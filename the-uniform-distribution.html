<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 The Uniform Distribution | Modern Probability and Statistical Inference</title>
  <meta name="description" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-poisson-and-related-distributions.html"/>
<link rel="next" href="multivariate-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.3</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.6</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-uniform-distribution" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> The Uniform Distribution<a href="the-uniform-distribution.html#the-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="properties" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Properties<a href="the-uniform-distribution.html#properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The uniform distribution is often used within the realm of probability,
in part because of its utility and in part because of its simplicity.
We briefly touched upon this distribution at times earlier in this book,
such as, for instance,
when we talked about hypothesis test <span class="math inline">\(p\)</span>-values (which are distributed
uniformly between 0 and 1 when the null hypothesis is correct).
Why do we return to the uniform distribution
now? Because it is slightly different from other distributions: its two
parameters, often denoted <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (where <span class="math inline">\(b &gt; a\)</span>), do not dictate the
shape of its probability density function, but rather its domain. This
affects aspects of estimation, such as determining sufficient statistics
and deriving maximum likelihood estimates, etc.
We highlight these “quirks” of the uniform pdf throughout this chapter.</p>
<p><strong>Recall</strong>: <em>a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) <span class="math inline">\(f_X(x) \geq 0\)</span> and (b) <span class="math inline">\(\int_x f_X(x) dx = 1\)</span>, where the integral is over all values of <span class="math inline">\(x\)</span> in the dist
ribution’s domain.</em></p>
<p>The uniform pdf is defined as
<span class="math display">\[
f_X(x) = \frac{1}{b-a} ~~\mbox{where}~~ x \in [a,b] \,.
\]</span>
<span class="math inline">\(f_X(x)\)</span> is thus constant between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.
(See Figure <a href="the-uniform-distribution.html#fig:unifpdf">5.1</a>.)
This means that we can
think of the uniform distribution “geometrically,” as the following
is true:
<span class="math display">\[
\underbrace{(b-a)}_{\mbox{domain}} \cdot \underbrace{\frac{1}{b-a}}_{f_X(x)} = 1
\]</span>
If we know the domain of the pdf, we immediately know <span class="math inline">\(f_X(x)\)</span>; conversely,
if we know
<span class="math inline">\(f_X(x)\)</span>, we immediately know the width of the domain (but not <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>
themselves).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unifpdf"></span>
<img src="_main_files/figure-html/unifpdf-1.png" alt="\label{fig:unifpdf}Three examples of uniform probability mass functions: Uniform(0,1) (solid red line), Uniform(0.5,2) (dashed green line), and Uniform(-1.5,1.5) (dotted blue line)." width="50%" />
<p class="caption">
Figure 5.1: Three examples of uniform probability mass functions: Uniform(0,1) (solid red line), Uniform(0.5,2) (dashed green line), and Uniform(-1.5,1.5) (dotted blue line).
</p>
</div>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as <span class="math inline">\(F_X(x) = \int_{y \leq x} f_Y(y) dy\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>The cdf for a uniformly distributed random variable is
<span class="math display">\[
F_X(x) = \int_a^x f_Y(y) dy = \int_a^x \frac{1}{b-a} dy = \frac{x-a}{b-a} ~~ x \in [a,b] \,,
\]</span>
with a value of 0 for <span class="math inline">\(x &lt; a\)</span> and 1 for <span class="math inline">\(x &gt; b\)</span>. (We can quickly confirm that
the derivative of the cdf yields the pdf. Recall that for continuous
distributions, <span class="math inline">\(f_X(x) = dF_X(x)/dx\)</span>.)</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(q = F_X(x)\)</span>.</em></p>
<p>The inverse cdf is exceptionally simple to compute:
<span class="math display">\[
q = \frac{x-a}{b-a} ~~ \Rightarrow ~~ x = (b-a)q + a \,.
\]</span>
Technically the inverse cdf has no unique solution when <span class="math inline">\(q = 0\)</span> or
<span class="math inline">\(q = 1\)</span>. However, it is convention (for instance, within <code>R</code>) that
the inverse cdf output for continuous distributions be the largest
value for which <span class="math inline">\(q = 0\)</span> and the smallest value for which <span class="math inline">\(q = 1\)</span>. Thus,
for a Uniform(<span class="math inline">\(a,b\)</span>) distribution,
when <span class="math inline">\(q = 0\)</span>, then <span class="math inline">\(x = a\)</span>, and when <span class="math inline">\(q = 1\)</span>, <span class="math inline">\(x = b\)</span>.</p>
<hr />
<p>A discrete analogue to the uniform distribution is the
<em>discrete uniform distribution</em>, which is defined over a range of
integers <span class="math inline">\([a,b]\)</span>. (The rolls of a fair, six-sided die would, for
instance, be governed by the discrete uniform distribution.) The pmf
for the discrete uniform is
<span class="math display">\[
p_X(x) = \frac{1}{n} ~~ x \in [a,b] \,,
\]</span>
where <span class="math inline">\(n = b - a + 1\)</span> is the number of possible experimental outcomes.
The cdf is
<span class="math display">\[
F_X(x) = \frac{\lfloor x \rfloor - a + 1}{n} ~~ x \in [a,b] \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> is the largest integer that is smaller than
or equal to <span class="math inline">\(x\)</span>, while the inverse cdf is given by the generalized inverse
cdf formalism that we’ve previously seen for discrete distributions.</p>
<p>Note that there are no standard <code>R</code> functions of the form <code>xdiscunif()</code>
for computing the pmf or cdf of
the discrete uniform distribution, or for sampling from it. We will
show how one can create such functions in an example below.</p>
<hr />
<div id="the-expected-value-and-variance-of-a-uniform-random-variable" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> The Expected Value and Variance of a Uniform Random Variable<a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the expected value of a continuously distributed random variable is</em>
<span class="math display">\[
E[X] = \int_x x f_X(x) dx\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(f_X(x)\)</span>.</em></p>
<blockquote>
<p>The expected value of a random variable drawn from a Uniform(<span class="math inline">\(a,b\)</span>)
distribution is
<span class="math display">\[
E[X] = \int_a^b x f_X(x) dx = \int_a^b \frac{x}{b-a} dx = \frac{1}{b-a} \left. \frac{x^2}{2} \right|_a^b = \frac{1}{b-a} \frac{b^2-a^2}{2} = \frac{1}{b-a} \frac{(b-a)(b+a)}{2} = \frac{a+b}{2} \,.
\]</span></p>
</blockquote>
<p><strong>Recall:</strong> <em>the variance of a continuously distributed random variable is</em>
<span class="math display">\[
V[X] = \int_x (x-\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(f_X(x)\)</span> is effectively non-zero.</em></p>
<blockquote>
<p>To find the variance, we work with the shortcut formula:
<span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>. We know <span class="math inline">\(E[X]\)</span> already; as for <span class="math inline">\(E[X^2]\)</span>,
we utilize the Law of the Unconscious Statistician:
<span class="math display">\[\begin{align*}
E[X^2] = \int_a^b x^2 f_X(x) dx &amp;= \int_a^b \frac{x^2}{b-a} dx \\
&amp;= \frac{1}{b-a} \left. \frac{x^3}{3} \right|_a^b \\
&amp;= \frac{b^3-a^3}{3(b-a)} \\
&amp;= \frac{(b-a)(a^2+ab+b^2)}{3(b-a)} = \frac{1}{3}\left(a^2 + ab + b^2\right) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
V[X] &amp;= \frac{1}{3}\left(a^2 + ab + b^2\right) - \left(\frac{a+b}{2}\right)^2 \\
&amp;= \frac{1}{3}\left(a^2 + ab + b^2\right) - \frac{1}{4}\left(a^2+2ab+b^2\right) \\
&amp;= \frac{1}{12}\left(4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\right) \\
&amp;= \frac{1}{12}\left(a^2 - 2ab + b^2 \right) \\
&amp;= \frac{(a-b)^2}{12} \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="coding-r-style-functions-for-the-discrete-uniform-distribution" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Coding R-Style Functions for the Discrete Uniform Distribution<a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>There are four standard functions associated with any distribution:
the one prefaced by <code>d</code> that returns the output of the
probability mass function or probability density function, given a
coordinate <span class="math inline">\(x\)</span>; the one prefaced by <code>p</code> that returns the output
of the cumulative distribution function, given <span class="math inline">\(x\)</span>; the one prefaced
<code>q</code> that returns the output of the inverse cdf, given a quantile
<span class="math inline">\(q \in [0,1]\)</span>; and the random sampler, a function prefaced by <code>r</code>.</p>
</blockquote>
<blockquote>
<p>For the discrete uniform distribution, one can code the
probability mass function as follows:</p>
</blockquote>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="the-uniform-distribution.html#cb291-1" tabindex="-1"></a>ddiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(x,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb291-2"><a href="the-uniform-distribution.html#cb291-2" tabindex="-1"></a>{</span>
<span id="cb291-3"><a href="the-uniform-distribution.html#cb291-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb291-4"><a href="the-uniform-distribution.html#cb291-4" tabindex="-1"></a>  <span class="cf">if</span> ( x <span class="sc">%in%</span> y ) <span class="fu">return</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">length</span>(y))</span>
<span id="cb291-5"><a href="the-uniform-distribution.html#cb291-5" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb291-6"><a href="the-uniform-distribution.html#cb291-6" tabindex="-1"></a>}</span>
<span id="cb291-7"><a href="the-uniform-distribution.html#cb291-7" tabindex="-1"></a><span class="fu">ddiscunif</span>(<span class="dv">4</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>) <span class="co"># assume a fair six-sided die</span></span></code></pre></div>
<pre><code>## [1] 0.1666667</code></pre>
<blockquote>
<p>As for the cumulative distribution function:</p>
</blockquote>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="the-uniform-distribution.html#cb293-1" tabindex="-1"></a>pdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(x,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb293-2"><a href="the-uniform-distribution.html#cb293-2" tabindex="-1"></a>{</span>
<span id="cb293-3"><a href="the-uniform-distribution.html#cb293-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb293-4"><a href="the-uniform-distribution.html#cb293-4" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">which</span>(y<span class="sc">&lt;=</span>x)</span>
<span id="cb293-5"><a href="the-uniform-distribution.html#cb293-5" tabindex="-1"></a>  <span class="cf">if</span> ( <span class="fu">length</span>(w) <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb293-6"><a href="the-uniform-distribution.html#cb293-6" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">length</span>(w)<span class="sc">/</span><span class="fu">length</span>(y))</span>
<span id="cb293-7"><a href="the-uniform-distribution.html#cb293-7" tabindex="-1"></a>}</span>
<span id="cb293-8"><a href="the-uniform-distribution.html#cb293-8" tabindex="-1"></a><span class="fu">pdiscunif</span>(<span class="dv">4</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 0.6666667</code></pre>
<blockquote>
<p>The inverse cdf implements the generalized inverse algorithm:</p>
</blockquote>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="the-uniform-distribution.html#cb295-1" tabindex="-1"></a>qdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(q,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb295-2"><a href="the-uniform-distribution.html#cb295-2" tabindex="-1"></a>{</span>
<span id="cb295-3"><a href="the-uniform-distribution.html#cb295-3" tabindex="-1"></a>  y   <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb295-4"><a href="the-uniform-distribution.html#cb295-4" tabindex="-1"></a>  <span class="cf">if</span> ( q <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="fu">min</span>(y))</span>
<span id="cb295-5"><a href="the-uniform-distribution.html#cb295-5" tabindex="-1"></a>  <span class="cf">if</span> ( q <span class="sc">==</span> <span class="dv">1</span> ) <span class="fu">return</span>(<span class="fu">max</span>(y))</span>
<span id="cb295-6"><a href="the-uniform-distribution.html#cb295-6" tabindex="-1"></a>  cdf <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y))<span class="sc">/</span><span class="fu">length</span>(y)</span>
<span id="cb295-7"><a href="the-uniform-distribution.html#cb295-7" tabindex="-1"></a>  w   <span class="ot">&lt;-</span> <span class="fu">which</span>(cdf<span class="sc">&gt;=</span>q)</span>
<span id="cb295-8"><a href="the-uniform-distribution.html#cb295-8" tabindex="-1"></a>  <span class="cf">if</span> ( <span class="fu">length</span>(w) <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="fu">max</span>(y))</span>
<span id="cb295-9"><a href="the-uniform-distribution.html#cb295-9" tabindex="-1"></a>  <span class="fu">return</span>(y[<span class="fu">min</span>(w)])</span>
<span id="cb295-10"><a href="the-uniform-distribution.html#cb295-10" tabindex="-1"></a>}</span>
<span id="cb295-11"><a href="the-uniform-distribution.html#cb295-11" tabindex="-1"></a><span class="fu">qdiscunif</span>(<span class="fl">0.55</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<blockquote>
<p>And last, the random data generator:</p>
</blockquote>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="the-uniform-distribution.html#cb297-1" tabindex="-1"></a>rdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb297-2"><a href="the-uniform-distribution.html#cb297-2" tabindex="-1"></a>{</span>
<span id="cb297-3"><a href="the-uniform-distribution.html#cb297-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb297-4"><a href="the-uniform-distribution.html#cb297-4" tabindex="-1"></a>  s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">length</span>(y),n,<span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb297-5"><a href="the-uniform-distribution.html#cb297-5" tabindex="-1"></a>  <span class="fu">return</span>(y[s])</span>
<span id="cb297-6"><a href="the-uniform-distribution.html#cb297-6" tabindex="-1"></a>}</span>
<span id="cb297-7"><a href="the-uniform-distribution.html#cb297-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">235</span>) <span class="co"># set to ensure consistent output</span></span>
<span id="cb297-8"><a href="the-uniform-distribution.html#cb297-8" tabindex="-1"></a><span class="fu">rdiscunif</span>(<span class="dv">10</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>##  [1] 6 5 5 6 2 1 5 1 3 6</code></pre>
</div>
</div>
<div id="linear-functions-of-uniform-random-variables" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Linear Functions of Uniform Random Variables<a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s assume that we are given <span class="math inline">\(n\)</span> iid Uniform random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Uniform(<span class="math inline">\(a,b\)</span>). What is the distribution
of the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>?</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(E[e^{tX}]\)</span>. If <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> os the mgf for a known family of distributions, then we can immediately identify the distribution of <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for the uniform distribution is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \int_a^b \frac{e^{tx}}{b-a} dx \\
&amp;= \frac{1}{b-a} \left. \frac{1}{t}e^{tx} \right|_a^b \\
&amp;= \frac{e^{tb}-e^{ta}}{t(b-a)} \,.
\end{align*}\]</span>
The mgf for the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is thus
<span class="math display">\[
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = \left( \frac{e^{tb}-e^{ta}}{t(b-a)} \right)^n \,.
\]</span>
This expression does not simplify such that we recognize the distribution
of <span class="math inline">\(Y\)</span>. If <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = 1\)</span>, it turns out that the mgf does take on the
form of that for an Irwin-Hall distribution. An Irwin-Hall random variable
converges in distribution to a normal random variable as
<span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>We are placed in a similar situation if we look at the sample mean
<span class="math inline">\(\bar{X} = Y/n\)</span>:
<span class="math display">\[
m_{\bar{X}}(t) = \prod_{i=1}^n m_{X_i}\left(\frac{t}{n}\right) = \left( \frac{n(e^{tb/n}-e^{ta/n})}{t(b-a)} \right)^n \,.
\]</span>
If <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = 1\)</span>, <span class="math inline">\(\bar{X}\)</span> is sampled from the Bates distribution.
A Bates random variable converges in distribution to a normal random variable
as <span class="math inline">\(n \rightarrow \infty\)</span>. For all other combinations of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we
cannot write down a specific functional form for the sampling distribution of
<span class="math inline">\(\bar{X}\)</span> and thus we would have to perform simulations to test hypotheses,
etc. (However, we note that because statistical inference for a uniform
distribution involves determining
the lower and/or upper bounds, we can utilize order
statistics for inference instead of <span class="math inline">\(\bar{X}\)</span>. See the next section below.)</p>
<hr />
<div id="the-moment-generating-function-for-a-discrete-uniform-distribution" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> The Moment-Generating Function for a Discrete Uniform Distribution<a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The mgf for a discrete uniform random variable is
<span class="math display">\[
E[e^{tX}] = \sum_{x=a}^b e^{tx} p_X(x) = \frac{1}{n} \sum_{x=a}^b e^{tx} \,.
\]</span>
We cannot say anything further without making an assumption. If we say
that <span class="math inline">\(x \in [a,a+1,\ldots,b-1,b]\)</span>, i.e., that there are integer steps
between the probability masses, then
<span class="math display">\[
E[e^{tX}] = \frac{1}{n} \sum_{x=a}^b e^{tx} = \frac{1}{n}e^{ta} \left( 1 + e^{t(a+1)} + \cdots + e^{t(b-a)} \right) \,.
\]</span>
If <span class="math inline">\(t\)</span> is negative, then we can make use of a geometric sum:
<span class="math display">\[
1 + e^t + \cdots = \frac{1}{1-e^t} = \underbrace{1 + \cdots + e^{t(b-a)}}_{} + \underbrace{e^{t(b-a+1)} + \cdots}_{} \,,
\]</span>
where the first underbraced quantity is what appears above in the
expected value. Thus we can rearrange terms and write
<span class="math display">\[\begin{align*}
1 + e^{t(a+1)} + \cdots + e^{t(b-a)} &amp;= \frac{1}{1-e^t} - \left( e^{t(b-a+1)} + \cdots \right) \\
&amp;= \frac{1}{1-e^t} - e^{t(b-a+1)}\left(1 + e^t + \cdots\right) \\
&amp;= \frac{1}{1-e^t} - \frac{e^{t(b-a+1)}}{1-e^t} = \frac{1-e^{t(b-a+1)}}{1-e^t} \,.
\end{align*}\]</span>
Putting everything together, we find that
<span class="math display">\[
m_X(t) = \frac{1}{n}e^{ta} \frac{1-e^{t(b-a+1)}}{1-e^t} = \frac{e^{ta}-e^t(b+1)}{n(1-e^t)} \,.
\]</span>
This is the usual form of the mgf presented for the discrete uniform
distribution, but again, this is only valid if the masses are separated
by one unit: <span class="math inline">\(x \in [a,a+1,\ldots,b-1,b]\)</span>.</p>
</blockquote>
</div>
</div>
<div id="sufficient-statistics-and-the-minimum-variance-unbiased-estimator" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Sufficient Statistics and the Minimum Variance Unbiased Estimator<a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a sufficient statistic for a population parameter <span class="math inline">\(\theta\)</span> captures all information about <span class="math inline">\(\theta\)</span> contained in a data sample; no additional statistic will provide more information about <span class="math inline">\(\theta\)</span>. Sufficient statistics are not unique: functions of sufficient statistics are themselves sufficient statistics.</em></p>
<p>Before we discuss sufficient statistics in the context of the uniform
distribution, it is useful to (re-)introduce the <em>indicator function</em>. This
function, mentioned briefly in Chapter 1, takes on the value 1 if a
specified condition is met and 0 otherwise. For instance,
<span class="math display">\[
\mathbb{I}_{x_i \in [0,1]} = \left\{ \begin{array}{cl} 1 &amp; x_i \in [0,1] \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
One use for the indicator function is to, well, <em>indicate</em> the domain of a pmf
or pdf. For instance, we can write
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{ll} e^{-x} &amp; x \geq 0 \\ 0 &amp; \mbox{otherwise} \end{array} \right.
\]</span>
to express that the exponential distribution with rate <span class="math inline">\(\beta = 1\)</span> is
defined within the domain <span class="math inline">\(x \in [0,\infty)\)</span>, or, equivalently, we can write
<span class="math display">\[
f_X(x) = e^{-x} \mathbb{I}_{x \in [0,\infty)} \,.
\]</span>
The latter form expresses the same information in a more condensed fashion.</p>
<p>So…what do indicator functions have to do with uniform distributions?</p>
<p>Let’s suppose we sample <span class="math inline">\(n\)</span> iid data <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> from
a uniform distribution with lower bound 0 and upper bound <span class="math inline">\(\theta\)</span>, and
our goal is to define a sufficient statistic for <span class="math inline">\(\theta\)</span>. Let’s work
with the factorization criterion:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = g(\mathbf{x},\theta) \cdot h(\mathbf{x}) \,.
\]</span>
The likelihood is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \prod_{i=1}^n \frac{1}{\theta} = \frac{1}{\theta^n} \,.
\]</span>
OK…no…wait, there are no data in this expression, so we cannot define
a sufficient statistic. The way around this is to re-express the pdf as
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \mathbb{I}_{x \in [0,\theta]}
\]</span>
and to re-write the likelihood as
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \frac{1}{\theta^n} \prod_{i=1}^n \mathbb{I}_{x \in [0,\theta]} \,.
\]</span>
A product of indicator functions will equal 1 if and only if <em>all</em> data lie
in the domain <span class="math inline">\(x \in [0,\theta]\)</span>. This is equivalent to saying that
<span class="math inline">\(\theta \geq X_{(n)}\)</span>, the order statistic representing the maximum observed
datum. Thus <span class="math inline">\(X_{(n)}\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>: we know
<span class="math inline">\(\theta\)</span> is greater than this statistic’s value, and none of the data
aside from <span class="math inline">\(X_{(n)}\)</span> provide any additional information about <span class="math inline">\(\theta\)</span>.</p>
<p>The upshot: when the parameter <span class="math inline">\(\theta\)</span> dictates (at least in part)
the domain of a distribution, the sufficient statistics for <span class="math inline">\(\theta\)</span> will be
functions of an order statistic.</p>
<p>When we first introduced the factorization criterion and sufficient statistics
back in Chapter 3, we did it so that ultimately we could write down the
minimum variance unbiased estimator (or MVUE).</p>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p><strong>Recall</strong>: <em>deriving the minimum variance unbiased estimator involves two steps:</em></p>
<ol style="list-style-type: decimal">
<li><em>factorizing the likelihood function to uncover a sufficient statistic <span class="math inline">\(U\)</span> (that we assume is both minimal and complete); and</em></li>
<li><em>finding a function <span class="math inline">\(h(U)\)</span> such that <span class="math inline">\(E[h(U)] = \theta\)</span>.</em></li>
</ol>
<p>When <span class="math inline">\(\{X_1,\ldots,X_n\} \stackrel{iid}{\sim}\)</span> Uniform(<span class="math inline">\(0,\theta\)</span>), can we
define an MVUE for <span class="math inline">\(\theta\)</span>? The answer is yes…but we have to recall how
we define the pdf of <span class="math inline">\(X_{(n)}\)</span> first.</p>
<p><strong>Recall</strong>: <em>the maximum of <span class="math inline">\(n\)</span> iid random variables sampled from a pdf
<span class="math inline">\(f_X(x)\)</span> has a sampling distribution given by</em>
<span class="math display">\[
f_{(n)}(x) = n f_X(x) [ F_X(x) ]^{n-1} \,,
\]</span>
<em>where <span class="math inline">\(F_X(x)\)</span> is the associated cdf.</em></p>
<p>For the Uniform(<span class="math inline">\(0,\theta\)</span>) distribution,
<span class="math display">\[
f_X(x) = \frac{1}{\theta} ~~\mbox{and}~~ F_X(x) = \int_0^x f_Y(y) dy = \int_0^x \frac{1}{\theta} dy = \frac{x}{\theta} \,,
\]</span>
so
<span class="math display">\[
f_{(n)}(x) = n \frac{1}{\theta} \left[ \frac{x}{\theta} \right]^{n-1} = n \frac{x^{n-1}}{\theta^n} \,.
\]</span>
To find the MVUE, we first compute the expected value of the sufficient
statistic <span class="math inline">\(X_{(n)}\)</span>:
<span class="math display">\[
E[X_{(n)}] = \int_0^\theta x n \frac{x^{n-1}}{\theta^n} dx = \left. \frac{n}{(n+1)\theta^n} x^{n+1} \right|_0^\theta = \frac{n}{n+1} \theta \,,
\]</span>
and then rearrange terms:
<span class="math display">\[
E\left[\frac{n+1}{n}X_{(n)}\right] = \theta \,.
\]</span>
Thus
<span class="math display">\[
\hat{\theta}_{MVUE} = \frac{n+1}{n}X_{(n)}
\]</span>
is the MVUE for <span class="math inline">\(\theta\)</span>. (Note that a similar calculation to this one
can be used to determine, e.g., the MVUE for the <span class="math inline">\(\theta\)</span> when the
data are sampled from a Uniform(<span class="math inline">\(\theta,b\)</span>) distribution.)</p>
<div id="the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution<a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Pareto [puh-RAY-toh] distribution, also known in some quarters as
the power-law, is
<span class="math display">\[
f_X(x) = \frac{\alpha \beta^\alpha}{x^{\alpha+1}} \,,
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> is the shape parameter and <span class="math inline">\(x \in [\beta,\infty)\)</span>,
where <span class="math inline">\(\beta\)</span> is the scale (or location)
parameter. Let’s assume <span class="math inline">\(\alpha\)</span> is known.
A sufficient statistic for <span class="math inline">\(\beta\)</span>, found via likelihood factorization,
is
<span class="math display">\[
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i) = \underbrace{\beta^{n\alpha}}_{g(\mathbf{x},\beta)} \cdot \underbrace{\frac{\alpha^n}{(\prod_{i=1}^n x_i)^{\alpha+1}}}_{h(\mathbf{x})} \,.
\]</span>
Wait…again, as is the case for the uniform distribution, no data appear
in the
expression <span class="math inline">\(g(\cdot)\)</span>. So we would go back and introduce
an indicator function into the pdf; it should be clear that when we do so,
<span class="math inline">\(g(\mathbf{x},\beta)\)</span> changes to
<span class="math display">\[
g(\mathbf{x},\beta) = \beta^{n\alpha} \prod_{i=1}^n \mathbb{I}_{x_i \in [\beta,\infty)}
\]</span>
and thus that because all data have to be larger than <span class="math inline">\(\beta\)</span>, the sufficient
statistic is the minimum observed datum, <span class="math inline">\(X_{(1)}\)</span>.</p>
</blockquote>
</div>
<div id="mvue-properties-for-uniform-distribution-bounds" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> MVUE Properties for Uniform Distribution Bounds<a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The properties of estimators that we have examined thus far include the
bias (are our estimates offset from the truth, on average?), the variance
(over how large a range do our estimates vary?), etc. Let’s look at some
of these properties here, assuming we sample <span class="math inline">\(n\)</span> iid data from a
Uniform(<span class="math inline">\(0,\theta\)</span>) distribution.</p>
</blockquote>
<blockquote>
<p>Bias: the MVUE is by definition unbiased, since
<span class="math inline">\(E[\frac{n+1}{n}X_{(n)}] = \theta\)</span>.</p>
</blockquote>
<blockquote>
<p>Variance: the variance of <span class="math inline">\(\hat{\theta}_{MVUE}\)</span> is
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MVUE}] &amp;= E\left[\left(\hat{\theta}_{MVUE}\right)^2\right] - \left( E\left[ \hat{\theta}_{MVUE} \right] \right)^2 \\
&amp;= \frac{(n+1)^2}{n^2} \left( E[X_{(n)}^2] - (E[X_{(n)}])^2 \right) \,,
\end{align*}\]</span>
where
<span class="math display">\[
E[X_{(n)}^2] = \int_0^\theta x^2 n \frac{x^{n-1}}{\theta^n} dx = \left. \frac{n}{(n+2)\theta^n} x^{n+2} \right|_0^\theta = \frac{n}{n+2} \theta^2 \,.
\]</span>
Thus
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MVUE}] &amp;= \frac{(n+1)^2}{n^2} \left( \frac{n}{n+2} \theta^2 - \frac{n^2}{(n+1)^2} \theta^2 \right) \\
&amp;= \frac{(n+1)^2}{n^2} \left( \frac{n(n+1)^2 - n^2(n+2)}{(n+2)(n+1)^2} \theta^2 \right) \\
&amp;= \frac{(n+1)^2}{n^2} \left( \frac{n}{(n+2)(n+1)^2} \theta^2 \right) \\
&amp;= \frac{1}{n(n+2)} \theta^2 \rightarrow \frac{\theta^2}{n^2} ~~\mbox{as}~~ n \rightarrow \infty \,.
\end{align*}\]</span>
We observe that since the variance goes to zero as <span class="math inline">\(n \rightarrow \infty\)</span>,
the MVUE is a consistent estimator…</p>
</blockquote>
<blockquote>
<p>…but does the MVUE achieve the Cramer-Rao Lower Bound (CRLB), the
theoretical
lower bound on the variance of unbiased estimators? It turns out that
not only does it achieve the lower bound (which one can show equals
<span class="math inline">\(\theta^2/n\)</span>), but it even surpasses that bound! This seemingly worrisome
result is actually fine, however, because
<strong>the CRLB theorem is not applicable in situations where the domain
of a pmf or pdf depends on <span class="math inline">\(\theta\)</span></strong>.</p>
</blockquote>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Maximum Likelihood Estimation<a href="the-uniform-distribution.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is, thus far, found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>Now that we have recalled how maximum likelihood estimation works, we can
state that this is <em>not</em> how the MLE is found for a domain-affecting
parameter! (Hence the “thus far” in the recall statement above.)
Let’s assume, for instance, that we sample <span class="math inline">\(n\)</span> iid random
variables from a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution. As stated above (without
the indicator function), the likelihood is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \frac{1}{\theta^n} \,.
\]</span>
This means that the smaller <span class="math inline">\(\theta\)</span> is, the larger the likelihood will be.
So how small can <span class="math inline">\(\theta\)</span> be? We can answer this intuitively: the domain
<span class="math inline">\([0,\theta]\)</span> has to just encompass all the observed data, i.e.,
<span class="math display">\[
\hat{\theta}_{MLE} = X_{(n)} \,.
\]</span>
If <span class="math inline">\(\theta\)</span> were smaller, <span class="math inline">\(X_{(n)}\)</span> would lie outside the domain. It is
fine for <span class="math inline">\(\theta\)</span> to be larger, since then all the data lie in the
domain <span class="math inline">\([0,\theta]\)</span>…but the larger <span class="math inline">\(\theta\)</span> is, the smaller the
likelihood.</p>
<p>We plot an example likelihood function in Figure <a href="the-uniform-distribution.html#fig:uniflik">5.2</a>. We observe
immediately that the usual MLE algorithm will not work here, as the likelihood
function is discontinuous at <span class="math inline">\(\theta = X_{(n)}\)</span> and thus we cannot compute
a first derivative. All we can do is, e.g., plot the likelihood and identify
the MLE as that value for which the likelihood is maximized (or identify the
value intuitively as we do above).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uniflik"></span>
<img src="_main_files/figure-html/uniflik-1.png" alt="\label{fig:uniflik}The likelihood function given $n=5$ data drawn from a Uniform(0,$\theta$) distribution, with $\theta = 1$. As $\theta$ cannot be smaller than the maximum observed value, the likelihood is zero for $\theta &lt; X_{(n)}$; it is $1/\theta^n$ for $\theta \geq X_{(n)}$. The maximum likelihood estimate is thus $X_{(n)}$ itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters." width="50%" />
<p class="caption">
Figure 5.2: The likelihood function given <span class="math inline">\(n=5\)</span> data drawn from a Uniform(0,<span class="math inline">\(\theta\)</span>) distribution, with <span class="math inline">\(\theta = 1\)</span>. As <span class="math inline">\(\theta\)</span> cannot be smaller than the maximum observed value, the likelihood is zero for <span class="math inline">\(\theta &lt; X_{(n)}\)</span>; it is <span class="math inline">\(1/\theta^n\)</span> for <span class="math inline">\(\theta \geq X_{(n)}\)</span>. The maximum likelihood estimate is thus <span class="math inline">\(X_{(n)}\)</span> itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters.
</p>
</div>
<hr />
<div id="the-mle-for-the-domain-parameter-of-the-pareto-distribution" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> The MLE for the Domain Parameter of the Pareto Distribution<a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the last section above, we introduce the Pareto distribution,
<span class="math display">\[
f_X(x) = \frac{\alpha\beta^\alpha}{x^{\alpha+1}} \,,
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(x \in [\beta,\infty)\)</span>, and we show that the
sufficient statistic for <span class="math inline">\(\beta\)</span> (with <span class="math inline">\(\alpha\)</span> fixed) is the smallest
observed datum, <span class="math inline">\(X_{(1)}\)</span>. Because <span class="math inline">\(\beta\)</span> is a parameter that dictates
the domain, we find the MLE not via differentiation but rather by
identifying that the likelihood is maximized when <span class="math inline">\(\beta\)</span> is exactly
equal to <span class="math inline">\(X_{(1)}\)</span>, i.e., <span class="math inline">\(\hat{\beta}_{MLE} = X_{(1)}\)</span>. See Figure
<a href="the-uniform-distribution.html#fig:parlik">5.3</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parlik"></span>
<img src="_main_files/figure-html/parlik-1.png" alt="\label{fig:parlik}The likelihood function given $n=5$ data drawn from a Pareto(1,$\beta$) distribution, with $\beta = 1$. As $\beta$ cannot be larger than the minimum observed value, the likelihood is zero for $\beta \geq X_{(1)}$; it is $\theta^n(1/\prod_{i=1}^n x_i)^2$ for $\beta &lt; X_{(n)}$. The maximum likelihood estimate is thus $X_{(1)}$ itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters." width="50%" />
<p class="caption">
Figure 5.3: The likelihood function given <span class="math inline">\(n=5\)</span> data drawn from a Pareto(1,<span class="math inline">\(\beta\)</span>) distribution, with <span class="math inline">\(\beta = 1\)</span>. As <span class="math inline">\(\beta\)</span> cannot be larger than the minimum observed value, the likelihood is zero for <span class="math inline">\(\beta \geq X_{(1)}\)</span>; it is <span class="math inline">\(\theta^n(1/\prod_{i=1}^n x_i)^2\)</span> for <span class="math inline">\(\beta &lt; X_{(n)}\)</span>. The maximum likelihood estimate is thus <span class="math inline">\(X_{(1)}\)</span> itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters.
</p>
</div>
<hr />
</div>
<div id="mle-properties-for-uniform-distribution-bounds" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> MLE Properties for Uniform Distribution Bounds<a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In this example, we will mimic what we do above when discussing the
properties of the MVUE, and look at estimator bias and variance, etc.,
assuming that <span class="math inline">\(\{X_1,\ldots,X_n\} \stackrel{iid}{\sim}\)</span> Uniform(<span class="math inline">\(0,\theta\)</span>).</p>
</blockquote>
<blockquote>
<p>Bias: we know, from our derivation of the MVUE, that
<span class="math display">\[
E[\hat{\theta}_{MLE}] = E[X_{(n)}] = \frac{n}{n+1}\theta \,,
\]</span>
and thus the estimator bias is
<span class="math display">\[
B[\hat{\theta}_{MLE}] = E[\hat{\theta}_{MLE}] - \theta = \frac{n}{n+1}\theta - \theta = \frac{1}{n+1}\theta \,.
\]</span>
As we expect for the MLE, the estimator is at least asymptotically unbiased,
as the bias goes to zero as the sample size <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</blockquote>
<blockquote>
<p>Variance: the variance of the MLE is
<span class="math display">\[
V[\hat{\theta}_{MLE}] = E[\hat{\theta}_{MLE}^2] - \left(E[\hat{\theta}_{MLE}\right)^2 = E[X_{(n)}^2] - (E[X_{(n)}])^2 \,.
\]</span>
We derived both <span class="math inline">\(E[X_{(n)}]\)</span> and <span class="math inline">\(E[X_{(n)}^2]\)</span> above when discussing the
MVUE, so we can write down immediately that
<span class="math display">\[
V[\hat{\theta}_{MLE}] = \frac{n}{n+2}\theta^2 - \left( \frac{n}{n+1}\theta\right)^2 = \frac{n}{(n+2)(n+1)^2}\theta^2 \rightarrow \frac{\theta^2}{n^2} ~~\mbox{as}~~ n \rightarrow \infty\,.
\]</span>
We observe that because the variance goes
to zero as <span class="math inline">\(n \rightarrow \infty\)</span>, the MLE is a consistent estimator.
The variance of the MLE is similar to, but not exactly the same as,
the variance for the MVUE, although the variances converge to the same
value in asymtopia.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-4" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Confidence Intervals<a href="the-uniform-distribution.html#confidence-intervals-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_U\)</span> by, e.g., solving for the root
<span class="math inline">\(\theta_q\)</span> in each of the following equations:</em>
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta_{\alpha/2}) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{\rm obs} \vert \theta_{1-\alpha/2}) - \left(1-\frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
<em>The construction of confidence intervals thus relies on knowing the
sampling distribution of the adopted statistic <span class="math inline">\(Y\)</span>. One maps
<span class="math inline">\(\theta_{\alpha/2}\)</span> and <span class="math inline">\(\theta_{1-\alpha/2}\)</span> to
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_U\)</span> by taking into account how
the expected value <span class="math inline">\(E[Y]\)</span> varies with the parameter <span class="math inline">\(\theta\)</span>. (See the
table in section 14 of Chapter 1.)</em></p>
<p>Here, we will recall something else about confidence interval
construction: we can choose the statistic that we use. This is important,
because as we have seen, if we are given <span class="math inline">\(n\)</span> iid data drawn from,
e.g., a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution, we do <em>not</em> know the distribution
of <span class="math inline">\(\bar{X}\)</span> (unless <span class="math inline">\(\theta=1\)</span>)…while we <em>do</em> know the distribution of
the order statistic <span class="math inline">\(Y = X_{(n)}\)</span>. We derive it above:
<span class="math display">\[
f_{(n)}(x) = n \frac{x^{n-1}}{\theta^n} \,.
\]</span>
The cdf is thus
<span class="math inline">\(F_Y(y) = F_{(n)}(x) = (x/\theta)^n\)</span>. We work with this expression
in an example below, noting that <span class="math inline">\(E[Y] = (n-1)\theta/n\)</span>, i.e., that
<span class="math inline">\(E[Y]\)</span> <em>increases</em> with <span class="math inline">\(\theta\)</span>.</p>
<hr />
<p>We conclude our coverage (so to speak) of confidence intervals by going back
to the notion of the confidence coefficient <span class="math inline">\(1 - \alpha\)</span>. In a footnote
in Chapter 1, we make the point that technically, the confidence coefficient
is the infimum, or minimum value, of the probability
<span class="math inline">\(P(\hat{\theta}_L \leq \theta \leq \hat{\theta}_U)\)</span>. What does this
actually mean?</p>
<p>Let’s suppose that we have sampled <span class="math inline">\(n\)</span> iid data from a normal distribution,
and that we are going to construct a confidence interval of the form
<span class="math display">\[
P(S^2 - a \leq \sigma^2 \leq S^2 + a)
\]</span>
for the population variance <span class="math inline">\(\sigma^2\)</span>. We can do this, right? Let’s see…
<span class="math display">\[\begin{align*}
P(S^2 - a \leq \sigma^2 \leq S^2 + a) &amp;= P(-a \leq S^2-\sigma^2 \leq a) \\
&amp;= P\left(1 - \frac{a}{\sigma^2} \leq \frac{S^2}{\sigma^2} \leq 1 + \frac{a}{\sigma^2}\right) \\
&amp;= P\left( (n-1)\left(1 - \frac{a}{\sigma^2}\right) \leq \frac{(n-1)S^2}{\sigma^2} \leq (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right)\\
&amp;= F_{W(n-1)}\left( (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right) - F_{W(n-1)}\left( (n-1)\left(1 - \frac{a}{\sigma^2}\right) \right) \,.
\end{align*}\]</span>
The key to interpreting the last line above is that <span class="math inline">\(\sigma^2\)</span> is <em>unknown</em>
(otherwise, why would we be constructing a confidence interval for it
in the first place?), and thus
can take on any positive value. What if <span class="math inline">\(\sigma^2\)</span> is very large?
<span class="math display">\[
\lim_{\sigma^2 \to \infty} F_{W(n-1)}\left[ (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right] - F_{W(n-1)}\left[ (n-1)\left(1 - \frac{a}{\sigma^2}\right) \right] = F_{W(n-1)}(n-1) - F_{W(n-1)}(n-1) = 0 \,.
\]</span>
Thus the confidence coefficient for the interval
<span class="math inline">\(S^2 - a \leq \sigma^2 \leq S^2 + a\)</span> is <span class="math inline">\(1 - \alpha = 0\)</span> (or,
we have that <span class="math inline">\(\alpha = 1\)</span>).</p>
<p>The upshot: one cannot write down just any interval and assume that it is
a valid one!</p>
<div id="interval-estimation-given-order-statistics" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Interval Estimation Given Order Statistics<a href="the-uniform-distribution.html#interval-estimation-given-order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We assume that we are given <span class="math inline">\(n\)</span> iid data drawn from a Uniform(<span class="math inline">\(0,\theta\)</span>)
distribution. Above, we note that the cdf for the maximum observed datum,
<span class="math inline">\(X_{(n)}\)</span>, is <span class="math inline">\(F_{(n)}(x) = (x/\theta)^n\)</span>. To find the lower and upper
bounds on <span class="math inline">\(\theta\)</span>, respectively, we solve for <span class="math inline">\(\theta\)</span> in the expressions
<span class="math display">\[\begin{align*}
\left(\frac{X_{(n)}}{\theta_{1-\alpha/2}}\right)^n &amp;= 1 - \frac{\alpha}{2} ~~ \mbox{(lower)} \\
\left(\frac{X_{(n)}}{\theta_{\alpha/2}}\right)^n &amp;= \frac{\alpha}{2} ~~ \mbox{(upper)} \,.
\end{align*}\]</span>
(We assume that we are constructing a two-sided interval. Similar
expressions would yield the lower or upper bound.) We thus find that
<span class="math display">\[
\hat{\theta}_L = \theta_{1-\alpha/2} = \frac{X_{(n)}}{(1-\alpha/2)^{1/n}} ~~\mbox{and}~~ \hat{\theta}_U = \theta_{\alpha/2} = \frac{X_{(n)}}{(\alpha/2)^{1/n}} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="confidence-coefficient-for-a-uniform-based-interval-estimator" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Confidence Coefficient for a Uniform-Based Interval Estimator<a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the example above, we show that the interval estimate with confidence
coefficient <span class="math inline">\(1-\alpha\)</span> for the uniform upper bound <span class="math inline">\(\theta\)</span> has
the form <span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span>. Can we also define an appropriate
interval estimator if, for instance, it has the form
<span class="math inline">\([X_{(n)} + a,X_{(n)} + b]\)</span>? The short answer is no…because the
confidence coefficient will be <em>zero</em>! To see why, let’s expand out and
solve:
<span class="math display">\[\begin{align*}
P(X_{(n)} + a \leq \theta \leq X_{(n)} + b) &amp;= P(\theta - b \leq X_{(n)} \leq \theta - a)\\
&amp;= P(X_{(n)} \leq \theta - a) - P(X_{(n)} \leq \theta - b)\\
&amp;= F_{(n)}(\theta-a) - F_{(n)}(\theta-b)\\
&amp;= \left(\frac{\theta-a}{\theta}\right)^2 - \left(\frac{\theta-b}{\theta}\right)^2\\
&amp;= \left(1-\frac{a}{\theta}\right)^2 - \left(1-\frac{b}{\theta}\right)^2 \,.
\end{align*}\]</span>
The confidence coefficient is the infimum (or minimum value)
that this expression can take on. For an interval of the form
<span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span>, <span class="math inline">\(\theta\)</span> does not appear, and thus the infimum
is a constant. Here, however,
<span class="math display">\[
\lim_{\theta \to \infty} P(X_{(n)} + a \leq \theta \leq X_{(n)} + b) = 0 \,,
\]</span>
and thus
the confidence coefficient is (i.e., the proportion of computed
intervals that overlap the true value <span class="math inline">\(\theta\)</span> goes to zero).
Thus an interval estimator of the form <span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span> is a better
one than one of the form <span class="math inline">\([X_{(n)} + a,X_{(n)} + b]\)</span>.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-3" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Hypothesis Testing<a href="the-uniform-distribution.html#hypothesis-testing-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>One aspect of hypothesis testing that we reiterate here is that the hypotheses
are always to be established, along with the level of the test,
<em>before</em> we collect data. This should be obvious<span class="math inline">\(-\)</span>looking at the data
prior to establishing hypotheses and test levels can (and often will) lead
to bias<span class="math inline">\(-\)</span>so why are we reiterating this now? We are making this point
because when we perform tests involving domain-specifying parameters, there
are some quirks that we observe when we establish rejection regions.</p>
<p>Let’s look at an example: we sample <span class="math inline">\(n\)</span> iid data from a Uniform(<span class="math inline">\(0,\theta\)</span>)
distribution, and we use these data to test the hypothesis
<span class="math inline">\(H_o : \theta = \theta_o\)</span> versus the hypothesis <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>
at level <span class="math inline">\(\alpha\)</span>. We know that the sufficient statistic upon which we
will build our test is <span class="math inline">\(X_{(n)}\)</span>.</p>
<p>Given this, what can we say about the rejection region right away?
We can say that we will reject the null if <span class="math inline">\(X_{(n)} &gt; \theta_o\)</span>.
This is a “trivial” statement, as no mathematics is involved. Initially,
this might seem off-putting: we would, of course, never set <span class="math inline">\(\theta_o\)</span> to
be less than the maximum datum, would we? (That would be silly.)
But…that’s an incorrect way of looking
at this situation, since that implies that we looked at the data first and only
established the hypotheses afterwards. If we do things in the proper order,
then it can be very much the case that the maximum datum will exceed <span class="math inline">\(\theta_o\)</span>.
If we observe this, then life is easy: we simply reject the null and move on.</p>
<p>But…how do we establish the part of the rejection region involving
values of <span class="math inline">\(X_{(n)}\)</span> that are less than <span class="math inline">\(\theta_o\)</span>? That seems simple enough:
we are performing a two-tail test, so we set the cdf for <span class="math inline">\(X_{(n)}\)</span> to
<span class="math inline">\(\alpha/2\)</span> and invert and…oh, but there’s a problem. If the null is
actually correct, then the power of the test for <span class="math inline">\(\theta = \theta_o\)</span> would
be <span class="math inline">\(\alpha/2\)</span> and not <span class="math inline">\(\alpha\)</span>. (If the null is correct, it is impossible
to observe <span class="math inline">\(X_{(n)} &gt; \theta_o\)</span>, so all rejections would happen “to the left”
of <span class="math inline">\(\theta_o\)</span>!) So this is quirk number two: we have to be careful about
whether we use, e.g., <span class="math inline">\(\alpha/2\)</span> or <span class="math inline">\(\alpha\)</span> we finding the rejection region
boundary. If in doubt, think about the test power and how we can reject
the null if <span class="math inline">\(\theta = \theta_o\)</span>, and make sure the power is actually <span class="math inline">\(\alpha\)</span>.</p>
<hr />
<p>The last hypothesis-test-related topic that we will touch upon is the
concept of <em>multiple comparisons</em>. This is a somewhat opaque term that
denotes the situation in which we perform many hypothesis tests
<em>simultaneously</em> and need to correct for the fact that if the null is
correct in all cases, it becomes more and more likely that we will observe
(multiple) instances in which we decide to reject the null. We can illustrate
this using the binomial distribution: if we collect <span class="math inline">\(k\)</span> sets of data
(e.g., <span class="math inline">\(k\)</span> separate sets of <span class="math inline">\(n\)</span> iid data sampled from a Uniform(0,<span class="math inline">\(\theta\)</span>)
distribution), and perform level-<span class="math inline">\(\alpha\)</span> hypothesis tests for each, then
the number of tests results in which we reject the null is
<span class="math display">\[
X \sim \mbox{Binom}(k,\alpha) \,,
\]</span>
The expected value of <span class="math inline">\(X\)</span> is <span class="math inline">\(E[X] = k\alpha\)</span>, which increases with <span class="math inline">\(k\)</span>.
The <em>family-wise error rate</em>, or <em>FWER</em>, is the probability that
at least one test will result in a rejection when the null is correct:
<span class="math display">\[
FWER = P(X &gt; 0) = 1 - P(X = 0) = 1 - \binom{k}{x} \alpha^0 (1-\alpha)^k = 1 - (1-\alpha)^k \,.
\]</span>
For instance, if <span class="math inline">\(k = 10\)</span> and <span class="math inline">\(\alpha = 0.05\)</span>, the family-wise error rate is
0.401: for every 10 tests we perform, the probability of erroneously rejecting
one or more null hypotheses (i.e., detecting one or more <em>false positives</em>)
is about 40 percent. This increase in the FWER with <span class="math inline">\(k\)</span> is not good,
and is well-known to be associated with a commonly seen
data analysis issue dubbed <em>data dredging</em> or <em>p-hacking</em>.
<span class="math inline">\(p\)</span>-hacking greatly increases the probability that researchers
will make incorrect claims about what their data say, and worse yet, that
they will publish papers purporting these claims.</p>
<p>To mitigate this issue, we can attempt to change the test level for individual
tests such that
the overall FWER is reduced to <span class="math inline">\(\alpha\)</span>. There are many procedures for how we
might go about changing the test level for individual tests, but the most
commonly used one is the <em>Bonferroni correction</em>:
<span class="math display">\[
\alpha \rightarrow \frac{\alpha}{k} \,.
\]</span>
What is the FWER given this correction? Let’s assume the null is correct for
all <span class="math inline">\(k\)</span> tests. Then
<span class="math display">\[
FWER = P\left( p_1 \leq \frac{\alpha}{k} \cup \cdots \cup p_k \leq \frac{\alpha}{k} \right) = \sum_{i=1}^k P\left( p_i \leq \frac{\alpha}{k}\right) = \sum_{i=1}^k \frac{\alpha}{k} = \alpha \,.
\]</span>
This works! Except…what happens if actually only <span class="math inline">\(k&#39;\)</span> out of the <span class="math inline">\(k\)</span> are
actually true? The FWER becomes <span class="math inline">\(k&#39; \alpha / k \leq \alpha\)</span>. Thus when there
are incorrect nulls sprinkled into the mix, the FWER is too low…which means
that the Bonferroni correction is unduly conservative. Using it will lead to
us possibly not detecting false nulls that we should have detected! We
illustrate this issue in an example below.</p>
<p>An alternative to the Bonferroni correction and related procedures is to not
focus upon the FWER, but to attempt to limit the <em>false discovery rate</em>,
or <em>FDR</em>, instead. The simplest and most often used FDR-based procedure
is the one of Benjamini and Hochberg (1995):</p>
<ul>
<li>compute all <span class="math inline">\(k\)</span> <span class="math inline">\(p\)</span>-values;</li>
<li>sort the <span class="math inline">\(p\)</span>-values into ascending order: <span class="math inline">\(p_{(1)},\ldots,p_{(k)}\)</span>;</li>
<li>determine the largest value <span class="math inline">\(k&#39;\)</span> such that <span class="math inline">\(p_{(k&#39;)} \leq k&#39; \alpha / k\)</span>; and</li>
<li>reject the null for all tests that map to <span class="math inline">\(p_{(1)},\ldots,p_{(k&#39;)}\)</span>.</li>
</ul>
<p>In an example below, we illustrate the use of the BH procedure.
To be clear: <span class="math inline">\(\alpha\)</span> here represents the proportion of rejected null
hypotheses that are actually correct. (“We reject the null 20 times. Assuming
<span class="math inline">\(\alpha = 0.05\)</span>, then we expect that we were right to reject the null 19
times, and that we’d be mistaken once.”)
This is different from the FWER setting,
where <span class="math inline">\(\alpha\)</span> represents the probability of erroneously rejecting one or
more null hypotheses. (“We perform 100 independent tests. Assuming <span class="math inline">\(\alpha =
0.05\)</span>, we expect to erroneously reject the null five percent of the time when
the null is correct…but we can say nothing about how often we correctly
reject the null.”) We can further illustrate this point with the following
table:</p>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="center">Null Correct</th>
<th align="center">Null False</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Null Rejected</td>
<td align="center">V</td>
<td align="center">S</td>
<td align="center">R</td>
</tr>
<tr class="even">
<td align="right">Fail to Reject</td>
<td align="center">U</td>
<td align="center">T</td>
<td align="center">k-R</td>
</tr>
<tr class="odd">
<td align="right">Total</td>
<td align="center">k’</td>
<td align="center">k-k’</td>
<td align="center">k</td>
</tr>
</tbody>
</table>
<p>The only observable random variable here is <span class="math inline">\(R\)</span>, the total number of rejected
null hypotheses. In the FDR procedure, we focus on the first <em>row</em>. We know
that
<span class="math display">\[
E[V] = \alpha k&#39; \leq \alpha k
\]</span>
and we know that <span class="math inline">\(V+S \leq k\)</span>, so
<span class="math display">\[
E\left[\frac{V}{V+S}\right] = E\left[\frac{V}{R}\right] \leq \frac{\alpha k&#39;}{k} \leq \alpha \,.
\]</span>
The FWER procedure, on the other hand, focuses on the first <em>column</em>, with
<span class="math display">\[
E\left[\frac{V}{V+U}\right] = \frac{\alpha k&#39;}{k&#39;} = \alpha \,.
\]</span></p>
<hr />
<div id="the-power-curve-for-testing-the-uniform-distribution-upper-bound" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> The Power Curve for Testing the Uniform Distribution Upper Bound<a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume, as we do above, that we have sampled <span class="math inline">\(n\)</span> iid
data from a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution, and that we will
use these data to test the hypotheses
<span class="math display">\[
H_o: \theta = \theta_o ~~\mbox{versus}~~ H_a: \theta \neq \theta_o \,.
\]</span></p>
</blockquote>
<blockquote>
<p>The sufficient statistic is the maximum datum <span class="math inline">\(X_{(n)}\)</span>. As stated above,
we know that we will reject the null when <span class="math inline">\(X_{(n)} &gt; \theta_o\)</span>; that’s
a “trivial” statement. As for the rejection region boundary
when <span class="math inline">\(X_{(n)} \leq \theta_o\)</span>: we know that the cdf for <span class="math inline">\(X_{(n)}\)</span> is
<span class="math inline">\(F_{(n)}(x) = (x/\theta)^n\)</span>, so the lower boundary is
<span class="math display">\[
\left(\frac{x_{\alpha/2}}{\theta_o}\right)^n = \frac{\alpha}{2} ~~~ \Rightarrow ~~~ \ldots \,.
\]</span>
Except, this is wrong: what would be the power if <span class="math inline">\(\theta = \theta_o\)</span>?
It would be <span class="math inline">\(\alpha/2\)</span> and not <span class="math inline">\(\alpha\)</span>. So despite the fact that we are
carrying out a two-tail test, all the <span class="math inline">\(\alpha\)</span> “goes to the left” of
<span class="math inline">\(\theta_o\)</span> (because it is impossible to reject “to the right”: if the null
is correct, <span class="math inline">\(X_{(n)} &gt; \theta_o\)</span> is impossible. So we have that
<span class="math display">\[
\left(\frac{x_{\alpha}}{\theta_o}\right)^n = \alpha
\]</span>
and thus that
<span class="math display">\[
x_{\alpha} = \theta_o \alpha^{1/n} \,.
\]</span>
If <span class="math inline">\(X_{(n)} &lt; x_{\alpha}\)</span>, we reject the null. Full stop.</p>
</blockquote>
<blockquote>
<p>The power of this test is
<span class="math display">\[\begin{align*}
P(\mbox{reject}~\mbox{null} \vert \theta) &amp;= P(X_{(n)} &lt; x_\alpha \cup X_{(n)} &gt; \theta_o \vert \theta) \\
&amp;= F_{(n)}(x_\alpha \vert \theta) + [1 - F_{(n)}(\theta_o \vert \theta)] \\
&amp;= \left\{ \begin{array}{rl} 1 &amp; \theta &lt; x_\alpha \\ \left(\frac{x_\alpha}{\theta}\right)^n &amp; x_\alpha &lt; \theta \leq \theta_o \\ 1 + \left(\frac{x_\alpha}{\theta}\right)^n - \left(\frac{\theta_o}{\theta}\right)^n &amp; \theta &gt; \theta_o \end{array} \right. \,.
\end{align*}\]</span>
For the first condition above: if <span class="math inline">\(\theta &lt; x_\alpha\)</span>, then
<span class="math inline">\(X_{(n)} &lt; x_\alpha\)</span>, so every test will result in a rejection, and
the power is thus 1.
We plot out the power curve for <span class="math inline">\(\theta_o = 1\)</span> and <span class="math inline">\(n = 10\)</span>
in Figure <a href="the-uniform-distribution.html#fig:uhyppow">5.4</a>.</p>
</blockquote>
<pre><code>## Warning in geom_point(aes(x = theta.o, y = alpha), size = 4, col = &quot;red&quot;): All aesthetics have length 1, but the data has 101 rows.
## ℹ Please consider using `annotate()` or provide this layer with data containing a single
##   row.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uhyppow"></span>
<img src="_main_files/figure-html/uhyppow-1.png" alt="\label{fig:uhyppow}The power curve for the test of $H_o : \theta = \theta_o = 1$ versus $H_a : \theta \neq \theta_o$. The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, $\alpha$, at $\theta = 1$." width="50%" />
<p class="caption">
Figure 5.4: The power curve for the test of <span class="math inline">\(H_o : \theta = \theta_o = 1\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>. The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, <span class="math inline">\(\alpha\)</span>, at <span class="math inline">\(\theta = 1\)</span>.
</p>
</div>
<hr />
</div>
<div id="an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean" class="section level3 hasAnchor" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> An Illustration of Multiple Comparisons When Testing for the Normal Mean<a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the code chunk below, we generate <span class="math inline">\(k = 100\)</span> independent datasets of
size <span class="math inline">\(n = 40\)</span>; for <span class="math inline">\(k&#39; = 80\)</span> datasets, <span class="math inline">\(\mu = 0\)</span>, and for the remainder,
<span class="math inline">\(\mu = 0.5\)</span>. For simplicity, we assume <span class="math inline">\(\sigma^2\)</span> is known and is equal
to one. For each dataset, we test the hypotheses <span class="math inline">\(H_o : \mu = 0\)</span>
versus <span class="math inline">\(H_a : \mu &gt; 0\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="the-uniform-distribution.html#cb300-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb300-2"><a href="the-uniform-distribution.html#cb300-2" tabindex="-1"></a></span>
<span id="cb300-3"><a href="the-uniform-distribution.html#cb300-3" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb300-4"><a href="the-uniform-distribution.html#cb300-4" tabindex="-1"></a>k      <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb300-5"><a href="the-uniform-distribution.html#cb300-5" tabindex="-1"></a>k.p    <span class="ot">&lt;-</span> <span class="dv">80</span></span>
<span id="cb300-6"><a href="the-uniform-distribution.html#cb300-6" tabindex="-1"></a>mu     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,k.p),<span class="fu">rep</span>(<span class="fl">0.5</span>,k<span class="sc">-</span>k.p))</span>
<span id="cb300-7"><a href="the-uniform-distribution.html#cb300-7" tabindex="-1"></a>mu.o   <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb300-8"><a href="the-uniform-distribution.html#cb300-8" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb300-9"><a href="the-uniform-distribution.html#cb300-9" tabindex="-1"></a></span>
<span id="cb300-10"><a href="the-uniform-distribution.html#cb300-10" tabindex="-1"></a>p      <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,k)</span>
<span id="cb300-11"><a href="the-uniform-distribution.html#cb300-11" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k ) {</span>
<span id="cb300-12"><a href="the-uniform-distribution.html#cb300-12" tabindex="-1"></a>  X     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span>mu[ii],<span class="at">sd=</span>sigma2)</span>
<span id="cb300-13"><a href="the-uniform-distribution.html#cb300-13" tabindex="-1"></a>  p[ii] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fu">mean</span>(X),<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))</span>
<span id="cb300-14"><a href="the-uniform-distribution.html#cb300-14" tabindex="-1"></a>}</span></code></pre></div>
<blockquote>
<p>Below, we try two separate corrections for multiple comparisons: the
Bonferroni correction (controlling FWER), and the Benjamini-Hochberg
procedure (controlling FDR).</p>
</blockquote>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="the-uniform-distribution.html#cb301-1" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb301-2"><a href="the-uniform-distribution.html#cb301-2" tabindex="-1"></a></span>
<span id="cb301-3"><a href="the-uniform-distribution.html#cb301-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The number of rejected null hypotheses for Bonferroni: &quot;</span>,</span>
<span id="cb301-4"><a href="the-uniform-distribution.html#cb301-4" tabindex="-1"></a>    <span class="fu">sum</span>(p <span class="sc">&lt;</span> alpha<span class="sc">/</span>k),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The number of rejected null hypotheses for Bonferroni:  9</code></pre>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="the-uniform-distribution.html#cb303-1" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">which</span>(p <span class="sc">&lt;</span> alpha<span class="sc">/</span>k)</span>
<span id="cb303-2"><a href="the-uniform-distribution.html#cb303-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The number of falsely rejected null hypotheses is:     &quot;</span>,<span class="fu">sum</span>(w<span class="sc">&lt;=</span>k.p),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The number of falsely rejected null hypotheses is:      0</code></pre>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="the-uniform-distribution.html#cb305-1" tabindex="-1"></a>p.sort <span class="ot">&lt;-</span> <span class="fu">sort</span>(p)</span>
<span id="cb305-2"><a href="the-uniform-distribution.html#cb305-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The number of rejected null hypotheses for FDR:        &quot;</span>,</span>
<span id="cb305-3"><a href="the-uniform-distribution.html#cb305-3" tabindex="-1"></a>    <span class="fu">sum</span>(p.sort <span class="sc">&lt;</span> (<span class="dv">1</span><span class="sc">:</span>k)<span class="sc">*</span>alpha<span class="sc">/</span>k),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The number of rejected null hypotheses for FDR:         17</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="the-uniform-distribution.html#cb307-1" tabindex="-1"></a>p.rej <span class="ot">&lt;-</span> p.sort[p.sort<span class="sc">&lt;</span>(<span class="dv">1</span><span class="sc">:</span>k)<span class="sc">*</span>alpha<span class="sc">/</span>k]</span>
<span id="cb307-2"><a href="the-uniform-distribution.html#cb307-2" tabindex="-1"></a>w <span class="ot">&lt;-</span> p <span class="sc">%in%</span> p.rej</span>
<span id="cb307-3"><a href="the-uniform-distribution.html#cb307-3" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">which</span>(w<span class="sc">==</span><span class="cn">TRUE</span>)</span>
<span id="cb307-4"><a href="the-uniform-distribution.html#cb307-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The number of falsely rejected null hypotheses is:     &quot;</span>,<span class="fu">sum</span>(w<span class="sc">&lt;=</span>k.p),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The number of falsely rejected null hypotheses is:      0</code></pre>
<blockquote>
<p>With the Bonferroni correction, we reject nine null hypotheses (with the
guarantee that there is, on average, a five percent chance that we
erroneously reject one or more of the true nulls…here, we reject no correct
null hypotheses. See Figure <a href="the-uniform-distribution.html#fig:mcs">5.5</a>.</p>
</blockquote>
<blockquote>
<p>With the BH procedure, we reject 17 null hypotheses (with the guarantee
that on average, five percent of these 17 [meaning, effectively, 1] is
an erroneous rejection…here, we reject no correct null hypotheses).</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mcs"></span>
<img src="_main_files/figure-html/mcs-1.png" alt="\label{fig:mcs}An illustration of the difference between the Bonferroni correction and the Benjamini-Hochberg procedure. The blue dots represent sorted $p$-values resulting from a simulation in which 80 of 100 null hypotheses are correct (so that a perfect disambiguation between null and non-null hypotheses would result in 20 rejected nulls, with none falsely rejected. The Bonferroni correction shifts $\alpha = 0.05$ downwards to the green short-dashed line; 9 $p$-values lie below the line, so 9 (true) null hypotheses are rejected in all. The BH procedure looks for the number of $p$-values lying below the red dashed line; that number is 17 (with no false rejections)." width="50%" />
<p class="caption">
Figure 5.5: An illustration of the difference between the Bonferroni correction and the Benjamini-Hochberg procedure. The blue dots represent sorted <span class="math inline">\(p\)</span>-values resulting from a simulation in which 80 of 100 null hypotheses are correct (so that a perfect disambiguation between null and non-null hypotheses would result in 20 rejected nulls, with none falsely rejected. The Bonferroni correction shifts <span class="math inline">\(\alpha = 0.05\)</span> downwards to the green short-dashed line; 9 <span class="math inline">\(p\)</span>-values lie below the line, so 9 (true) null hypotheses are rejected in all. The BH procedure looks for the number of <span class="math inline">\(p\)</span>-values lying below the red dashed line; that number is 17 (with no false rejections).
</p>
</div>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-poisson-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 The Uniform Distribution | Modern Probability and Statistical Inference</title>
  <meta name="description" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 The Uniform Distribution | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-poisson-and-related-distributions.html"/>
<link rel="next" href="multivariate-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.19" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#exercises"><i class="fa fa-check"></i><b>1.19</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-exponential-family-of-distributions"><i class="fa fa-check"></i><b>2.19</b> The Exponential Family of Distributions</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exponential-family-with-a-vector-of-parameters"><i class="fa fa-check"></i><b>2.19.1</b> Exponential Family with a Vector of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exercises-1"><i class="fa fa-check"></i><b>2.20</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.2.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-binomial-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>3.2.5</b> The Binomial Distribution as Part of the Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sufficiency-principle-examples-of-when-we-cannot-reduce-data"><i class="fa fa-check"></i><b>3.6.3</b> The Sufficiency Principle: Examples of When We Cannot Reduce Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.5</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#utilizing-simulations-when-data-reduction-is-not-possible"><i class="fa fa-check"></i><b>3.8.6</b> Utilizing Simulations When Data Reduction is Not Possible</a></li>
<li class="chapter" data-level="3.8.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.7</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises-2"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-poisson-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>4.2.1</b> The Poisson Distribution as Part of the Exponential Family</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.2</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
<li class="chapter" data-level="4.6.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation"><i class="fa fa-check"></i><b>4.6.5</b> Confidence Interval Estimation via Simulation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#exercises-3"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.3.1</b> Sufficient Statistics for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-using-an-order-statistic"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Using an Order Statistic</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-test-for-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> Hypothesis Test for the Uniform Distribution Upper Bound</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#using-numerical-integration-to-characterize-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Using Numerical Integration to Characterize a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.4</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
<li class="chapter" data-level="6.4.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#mutual-information"><i class="fa fa-check"></i><b>6.4.3</b> Mutual Information</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#multiple-comparisons"><i class="fa fa-check"></i><b>7.6</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>7.6.1</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><a href="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><i class="fa fa-check"></i>Appendix B: Confidence Interval and Hypothesis Test Reference Tables</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-1"><i class="fa fa-check"></i>Chapter 1</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-2"><i class="fa fa-check"></i>Chapter 2</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-6"><i class="fa fa-check"></i>Chapter 6</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-uniform-distribution" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> The Uniform Distribution<a href="the-uniform-distribution.html#the-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="graybox">
<p>Let <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> be <span class="math inline">\(n\)</span> independent and identically distributed
data sampled according to a distribution <span class="math inline">\(P(\theta)\)</span>.</p>
<ul>
<li><p><span class="math inline">\(\theta\)</span> is a <em>domain-specifying parameter</em> if, e.g., all the sampled
data have values <span class="math inline">\(\leq \theta\)</span>.</p></li>
<li><p>This chapter is all about the quirks that we observe when we work with
domain-specifying parameters, ones that affect the determination of
maximum-likelihood and minimum-variance unbiased estimators, how we
go about constructing hypothesis tests, etc.</p></li>
</ul>
</div>
<div id="properties" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Properties<a href="the-uniform-distribution.html#properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The uniform distribution is often used within the realm of probability,
in part because of its utility and in part because of its simplicity.
We briefly touched upon this distribution at times earlier in this book,
such as, for instance,
when we talked about hypothesis test <span class="math inline">\(p\)</span>-values (which are distributed
uniformly between 0 and 1 when the null hypothesis is correct).
Why do we return to the uniform distribution
now? Because it is slightly different from other distributions: its two
parameters, often denoted <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (where <span class="math inline">\(b &gt; a\)</span>), do not dictate the
shape of its probability density function, but rather its domain. This
affects aspects of estimation, such as determining sufficient statistics
and deriving maximum likelihood estimates, etc.
We highlight these quirks of the uniform distribution (and, indeed, of
any distribution with domain-specifying parameters) throughout this chapter.</p>
<p><strong>Recall</strong>: <em>a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) <span class="math inline">\(f_X(x) \geq 0\)</span> and (b) <span class="math inline">\(\int_x f_X(x) dx = 1\)</span>, where the integral is over all values of <span class="math inline">\(x\)</span> in the distribution’s domain.</em></p>
<p>The uniform pdf is defined as
<span class="math display">\[
f_X(x) = \frac{1}{b-a} ~~\mbox{where}~~ x \in [a,b] \,.
\]</span>
<span class="math inline">\(f_X(x)\)</span> is thus constant between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.
(See Figure <a href="the-uniform-distribution.html#fig:unifpdf">5.1</a>.)
This means that we can
think of the uniform distribution “geometrically,” as the following
is true:
<span class="math display">\[
\underbrace{(b-a)}_{\mbox{domain}} \cdot \underbrace{\frac{1}{b-a}}_{f_X(x)} = 1
\]</span>
If we know the domain of the pdf, we immediately know <span class="math inline">\(f_X(x)\)</span>; conversely,
if we know
<span class="math inline">\(f_X(x)\)</span>, we immediately know the width of the domain (but not <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>
themselves).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unifpdf"></span>
<img src="_main_files/figure-html/unifpdf-1.png" alt="\label{fig:unifpdf}Three examples of uniform probability mass functions: Uniform(0,1) (solid red line), Uniform(0.5,2) (dashed green line), and Uniform(-1.5,1.5) (dotted blue line)." width="50%" />
<p class="caption">
Figure 5.1: Three examples of uniform probability mass functions: Uniform(0,1) (solid red line), Uniform(0.5,2) (dashed green line), and Uniform(-1.5,1.5) (dotted blue line).
</p>
</div>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as <span class="math inline">\(F_X(x) = \int_{y \leq x} f_Y(y) dy\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>The cdf for a uniformly distributed random variable is
<span class="math display">\[
F_X(x) = \int_a^x f_Y(y) dy = \int_a^x \frac{1}{b-a} dy = \frac{x-a}{b-a} ~~ x \in [a,b] \,,
\]</span>
with a value of 0 for <span class="math inline">\(x &lt; a\)</span> and 1 for <span class="math inline">\(x &gt; b\)</span>. (We can quickly confirm that
the derivative of the cdf yields the pdf. Recall that for continuous
distributions, <span class="math inline">\(f_X(x) = dF_X(x)/dx\)</span>.)</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(q = F_X(x)\)</span>.</em></p>
<p>The inverse cdf is exceptionally simple to compute:
<span class="math display">\[
q = \frac{x-a}{b-a} ~~ \Rightarrow ~~ x = (b-a)q + a \,.
\]</span>
Technically the inverse cdf has no unique solution when <span class="math inline">\(q = 0\)</span> or
<span class="math inline">\(q = 1\)</span>. However, it is convention (for instance, within <code>R</code>) that
the inverse cdf output for continuous distributions be the largest
value for which <span class="math inline">\(q = 0\)</span> and the smallest value for which <span class="math inline">\(q = 1\)</span>. Thus,
for a Uniform(<span class="math inline">\(a,b\)</span>) distribution,
when <span class="math inline">\(q = 0\)</span>, then <span class="math inline">\(x = a\)</span>, and when <span class="math inline">\(q = 1\)</span>, <span class="math inline">\(x = b\)</span>.</p>
<hr />
<p>A discrete analogue to the uniform distribution is the
<em>discrete uniform distribution</em>, which is defined over a range of
integers <span class="math inline">\([a,b]\)</span>. (The rolls of a fair, six-sided die would, for
instance, be governed by the discrete uniform distribution.) The pmf
for the discrete uniform is
<span class="math display">\[
p_X(x) = \frac{1}{n} ~~ x \in [a,b] \,,
\]</span>
where <span class="math inline">\(n = b - a + 1\)</span> is the number of possible experimental outcomes.
The cdf is
<span class="math display">\[
F_X(x) = \frac{\lfloor x \rfloor - a + 1}{n} ~~ x \in [a,b] \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> is the largest integer that is smaller than
or equal to <span class="math inline">\(x\)</span>, while the inverse cdf is given by the generalized inverse
cdf formalism that we’ve previously seen for discrete distributions.</p>
<p>Note that there are no standard <code>R</code> functions of the form <code>xdiscunif()</code>
for computing the pmf or cdf of
the discrete uniform distribution, or for sampling from it. (See, however,
the <code>xdunif()</code> functions defined within the contributed <code>extraDistr</code>
package.) We show how one can easily create such functions for one’s own
use in an example below.</p>
<hr />
<div id="the-expected-value-and-variance-of-a-uniform-random-variable" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> The Expected Value and Variance of a Uniform Random Variable<a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the expected value of a continuously distributed random variable is</em>
<span class="math display">\[
E[X] = \int_x x f_X(x) dx\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(f_X(x)\)</span>.</em></p>
<blockquote>
<p>The expected value of a random variable drawn from a Uniform(<span class="math inline">\(a,b\)</span>)
distribution is
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_a^b x f_X(x) dx = \int_a^b \frac{x}{b-a} dx \\
&amp;= \frac{1}{b-a} \left. \frac{x^2}{2} \right|_a^b = \frac{1}{b-a} \frac{b^2-a^2}{2} = \frac{1}{b-a} \frac{(b-a)(b+a)}{2} = \frac{a+b}{2} \,.
\end{align*}\]</span></p>
</blockquote>
<p><strong>Recall:</strong> <em>the variance of a continuously distributed random variable is</em>
<span class="math display">\[
V[X] = \int_x (x-\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(f_X(x)\)</span> is effectively non-zero.</em></p>
<blockquote>
<p>To find the variance, we work with the shortcut formula:
<span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>. We know <span class="math inline">\(E[X]\)</span> already; as for <span class="math inline">\(E[X^2]\)</span>,
we utilize the Law of the Unconscious Statistician:
<span class="math display">\[\begin{align*}
E[X^2] = \int_a^b x^2 f_X(x) dx &amp;= \int_a^b \frac{x^2}{b-a} dx \\
&amp;= \frac{1}{b-a} \left. \frac{x^3}{3} \right|_a^b \\
&amp;= \frac{b^3-a^3}{3(b-a)} \\
&amp;= \frac{(b-a)(a^2+ab+b^2)}{3(b-a)} = \frac{1}{3}\left(a^2 + ab + b^2\right) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
V[X] &amp;= \frac{1}{3}\left(a^2 + ab + b^2\right) - \left(\frac{a+b}{2}\right)^2 \\
&amp;= \frac{1}{3}\left(a^2 + ab + b^2\right) - \frac{1}{4}\left(a^2+2ab+b^2\right) \\
&amp;= \frac{1}{12}\left(4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\right) \\
&amp;= \frac{1}{12}\left(a^2 - 2ab + b^2 \right) \\
&amp;= \frac{(a-b)^2}{12} \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="coding-r-style-functions-for-the-discrete-uniform-distribution" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Coding R-Style Functions for the Discrete Uniform Distribution<a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>There are four standard functions associated with any distribution:
the one prefaced by <code>d</code> that returns the output of the
probability mass function or probability density function, given a
coordinate <span class="math inline">\(x\)</span>; the one prefaced by <code>p</code> that returns the output
of the cumulative distribution function, given <span class="math inline">\(x\)</span>; the one prefaced
<code>q</code> that returns the output of the inverse cdf, given a quantile
<span class="math inline">\(q \in [0,1]\)</span>; and the random sampler, a function prefaced by <code>r</code>.</p>
</blockquote>
<blockquote>
<p>For the discrete uniform distribution, one can code the
probability mass function as follows:</p>
</blockquote>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="the-uniform-distribution.html#cb305-1" tabindex="-1"></a>ddiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(x,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb305-2"><a href="the-uniform-distribution.html#cb305-2" tabindex="-1"></a>{</span>
<span id="cb305-3"><a href="the-uniform-distribution.html#cb305-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb305-4"><a href="the-uniform-distribution.html#cb305-4" tabindex="-1"></a>  <span class="cf">if</span> ( x <span class="sc">%in%</span> y ) <span class="fu">return</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">length</span>(y))</span>
<span id="cb305-5"><a href="the-uniform-distribution.html#cb305-5" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb305-6"><a href="the-uniform-distribution.html#cb305-6" tabindex="-1"></a>}</span>
<span id="cb305-7"><a href="the-uniform-distribution.html#cb305-7" tabindex="-1"></a><span class="fu">ddiscunif</span>(<span class="dv">4</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>) <span class="co"># assume a fair six-sided die</span></span></code></pre></div>
<pre><code>## [1] 0.1666667</code></pre>
<blockquote>
<p>As for the cumulative distribution function:</p>
</blockquote>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="the-uniform-distribution.html#cb307-1" tabindex="-1"></a>pdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(x,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb307-2"><a href="the-uniform-distribution.html#cb307-2" tabindex="-1"></a>{</span>
<span id="cb307-3"><a href="the-uniform-distribution.html#cb307-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb307-4"><a href="the-uniform-distribution.html#cb307-4" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">which</span>(y<span class="sc">&lt;=</span>x)</span>
<span id="cb307-5"><a href="the-uniform-distribution.html#cb307-5" tabindex="-1"></a>  <span class="cf">if</span> ( <span class="fu">length</span>(w) <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb307-6"><a href="the-uniform-distribution.html#cb307-6" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">length</span>(w)<span class="sc">/</span><span class="fu">length</span>(y))</span>
<span id="cb307-7"><a href="the-uniform-distribution.html#cb307-7" tabindex="-1"></a>}</span>
<span id="cb307-8"><a href="the-uniform-distribution.html#cb307-8" tabindex="-1"></a><span class="fu">pdiscunif</span>(<span class="dv">4</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 0.6666667</code></pre>
<blockquote>
<p>The inverse cdf implements the generalized inverse algorithm:</p>
</blockquote>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="the-uniform-distribution.html#cb309-1" tabindex="-1"></a>qdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(q,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb309-2"><a href="the-uniform-distribution.html#cb309-2" tabindex="-1"></a>{</span>
<span id="cb309-3"><a href="the-uniform-distribution.html#cb309-3" tabindex="-1"></a>  y   <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb309-4"><a href="the-uniform-distribution.html#cb309-4" tabindex="-1"></a>  <span class="cf">if</span> ( q <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="fu">min</span>(y))</span>
<span id="cb309-5"><a href="the-uniform-distribution.html#cb309-5" tabindex="-1"></a>  <span class="cf">if</span> ( q <span class="sc">==</span> <span class="dv">1</span> ) <span class="fu">return</span>(<span class="fu">max</span>(y))</span>
<span id="cb309-6"><a href="the-uniform-distribution.html#cb309-6" tabindex="-1"></a>  cdf <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y))<span class="sc">/</span><span class="fu">length</span>(y)</span>
<span id="cb309-7"><a href="the-uniform-distribution.html#cb309-7" tabindex="-1"></a>  w   <span class="ot">&lt;-</span> <span class="fu">which</span>(cdf<span class="sc">&gt;=</span>q)</span>
<span id="cb309-8"><a href="the-uniform-distribution.html#cb309-8" tabindex="-1"></a>  <span class="cf">if</span> ( <span class="fu">length</span>(w) <span class="sc">==</span> <span class="dv">0</span> ) <span class="fu">return</span>(<span class="fu">max</span>(y))</span>
<span id="cb309-9"><a href="the-uniform-distribution.html#cb309-9" tabindex="-1"></a>  <span class="fu">return</span>(y[<span class="fu">min</span>(w)])</span>
<span id="cb309-10"><a href="the-uniform-distribution.html#cb309-10" tabindex="-1"></a>}</span>
<span id="cb309-11"><a href="the-uniform-distribution.html#cb309-11" tabindex="-1"></a><span class="fu">qdiscunif</span>(<span class="fl">0.55</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<blockquote>
<p>And last, the random data generator:</p>
</blockquote>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="the-uniform-distribution.html#cb311-1" tabindex="-1"></a>rdiscunif <span class="ot">&lt;-</span> <span class="cf">function</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>,<span class="at">step=</span><span class="dv">1</span>)</span>
<span id="cb311-2"><a href="the-uniform-distribution.html#cb311-2" tabindex="-1"></a>{</span>
<span id="cb311-3"><a href="the-uniform-distribution.html#cb311-3" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">seq</span>(min,max,<span class="at">by=</span>step)</span>
<span id="cb311-4"><a href="the-uniform-distribution.html#cb311-4" tabindex="-1"></a>  s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">length</span>(y),n,<span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb311-5"><a href="the-uniform-distribution.html#cb311-5" tabindex="-1"></a>  <span class="fu">return</span>(y[s])</span>
<span id="cb311-6"><a href="the-uniform-distribution.html#cb311-6" tabindex="-1"></a>}</span>
<span id="cb311-7"><a href="the-uniform-distribution.html#cb311-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">235</span>) <span class="co"># set to ensure consistent output</span></span>
<span id="cb311-8"><a href="the-uniform-distribution.html#cb311-8" tabindex="-1"></a><span class="fu">rdiscunif</span>(<span class="dv">10</span>,<span class="at">min=</span><span class="dv">1</span>,<span class="at">max=</span><span class="dv">6</span>)</span></code></pre></div>
<pre><code>##  [1] 6 5 5 6 2 1 5 1 3 6</code></pre>
</div>
</div>
<div id="linear-functions-of-uniform-random-variables" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Linear Functions of Uniform Random Variables<a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s assume that we are given <span class="math inline">\(n\)</span> iid Uniform random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Uniform(<span class="math inline">\(a,b\)</span>). What is the distribution
of the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>?</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(E[e^{tX}]\)</span>. If <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> os the mgf for a known family of distributions, then we can immediately identify the distribution of <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The moment-generating function for the uniform distribution is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \int_a^b \frac{e^{tx}}{b-a} dx = \frac{1}{b-a} \left. \frac{1}{t}e^{tx} \right|_a^b = \frac{e^{tb}-e^{ta}}{t(b-a)} \,,
\end{align*}\]</span>
thus the mgf for the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
<span class="math display">\[
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = \left( \frac{e^{tb}-e^{ta}}{t(b-a)} \right)^n \,.
\]</span>
This expression does not simplify such that we recognize the distribution
of <span class="math inline">\(Y\)</span>. If <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = 1\)</span>, it turns out that the mgf does take on the
form of that for an Irwin-Hall distribution. An Irwin-Hall random variable
converges in distribution to a normal random variable as
<span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>We find ourselves in a similar situation if we look at the sample mean
<span class="math inline">\(\bar{X} = Y/n\)</span>:
<span class="math display">\[
m_{\bar{X}}(t) = \prod_{i=1}^n m_{X_i}\left(\frac{t}{n}\right) = \left( \frac{n(e^{tb/n}-e^{ta/n})}{t(b-a)} \right)^n \,.
\]</span>
If <span class="math inline">\(a = 0\)</span> and <span class="math inline">\(b = 1\)</span>, <span class="math inline">\(\bar{X}\)</span> is sampled from a Bates distribution.
A Bates random variable converges in distribution to a normal random variable
as <span class="math inline">\(n \rightarrow \infty\)</span>. For all other combinations of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we
cannot write down a specific functional form for the sampling distribution of
<span class="math inline">\(\bar{X}\)</span> and thus we would have to perform simulations to test hypotheses,
etc. (However, we note that because statistical inference for a uniform
distribution involves determining
the lower and/or upper bounds, we can utilize order
statistics for inference instead of <span class="math inline">\(\bar{X}\)</span>. See the next section below.)</p>
<hr />
<div id="the-moment-generating-function-for-a-discrete-uniform-distribution" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> The Moment-Generating Function for a Discrete Uniform Distribution<a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The mgf for a discrete uniform random variable is
<span class="math display">\[
E[e^{tX}] = \sum_{x=a}^b e^{tx} p_X(x) = \frac{1}{n} \sum_{x=a}^b e^{tx} \,.
\]</span>
We cannot say anything further without making an assumption. If we say
that <span class="math inline">\(x \in [a,a+1,\ldots,b-1,b]\)</span>, i.e., that there are integer steps
between the probability masses, then
<span class="math display">\[
E[e^{tX}] = \frac{1}{n} \sum_{x=a}^b e^{tx} = \frac{1}{n}e^{ta} \left( 1 + e^{t(a+1)} + \cdots + e^{t(b-a)} \right) \,.
\]</span>
If <span class="math inline">\(t\)</span> is negative, then we can make use of a geometric sum:
<span class="math display">\[
1 + e^t + \cdots = \frac{1}{1-e^t} = \underbrace{1 + \cdots + e^{t(b-a)}}_{} + \underbrace{e^{t(b-a+1)} + \cdots}_{} \,,
\]</span>
where the first underbraced quantity is what appears above in the
expected value. Thus we can rearrange terms and write
<span class="math display">\[\begin{align*}
1 + e^{t(a+1)} + \cdots + e^{t(b-a)} &amp;= \frac{1}{1-e^t} - \left( e^{t(b-a+1)} + \cdots \right) \\
&amp;= \frac{1}{1-e^t} - e^{t(b-a+1)}\left(1 + e^t + \cdots\right) \\
&amp;= \frac{1}{1-e^t} - \frac{e^{t(b-a+1)}}{1-e^t} = \frac{1-e^{t(b-a+1)}}{1-e^t} \,.
\end{align*}\]</span>
Putting everything together, we find that
<span class="math display">\[
m_X(t) = \frac{1}{n}e^{ta} \frac{1-e^{t(b-a+1)}}{1-e^t} = \frac{e^{ta}-e^{t(b+1)}}{n(1-e^t)} \,.
\]</span>
This is the usual form of the mgf presented for the discrete uniform
distribution, but again, this is only valid if the masses are separated
by one unit: <span class="math inline">\(x \in [a,a+1,\ldots,b-1,b]\)</span>.</p>
</blockquote>
</div>
</div>
<div id="sufficient-statistics-and-the-minimum-variance-unbiased-estimator" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Sufficient Statistics and the Minimum Variance Unbiased Estimator<a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a sufficient statistic for a population parameter <span class="math inline">\(\theta\)</span> captures all information about <span class="math inline">\(\theta\)</span> contained in a data sample; no additional statistic will provide more information about <span class="math inline">\(\theta\)</span>. Sufficient statistics are not unique: one-to-one functions of sufficient statistics are themselves sufficient statistics.</em></p>
<p>Before we discuss sufficient statistics in the context of the uniform
distribution, it is useful to (re-)introduce the <em>indicator function</em>. This
function, mentioned briefly in Chapter 1, takes on the value 1 if a
specified condition is met and 0 otherwise. For instance,
<span class="math display">\[
\mathbb{I}_{x_i \in [0,1]} = \left\{ \begin{array}{cl} 1 &amp; x_i \in [0,1] \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
One use for the indicator function is to, well, <em>indicate</em> the domain of a pmf
or pdf. For instance, we can write
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{ll} e^{-x} &amp; x \geq 0 \\ 0 &amp; \mbox{otherwise} \end{array} \right.
\]</span>
to express that the exponential distribution with rate <span class="math inline">\(\beta = 1\)</span> is
defined within the domain <span class="math inline">\(x \in [0,\infty)\)</span>, or, equivalently, we can write
<span class="math display">\[
f_X(x) = e^{-x} \mathbb{I}_{x \in [0,\infty)} \,.
\]</span>
The latter form expresses the same information in a more condensed fashion.</p>
<p>So…what do indicator functions have to do with uniform distributions?</p>
<p>Let’s suppose we sample <span class="math inline">\(n\)</span> iid data <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> from
a uniform distribution with lower bound 0 and upper bound <span class="math inline">\(\theta\)</span>, and
our goal is to define a sufficient statistic for <span class="math inline">\(\theta\)</span>. Let’s work
with the factorization criterion:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = g(\mathbf{x},\theta) \cdot h(\mathbf{x}) \,.
\]</span>
The likelihood is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \prod_{i=1}^n \frac{1}{\theta} = \frac{1}{\theta^n} \,.
\]</span>
OK…no…wait, there are no data in this expression, so we cannot define
a sufficient statistic. The way around this is to re-express the pdf as
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \mathbb{I}_{x \in [0,\theta]}
\]</span>
and to rewrite the likelihood as
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \frac{1}{\theta^n} \prod_{i=1}^n \mathbb{I}_{x_i \in [0,\theta]} \,.
\]</span>
A product of indicator functions will equal 1 if and only if <em>all</em> data lie
in the domain <span class="math inline">\(x \in [0,\theta]\)</span>. This is equivalent to saying that
<span class="math inline">\(\theta \geq X_{(n)}\)</span>, the order statistic representing the maximum observed
datum. Thus <span class="math inline">\(X_{(n)}\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>: we know
<span class="math inline">\(\theta\)</span> is greater than this statistic’s value, and none of the data
aside from <span class="math inline">\(X_{(n)}\)</span> provide any additional information about <span class="math inline">\(\theta\)</span>.</p>
<p>The upshot: when the parameter <span class="math inline">\(\theta\)</span> dictates (at least in part)
the domain of a distribution, a sufficient statistic for <span class="math inline">\(\theta\)</span> will be
an order statistic (or one-to-one functions of an order statistic).</p>
<p>When we first introduced the factorization criterion and sufficient statistics
back in Chapter 3, we did it so that ultimately we could write down the
minimum variance unbiased estimator (or MVUE).</p>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p><strong>Recall</strong>: <em>deriving the minimum variance unbiased estimator involves two steps:</em></p>
<ol style="list-style-type: decimal">
<li><em>factorizing the likelihood function to uncover a sufficient statistic <span class="math inline">\(U\)</span> (that we assume is both minimal and complete); and</em></li>
<li><em>finding a function <span class="math inline">\(h(U)\)</span> such that <span class="math inline">\(E[h(U)] = \theta\)</span>.</em></li>
</ol>
<p>If <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> are iid data sampled according to a
Uniform(<span class="math inline">\(0,\theta\)</span>) distribution, can we
define an MVUE for <span class="math inline">\(\theta\)</span>? The answer is yes…but we have to recall how
we define the pdf of <span class="math inline">\(X_{(n)}\)</span> first.</p>
<p><strong>Recall</strong>: <em>the maximum of <span class="math inline">\(n\)</span> iid random variables sampled from a pdf
<span class="math inline">\(f_X(x)\)</span> has a sampling distribution given by</em>
<span class="math display">\[
f_{(n)}(x) = n f_X(x) [ F_X(x) ]^{n-1} \,,
\]</span>
<em>where <span class="math inline">\(F_X(x)\)</span> is the associated cdf.</em></p>
<p>For the Uniform(<span class="math inline">\(0,\theta\)</span>) distribution,
<span class="math display">\[
f_X(x) = \frac{1}{\theta} ~~\mbox{and}~~ F_X(x) = \int_0^x f_Y(y) dy = \int_0^x \frac{1}{\theta} dy = \frac{x}{\theta} \,,
\]</span>
so
<span class="math display">\[
f_{(n)}(x) = n \frac{1}{\theta} \left[ \frac{x}{\theta} \right]^{n-1} = n \frac{x^{n-1}}{\theta^n} \,.
\]</span>
To find the MVUE, we first compute the expected value of <span class="math inline">\(X_{(n)}\)</span>:
<span class="math display">\[
E[X_{(n)}] = \int_0^\theta x n \frac{x^{n-1}}{\theta^n} dx = \left. \frac{n}{(n+1)\theta^n} x^{n+1} \right|_0^\theta = \frac{n}{n+1} \theta \,,
\]</span>
and then rearrange terms:
<span class="math display">\[
E\left[\frac{n+1}{n}X_{(n)}\right] = \theta \,.
\]</span>
Thus
<span class="math display">\[
\hat{\theta}_{MVUE} = \frac{n+1}{n}X_{(n)}
\]</span>
is the MVUE for <span class="math inline">\(\theta\)</span>. (Note that we can utilize
a similar calculation to this one
to determine, e.g., the MVUE for <span class="math inline">\(\theta\)</span> when
data are sampled according to a Uniform(<span class="math inline">\(\theta,b\)</span>) distribution.)</p>
<div id="sufficient-statistics-for-the-pareto-domain-parameter" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Sufficient Statistics for the Pareto Domain Parameter<a href="the-uniform-distribution.html#sufficient-statistics-for-the-pareto-domain-parameter" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Pareto [puh-RAY-toh] distribution, also known in some quarters as
the power-law distribution, is
<span class="math display">\[
f_X(x) = \frac{\alpha \beta^\alpha}{x^{\alpha+1}} \,,
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> is the shape parameter and <span class="math inline">\(x \in [\beta,\infty)\)</span>,
where <span class="math inline">\(\beta\)</span> is the scale (or location)
parameter. Let’s assume <span class="math inline">\(\alpha\)</span> is known.
A sufficient statistic for <span class="math inline">\(\beta\)</span>, found via likelihood factorization,
is
<span class="math display">\[
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i) = \underbrace{\beta^{n\alpha}}_{g(\mathbf{x},\beta)} \cdot \underbrace{\frac{\alpha^n}{(\prod_{i=1}^n x_i)^{\alpha+1}}}_{h(\mathbf{x})} \,.
\]</span>
Wait…again, as is the case for the uniform distribution, no data appear
in the
expression <span class="math inline">\(g(\cdot)\)</span>. So we would go back and introduce
an indicator function into the pdf; it should be clear that when we do so,
<span class="math inline">\(g(\mathbf{x},\beta)\)</span> changes to
<span class="math display">\[
g(\mathbf{x},\beta) = \beta^{n\alpha} \prod_{i=1}^n \mathbb{I}_{x_i \in [\beta,\infty)}
\]</span>
and thus that because all data have to be larger than <span class="math inline">\(\beta\)</span>, a sufficient
statistic will be the minimum observed datum, <span class="math inline">\(X_{(1)}\)</span>.</p>
</blockquote>
</div>
<div id="mvue-properties-for-uniform-distribution-bounds" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> MVUE Properties for Uniform Distribution Bounds<a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The properties of estimators that we have examined thus far include the
bias (are our estimates offset from the truth, on average?), the variance
(over how large a range do our estimates vary?), etc. Let’s look at some
of these properties here, assuming we sample <span class="math inline">\(n\)</span> iid data from a
Uniform(<span class="math inline">\(0,\theta\)</span>) distribution.</p>
</blockquote>
<blockquote>
<p>Bias: the MVUE is by definition unbiased, since
<span class="math inline">\(E[(n+1/n)X_{(n)}] = \theta\)</span>.</p>
</blockquote>
<blockquote>
<p>Variance: the variance of <span class="math inline">\(\hat{\theta}_{MVUE}\)</span> is
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MVUE}] &amp;= E\left[\left(\hat{\theta}_{MVUE}\right)^2\right] - \left( E\left[ \hat{\theta}_{MVUE} \right] \right)^2 \\
&amp;= \frac{(n+1)^2}{n^2} \left( E[X_{(n)}^2] - (E[X_{(n)}])^2 \right) \,,
\end{align*}\]</span>
where
<span class="math display">\[
E[X_{(n)}^2] = \int_0^\theta x^2 n \frac{x^{n-1}}{\theta^n} dx = \left. \frac{n}{(n+2)\theta^n} x^{n+2} \right|_0^\theta = \frac{n}{n+2} \theta^2 \,.
\]</span>
Thus
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MVUE}] &amp;= \frac{(n+1)^2}{n^2} \left( \frac{n}{n+2} \theta^2 - \frac{n^2}{(n+1)^2} \theta^2 \right) \\
&amp;= \frac{(n+1)^2}{n^2} \left( \frac{n(n+1)^2 - n^2(n+2)}{(n+2)(n+1)^2} \theta^2 \right) \\
&amp;= \frac{(n+1)^2}{n^2} \left( \frac{n}{(n+2)(n+1)^2} \theta^2 \right) \\
&amp;= \frac{1}{n(n+2)} \theta^2 \rightarrow \frac{\theta^2}{n^2} ~~\mbox{as}~~ n \rightarrow \infty \,.
\end{align*}\]</span>
We observe that since the variance goes to zero as <span class="math inline">\(n \rightarrow \infty\)</span>,
the MVUE is a consistent estimator…</p>
</blockquote>
<blockquote>
<p>…but does the MVUE achieve the Cramer-Rao Lower Bound (CRLB), the
theoretical
lower bound on the variance of unbiased estimators? It turns out that
not only does it achieve the lower bound (which one can show equals
<span class="math inline">\(\theta^2/n\)</span>), but it even surpasses that bound!</p>
</blockquote>
<blockquote>
<p>Ultimately, we need not worry about
this seemingly worrisome result, because one of the so-called regularity
conditions that must hold for the bound calculation to be valid is
that the log-likelihood is differentiable everywhere within a
distribution’s domain. As we will see in the next section, this
condition does <em>not</em> hold when we are working with domain-specifying
parameters.</p>
</blockquote>
</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Maximum Likelihood Estimation<a href="the-uniform-distribution.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is, thus far, found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>Now that we have recalled how maximum likelihood estimation works, we can
state that this is <em>not</em> how the MLE is found for a domain-affecting
parameter! (Hence the “thus far” in the recall statement above.)
Let’s assume, for instance, that we sample <span class="math inline">\(n\)</span> iid random
variables from a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution. As stated above (without
the indicator function), the likelihood is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \frac{1}{\theta^n} \,.
\]</span>
This means that the smaller <span class="math inline">\(\theta\)</span> is, the larger the likelihood will be.
So how small can <span class="math inline">\(\theta\)</span> be? We can answer this intuitively: the domain
<span class="math inline">\([0,\theta]\)</span> has to just encompass all the observed data, i.e.,
<span class="math display">\[
\hat{\theta}_{MLE} = X_{(n)} \,.
\]</span>
If <span class="math inline">\(\theta\)</span> were smaller, <span class="math inline">\(X_{(n)}\)</span> would lie outside the domain. It is
fine for <span class="math inline">\(\theta\)</span> to be larger, since then all the data lie in the
domain <span class="math inline">\([0,\theta]\)</span>…but the larger <span class="math inline">\(\theta\)</span> is, the smaller the
likelihood.</p>
<p>We plot an example likelihood function in Figure <a href="the-uniform-distribution.html#fig:uniflik">5.2</a>. We observe
immediately that the usual MLE algorithm will not work here, as the likelihood
function is not differentiable at <span class="math inline">\(\theta = X_{(n)}\)</span>.
All we can do is, e.g., plot the likelihood and identify
the MLE as that value for which the likelihood is maximized (or identify the
value intuitively as we do above).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uniflik"></span>
<img src="_main_files/figure-html/uniflik-1.png" alt="\label{fig:uniflik}The likelihood function given $n=5$ data drawn from a Uniform(0,$\theta$) distribution, with $\theta = 1$. As $\theta$ cannot be smaller than the maximum observed value, the likelihood is zero for $\theta &lt; X_{(n)}$; it is $1/\theta^n$ for $\theta \geq X_{(n)}$. The maximum likelihood estimate is thus $X_{(n)}$ itself; as the likelihood function is not differentiable at this point, the MLE cannot be found via the algorithm that we have used previously." width="50%" />
<p class="caption">
Figure 5.2: The likelihood function given <span class="math inline">\(n=5\)</span> data drawn from a Uniform(0,<span class="math inline">\(\theta\)</span>) distribution, with <span class="math inline">\(\theta = 1\)</span>. As <span class="math inline">\(\theta\)</span> cannot be smaller than the maximum observed value, the likelihood is zero for <span class="math inline">\(\theta &lt; X_{(n)}\)</span>; it is <span class="math inline">\(1/\theta^n\)</span> for <span class="math inline">\(\theta \geq X_{(n)}\)</span>. The maximum likelihood estimate is thus <span class="math inline">\(X_{(n)}\)</span> itself; as the likelihood function is not differentiable at this point, the MLE cannot be found via the algorithm that we have used previously.
</p>
</div>
<hr />
<div id="the-mle-for-the-pareto-domain-parameter" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> The MLE for the Pareto Domain Parameter<a href="the-uniform-distribution.html#the-mle-for-the-pareto-domain-parameter" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that the Pareto distribution has the probability density function
<span class="math display">\[
f_X(x) = \frac{\alpha\beta^\alpha}{x^{\alpha+1}} \,,
\]</span>
where <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(x \in [\beta,\infty)\)</span>, and we show that a
sufficient statistic for <span class="math inline">\(\beta\)</span> (with <span class="math inline">\(\alpha\)</span> fixed) is the smallest
observed datum, <span class="math inline">\(X_{(1)}\)</span>. Because <span class="math inline">\(\beta\)</span> is a parameter that dictates
the domain, we find the MLE not via differentiation but rather by
identifying that the likelihood is maximized when <span class="math inline">\(\beta\)</span> is exactly
equal to <span class="math inline">\(X_{(1)}\)</span>, i.e., <span class="math inline">\(\hat{\beta}_{MLE} = X_{(1)}\)</span>. See Figure
<a href="the-uniform-distribution.html#fig:parlik">5.3</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parlik"></span>
<img src="_main_files/figure-html/parlik-1.png" alt="\label{fig:parlik}The likelihood function given $n=5$ data drawn from a Pareto(1,$\beta$) distribution, with $\beta = 1$. As $\beta$ cannot be larger than the minimum observed value, the likelihood is zero for $\beta \geq X_{(1)}$; it is $\theta^n(1/\prod_{i=1}^n x_i)^2$ for $\beta &lt; X_{(n)}$. The maximum likelihood estimate is thus $X_{(1)}$ itself; as the likelihood function is not differentiable at this point, the MLE cannot be found via the algorithm we have used previously." width="50%" />
<p class="caption">
Figure 5.3: The likelihood function given <span class="math inline">\(n=5\)</span> data drawn from a Pareto(1,<span class="math inline">\(\beta\)</span>) distribution, with <span class="math inline">\(\beta = 1\)</span>. As <span class="math inline">\(\beta\)</span> cannot be larger than the minimum observed value, the likelihood is zero for <span class="math inline">\(\beta \geq X_{(1)}\)</span>; it is <span class="math inline">\(\theta^n(1/\prod_{i=1}^n x_i)^2\)</span> for <span class="math inline">\(\beta &lt; X_{(n)}\)</span>. The maximum likelihood estimate is thus <span class="math inline">\(X_{(1)}\)</span> itself; as the likelihood function is not differentiable at this point, the MLE cannot be found via the algorithm we have used previously.
</p>
</div>
<hr />
</div>
<div id="mle-properties-for-uniform-distribution-bounds" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> MLE Properties for Uniform Distribution Bounds<a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In this example, we mimic what we do above when discussing the
properties of the MVUE: we look at estimator bias and variance, etc.,
assuming that <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> are iid data sampled according to
a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution.</p>
</blockquote>
<blockquote>
<p>Bias: we know, from our derivation of the MVUE, that
<span class="math display">\[
E[\hat{\theta}_{MLE}] = E[X_{(n)}] = \frac{n}{n+1}\theta \,,
\]</span>
and thus the estimator bias is
<span class="math display">\[
B[\hat{\theta}_{MLE}] = E[\hat{\theta}_{MLE}] - \theta = \frac{n}{n+1}\theta - \theta = -\frac{1}{n+1}\theta \,.
\]</span>
As we expect for the MLE, the estimator is at least asymptotically unbiased,
as the bias goes to zero as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</blockquote>
<blockquote>
<p>Variance: the variance of the MLE is
<span class="math display">\[
V[\hat{\theta}_{MLE}] = E[\hat{\theta}_{MLE}^2] - \left(E[\hat{\theta}_{MLE}\right)^2 = E[X_{(n)}^2] - (E[X_{(n)}])^2 \,.
\]</span>
We derived both <span class="math inline">\(E[X_{(n)}]\)</span> and <span class="math inline">\(E[X_{(n)}^2]\)</span> above when discussing the
MVUE, so we can write down immediately that
<span class="math display">\[
V[\hat{\theta}_{MLE}] = \frac{n}{n+2}\theta^2 - \left( \frac{n}{n+1}\theta\right)^2 = \frac{n}{(n+2)(n+1)^2}\theta^2 \rightarrow \frac{\theta^2}{n^2} ~~\mbox{as}~~ n \rightarrow \infty\,.
\]</span>
We observe that because the variance goes
to zero as <span class="math inline">\(n \rightarrow \infty\)</span>, the MLE is a consistent estimator.
The variance of the MLE is similar to, but not exactly the same as,
the variance for the MVUE, although the two variances converge to the same
value in as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-4" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Confidence Intervals<a href="the-uniform-distribution.html#confidence-intervals-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}\)</span> by solving the following equation:</em>
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span>, <span class="math inline">\(y_{\rm obs}\)</span> is the observed value of the statistic,
and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined using
the confidence interval reference table introduced in
section 16 of Chapter 1.</em></p>
<p>Recall that a sufficient statistic for a domain-specifying parameter is
an order statistic. For instance, when we sample <span class="math inline">\(n\)</span> iid data according to
a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution, a sufficient statistic is <span class="math inline">\(Y = X_{(n)}\)</span>,
with probability density function
<span class="math display">\[
f_Y(y) = f_{(n)}(x) = n \frac{x^{n-1}}{\theta^n}
\]</span>
and cumulative distribution function
<span class="math display">\[
F_Y(y) = F_{(n)}(x) = (x/\theta)^n \,.
\]</span>
We work with this cdf in an example below to derive a two-sided
confidence interval for <span class="math inline">\(\theta\)</span>.</p>
<hr />
<p>We conclude our coverage (so to speak) of confidence intervals by going back
to the notion of the confidence coefficient <span class="math inline">\(1 - \alpha\)</span>. In a footnote
in Chapter 1, we make the point that technically, the confidence coefficient
is the infimum, or minimum value, of the probability
<span class="math inline">\(P(\hat{\theta}_L \leq \theta \leq \hat{\theta}_U)\)</span>. What does this
actually mean?</p>
<p>Let’s suppose that we have sampled <span class="math inline">\(n\)</span> iid data from a normal distribution,
and that we are going to construct a confidence interval of the form
<span class="math display">\[
P(S^2 - a \leq \sigma^2 \leq S^2 + a)
\]</span>
for the population variance <span class="math inline">\(\sigma^2\)</span>. We can do this, right? Let’s see…
<span class="math display">\[\begin{align*}
P(S^2 - a \leq \sigma^2 \leq S^2 + a) &amp;= P(-a \leq S^2-\sigma^2 \leq a) \\
&amp;= P\left(1 - \frac{a}{\sigma^2} \leq \frac{S^2}{\sigma^2} \leq 1 + \frac{a}{\sigma^2}\right) \\
&amp;= P\left( (n-1)\left(1 - \frac{a}{\sigma^2}\right) \leq \frac{(n-1)S^2}{\sigma^2} \leq (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right)\\
&amp;= F_{W}\left( (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right) - F_{W}\left( (n-1)\left(1 - \frac{a}{\sigma^2}\right) \right) \,,
\end{align*}\]</span>
where <span class="math inline">\(W\)</span> is a chi-square-distributed random variable for <span class="math inline">\(n-1\)</span> degrees of
freedom.
The key to interpreting the last line above is that <span class="math inline">\(\sigma^2\)</span> is <em>unknown</em>
(otherwise, why would we be constructing a confidence interval for it
in the first place?), and thus
can plausibly have any positive value. What if <span class="math inline">\(\sigma^2\)</span> is very large?
<span class="math display">\[
\lim_{\sigma^2 \to \infty} F_{W}\left[ (n-1)\left(1 + \frac{a}{\sigma^2}\right) \right] - F_{W}\left[ (n-1)\left(1 - \frac{a}{\sigma^2}\right) \right] = F_{W}(n-1) - F_{W}(n-1) = 0 \,.
\]</span>
Thus the confidence coefficient for the interval
<span class="math inline">\(S^2 - a \leq \sigma^2 \leq S^2 + a\)</span> is <span class="math inline">\(1 - \alpha = 0\)</span> (or,
we have that <span class="math inline">\(\alpha = 1\)</span>).</p>
<p>The upshot: one cannot just write down any interval and assume that it comes
with a guarantee of non-zero coverage!</p>
<div id="interval-estimation-using-an-order-statistic" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Interval Estimation Using an Order Statistic<a href="the-uniform-distribution.html#interval-estimation-using-an-order-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We assume that we are given <span class="math inline">\(n\)</span> iid data that are sampled according
to a Uniform(<span class="math inline">\(0,\theta\)</span>)
distribution. Above, we write down that the cdf for the maximum observed
datum, <span class="math inline">\(X_{(n)}\)</span>, is <span class="math inline">\(F_{(n)}(x) = (x/\theta)^n\)</span>.
Before we continue, however, we need to determine whether or not
the expected value of the maximum observed datum, <span class="math inline">\(E[X_{(n)}]\)</span>, increases
with <span class="math inline">\(\theta\)</span>. The expected value is easily derived, but here we will appeal
to reason: if we increase <span class="math inline">\(\theta\)</span>, the upper bound of the domain, then
on average the maximum observed value <em>must</em> increase. Thus we know that
we will work with the quantities on the “yes” line of the confidence
interval reference table.</p>
</blockquote>
<blockquote>
<p>To find the lower and upper
bounds on <span class="math inline">\(\theta\)</span>, respectively, we solve for <span class="math inline">\(\theta\)</span> in the expressions
<span class="math display">\[\begin{align*}
\left(\frac{X_{(n)}}{\theta}\right)^n - \left(1 - \frac{\alpha}{2}\right) &amp;= 0 ~~~ \mbox{(lower)} \\
\left(\frac{X_{(n)}}{\theta}\right)^n - \frac{\alpha}{2} &amp;= 0 ~~~ \mbox{(upper)} \,.
\end{align*}\]</span>
We find that
<span class="math display">\[
\hat{\theta}_L = \frac{X_{(n)}}{(1-\alpha/2)^{1/n}} ~~\mbox{and}~~ \hat{\theta}_U = \frac{X_{(n)}}{(\alpha/2)^{1/n}} \,.
\]</span></p>
</blockquote>
<blockquote>
<p>In Figure <a href="the-uniform-distribution.html#fig:unifci">5.4</a>, we display 10 separate 90 percent
confidence intervals generated using data sampled according to a
Uniform(<span class="math inline">\(0,\theta\)</span>) distribution (where here <span class="math inline">\(\theta = 1\)</span>). In this
figure, the maximum observed values for each dataset are shown as red
crosses, while the intervals are displayed as blue lines. We immediately
see one of the quirks associated with domain-specifying parameters:
the observed data do not lie within the intervals (as they have previously)
but rather outside of them. This is good: no observed value
of <span class="math inline">\(X_{(n)}\)</span> should be larger than the derived lower bound! (Otherwise
it would be impossible to observe that value, if indeed the derived lower
bound is the true value.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unifci"></span>
<img src="_main_files/figure-html/unifci-1.png" alt="\label{fig:unifci}Ten 90 percent confidence intervals, generated using data from 10 separate datasets of size $n = 10$ sampled according to a Uniform($0,\theta$) distribution. (Here, $\theta = 1$.) The observed statistic values $X_{(n)}$ are shown as red crosses; in each case, the values lie outside the derived intervals." width="50%" />
<p class="caption">
Figure 5.4: Ten 90 percent confidence intervals, generated using data from 10 separate datasets of size <span class="math inline">\(n = 10\)</span> sampled according to a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution. (Here, <span class="math inline">\(\theta = 1\)</span>.) The observed statistic values <span class="math inline">\(X_{(n)}\)</span> are shown as red crosses; in each case, the values lie outside the derived intervals.
</p>
</div>
<hr />
</div>
<div id="confidence-coefficient-for-a-uniform-based-interval-estimator" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Confidence Coefficient for a Uniform-Based Interval Estimator<a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the example above, we show that the interval estimate with confidence
coefficient <span class="math inline">\(1-\alpha\)</span> for the uniform upper bound <span class="math inline">\(\theta\)</span> has
the form <span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span>. Can we also define an appropriate
interval estimator if, for instance, it has the form
<span class="math inline">\([X_{(n)} + a,X_{(n)} + b]\)</span>? The short answer is no…because the
confidence coefficient will be <em>zero</em>! To see why, let’s expand out and
solve:
<span class="math display">\[\begin{align*}
P(X_{(n)} + a \leq \theta \leq X_{(n)} + b) &amp;= P(\theta - b \leq X_{(n)} \leq \theta - a)\\
&amp;= P(X_{(n)} \leq \theta - a) - P(X_{(n)} \leq \theta - b)\\
&amp;= F_{(n)}(\theta-a) - F_{(n)}(\theta-b)\\
&amp;= \left(\frac{\theta-a}{\theta}\right)^2 - \left(\frac{\theta-b}{\theta}\right)^2\\
&amp;= \left(1-\frac{a}{\theta}\right)^2 - \left(1-\frac{b}{\theta}\right)^2 \,.
\end{align*}\]</span>
The confidence coefficient is the infimum (or minimum value)
that this expression can take on. For an interval of the form
<span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span>, <span class="math inline">\(\theta\)</span> does not appear, and thus the infimum
is a constant. Here, however,
<span class="math display">\[
\lim_{\theta \to \infty} P(X_{(n)} + a \leq \theta \leq X_{(n)} + b) = 0 \,,
\]</span>
and thus
the confidence coefficient is (i.e., the proportion of computed
intervals that overlap the true value <span class="math inline">\(\theta\)</span>) goes to zero.
Thus an interval estimator of the form <span class="math inline">\([aX_{(n)},bX_{(n)}]\)</span> is a better
one than one of the form <span class="math inline">\([X_{(n)} + a,X_{(n)} + b]\)</span>.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-3" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Hypothesis Testing<a href="the-uniform-distribution.html#hypothesis-testing-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>One aspect of hypothesis testing that we reiterate here is that the hypotheses
are always to be established, along with the level of the test,
<em>before</em> we collect data. This should be obvious<span class="math inline">\(-\)</span>looking at the data
prior to establishing hypotheses and test levels can (and often will) lead
to biased outcomes<span class="math inline">\(-\)</span>so why are we reiterating this now? We are making this point
because when we perform tests involving domain-specifying parameters, there
is a quirk that we (sometimes) observe when we attempt to
establish rejection-region boundaries.</p>
<p>For instance, let’s say that we sample <span class="math inline">\(n\)</span> iid data according to
a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution, and that we have previously decided
to use these data to test the hypothesis
<span class="math inline">\(H_o : \theta = \theta_o\)</span> versus the hypothesis <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>
at level <span class="math inline">\(\alpha\)</span>. We know that the sufficient statistic upon which we
will build our test is <span class="math inline">\(X_{(n)}\)</span>.</p>
<p>Given this, what can we say about the rejection regions?</p>
<p>The first thing that we can say that we will reject the null
if <span class="math inline">\(X_{(n)} &gt; \theta_o\)</span>; if the null is correct, it is <em>impossible</em> to
sample a datum with a larger value. This rejection region is what we
call the “trivial” rejection region…it is trivial in the sense that we need
not use any mathematics to establish a boundary.</p>
<p>So…how then do we establish the “other” rejection-region boundary, the
one with value <span class="math inline">\(&lt; \theta_o\)</span>? Do we derive that boundary using the
value <span class="math inline">\(\alpha/2\)</span> in our calculations, as we have in the past when working
through the definition of two-tail tests? Or would we use <span class="math inline">\(\alpha\)</span> instead? The answer lies in the idea of test power. If the null hypothesis is correct, then
the probability of rejecting it is, by definition, <span class="math inline">\(\alpha\)</span>.
(That’s the power of the test when <span class="math inline">\(\theta = \theta_o\)</span>.)
However, if the
null is correct, then it is also impossible to sample values of <span class="math inline">\(X_{(n)}\)</span> that
are larger than <span class="math inline">\(\theta\)</span>…we can only sample values that are <span class="math inline">\(\leq \theta\)</span>,
and thus we can only reject the null when <span class="math inline">\(X_{(n)} \ll \theta\)</span>.
Hence we would use <span class="math inline">\(\alpha\)</span> in our derivation, not <span class="math inline">\(\alpha/2\)</span>, to ensure
that the power of the test when <span class="math inline">\(\theta = \theta_o\)</span> is indeed <span class="math inline">\(\alpha\)</span>.</p>
<p>Another way of stating this result is that <em>there really is no such thing as a two-tail test for a domain-specifying parameter</em>: if the parameter specifies an upper bound, then the only test we can define is a lower-tail test, and if the parameter specifies a lower bound, then the only test we can define is an upper-tail test.</p>
<p>However, there is yet one more quirk to discuss, and that is how we would
compute the power of the test for values <span class="math inline">\(\theta &gt; \theta_o\)</span>. In this
situation, there are <em>two</em> possible ways to reject the null:</p>
<ol style="list-style-type: decimal">
<li>by observing <span class="math inline">\(x_{(n),\rm obs} &lt; x_{\rm RR}\)</span> (traditional rejection), and</li>
<li>by observing <span class="math inline">\(\theta_o &lt; x_{(n),\rm obs} \leq \theta\)</span> (trivial rejection).</li>
</ol>
<p>We work out the mathematics of the power calculation in an example below.</p>
<hr />
<div id="hypothesis-test-for-the-uniform-distribution-upper-bound" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Hypothesis Test for the Uniform Distribution Upper Bound<a href="the-uniform-distribution.html#hypothesis-test-for-the-uniform-distribution-upper-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We sample <span class="math inline">\(n\)</span> iid data according to a Uniform(<span class="math inline">\(0,\theta\)</span>) distribution,
and we use these data to test
<span class="math display">\[
H_o: \theta = \theta_o ~~\mbox{versus}~~ H_a: \theta &lt; \theta_o \,.
\]</span>
The sufficient statistic is the maximum datum <span class="math inline">\(X_{(n)}\)</span>; as stated previously,
we can appeal to reason to state that the expected value of this quantity
must increase as <span class="math inline">\(\theta\)</span> increases, so we know that we are using the
“yes” line of the hypothesis test reference tables and that <span class="math inline">\(q = \alpha\)</span>:
<span class="math display">\[\begin{align*}
F_Y(y \vert \theta) - q = 0 ~~~ \Rightarrow ~~~ F_{(n)}(x_{\rm RR} \vert \theta_o) - \alpha = 0 ~~~ \Rightarrow ~~~ \left(\frac{x_{\rm RR}}{\theta_o}\right)^n - \alpha = 0 \,.
\end{align*}\]</span>
Solving for <span class="math inline">\(x_{\rm RR}\)</span>, we find that
<span class="math display">\[
x_{\rm RR} = \theta_o \alpha^{1/n} \,.
\]</span></p>
</blockquote>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is straightforward to compute: according to the reference tables, it is
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta_o) ~~~ \Rightarrow ~~~ F_{(n)}(x_{(n),\rm obs} \vert \theta_o) ~~~ \Rightarrow ~~~ \left(\frac{x_{(n),\rm obs}}{\theta_o}\right)^n \,.
\end{align*}\]</span></p>
</blockquote>
<blockquote>
<p>However, as first mentioned above, the test power is less straightforward
to compute.</p>
</blockquote>
<blockquote>
<p>If <span class="math inline">\(\theta &lt; \theta_o\)</span>, then we can utilize the reference tables
directly to write that the power is
<span class="math display">\[\begin{align*}
F_Y(y_{\rm RR} \vert \theta) ~~~ \Rightarrow ~~~ F_{(n)}(x_{\rm RR} \vert \theta) ~~~ \Rightarrow ~~~ \left(\frac{x_{\rm RR}}{\theta}\right)^n \,.
\end{align*}\]</span>
The power rises from <span class="math inline">\(\alpha\)</span> to 1 as <span class="math inline">\(\theta\)</span> decreases from <span class="math inline">\(\theta_o\)</span>
to <span class="math inline">\(x_{\rm RR}\)</span>, and for smaller values of <span class="math inline">\(\theta\)</span> it is 1 by definition
(as it becomes impossible to sample a datum outside of the rejection
region). (See the left-hand side of Figure <a href="the-uniform-distribution.html#fig:uhyppow">5.5</a> and
the left panel of Figure <a href="the-uniform-distribution.html#fig:uhyppow2">5.6</a>.)</p>
</blockquote>
<blockquote>
<p>If <span class="math inline">\(\theta &gt; \theta_o\)</span>, then we would reject the null hypothesis if
<span class="math inline">\(x_{(n),\rm obs} &lt; x_{\rm RR}\)</span> <em>or</em> <span class="math inline">\(x_{(n),\rm obs} &gt; \theta_o\)</span>. Thus
<span class="math display">\[\begin{align*}
P(\mbox{reject}~\mbox{null} \vert \theta) &amp;= P(X_{(n)} &lt; x_{\rm RR} \cup X_{(n)} &gt; \theta_o \vert \theta) \\
&amp;= F_{(n)}(x_{\rm RR} \vert \theta) + [1 - F_{(n)}(\theta_o \vert \theta)] \\
&amp;= \left(\frac{x_\alpha}{\theta}\right)^n + \left( 1 - \left(\frac{\theta_o}{\theta}\right)^n \right) \,.
\end{align*}\]</span>
(See the right-hand side of Figure <a href="the-uniform-distribution.html#fig:uhyppow">5.5</a> and
the right panel of Figure <a href="the-uniform-distribution.html#fig:uhyppow2">5.6</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uhyppow"></span>
<img src="_main_files/figure-html/uhyppow-1.png" alt="\label{fig:uhyppow}The power curve for the test of $H_o : \theta = \theta_o = 1$ versus $H_a : \theta \neq \theta_o$, assuming $n = 10$. The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, $\alpha = 0.05$, at $\theta = 1$." width="50%" />
<p class="caption">
Figure 5.5: The power curve for the test of <span class="math inline">\(H_o : \theta = \theta_o = 1\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>, assuming <span class="math inline">\(n = 10\)</span>. The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, <span class="math inline">\(\alpha = 0.05\)</span>, at <span class="math inline">\(\theta = 1\)</span>.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uhyppow2"></span>
<img src="_main_files/figure-html/uhyppow2-1.png" alt="\label{fig:uhyppow2}The rejection regions that inform the power curve calculation shown in Figure \@ref(fig:uhyppow). To the left: if $\theta &lt; \theta_o$, then we reject the null hypothesis if $x_{(n),\rm obs} &lt; x_{\rm RR}$. The power is thus the area under the curve shown in red. To the right: if $\theta &gt; \theta_o$, then we reject the null hypothesis if $x_{(n),\rm obs} &lt; x_{\rm RR}$ or if $x_{(n),\rm obs} &gt; \theta_o = 1$. The power is thus the sum of the two areas under the curve shown in red." width="50%" /><img src="_main_files/figure-html/uhyppow2-2.png" alt="\label{fig:uhyppow2}The rejection regions that inform the power curve calculation shown in Figure \@ref(fig:uhyppow). To the left: if $\theta &lt; \theta_o$, then we reject the null hypothesis if $x_{(n),\rm obs} &lt; x_{\rm RR}$. The power is thus the area under the curve shown in red. To the right: if $\theta &gt; \theta_o$, then we reject the null hypothesis if $x_{(n),\rm obs} &lt; x_{\rm RR}$ or if $x_{(n),\rm obs} &gt; \theta_o = 1$. The power is thus the sum of the two areas under the curve shown in red." width="50%" />
<p class="caption">
Figure 5.6: The rejection regions that inform the power curve calculation shown in Figure <a href="the-uniform-distribution.html#fig:uhyppow">5.5</a>. To the left: if <span class="math inline">\(\theta &lt; \theta_o\)</span>, then we reject the null hypothesis if <span class="math inline">\(x_{(n),\rm obs} &lt; x_{\rm RR}\)</span>. The power is thus the area under the curve shown in red. To the right: if <span class="math inline">\(\theta &gt; \theta_o\)</span>, then we reject the null hypothesis if <span class="math inline">\(x_{(n),\rm obs} &lt; x_{\rm RR}\)</span> or if <span class="math inline">\(x_{(n),\rm obs} &gt; \theta_o = 1\)</span>. The power is thus the sum of the two areas under the curve shown in red.
</p>
</div>
<hr />
</div>
</div>
<div id="exercises-4" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Exercises<a href="the-uniform-distribution.html#exercises-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> denote independent and identically distributed uniform random variables on the interval <span class="math inline">\([0, 3\theta]\)</span>. Derive the method-of-moments estimator for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Compute <span class="math inline">\(P(X &gt; a+b \vert X &gt; b)\)</span> for a Uniform(0,1) distribution. (Assume <span class="math inline">\(0 &lt; b &lt; a+b &lt; 1\)</span>.) Does the Uniform(0,1) distribution exhibit the property of memorylessness? Why or why not?</p></li>
<li><p>A woman goes to her local bus stop every day at a random time between noon and 1 PM, for five days total. If a bus doesn’t appear to pick her up within 10 minutes, she immediately hops into a waiting Uber and is driven off. On every day, there is only one bus that will arrive between noon and 1:10 PM, and it will arrive at a random time <span class="math inline">\(X\)</span> minutes after noon. <span class="math inline">\(X\)</span> is sampled from the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \left\{ \begin{array}{ll} 1/70 &amp; x \in [0,70] \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{eqnarray*}\]</span>
(a) On any one day, what is the probability that the woman catches the bus? (b) Over the five days, what is the probability that the woman catches the bus one or more times? (You may leave fractions raised to powers in your final answer, such as <span class="math inline">\((3/4)^3\)</span> or <span class="math inline">\((7/15)^5\)</span>, if they are part of your answer.) (Also, if you are in doubt about your answer to (a), just use the variable <span class="math inline">\(p\)</span> in place of your answer for (a) in part (b).)</p></li>
<li><p>You sample a datum <span class="math inline">\(X\)</span> from a Uniform(0,1) distribution. What is <span class="math inline">\(P(X \leq 2u \vert X \geq u])\)</span>, where <span class="math inline">\(0 \leq u \leq 0.5\)</span>?</p></li>
<li><p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be two iid random variables sampled from a Uniform(0,1) distribution. What is <span class="math inline">\(P(X_1 &lt; 2X_2 \vert X_2 &lt; 1/2)\)</span>? (Note: <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are <em>not</em> order statistics, so do not treat them as such!)</p></li>
<li><p>Assume that we have sampled <span class="math inline">\(n\)</span> iid random variables from a Uniform(<span class="math inline">\(\theta,0\)</span>) distribution, where <span class="math inline">\(\theta &lt; 0\)</span>. (a) What is a sufficient statistic for <span class="math inline">\(\theta\)</span>? (b) What is the cdf for this sufficient statistic? Be careful when deriving <span class="math inline">\(F_X(x)\)</span>: the pdf <span class="math inline">\(f_X(x)\)</span> is <span class="math inline">\(1/(0-\theta) = -1/\theta\)</span> and not <span class="math inline">\(1/\theta\)</span>. Also, take care when writing down the integral bounds. (c) We wish to test <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>. Recall that hypothesis tests are written down (in theory!) before the collection of data. Given that factoid, write down the trivial part of the test rejection region, i.e., the part of the overall rejection region that one can write down without having to work with the sufficient statistic cdf. (d) To derive the other part of the rejection region, do we set the cdf for the sufficient statistic to <span class="math inline">\(1-\alpha\)</span> or <span class="math inline">\(1-\alpha/2\)</span>? Choose one and write it in the answer box. Recall that the power of the hypothesis test when <span class="math inline">\(\theta = \theta_o\)</span> is exactly <span class="math inline">\(\alpha\)</span>. (e) Given your answers for (b) and (d), derive the boundary of the other part of the rejection region (the non-trivial part).</p></li>
<li><p>You sample <span class="math inline">\(n\)</span> iid data from the following (unnamed) distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{2}{\theta^2} x  ~~~ x \in [0,\theta] \,.
\end{eqnarray*}\]</span>
The cdf for this distribution is <span class="math inline">\(F_X(x) = (x/\theta)^2\)</span>. (a) What is the MLE for <span class="math inline">\(\theta\)</span>? (b) What is <span class="math inline">\(E[X_{(n)}]\)</span>? (c) What is the MVUE for <span class="math inline">\(\theta\)</span>?</p></li>
<li><p>Let’s assume we have sampled <span class="math inline">\(n\)</span> iid data from the following distribution:
<span class="math display">\[\begin{align*}
f_X(x) = e^{-(x-\theta)} ~~~ x \in [\theta,\infty)
\end{align*}\]</span>
where <span class="math inline">\(\theta &gt; 0\)</span>. The cdf for this distribution, for <span class="math inline">\(x \geq \theta\)</span>, is
<span class="math display">\[\begin{align*}
F_X(x) = 1-e^{-(x-\theta)} \,.
\end{align*}\]</span>
(a) Identify a sufficient statistic for <span class="math inline">\(\theta\)</span>. (b) Identify the maximum likelihood estimator for <span class="math inline">\(\theta\)</span>. No work need be shown. (c) Determine the sampling distribution (specifically, the pdf, and not the cdf) for the sufficient statistic identified in part (a). (d) Determine the minimum variance unbiased estimator for <span class="math inline">\(\theta\)</span>. You will want to utilize a variable subsitution here. Recall that
<span class="math display">\[\begin{align*}
\Gamma(a+1) = a! = \int_0^\infty u^a e^{-u} du \,,
\end{align*}\]</span>
assuming that <span class="math inline">\(a\)</span> is a non-negative integer. (Also recall that 0! = 1.)</p></li>
<li><p>We sample two iid data, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, from a Uniform(0,1) distribution. (a) What is <span class="math inline">\(P(X_1 &gt; 1/2 \vert X_2 &lt; 1/2)\)</span>? (b) What is <span class="math inline">\(P(X_1 &gt; 1/2 \vert X_1 &lt; 3/4)\)</span>? (c) What is <span class="math inline">\(P(X_1 &lt; 3X_2)\)</span>? (Hint: draw this out in a 1 <span class="math inline">\(\times\)</span> 1 box. Do the same for (d).) (d) What is <span class="math inline">\(P(X_2 &lt; X_1 \vert X_2 &lt; 1/2)\)</span>?</p></li>
<li><p>Let’s assume that we have sampled <span class="math inline">\(n\)</span> iid data from a particular distribution with domain <span class="math inline">\([\theta,\infty)\)</span>, and let the cdf of the sampling distribution of the appropriate statistic <span class="math inline">\(Y\)</span> to use to construct confidence intervals and perform hypothesis tests be
<span class="math display">\[\begin{align*}
F_Y(y) = 1 - e^{-n(y-\theta)} \,.
\end{align*}\]</span>
Assume the observed statistic value is <span class="math inline">\(y_{\rm obs}\)</span>, and that <span class="math inline">\(E[Y] = \theta + 1/n\)</span>. (Note that it is not necessary to know what <span class="math inline">\(Y\)</span> actually represents to answer the questions below.) (a) Determine a <span class="math inline">\(100(1-\alpha)\)</span>-percent lower bound on <span class="math inline">\(\theta\)</span>. (b) Assume we wish to test <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>. Derive the rejection-region boundary (or boundaries) <span class="math inline">\(y_{\rm RR}\)</span> in terms of <span class="math inline">\(\theta_o\)</span>, the Type I error <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(n\)</span>.</p></li>
<li><p>We are given the following probability mass function (which is an example of a discrete uniform distribution):
<span class="math display">\[\begin{align*}
p_X(x) = 1/2
\end{align*}\]</span>
for <span class="math inline">\(x \in \{1,2\}\)</span>. (a) Compute the moment-generating function for this distribution. (b) Using the mgf, compute the variance of <span class="math inline">\(X\)</span>. Do not compute <span class="math inline">\(V[X]\)</span> by any other method!</p></li>
<li><p>We sample a random variable <span class="math inline">\(X\)</span> from a Uniform(0,1) distribution. Let <span class="math inline">\(U = \sqrt{X}\)</span>. (a) Write down the pdf for <span class="math inline">\(U\)</span>. (b) Identify the distribution of <span class="math inline">\(U\)</span>, if it is known. Include the name and any parameter values.</p></li>
<li><p>Let <span class="math inline">\(X \sim\)</span> Uniform(0,1), i.e.,
<span class="math display">\[\begin{eqnarray*}
f_X(x) = 1
\end{eqnarray*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. Now, let <span class="math inline">\(U = X^2\)</span>. (a) We will derive <span class="math inline">\(f_U(u)\)</span> in part (c). For now: what is the domain of this probability density function? (b) What is the functional form of <span class="math inline">\(F_U(u)\)</span> within the domain of <span class="math inline">\(f_U(u)\)</span>? (c) What is the functional form of <span class="math inline">\(f_U(u)\)</span> within its domain? (d) What is <span class="math inline">\(E[U]\)</span>?</p></li>
<li><p>In an experiment, we sample <em>one datum</em> according to the cumulative distribution function
<span class="math display">\[\begin{eqnarray*}
F_X(x) = c\left( 1 - e^{-x/\theta} \right) \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \in [0,-\theta\log(1-1/c)]\)</span> (and where <span class="math inline">\(\theta\)</span> is a known positive constant). One may picture this as a distribution that is truncated at the coordinate <span class="math inline">\(x_c = -\theta\log(1-1/c)\)</span>, with the unknown parameter <span class="math inline">\(c &gt; 0\)</span> having a value such that the integral of <span class="math inline">\(f_X(x)\)</span> over the whole domain is 1. (a) What is the functional form of <span class="math inline">\(f_X(x)\)</span>? (b) We wish to test the hypothesis <span class="math inline">\(H_o : c = c_o\)</span> versus <span class="math inline">\(H_a : c &gt; c_o\)</span>. What is the rejection region boundary <span class="math inline">\(x_{\rm RR}\)</span> for this test? The answer should be in terms of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(c_o\)</span>, and the level of the test <span class="math inline">\(\alpha\)</span>. (c) What is the power of the test given an arbitrary value <span class="math inline">\(c\)</span>, where <span class="math inline">\(c_o &lt; c &lt; c&#39;\)</span> and where <span class="math inline">\(c&#39;\)</span> is the value of <span class="math inline">\(c\)</span> where the power achieves the value 1? Leave the answer in terms of <span class="math inline">\(c\)</span> and <span class="math inline">\(x_{\rm RR}\)</span>. (d) Over what range of observed values of <span class="math inline">\(X\)</span> would we trivially reject the null hypothesis, since if the null is correct, it would be impossible to observe values in this range? (e) If instead of sampling one datum, we sample <span class="math inline">\(n\)</span> iid data, what would be a sufficient statistic for <span class="math inline">\(c\)</span>?</p></li>
<li><p>In an experiment, we sample <em>one datum</em> <span class="math inline">\(X\)</span> according to the distribution
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{2}{\theta}\left(1-\frac{x}{\theta}\right) ~~~~~~ x \in [0,\theta] \,.
\end{eqnarray*}\]</span>
(a) What is the maximum likelihood estimate for <span class="math inline">\(\theta\)</span>? (b) What is the expected value <span class="math inline">\(E[X]\)</span>? (c) What is the bias of the MLE? (d) What is the minimum variance unbiased estimator for <span class="math inline">\(\theta\)</span>? (e) One can compute that <span class="math inline">\(V[X] = \theta^2/18\)</span>. It turns out that the mean-squared errors for the MLE and the MVUE are the same, as a function of <span class="math inline">\(\theta\)</span>. Write down an expression for the MSE.</p></li>
</ol>

<div style="page-break-after: always;"></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-poisson-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

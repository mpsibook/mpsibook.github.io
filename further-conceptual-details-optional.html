<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Further Conceptual Details (Optional) | Modern Probability and Statistical Inference</title>
  <meta name="description" content="7 Further Conceptual Details (Optional) | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Further Conceptual Details (Optional) | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Further Conceptual Details (Optional) | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multivariate-distributions.html"/>
<link rel="next" href="appendix-a-table-of-symbols.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.3</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.6</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="further-conceptual-details-optional" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">7</span> Further Conceptual Details (Optional)<a href="further-conceptual-details-optional.html#further-conceptual-details-optional" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="types-of-convergence" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Types of Convergence<a href="further-conceptual-details-optional.html#types-of-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two primary types of convergence which interest us in this
book: <em>convergence in probability</em> and <em>convergence in distribution</em>.</p>
<p>Let <span class="math inline">\(X_1,X_2,\ldots\)</span> be a sequence of random variables, and let <span class="math inline">\(Y\)</span> be
some other random variable. (We note that the
concept of a sequence can be initially confusing for students: here,
<span class="math inline">\(X_n\)</span> is a statistic formed from a set of data with sample size <span class="math inline">\(n\)</span>. A
classic example of a sequence is <span class="math inline">\(\bar{X}_1,\bar{X}_2,...\)</span>; the first element
is the mean for a sample of size 1 (the datum itself!), the second element
is the mean that we compute after we independently sample a second datum from
the same underlying distribution to go along with
the first, etc.) In addition, let <span class="math inline">\(F_{X_n}\)</span> denote the cumulative distribution
function for <span class="math inline">\(X_n\)</span> and <span class="math inline">\(F_Y\)</span> denote the cdf for <span class="math inline">\(Y\)</span>.
We say that</p>
<ul>
<li><span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(Y\)</span> if for every <span class="math inline">\(\epsilon &gt; 0\)</span>,
<span class="math display">\[
P(\vert X_n - Y \vert &gt; \epsilon) \rightarrow 0
\]</span>
as <span class="math inline">\(n \rightarrow \infty\)</span>; i.e., <span class="math inline">\(X_n \stackrel{p}{\rightarrow} Y\)</span>.</li>
<li><span class="math inline">\(X_n\)</span> converges in distribution to <span class="math inline">\(Y\)</span> if
<span class="math display">\[
\lim_{n \rightarrow \infty} F_{X_n}(u) = F_Y(u)
\]</span>
for all values of <span class="math inline">\(u\)</span> for which <span class="math inline">\(F_Y(u)\)</span> is continuous;
i.e., <span class="math inline">\(X_n \stackrel{d}{\rightarrow} Y\)</span>.</li>
</ul>
<p>An example of convergence in probability is the <em>weak law of large
numbers</em>, which states that <span class="math inline">\(\bar{X}_n \stackrel{p}{\rightarrow} \mu\)</span>.
(This is a weak statement because it does not invoke almost sure
convergence, a concept beyond the scope of this book.) What is weak
about this statement? The statement
<span class="math inline">\(P(\vert \bar{X}_n - Y \vert &gt; \epsilon) \rightarrow 0\)</span>, where <span class="math inline">\(Y\)</span> has
mean <span class="math inline">\(\mu\)</span>, is not as restrictive a statement as others that one can make;
for any chosen value of <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(\vert \bar{X}_n - Y \vert\)</span> can
stray so as to be greater than <span class="math inline">\(\epsilon\)</span> an infinite amount of times as
<span class="math inline">\(n \rightarrow \infty\)</span>. In other words, <span class="math inline">\(\bar{X}\)</span> asymptotically
approaches <span class="math inline">\(\mu\)</span>, but is still allowed to substantially deviate from
that value for any finite <span class="math inline">\(n\)</span>.</p>
</div>
<div id="the-central-limit-theorem-1" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> The Central Limit Theorem<a href="further-conceptual-details-optional.html#the-central-limit-theorem-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We introduce the Central Limit Theorem in Chapter 2.
It states that if
we have <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> with mean
<span class="math inline">\(E[X_i] = \mu\)</span> and finite variance <span class="math inline">\(V[X_i] = \sigma^2 &lt; \infty\)</span>, and
if <span class="math inline">\(n\)</span> is sufficiently large,
then <span class="math inline">\(\bar{X}\)</span> converges in distribution to a standard normal random variable:
<span class="math display">\[
\lim_{n \rightarrow \infty} P\left(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \leq z \right) = \Phi(z) \,.
\]</span>
We can prove this using moment-generating functions.</p>
<p>Let <span class="math inline">\(X_i \stackrel{iid}{\sim} P(\mu,\sigma^2)\)</span>, where <span class="math inline">\(P\)</span> is some distribution with finite
variance, and let
<span class="math display">\[
Y = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} = \frac{1}{\sqrt{n}} \left(\frac{n\bar{X} - n\mu}{\sigma}\right) = \frac{1}{\sqrt{n}} \left(\frac{\sum_{i=1}^n X_i - n\mu}{\sigma}\right) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \frac{X_i - \mu}{\sigma} = \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i \,.
\]</span>
Since the <span class="math inline">\(Z_i\)</span>s are standardized random variables, by definition <span class="math inline">\(E[Z_i] = 0\)</span>
and <span class="math inline">\(V[Z_i]\)</span> = 1.</p>
<!--
---
-->
<p>Heres where things break down if we do not know <span class="math inline">\(\sigma\)</span>, but instead
plug <span class="math inline">\(S\)</span> in in CLT-related problems: if we use <span class="math inline">\(S\)</span>, then
<span class="math inline">\(V[X_i] \neq 1\)</span>. However, a theoretical result known as <em>Slutskys
theorem</em> saves us here. As we are determining here and below,
<span class="math display">\[
\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \stackrel{d}{\rightarrow} Z \sim \mathcal{N}(0,1) \,,
\]</span>
and hence
<span class="math display">\[
\sqrt{n}(\bar{X} - \mu) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(0,\sigma^2) \,.
\]</span>
Furthermore, <span class="math inline">\(S^2 \stackrel{p}{\rightarrow} \sigma^2\)</span> (by the weak law of large numbers),
or equivalently <span class="math inline">\(S \stackrel{p}{\rightarrow} \sigma\)</span> (by the continuous mapping theorem).
Given these pieces of information, Slutskys theorem tells us
that <span class="math inline">\(\sqrt{n}(\bar{X} - \mu)/S \stackrel{d}{\rightarrow} Y/\sigma\)</span>, a normally
distributed random variable with mean 0 and variance 1.
Hence eventually the CLT is valid, even when we plug in <span class="math inline">\(S\)</span>!</p>
<!--
---
-->
<p>To determine the distribution of <span class="math inline">\(Y\)</span>, we use the method of moment-generating
functions:
<span class="math display">\[
m_Y(t) = m_{Z_1}\left(\frac{t}{\sqrt{n}}\right) \cdots m_{Z_n}\left(\frac{t}{\sqrt{n}}\right) = \left[m_{Z_i}\left(\frac{t}{\sqrt{n}}\right)\right]^n \,.
\]</span>
Wait, one might say. We dont know the mgf for the quantity <span class="math inline">\(Z_i\)</span>,
so how can this possibly be helpful? It is because we can work with the
expected value and variance to get at the final result. First,
<span class="math display">\[\begin{align*}
m_{Z_i}\left(\frac{t}{\sqrt{n}}\right) &amp;= m_{Z_i}\left.\left(\frac{t}{\sqrt{n}}\right)\right|_{t=0} + m_{Z_i}&#39;\left.\left(\frac{t}{\sqrt{n}}\right)\right|_{t=0} \frac{t}{\sqrt{n}} + m_{Z_i}&#39;&#39;\left.\left(\frac{t}{\sqrt{n}}\right)\right|_{t=0} \frac{t^2}{2n} + \cdots \\
&amp;\approx m_{Z_i}(0) + E[Z_i] \frac{t}{\sqrt{n}} + E[Z_i^2] \frac{t^2}{2n} \\
&amp;= E[\exp(0Z_i)] + 0 + (V[Z_i] + (E[Z_i])^2) \frac{t^2}{2n} \\
&amp;= 1 + V[Z_i] \frac{t^2}{2n} = 1 + \frac{t^2}{2n} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[
m_Y(t) \approx \left[ 1 + \frac{t^2}{2n} \right]^n = \left[ 1 + \frac{t^2/2}{n} \right]^n \,,
\]</span>
and, as <span class="math inline">\(n \rightarrow \infty\)</span>,
<span class="math display">\[
\lim_{n \rightarrow \infty} \left[ 1 + \frac{t^2/2}{n} \right]^n = \exp\left(\frac{t^2}{2}\right) \,.
\]</span>
This is the moment-generating function for a standard normalhence <span class="math inline">\(Y\)</span>
converges in distribution to a standard normal random variable
and <span class="math inline">\(\bar{X}\)</span> converges in distribution to normal random variable with
mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>.</p>
</div>
<div id="asymptotic-normality-of-maximum-likelihood-estimates" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Asymptotic Normality of Maximum Likelihood Estimates<a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When discussing point estimation in Chapter 2, we declared that as
<span class="math inline">\(n \rightarrow \infty\)</span>, the maximum likelihood estimate for a parameter
<span class="math inline">\(\theta\)</span> converges in distribution to a normal random variable:
<span class="math display">\[
\sqrt{n}(\hat{\theta}_{MLE}-\theta) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(0,\frac{1}{I(\theta)}\right) ~\mbox{or}~ (\hat{\theta}_{MLE}-\theta) \stackrel{d}{\rightarrow} Y&#39; \sim \mathcal{N}\left(0,\frac{1}{nI(\theta)}\right) \,.
\]</span>
Here, we sketch out why this is true. As we will see, this result rests upon
fundamental concepts covered earlier in this book and chapter: the
weak law of large numbers (aka, convergence in probability), and the
central limit theorem.</p>
<p>We start with the log-likelihood function <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span>.
(However, to simplify the notation in what follows, we will drop the
<span class="math inline">\(\mathbf{x}\)</span> for now.) By definition,
the MLE for <span class="math inline">\(\theta\)</span> is that value for which <span class="math inline">\(\ell&#39;(\theta) =
\ell&#39;(\hat{\theta}_{MLE}) = 0\)</span>. The Taylor expansion of <span class="math inline">\(\ell&#39;(\theta)\)</span>
around <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is
<span class="math display">\[
\ell&#39;(\theta) \approx \ell&#39;(\hat{\theta}_{MLE}) + (\theta - \hat{\theta}_{MLE})\ell&#39;&#39;(\theta) + \cdots = (\theta - \hat{\theta}_{MLE})\ell&#39;&#39;(\theta) + \cdots \,.
\]</span>
We rearrange terms (and introduce the factor <span class="math inline">\(\sqrt{n}\)</span>):
<span class="math display">\[\begin{align*}
(\hat{\theta}_{MLE}-\theta)\ell&#39;&#39;(\theta) &amp;\approx -\ell&#39;(\theta) \\
\sqrt{n} (\hat{\theta}_{MLE}-\theta)\ell&#39;&#39;(\theta) &amp;\approx -\sqrt{n}\ell&#39;(\theta) \\
\sqrt{n} (\hat{\theta}_{MLE}-\theta) &amp;\approx \frac{-\sqrt{n}\ell&#39;(\theta)}{\ell&#39;&#39;(\theta)} = \frac{\frac{1}{\sqrt{n}}\ell&#39;(\theta)}{-\frac{1}{n}\ell&#39;&#39;(\theta)} \,.
\end{align*}\]</span>
(Note that we reversed <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\hat{\theta}_{MLE}\)</span> in the parentheses!)</p>
<p>Lets look at the numerator and the denominator separately.</p>
<p>For the numerator:
<span class="math display">\[\begin{align*}
\frac{1}{\sqrt{n}}\ell&#39;(\theta) &amp;= \sqrt{n}\left(\frac{\ell&#39;(\theta \vert \mathbf{x})}{n} - 0\right) \\
&amp;= \sqrt{n}\left(\frac{\ell&#39;(\theta \vert \mathbf{x})}{n} - E[\ell&#39;(\theta \vert x_i)] \right) \\
&amp;= \sqrt{n}\left(\frac{\sum_{i=1}^n \ell&#39;(\theta \vert x_i)}{n} - E[\ell&#39;(\theta \vert x_i)] \right) \,.
\end{align*}\]</span>
Here we utilize the results that, e.g., <span class="math inline">\(\ell&#39;(\theta \vert \mathbf{x})\)</span> equals
the sum of the <span class="math inline">\(\ell&#39;\)</span>s for each datum, and that the average value of
the slope for the likelihood function is zero. What we have written
is equivalent in form to
<span class="math display">\[
\sqrt{n} (\bar{X} - \mu)
\]</span>
and given the CLT,
we know that as <span class="math inline">\(n \rightarrow \infty\)</span>, this quantity converges in
distribution to normal random variable <span class="math inline">\(Y \sim \mathcal{N}(0,\sigma^2)\)</span>. Thus
<span class="math display">\[
\frac{1}{\sqrt{n}}\ell&#39;(\theta) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(0,V[\ell&#39;(\theta \vert x_i)]\right) \,,
\]</span>
and, since <span class="math inline">\(V[\ell&#39;(\theta \vert x_i)] = E[(\ell&#39;(\theta \vert x_i))^2] - (E[\ell&#39;(\theta \vert x_i)])^2 = E[(\ell&#39;(\theta \vert x_i))^2] = I(\theta)\)</span>,
<span class="math display">\[
\frac{1}{\sqrt{n}}\ell&#39;(\theta) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(0,I(\theta)\right) \,.
\]</span></p>
<p>For the denominator, we can utilize the weak law of large numbers:
<span class="math display">\[
-\frac{1}{n} \ell&#39;&#39;(\theta \vert \mathbf{x}) = -\frac{1}{n} \left( \sum_{i=1}^n \ell&#39;&#39;(\theta \vert x_i) \right) \stackrel{p}{\rightarrow} -E[\ell&#39;&#39;(\theta \vert x_i)] = I(\theta) \,.
\]</span></p>
<p>At last, we can determine the variance of the ratio:
<span class="math display">\[
\lim_{n \rightarrow \infty} V[\sqrt{n}(\hat{\theta}_{MLE}-\theta)] = V\left[ \frac{\frac{1}{\sqrt{n}}\ell&#39;(\theta)}{I(\theta)} \right] = \frac{1}{I(\theta)^2} V\left[ \frac{1}{\sqrt{n}}\ell&#39;(\theta) \right] = \frac{1}{I(\theta)^2} I(\theta) = \frac{1}{I(\theta)} \,.
\]</span>
We combine this information with the finding above that
the numerator in the ratio is, asymptotically, a normally distributed
random variable with mean 0 and variance <span class="math inline">\(I(\theta)\)</span> to write
<span class="math display">\[
\sqrt{n}(\hat{\theta}_{MLE}-\theta) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(0,\frac{1}{I(\theta)}\right) \,.
\]</span></p>
</div>
<div id="point-estimation-relative-efficiency" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Point Estimation: (Relative) Efficiency<a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The efficiency of an unbiased estimator of a parameter <span class="math inline">\(\theta\)</span> is
<span class="math display">\[
e(\hat{\theta}) = \frac{1/I(\theta)}{V[\hat{\theta}]} \,,
\]</span>
i.e., the ratio of the minimum possible variance for an unbiased
estimator of <span class="math inline">\(\theta\)</span> (the Cramer-Rao lower bound)
to the variance for the estimator in question. If an unbiased
estimator attains <span class="math inline">\(e(\hat{\theta})\)</span> for all possible values of
<span class="math inline">\(\theta\)</span>, the estimator is <em>efficient</em>. (It is also the MVUE!)</p>
<p>The relative efficiency is a metric that we can use to compare two
unbiased estimators:
<span class="math display">\[
e(\hat{\theta}_1,\hat{\theta}_2) = \frac{V[\hat{\theta}_2]}{V[\hat{\theta}_1]} \,.
\]</span>
If <span class="math inline">\(e(\hat{\theta}_1,\hat{\theta}_2) &gt; 1\)</span>, we would opt to use
<span class="math inline">\(\hat{\theta}_1\)</span>; otherwise, if <span class="math inline">\(e(\hat{\theta}_1,\hat{\theta}_2) &lt; 1\)</span>,
we would opt to use <span class="math inline">\(\hat{\theta}_2\)</span>. (We could use either if the
relative efficiency is exactly one.)</p>
<p>As an example, let <span class="math inline">\(X_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, and let <span class="math inline">\(\hat{\mu}_1 =
\bar{X}\)</span> and <span class="math inline">\(\hat{\mu}_2 = (X_1+X_2)/2\)</span>. What is the relative efficiency of
these two estimators?</p>
<p>We know the general result that <span class="math inline">\(V[\hat{\mu}_1] =
V[\bar{X}] = \sigma^2/n\)</span>, and we know
that <span class="math inline">\(\hat{\mu}_2\)</span> is simply the sample mean of the first two data, so
<span class="math inline">\(V[\hat{\mu}_2] = V[(X_1+X_2)/2] = \sigma^2/2\)</span>. The relative efficiency
is thus
<span class="math display">\[
e(\hat{\mu}_1,\hat{\mu}_2) = \frac{V[\hat{\mu}_2]}{V[\hat{\mu}_1]} = \frac{\sigma^2/2}{\sigma^2/n} = \frac{n}{2} \,.
\]</span>
This value is <span class="math inline">\(&gt; 1\)</span> for all <span class="math inline">\(n &gt; 2\)</span>, and it is never less than 1, so
we would choose to use <span class="math inline">\(\hat{\mu}_1 = \bar{X}\)</span> as our estimator of the
population mean.</p>
</div>
<div id="sufficient-statistics" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Sufficient Statistics<a href="further-conceptual-details-optional.html#sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let us assume that we are given <span class="math inline">\(n\)</span> independent and identically distributed
(iid) data sampled from some distribution <span class="math inline">\(P\)</span> whose shape is (without loss
of generality) governed by a single parameter <span class="math inline">\(\theta\)</span>:
<span class="math display">\[
\{X_1,\ldots,X_n\} \overset{iid}{\sim} P(\theta)
\]</span>
A sufficient statistic <span class="math inline">\(U\)</span> is a function of <span class="math inline">\(\mathbf{X}\)</span> that
encapsulates all the information needed to estimate <span class="math inline">\(\theta\)</span>, i.e.,
if <span class="math inline">\(U\)</span> is sufficient,
no other computed statistic would provide any additional information
that would help us estimate <span class="math inline">\(\theta\)</span>. Sufficient statistics are not
unique: if <span class="math inline">\(U\)</span> is a sufficient statistic, then every one-to-one function
<span class="math inline">\(f(U)\)</span> is as well, so long as <span class="math inline">\(f(U)\)</span> does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>In Chapter 3 we show how one finds a sufficient statistic by factorizing
the likelihood function, and uses it to define the minimum variance
unbiased estimator (the MVUE). In that chapter, we sweep a number of
things under the metaphorical rug, such as the fact that to be used to
determine the MVUE, a sufficient statistic has to be both minimal and
complete. We elaborate on those points below. However, we start by
providing an alternate means by which to determine a sufficient statistic.</p>
<div id="a-formal-definition-of-sufficiency" class="section level3 hasAnchor" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> A Formal Definition of Sufficiency<a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A statistic <span class="math inline">\(U(\mathbf{X})\)</span> is a sufficient statistic for
the parameter <span class="math inline">\(\theta\)</span> if the conditional distribution of <span class="math inline">\(\mathbf{X}\)</span>
given <span class="math inline">\(U(\mathbf{X})\)</span> does not depend on <span class="math inline">\(\theta\)</span>, i.e., if
<span class="math inline">\(P(X_1=x_1,\ldots,X_n=x_n \vert U(\mathbf{X}) = k)\)</span> does not
depend on <span class="math inline">\(\theta\)</span>.</p>
<p>As an example, let <span class="math inline">\(\{X_1,\ldots,X_n\} \overset{iid}{\sim}\)</span> Bernoulli(<span class="math inline">\(p\)</span>),
and let us propose <span class="math inline">\(U(\mathbf{X}) = \sum_{i=1}^n X_i\)</span> as a sufficient
ststistic. To determine if it is, we need to determine if
<span class="math display">\[
P(X_1=x_1,\ldots,X_n=x_n \vert U(\mathbf{X}=k) = \frac{P(X_1=x_1,\ldots,X_n=x_n,U(\mathbf{X})=k}{P(U(\mathbf{X}=k)}
\]</span>
does not depend on <span class="math inline">\(p\)</span>. The first thing to note is that for the
numerator to be non-zero, <span class="math inline">\(k = x_1+\cdots+x_n = \sum_{i=1}^n x_i\)</span>, and thus
<span class="math inline">\(U(\mathbf{Y})=k\)</span> is, from an information standpoint, <em>completely redundant</em>
with respect to <span class="math inline">\(X_1=x_1,\ldots,X_n=x_n\)</span>so we can ignore it:
<span class="math display">\[
P(X_1=x_1,\ldots,X_n=x_n \vert U(\mathbf{X}=k) = \frac{P(X_1=x_1,\ldots,X_n=x_n)}{P(U(\mathbf{X}=k)} \,.
\]</span>
The next thing to note is that because the data are iid, we can rewrite
the numerator as a product of probabilities:
<span class="math display">\[
P(X_1=x_1,\ldots,X_n=x_n) = P(X_1=x_1) \cdots P(X_n=x_n) = \prod_{i=1}^n P(X_i=x_i) \,,
\]</span>
which means, in the context of Bernoulli random variables, that the numerator is
<span class="math display">\[
\prod_{i=1}^n P(X_i=x_i) = p^k (1-p)^{n-k} \,.
\]</span>
(Why <span class="math inline">\(p^k\)</span>, etc.? We are given that <span class="math inline">\(U(\mathbf{X}) = k\)</span>, i.e., that there
are <span class="math inline">\(k\)</span> observed successes (and <span class="math inline">\(n-k\)</span> observed failures) in the sample.</p>
<p>That takes care of the numerator. Now for the denominator:
<span class="math display">\[
P(U(\mathbf{X} = k) = \binom{n}{k} p^k (1-p)^{n-k} \,,
\]</span>
because the sum of <span class="math inline">\(n\)</span> Bernoulli-distributed random variables is
binomially distributed (as you can show yourself using the method of
moment-generating functions).</p>
<p>Thus
<span class="math display">\[
P(X_1=x_1,\ldots,X_n=x_n \vert U(\mathbf{X}=k) = \frac{p^k(1-p)^{n-k}}{\binom{n}{k} p^k(1-p)^{n-k}} = \frac{1}{\binom{n}{k}} \,,
\]</span>
which does not depend on the parameter <span class="math inline">\(p\)</span>. Thus
<span class="math inline">\(U(\mathbf{X}) = \sum_{i=1}^n X_i\)</span> is indeed a sufficient statistic for <span class="math inline">\(p\)</span>.</p>
<p>(The foregoing clearly illustrates why factorization is a preferred means
by which to determine sufficient statistics!)</p>
</div>
<div id="minimal-sufficiency" class="section level3 hasAnchor" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Minimal Sufficiency<a href="further-conceptual-details-optional.html#minimal-sufficiency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A question that naturally arises when dealing with sufficient statistics is:
if we have many sufficient statistics, which one is the best one? It would
be the one that reduces the data the most. In a given context,
<span class="math inline">\(U(\mathbf{X})\)</span> is minimal
sufficient if, given any another sufficient statistic <span class="math inline">\(T(\mathbf{X})\)</span>,
<span class="math inline">\(U(\mathbf{X}) = f(T(\mathbf{X}))\)</span>.</p>
<p>To generate a minimal sufficient statistic, one can make use of the
Lehmann-Scheffe theorem. If we have two iid samples of data of
the same size, <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> and <span class="math inline">\(\{Y_1,\ldots,Y_n\}\)</span>, then,
if the ratio of likelihoods
<span class="math display">\[
\frac{\mathcal{L}(x_1,\ldots,x_n \vert \theta)}{\mathcal{L}(y_1,\ldots,y_n \vert \theta)}
\]</span>
is constant as a function of <span class="math inline">\(\theta\)</span> if and only if <span class="math inline">\(g(x_1,\ldots,x_n) =
g(y_1,\ldots,y_n)\)</span> for a function <span class="math inline">\(g(\cdot)\)</span>, then <span class="math inline">\(g(X_1,\ldots,X_n)\)</span> is
a minimal sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
<p>As an example, let <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> and
<span class="math inline">\(\{Y_1,\ldots,Y_n\} \overset{iid}{\sim}\)</span> Poisson(<span class="math inline">\(\lambda\)</span>).
The ratio of likelihoods is
<span class="math display">\[
\frac{\mathcal{L}(x_1,\ldots,x_n \vert \theta)}{\mathcal{L}(y_1,\ldots,y_n \vert \theta)} = \frac{\prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda}}{\prod_{i=1}^n \frac{\lambda^{y_i}}{y_i!} e^{-\lambda}} = \frac{\prod_{i=1}^n \frac{1}{x_i!}}{\prod_{i=1}^n \frac{1}{y_i!}} \frac{\lambda^{\sum_{i=1}^n x_i}}{\lambda^{\sum_{i=1}^n y_i}} = \frac{\prod_{i=1}^n \frac{1}{x_i!}}{\prod_{i=1}^n \frac{1}{y_i!}} \lambda^{\left(\sum_{i=1}^n x_i - \sum_{i=1}^n y_i\right)} \,.
\]</span>
This is only constant as a function of <span class="math inline">\(\lambda\)</span> if we can get rid of
<span class="math inline">\(\lambda\)</span>, by setting its exponent to 0i.e., by setting
<span class="math inline">\(\sum_{i=1}^n x_i = \sum_{i=1}^n y_i\)</span>. Thus <span class="math inline">\(g(\mathbf{x}) =
\sum_{i=1}^n x_i\)</span> is a minimal sufficient statistic for <span class="math inline">\(\lambda\)</span>.</p>
<p>Note that in the context of the present course, any and all sufficient
statistics that we define via, e.g., factorization are minimal sufficient.</p>
<p>One might ask what is an example of a sufficient statistic that is not
minimally sufficient? The answer is simple but may not initially be intuitive,
in that we naturally think of a statistic as being a single number (the
output from applying a given function to a full dataset). However, e.g.,
a full dataset is a statistic, and it is a sufficient statistic:
there is sufficient information in a full dataset so as
to compute the likelihood of a population parameter
<span class="math inline">\(\theta\)</span>. However, a full dataset is
not minimally sufficient! If <span class="math inline">\(T(\mathbf{X}) = \bar{X}\)</span>,
and <span class="math inline">\(U(\mathbf{X}) = \{X_1,\ldots,X_n\}\)</span>, then we
cannot define a function <span class="math inline">\(f\)</span> such that <span class="math inline">\(\{X_1,\ldots,X_n\} = f(\bar{X})\)</span>.</p>
</div>
<div id="completeness" class="section level3 hasAnchor" number="7.5.3">
<h3><span class="header-section-number">7.5.3</span> Completeness<a href="further-conceptual-details-optional.html#completeness" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The concept of the completeness of a statistic is a general concept, i.e., it
is not limited to sufficient statistics. One can think of it as the idea that
each value of <span class="math inline">\(\theta\)</span> in a distribution maps to a distinct pmf or pdf.
We bring up completeness here because
the concept appears in the context of determining an MVUE: the sufficient
statistic that we find via, e.g., factorization technically needs to be both
minimal and complete.</p>
<p>Let <span class="math inline">\(f_U(u \vert \theta)\)</span> be the family of pmfs or pdfs for the
statistic <span class="math inline">\(U(\mathbf{X})\)</span>. <span class="math inline">\(U\)</span> is dubbed a <em>complete</em> statistic if
<span class="math inline">\(E[g(U)] = 0\)</span> for all <span class="math inline">\(\theta\)</span> implies <span class="math inline">\(P(g(U) = 0) = 1\)</span> for all <span class="math inline">\(\theta\)</span>.</p>
<p>As an example, let <span class="math inline">\(U \sim\)</span> Binomial(<span class="math inline">\(n,p\)</span>), with <span class="math inline">\(0 &lt; p &lt; 1\)</span>, and let
<span class="math inline">\(g(\cdot)\)</span> be a function such that <span class="math inline">\(E[g(U)] = 0\)</span>. Then
<span class="math display">\[
0 = E[g(U)] = \sum_{u=0}^n g(u) \binom{n}{u} p^u (1-p)^{n-u} = (1-p)^n \sum_{u=0}^n g(u) \binom{n}{u} \left( \frac{p}{1-p}\right)^u \,.
\]</span>
For any choice of <span class="math inline">\(u\)</span> and <span class="math inline">\(n\)</span>, <span class="math inline">\(\binom{n}{u} [p/(1-p)]^u &gt; 0\)</span>. So, in order
to have <span class="math inline">\(E[g(U)] = 0\)</span> for all <span class="math inline">\(p\)</span>, <span class="math inline">\(g(u) = 0\)</span> for all <span class="math inline">\(u\)</span>, i.e.,
<span class="math inline">\(P(g(U) = 0) = 1\)</span>. Therefore <span class="math inline">\(U\)</span> is a complete statistic.</p>
<p>Note that a complete statistic is also minimal sufficient, but the converse
is not necessarily true: a minimal sufficient statistic may not be complete.</p>
</div>
<div id="the-rao-blackwell-theorem" class="section level3 hasAnchor" number="7.5.4">
<h3><span class="header-section-number">7.5.4</span> The Rao-Blackwell Theorem<a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Rao-Blackwell theorem implies that an unbiased estimator
for <span class="math inline">\(\theta\)</span> with a small variance is, or can be made to be, a
function of a sufficient statistic:
if we have an unbiased estimator for <span class="math inline">\(\theta\)</span>, we might be able to improve
it using the prescription of the theorem. In the end, the theorem provides
a mathematically more challenging route to defining a minimum variance
unbiased estimator than what we show in the main text<span class="math inline">\(-\)</span>likelihood factorization
followed by the de-biasing of a sufficient statistic<span class="math inline">\(-\)</span>and hence we
relegate it here, to the chapter of optional material.</p>
<p>Let <span class="math inline">\(\hat\theta\)</span> be an unbiased estimator for <span class="math inline">\(\theta\)</span> such
that <span class="math inline">\(V[\hat\theta] &lt; \infty\)</span>. If <span class="math inline">\(U\)</span> is a sufficient
statistic for <span class="math inline">\(\theta\)</span>, define <span class="math inline">\(\hat\theta^* = E[\hat\theta \vert U]\)</span>.
Then, for all <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
E[\hat\theta^*] = \theta ~~~ \text{and} ~~~ V[\hat\theta^*] \leq V[\hat\theta] \,.
\]</span></p>
<p>Lets suppose we have sampled <span class="math inline">\(n\)</span> iid data from a Binomial distribution with
number of trials <span class="math inline">\(k\)</span> and proportion <span class="math inline">\(p\)</span>.
We propose an estimator for <span class="math inline">\(p\)</span>: <span class="math inline">\(X_1/k\)</span>.
First, is <span class="math inline">\(\hat{p} = X_1/k\)</span> unbiased? We have that
<span class="math display">\[
E[\hat{p}-p] = E\left[\frac{X_1}{k}\right] - p = \frac{1}{k}E[X_1] - p = \frac{1}{k}kp - p = 0 \,.
\]</span>
Soyes.
Second, is <span class="math inline">\(V[\hat{p}] &lt; \infty\)</span>? <span class="math inline">\(V[X_1/k] = V[X_1]/k^2 = p(1-p)/k\)</span>so,
also yes. We are thus free to use the Rao-Blackwell theorem to improve upon
<span class="math inline">\(\hat{p} = X_1/k\)</span>:
<span class="math display">\[
\hat\theta^* = E[\hat\theta \vert U] = E\left[\frac{X_1}{k} \left| ~ U = \sum_{i=1}^n X_i \right. \right] \,.
\]</span>
Since we are given the sum of the data, and since the data are iid, we
would expect, on average, that <span class="math inline">\(X_1\)</span> has the value <span class="math inline">\(U/n\)</span>, and thus that
<span class="math inline">\(X_1/k\)</span> has the value <span class="math inline">\(U/(nk)\)</span>. Thus
<span class="math display">\[
\hat\theta^* = \frac{\bar{X}}{k}
\]</span>
is the MVUE for <span class="math inline">\(p\)</span>.</p>
</div>
<div id="exponential-family-of-distributions" class="section level3 hasAnchor" number="7.5.5">
<h3><span class="header-section-number">7.5.5</span> Exponential Family of Distributions<a href="further-conceptual-details-optional.html#exponential-family-of-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The exponential family is a set of distributions whose probability mass
or density functions can be expressed as either
<span class="math display">\[
f_X(x \vert \theta) = a(\theta) b(x) \exp[ c(\theta) t(x) ]
\]</span>
or
<span class="math display">\[
f_X(x \vert \theta) = b(x) \exp[ c(\theta) t(x) - d(\theta)] \,.
\]</span>
These two forms are equivalent, with <span class="math inline">\(a(\theta) = \exp[-d(\theta)]\)</span>.
Note that the domain of <span class="math inline">\(f_X(x \vert \theta)\)</span> cannot depend on <span class="math inline">\(\theta\)</span>,
meaning that distributions like the uniform and Pareto cannot be members
of the exponential family even if their density functions could be expressed
in the form above.</p>
<p>There are many concepts related to the exponential family that are beyond
the scope of this book. The one factoid that we will note here is that
sufficient statistics for the parameter <span class="math inline">\(\theta\)</span> can be read directly off
a distributions exponential family form. Assuming we have <span class="math inline">\(n\)</span> iid random
variables, we can write the likelihood and factorize it, and isolate the
sufficient statistic:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) &amp;= \prod_{i=1}^n f_X(x_i \vert \theta) \\
&amp;= \underbrace{[a(\theta)]^n \exp\left[ \sum_{i=1}^n c(\theta) t(x_i) \right]}_{g(\theta,u)} \underbrace{\left( \prod_{i=1}^n b(x_i) \right)}_{h(\mathbf{x})} \,.
\end{align*}\]</span>
The sufficient statistic is thus
<span class="math display">\[
U = \sum_{i=1}^n t(X_i) \,.
\]</span></p>
<p>As an example, what is the sufficient statistic for the mean of an exponential
distribution? The exponential pdf is <span class="math inline">\((1/\beta)\exp(-x/\beta)\)</span> for <span class="math inline">\(\beta &gt; 0\)</span>
and <span class="math inline">\(x \geq 0\)</span>. If we use the first exponential family form above, we can
read off that
<span class="math display">\[
a(\beta) = \frac{1}{\beta} ~~ b(x) = 1 ~~ c(\beta) = -\frac{1}{\beta} ~~ t(x) = x \,.
\]</span>
Thus, assuming we have an iid sample of size <span class="math inline">\(n\)</span>, the sufficient statistic
would be <span class="math inline">\(U = \sum_{i=1}^n t(X_i) = \sum_{i=1}^n X_i\)</span>.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multivariate-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-a-table-of-symbols.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter Exercises: Solutions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bibliography.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficiency-and-the-exponential-family-of-distributions"><i class="fa fa-check"></i><b>3.6.1</b> Sufficiency and the Exponential Family of Distributions</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.2</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.3</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.5</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.6</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
<li class="chapter" data-level="4.6.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation-and-numerical-optimization"><i class="fa fa-check"></i><b>4.6.5</b> Confidence Interval Estimation via Simulation and Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#exercises-1"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#exercises-2"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-exercises-solutions" class="section level1 unnumbered hasAnchor">
<h1>Chapter Exercises: Solutions<a href="chapter-exercises-solutions.html#chapter-exercises-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="chapter-3" class="section level2 unnumbered hasAnchor">
<h2>Chapter 3<a href="chapter-exercises-solutions.html#chapter-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>The median of a <span class="math inline">\(\mathcal{N}(2,4)\)</span> is <span class="math inline">\(\tilde{\mu} = 2\)</span>,
thus <span class="math inline">\(P(X_i &gt; 2) = 0.5\)</span> by inspection. Now, let <span class="math inline">\(Y\)</span> be the number of
values <span class="math inline">\(&gt; 2\)</span>. Then
<span class="math display">\[\begin{align*}
P(Y=m) = \binom{n}{m} \left(\frac12\right)^m \left(\frac12\right)^{n-m} = \binom{n}{m} \left(\frac12\right)^n \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>(a) We fix the number of successes to <span class="math inline">\(s=1\)</span>, with the
random variable being the number of tickets we need to buy <em>before</em> buying
the one that allows us to win the raffle. There are two correct answers here:
<span class="math inline">\(F\)</span> is sampled from
the geometric distribution, with <span class="math inline">\(p=0.4\)</span>, or from the negative binomial
distribution, with <span class="math inline">\(s=1\)</span> and <span class="math inline">\(p=0.4\)</span>.</p>
<p>(b) We have that <span class="math inline">\(W = 2-F\)</span>, so
<span class="math display">\[\begin{align*}
P(W &gt; 0) = P(2-F &gt; 0) &amp;= P(F &lt; 2) = p_X(0) + p_X(1) \\
&amp;= (0.4)^1(1-0.4)^0 + (0.4)^1(1-0.4)^1 = 0.4 + 0.24 = 0.64 \,.
\end{align*}\]</span></p>
<p>(c) Let <span class="math inline">\(X\)</span> be the number of winning tickets. Then
<span class="math display">\[\begin{align*}
P(X = 1 \vert X \geq 1) &amp;= \frac{P(X = 1 \cap X \geq 1)}{P(X \geq 1)} = \frac{P(X=1)}{1-P(X=0)} \\
&amp;= \frac{\binom{2}{1}(0.4)^1(0.6)^1}{1 - \binom{2}{0}(0.4)^0(0.6)^2} = \frac{2 \cdot 0.24}{1 - 0.36} = 0.48/0.64 = 0.75 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>The first step is to write down the probability mass function for the
number of insured drivers, given that the number of insured drivers is odd:
<span class="math display">\[\begin{align*}
P(X=1 \vert X=1 \cup X=3) &amp;= \frac{p_X(1)}{p_X(1)+p_X(3)} = \frac{\binom{3}{1}(1/2)^1(1/2)^2}{\binom{3}{1}(1/2)^1(1/2)^2 + \binom{3}{3}(1/2)^3(1/2)^0} \\
&amp;= \frac{3}{3 + 1} = \frac34 \\
P(X=3 \vert X=1 \cup X=3) &amp;= 1 - P(X=1 \vert X=1 \cup X=3) = \frac14 \,.
\end{align*}\]</span>
So the expected value is
<span class="math display">\[\begin{align*}
E[X \vert X=1 \cup X=3] = \sum_{1,3} x p_X(x \vert x=1 \cup x=3) = 1 \cdot 3/4 + 3 \cdot 1/4 = 3/2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) We are conducting a negative binomial experiment with <span class="math inline">\(s = 2\)</span>.
The random variable is the number of failures…here, <span class="math inline">\(X = 2\)</span>. So:
<span class="math display">\[\begin{align*}
p_X(2) = \binom{2+2-1}{2} \left(\frac{1}{2}\right)^2 \left(\frac{1}{2}\right)^2 = \frac{3!}{1!2!} \frac{1}{16} = \frac{3}{16} \,.
\end{align*}\]</span></p>
<p>(b) The sum of the data is negatively binomially distributed for
<span class="math inline">\(s=4\)</span> successes and probability of success <span class="math inline">\(p=1/2\)</span>.
The overall number of failures here is <span class="math inline">\(X = 1\)</span>. So
<span class="math display">\[\begin{align*}
p_X(1) = \binom{1+4-1}{1} \left(\frac{1}{2}\right)^4 \left(\frac{1}{2}\right)^1 = \frac{4!}{1!3!} \frac{1}{32} = \frac{4}{32} = \frac18 \,.
\end{align*}\]</span></p>
<p>(c) This is a negative binomial experiment, so
<span class="math inline">\(E[X] = s(1-p)/p\)</span>, which decreases as <span class="math inline">\(p\)</span> increases. Referring to the
confidence interval reference table, we see that <span class="math inline">\(q = 1-\alpha = 0.9\)</span>.</p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>(a) We have that <span class="math inline">\(f_X(x) = 3x^2\)</span> and thus that <span class="math inline">\(F_X(x) = x^3\)</span>. Plugging
these into the formula for <span class="math inline">\(f_{(j)}(x)\)</span> (along with <span class="math inline">\(j=1\)</span>) yields
<span class="math display">\[\begin{align*}
f_{(1)}(x) = \frac{4!}{3!0!}(1-x^3)^3 3 x^2 = 12(1-x^3)^3x^2 \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[X_{(4)}] = \int_0^1 x 12x^{11} = \int_0^1 12x^{12} = \left.\frac{12}{13}x^{13}\right|_0^1 = \frac{12}{13} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) The cdf is <span class="math inline">\(F_X(x) =  \int_0^x y dy = \frac{x^2}{2}\)</span>. Thus
<span class="math display">\[\begin{align*}
f_{(3)} = 3x\left[F_X(x)\right]^{2} = 3x\frac{x^4}{4} = \frac{3}{4} x^5
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,\sqrt{2}]\)</span>.</p>
<p>(b) The variance is <span class="math inline">\(V[X_{(3)}] = E[X_{(3)}^2] - (E[X_{(3)}])^2\)</span>, where
<span class="math display">\[\begin{align*}
E[X_{(3)}] &amp;= \int_0^{\sqrt{2}} x \left(\frac{3}{4} x^5\right)dx =\frac{3}{4} \frac{x^7}{7}\bigg|_0^{\sqrt{2}} = \frac{3\cdot 2^3}{28} \sqrt{2} = \frac{6}{7}\sqrt{2} = 1.212 \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X_{(3)}^2] &amp;= \int_0^{\sqrt{2}} x^2 \left(\frac{3}{4} x^5\right)dx =\frac{3}{4} \frac{x^8}{8}\bigg|_0^{\sqrt{2}} = \frac{3\cdot2^4}{32} = \frac{3}{2} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
V[X_{(3)}] = \frac{3}{2} - \frac{36\cdot 2}{49} = \frac{147 - 144}{98} = \frac{3}{98} = 0.031 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>We have that <span class="math inline">\(f_X(x) = 1\)</span>, <span class="math inline">\(F_X(x) = x\)</span>, and <span class="math inline">\(j = \frac{n+1}{2} = 2\)</span>, so
<span class="math display">\[\begin{align*}
f_{(2)}(x) = \frac{3!}{1!1!}x^1(1-x)^1(1) = 6x(1-x)
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. Therefore
<span class="math display">\[\begin{align*}
P\left(\frac{1}{3} \leq X_{(2)} \leq \frac{2}{3}\right) &amp;= \int_{1/3}^{2/3} 6x(1-x) dx = 6\left[ \frac{x^2}{2}\bigg|_{1/3}^{2/3} - \frac{x^3}{3}\bigg|_{1/3}^{2/3}\right]\\
&amp;= 6\left[ \frac{1}{2} \left( \frac{4}{9} - \frac{1}{9}\right) - \frac{1}{3} \left( \frac{8}{27} - \frac{1}{27}\right)  \right]\\
&amp;= 6\left[ \frac{3}{18} - \frac{7}{81}  \right] = 6\left[ \frac{27}{162} - \frac{14}{162}  \right] = \frac{13}{27} = 0.481 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>We have that <span class="math inline">\(f_X(x) = e^{-x}\)</span> for <span class="math inline">\(x \geq 0\)</span> and thus that
<span class="math inline">\(F_X(x) = 1-e^{-x}\)</span> over the same domain. Thus
<span class="math display">\[\begin{align*}
f_{(2)}(x) &amp;= \frac{3!}{1!1!}(1 - e^{-x})^1\left[1 - (1- e^{-x}) \right]^1 e^{-x} = 6(1 - e^{-x})e^{-x}e^{-x} \\
&amp;= 6(1 - e^{-x})e^{-2x} = 6e^{-2x}  - 6e^{-3x} \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X_{(2)}] &amp;= \underbrace{\int_0^{\infty} 6xe^{-2x} dx}_{\text{by } y=2x, \, dy/2 = dx} - \underbrace{\int_0^{\infty} 6xe^{-3x} dx}_{\text{by } y=3x, \, dy/3 = dx} \\
&amp;= \int_0^{\infty}  \frac{3}{2}y e^{-y} dy - \int_0^{\infty}  \frac{2}{3}y e^{-y} dy\\
&amp;= \frac{3}{2} \Gamma(2) - \frac{2}{3}\Gamma(2) = \frac{3}{2} - \frac{2}{3} =\frac{5}{6} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) A probability density function is the derivative of its associated
cumulative distribution function, so
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx} x^3 = 3x^2 \,.
\end{align*}\]</span></p>
<p>(b) The maximum order statistic has pdf
<span class="math display">\[\begin{align*}
f_{(n)}(x) &amp;= n f_X(x) [F_X(x)]^{n-1} \\
&amp;= n (3x^2) [x^3]^{n-1} = 3n x^2 x^{3n-3} = 3n x^{3n-1} \,.
\end{align*}\]</span></p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
F_{(n)}(x) = [F_X(x)]^n ~~ \Rightarrow ~~ F_{(n)}(x) = x^{3n} \,.
\end{align*}\]</span>
We can also show this via integration:
<span class="math display">\[\begin{align*}
F_{(n)}(x) = \int_0^x f_{(n)}(y) dy = \int_0^x 3n y^{3n-1} dy = \left. y^{3n}\right|_0^x = x^{3n} \,.
\end{align*}\]</span></p>
<p>(d) The expected value is
<span class="math display">\[\begin{align*}
E[X_{(n)}] = \int_0^1 x f_{(n)}(x) dx = \int_0^1 3n x^{3n} dx = \left. \frac{3n}{3n+1} x^{3n+1} \right|_0^1 = \frac{3n}{3n+1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) The cdf within the domain is
<span class="math display">\[\begin{align*}
F_X(x) = \int_0^x \frac12 y dy = \left. \frac14 y^2 \right|_0^x = \frac{x^2}{4} \,.
\end{align*}\]</span></p>
<p>(b) We plug <span class="math inline">\(n\)</span>, <span class="math inline">\(f_X(x)\)</span>, and <span class="math inline">\(F_X(x)\)</span> into the order statistic
pdf equation (where <span class="math inline">\(j = n = 2\)</span>):
<span class="math display">\[\begin{align*}
f_{(2)}(x) = 2 f_X(x) \left[ F_X(x) \right]^{2-1} \left[ 1 - F_X(x) \right]^{2-2} = 2 \left( \frac12 \right) x \left( \frac14 \right) x^2 = \frac14 x^3 \,.
\end{align*}\]</span></p>
<p>(c) The expected value is
<span class="math display">\[\begin{align*}
E[X_{(2)}] = \int_0^2 x f_{(2)}(x) dx = \int_0^2 \frac14 x^4 dx = \frac{1}{20} \left. x^5 \right|_0^2 = \frac{32}{20} = \frac85 = 1.6 \,.
\end{align*}\]</span></p>
<p>(d) They cannot be independent: given the value of one, the other
has to be either smaller (<span class="math inline">\(X_{(1)}\)</span>) or larger (<span class="math inline">\(X_{(2)}\)</span>).</p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>Let <span class="math inline">\(X_1, \ldots, X_n\)</span> denote the samples from the Bernoulli distribution.
The log-likelihood is
<span class="math display">\[\begin{align*}
\ell(X_1, \ldots, X_n | p) = \sum_{i = 1}^n [X_i \log(p) + (1 - X_i) \log (1-p)] \,.
\end{align*}\]</span>
The first two derivatives are
<span class="math display">\[\begin{align*}
\frac{d}{dp} \ell(X_1, \ldots, X_n | p) &amp;= \sum_{i = 1}^n\bigg[ \frac{X_i}{p}  - \frac{(1 - X_i)}{1-p}\bigg] \\
\frac{d^2}{dp^2} \ell(X_1,\ldots, X_n | p ) &amp;= \sum_{i = 1}^n \bigg[-\frac{X_i}{p^2}- \frac{(1- X_i)}{(1-p)^2}\bigg] \,.
\end{align*}\]</span>
The Fisher information is thus
<span class="math display">\[\begin{align*}
I_n(p) = E\bigg[-\frac{d^2}{dp^2} \ell(X_1,\ldots, X_n | p )\bigg] &amp;= \sum_{i=1}^n \bigg[\frac{E[X_i]}{p^2} + \frac{E[(1- X_i)]}{(1-p)^2}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{p}{p^2} + \frac{1-p}{(1-p)^2}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{1}{p} + \frac{1}{1-p}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{1-p}{p(1-p)} + \frac{p}{p(1-p)}\bigg] \\
&amp;= \frac{n}{p(1-p)} \,,
\end{align*}\]</span>
and the asymptotic distribution of the MLE is <span class="math inline">\(N(p,\frac{p(1-p)}{n})\)</span>.</p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) The log-likelihood and its derivative are
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) &amp;= \log (1-p) \sum_{i=1}^n (x_i - 1)  + n \log p\\
\ell&#39;(p \vert \mathbf{x}) &amp;= -\frac{\sum_{i=1}^n x_i - n}{1 - p} + \frac{n}{p} \,.
\end{align*}\]</span>
Setting the derivative to zero, we find that
<span class="math display">\[\begin{align*}
\frac{\sum_{i=1}^n x_i - n}{1 - p} &amp;= \frac{n}{p} \\
\Rightarrow ~~~ \left(\sum_{i=1}^n x_i - n\right) p &amp;= n (1 - p) \\
\Rightarrow ~~~ p \,\sum_{i=1}^n x_i &amp;= n \\
\Rightarrow ~~~ \hat{p} &amp;= \frac{1}{\bar{X}} \,.
\end{align*}\]</span>
Using the invariance property of the MLE, we find that
<span class="math inline">\(\widehat{1/p}_{MLE} = \bar{X}\)</span>.</p>
<p>(b) The variance of this estimator is
<span class="math display">\[\begin{align*}
V\left[\widehat{1/p}_{MLE}\right] = V \left[ \frac{\sum_{i=1}^n X_i}{n} \right] = \frac{V[X]}{n} = \frac{1 - p}{np^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>The likelihood for <span class="math inline">\(p\)</span> is
<span class="math display">\[\begin{align*}               
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n p_X(x_i \vert p) = \prod_{i=1}^n -\frac{1}{\log(1-p)} \frac{p^{x_i}}{x_i} = \underbrace{- \prod_{i=1}^n \frac{1}{x_i}}_{h(\mathbf{x})} \times \underbrace{\frac{1}{[\log(1-p)]^n} p^{\sum_{i=1}^n x_i}}_{g(p,\mathbf{x})} \,.
\end{align*}\]</span><br />
Given the expression for <span class="math inline">\(g(\cdot)\)</span>, we can see that a sufficient statistic
for <span class="math inline">\(p\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}           
\mathcal{L}(a,b \vert \mathbf{x}) &amp;= \prod_{i=1}^n a b x_i^{a-1} (1-x_i^a)^{b-1}\\
&amp;= a^n b^n \left(\prod_{i=1}^n  x_i\right)^{a-1} \left(\prod_{i=1}^n (1-x_i^a)\right)^{b-1}
\end{align*}\]</span>
At first glance, it seems that we can take
<span class="math display">\[\begin{align*}
\mathbf{Y} = \left\{ \prod_{i=1}^n x_i, \prod_{i=1}^n (1-x_i^a) \right\}
\end{align*}\]</span>
as the joint sufficient statistics. However, note that the parameter <span class="math inline">\(a\)</span>
occurs in the second statistic. Because this second statistic
includes a parameter value, it cannot be a sufficient statistic…and
thus we conclude that we cannot identify joint sufficient statistics for <span class="math inline">\(a\)</span>
and <span class="math inline">\(b\)</span>.</p>
<p>(b) With <span class="math inline">\(a=1\)</span> the density function becomes <span class="math inline">\(f_X(x) = b \cdot (1-x)^{b-1}\)</span>,
with resulting likelihood
<span class="math display">\[\begin{align*}
\mathcal{L}(b \vert \mathbf{x}) &amp;= \prod_{i=1}^n  b \cdot (1-x_i)^{b-1}\\
&amp;=  b^n \left(\prod_{i=1}^n (1-x_i)\right)^{b-1}\\
&amp;=  h(\mathbf{x}) g(b,\mathbf{x}) \,.
\end{align*}\]</span>
Hence, a sufficient statistic for <span class="math inline">\(b\)</span> is <span class="math inline">\(Y = \prod_{i=1}^n (1-X_i)\)</span>.</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \beta) &amp;= \prod_{i=1}^n \frac{x_i}{\beta^2} \exp\left(-\frac{x}{\beta}\right) \\
&amp;= \underbrace{\prod_{i=1}^n x_i}_{h(\mathbf{x})} \times \underbrace{\frac{1}{\beta^{2n}} \exp\left(-\frac{1}{\beta}\sum_{i=1}^n x_i\right)}_{g(\beta,\mathbf{x})} \,.
\end{align*}\]</span>
We can examine <span class="math inline">\(g(\cdot)\)</span> and immediately identify that a sufficient statistic
for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[Y] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n 2\beta = 2n\beta \,.
\end{align*}\]</span>
Hence
<span class="math display">\[\begin{align*}
E\left[\frac{Y}{2n}\right] = \beta
\end{align*}\]</span>
and <span class="math inline">\(\hat{\beta}_{MVUE} = Y/2n = \bar{X}/2\)</span>.</p>
<p>(c) Utilizing the general rule from 235:
<span class="math display">\[\begin{align*}
V[\hat{\beta}_{MVUE}] = V\left[\frac{\bar{X}}{2}\right] = \frac{V[\bar{X}]}{4} = \frac{V[X]}{4n} = \frac{2\beta^2}{4n} = \frac{\beta^2}{2n} \,.
\end{align*}\]</span></p>
<p>(d) The first step is to write down the log-likelihood for one datum:
<span class="math display">\[\begin{align*}
\ell(\beta \vert x) = \log f_X(x \vert \beta) = \log x - \frac{x}{\beta} - 2\log\beta \,.
\end{align*}\]</span>
We take the first two derivatives:
<span class="math display">\[\begin{align*}
\frac{d\ell}{d\beta} &amp;= \frac{x}{\beta^2} - \frac{2}{\beta} \\
\frac{d^2\ell}{d\beta^2} &amp;= -\frac{2x}{\beta^3} + \frac{2}{\beta^2} \,,
\end{align*}\]</span>
and then compute the expected value:
<span class="math display">\[\begin{align*}
I(\beta) = E\left[ \frac{2X}{\beta^3} - \frac{2}{\beta^2} \right] = \frac{2}{\beta^3}E[X] - \frac{2}{\beta^2} = \frac{2}{\beta^3}(2\beta) - \frac{2}{\beta^2} = \frac{2}{\beta^2} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(I_n(\beta) = (2n)/\beta^2\)</span> and the CRLB is <span class="math inline">\(1/I_n(\beta) = \beta^2/(2n)\)</span>.
The MVUE achieves the CRLB.</p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>(a) We can factorize the likelihood as follows:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \frac{1}{\theta} e^{x_i} e^{-e^{x_i}/\theta} = e^{\sum_{i=1}^n x_i} \theta^{-n} e^{-(\sum_{i=1}^n e^{x_i})/\theta} \,.
\end{align*}\]</span>
The first term does not contain <span class="math inline">\(\theta\)</span> and thus can be ignored. Thus
we identify <span class="math inline">\(Y = \sum_{i=1}^n e^{X_i}\)</span> as a sufficient statistic.</p>
<p>(b) We can determine <span class="math inline">\(E[Y]\)</span> by noticing that <span class="math inline">\(Y \sim\)</span> Gamma<span class="math inline">\((n,\theta)\)</span>, as stated in the question…so <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(E[Y/n] = \theta\)</span>. Thus the MVUE for <span class="math inline">\(\theta\)</span> is <span class="math inline">\((\sum_{i=1}^n e^{X_i})/n\)</span>.</p>
<p>(c) The MVUE will be a function of the sufficient statistic for <span class="math inline">\(\theta\)</span>, so let’s try <span class="math inline">\((\sum_{i=1}^n e^{X_i})^2\)</span>:
<span class="math display">\[\begin{align*}
E\left[\left(\sum_{i=1}^n e^{X_i}\right)^2\right] = V\left[\left(\sum_{i=1}^n e^{X_i}\right)\right] + E\left[\sum_{i=1}^n e^{X_i}\right]^2 = n \theta^2 + (n\theta)^2 = n(n+1)\theta^2 \,.
\end{align*}\]</span>
Therefore <span class="math inline">\((\sum_{i=1}^n e^{X_i})^2/(n(n+1))\)</span> is the MVUE for <span class="math inline">\(\theta^2\)</span>.</p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>(a) We factorize the likelihood:
<span class="math display">\[\begin{align*}
\mathcal{L}(a \vert \mathbf{x}) = \prod_{i=1}^n \sqrt{\frac{2}{\pi}} \frac{x_i^2}{a^3} e^{-x_i^2/(2a^2)} = \left[ \left(\frac{2}{\pi}\right)^{n/2} \left( \prod_{i=1}^n x_i^2 \right) \right] \times \left[ \frac{1}{a^{3n}} e^{-(\sum_{i=1}^n x_i^2)/(2a^2)} \right] = h(\mathbf{x}) \times g(a,\mathbf{x}) \,.
\end{align*}\]</span>
We can read off from the <span class="math inline">\(g(\cdot)\)</span> function term that
<span class="math inline">\(Y = \sum_{i=1}^n X_i^2\)</span>. (Including the minus sign, for instance,
is fine because a function of a
sufficient statistic is itself sufficent and we will get to the same
MVUE in the end.)</p>
<p>(b) We utilize the shortcut formula:
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = a^2 \frac{(3 \pi - 8)}{\pi} + (2a)^2 \frac{2}{\pi} = 3 a^2 + \frac{8 a^2}{\pi} - \frac{8 a^2}{\pi} = 3 a^2 \,.
\end{align*}\]</span></p>
<p>(c) We compute the expected value for <span class="math inline">\(Y\)</span>:
<span class="math display">\[\begin{align*}
E[Y] = E\left[\sum_{i=1}^n X_i^2\right] = \sum_{i=1}^n E[X_i^2] = n E[X^2] = 3 n a^2 \,.
\end{align*}\]</span>
Thus the expected value for <span class="math inline">\(Y/(3n)\)</span> is <span class="math inline">\(a^2\)</span>:
<span class="math display">\[\begin{align*}
\widehat{a^2}_{MVUE} = \frac{1}{3n} \sum_{i=1}^n X_i^2 \,.
\end{align*}\]</span></p>
<p>(d) There is no invariance principle for the MVUE. Maybe
the desired result holds and maybe it doesn’t, but we cannot simply
state that it does.</p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>We are constructing an upper-tail test where the test statistic is
trivially <span class="math inline">\(Y = X\)</span>. (So the NP Lemma does not <em>really</em> come into play here,
given the lack of choices for the test statistic.)
The expected value of <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{align*}
E[Y] = \int_0^2 y f_Y(y) dy = \int_0^2 \frac{\theta}{2^\theta} y^{\theta} dy = \left. \frac{\theta}{2^\theta} \frac{y^{\theta+1}}{\theta+1} \right|_0^2 = \frac{2\theta}{\theta+1} \,.
\end{align*}\]</span>
<span class="math inline">\(E[Y]\)</span> increases as <span class="math inline">\(\theta\)</span> increases, so we will be on the “yes” line of
the hypothesis test reference table. Hence the rejection region will be of
the form
<span class="math display">\[\begin{align*}
y_{\rm obs} &gt; F_Y^{-1}(1-\alpha \vert \theta_o) \,.
\end{align*}\]</span>
The cdf <span class="math inline">\(F_Y(y)\)</span> is
<span class="math display">\[\begin{align*}
F_Y(y) = \int_0^y \frac{\theta}{2^\theta} u^{\theta-1} du = \left. \frac{u^\theta}{2^\theta}\right|_0^y = \left(\frac{y}{2}\right)^\theta \,,
\end{align*}\]</span>
and the inverse cdf <span class="math inline">\(F_Y^{-1}(q)\)</span> is <span class="math inline">\(y = 2q^{1/\theta}\)</span>.
Hence the test we seek rejects the null hypothesis if
<span class="math display">\[\begin{align*}
y_{\rm obs} &gt; 2(1-\alpha)^{1/\theta_o} \,.
\end{align*}\]</span>
The rejection-region boundary does not
depend on <span class="math inline">\(\theta_a\)</span>, so we know that the test is the most powerful one
for all alternative values <span class="math inline">\(\theta_a &gt; \theta_o\)</span>…thus it is a
uniformly most powerful test.</p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n \frac{\theta}{\beta}x_i^{\theta-1}\exp\left(-\frac{x_i^\theta}{\beta}\right) = \left[ \theta x_i^{\theta-1} \right] \times \left[ \frac{1}{\beta} \exp\left(-\frac{1}{\beta} x_i^\theta \right) \right] = h(\mathbf{x}) \times g(\beta,\mathbf{x}) \,,
\end{align*}\]</span>
thus a sufficient statistic for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i^\theta\)</span>.</p>
<p>(b) We are given that <span class="math inline">\(X^\theta \sim\)</span> Exp(<span class="math inline">\(\beta\)</span>). The mgf for an
exponential distribution is
<span class="math display">\[\begin{align*}
m_X(t) = (1 - \theta t)^{-1} \,,
\end{align*}\]</span>
and hence the mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> will be
<span class="math display">\[\begin{align*}
m_Y(t) = \prod_{i=1}^n (1 - \theta t)^{-1} = \left[ (1 - \theta t)^{-1} \right]^n = (1 - \theta t)^{-n} \,.
\end{align*}\]</span>
Following the hint given in the question, we find that <span class="math inline">\(Y\)</span> is a
gamma-distributed random variable with “shape” parameter <span class="math inline">\(n\)</span> and
“scale” parameter <span class="math inline">\(\theta\)</span>. (There are two common parameterizations of
the gamma distribution<span class="math inline">\(-\)</span>shape/scale and shape/rate<span class="math inline">\(-\)</span>and it is imperative
to determine the correct one! This will impact the answer to part (c).)</p>
<p>(c) The statistic <span class="math inline">\(Y\)</span> has expected value <span class="math inline">\(E[Y] = n\theta\)</span>, which increases
with <span class="math inline">\(\theta\)</span>. Hence we utilize the upper-tail/yes line of the hypothesis
test reference table: <span class="math inline">\(y_{\rm RR} = F_Y^{-1}(1 - \alpha \vert \theta_o)\)</span>,
or, in code,</p>
<pre><code>y.rr &lt;- qgamma(1-alpha,shape=n,scale=theta)</code></pre>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) The moment-generating function for the random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
m_X(t) = E\left[e^{tX}\right] &amp;= \int_b^\infty e^{tx} \frac{1}{\theta} e^{-(x-b)/\theta} dx \\
&amp;= e^{b/\theta} \frac{1}{\theta} \int_b^\infty e^{-x(1/theta - t)} dx \\
&amp;= e^{b/\theta} \frac{1}{\theta} \frac{e^{-b(1/theta - t)}}{(1/theta-t)} \\
&amp;= e^{bt} (1-t\theta)^{-1} \,.
\end{align*}\]</span>
This is the final answer, but
recall that when <span class="math inline">\(X = U+b\)</span>, <span class="math inline">\(m_X(t) = e^{bt} m_U(t)\)</span>. Since we recognize
that <span class="math inline">\((1-t\theta)^{-1}\)</span> is the mgf for an exponential distribution,
we can state that <span class="math inline">\(U = X-b\)</span> is an exponentially distributed random variable.</p>
<p>(b) The mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
<span class="math display">\[\begin{align*}
m_Y(t) = \prod_{i=1}^n e^{bt} (1-t\theta)^{-1} = \left[ e^{bt} (1-t\theta)^{-1} \right]^n = e^{nbt} (1-t\theta)^{-n} \,.
\end{align*}\]</span></p>
<p>(c) Going back to our answer for (a) (and our answer for the previous
problem), we recognize that the mgf for <span class="math inline">\(\sum_{i=1}^n U_i\)</span> is
<span class="math inline">\((1-t\theta)^{-n}\)</span>, which is the mgf for a gamma distribution with
shape parameter <span class="math inline">\(n\)</span> and scale parameter <span class="math inline">\(\theta\)</span>. Hence
<span class="math inline">\(Y&#39; = Y - nb \sim \text{Gamma}(n,\theta)\)</span>.</p>
<p>(d) We are on the lower-tail/yes line of the hypothesis test reference
table: <span class="math inline">\(y_{\rm RR}&#39; = F_Y^{-1}(\alpha \vert \theta_o)\)</span>, or, in code,</p>
<pre><code>y.rr.prime &lt;- qgamma(alpha,shape=n,scale=theta)</code></pre>
<p>We would reject the null hypothesis if <span class="math inline">\(y_{\rm obs} - nb &lt; y_{\rm RR}&#39;\)</span>.
Because this test is constructed using a sufficient statistic and because
no value of the alternative hypothesis appears in the definition of the
rejection region, we indeed have defined a uniformly most powerful test
of <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>.</p>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>(a) Let’s first find a sufficient statistic:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \theta e^{-\theta x_i} = \theta^n e^{-\theta \sum_{i=1}^n x_i} \,.
\end{align*}\]</span>
A sufficient statistic is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.
We are conducting a lower-tail test, and
since <span class="math inline">\(E[X] = 1/\theta\)</span> decreases as <span class="math inline">\(\theta\)</span> increases,
we are on the “no” line of the reference table.
We reject the null if <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i &gt; y_{\rm RR}\)</span>.</p>
<p>(b) <span class="math inline">\(\theta_o\)</span> is plugged in to compute the rejection-region boundary, but
<span class="math inline">\(\theta_a\)</span> does not appear at all. Hence the defined test is uniformly
most powerful, since it is most powerful for any value of <span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>(a) The sampling distribution is Binom(<span class="math inline">\(nk,p\)</span>). We can determine this
using the method of moment-generating functions, if necessary.</p>
<p>(b) <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, so we are on the upper-tail/“yes” line
of the hypothesis test reference tables. The rejection-region
boundary is given by <span class="math inline">\(F_Y^{-1}(1-\alpha \vert \theta_o)\)</span>, or, in
code, with <span class="math inline">\(p_o\)</span> in place of <span class="math inline">\(\theta_o\)</span>,</p>
<pre><code>qbinom(1-alpha,n*k,p.o)</code></pre>
<p>(c) For an upper-tail/yes test, the <span class="math inline">\(p\)</span>-value is
<span class="math inline">\(1 - F_Y(y_{\rm obs} \vert \theta_o)\)</span>. In code, with <span class="math inline">\(p_o\)</span> in
place of <span class="math inline">\(\theta_o\)</span>, the <span class="math inline">\(p\)</span>-value is</p>
<pre><code>1 - pbinom(y.obs,n*k,p.o)</code></pre>
<p>However, we have to apply a discreteness correction, because otherwise
we will not be summing over the correct range of <span class="math inline">\(y\)</span> values, i.e., our
<span class="math inline">\(p\)</span>-value will be wrong. Here, that factor is <span class="math inline">\(-1\)</span>, applied to the input.
So…</p>
<pre><code>1 - pbinom(y.obs-1,n*k,p.o)</code></pre>
<p>is the final answer.</p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>This is straightforward <em>if</em> we remember to set the link
function to the equation for the line:
<span class="math display">\[\begin{align*}
-(Y \vert x)^{-1} = \beta_0 + \beta_1 x ~~~ \Rightarrow ~~~ (Y \vert x)^{-1} = -\beta_0 - \beta_1 x ~~~ \Rightarrow ~~~ Y \vert x = (-\beta_0 - \beta_1 x)^{-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>(a) The degrees of freedom for the residual deviance
is <span class="math inline">\(n-p\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters (here, two: <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span>). Hence <span class="math inline">\(n = 32\)</span>.</p>
<p>(b) <span class="math inline">\(\beta_1\)</span> is set to zero to compute the null deviance.
So <span class="math inline">\(-2\log\mathcal{L}_{\rm max} = 43.230\)</span>.</p>
<p>(c) The odds are <span class="math inline">\(O(x) = \exp(\hat{\beta}_0 + \hat{\beta}_1 x)\)</span> or
just <span class="math inline">\(\exp(\hat{\beta}_0)\)</span> for <span class="math inline">\(x = 0\)</span>, meaning thet <span class="math inline">\(O(x=0) = \exp(12.040)\)</span>.</p>
<p>(d) The estimated slope <span class="math inline">\(\hat{\beta}_1\)</span> is negative, and we
know that <span class="math inline">\(O(x+1) = O(x) \exp(\hat{\beta}_1)\)</span>, so we know that
<span class="math inline">\(O(x+1) &lt; O(x)\)</span>…the odds <em>decrease</em> as <span class="math inline">\(x\)</span> increases.</p>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
O(x) = \frac{p \vert x}{1 - p \vert x} = \frac{0.1}{1-0.1} = \frac19 = 0.111 \,.
\end{align*}\]</span></p>
<p>(b) The new odds are
<span class="math display">\[\begin{align*}
O(589+100) = \exp(\hat{\beta}_0 + \hat{\beta_1}(589+100)) = O(589) \exp(100\hat{\beta}_1) = \frac19 \exp(0.14684) = 0.129 \,.
\end{align*}\]</span></p>
<p>(c) We have that <span class="math inline">\(Y_1 = 0\)</span> and <span class="math inline">\(\hat{Y}_i = 0.07\)</span>, so
<span class="math display">\[\begin{align*}
d_1 &amp;= \mbox{sign}(Y_1-\hat{Y}_1)\sqrt{-2[Y_1\log\hat{Y}_1+(1-Y_1)\log(1-\hat{Y}_1)]} = \mbox{sign}(-0.07) \sqrt{-2\log(0.93)} \\
&amp;= -\sqrt{-2\log(0.93)} = 0.381 \,.
\end{align*}\]</span></p>
<p>(d) The null deviance is computed assuming <span class="math inline">\(\beta_1 = 0\)</span>. This
model lies “farther” from the observed data than the model with
<span class="math inline">\(\hat{\beta}_1 = 0.00147\)</span>, meaning it deviates more from the data, meaning
that the deviance would be higher.</p>
<hr />
<ul>
<li>Problem 26</li>
</ul>
<p>Let’s start by collecting the basic pieces of information that
we would combine in a Naive Bayes regression model:
<span class="math display">\[\begin{align*}
p(0) = 3/5 ~~\mbox{and}~~ p(1) = 2/5 \,,
\end{align*}\]</span>
where <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> are the two response (i.e., <span class="math inline">\(Y\)</span>) values.
Next up, the conditionals:
<span class="math display">\[\begin{align*}
p(x1 = N \vert 0) = 2/3 ~~ &amp;\mbox{and}&amp; ~~ p(x1 = Y \vert 0) = 1/3 \\
p(N \vert 1) = 1/2 ~~ &amp;\mbox{and}&amp; ~~ P(Y \vert 1) = 1/2 \\
\\
p(x2 = T \vert 0) = 2/3 ~~ &amp;\mbox{and}&amp; ~~ p(x2 = F \vert 0) = 1/3 \\
p(T \vert 1) = 1/2 ~~ &amp;\mbox{and}&amp; ~~ P(F \vert 1) = 1/2 \,.
\end{align*}\]</span>
The estimated probability of observing a datum of Class 0 given <code>Y</code> and <code>F</code> is
thus
<span class="math display">\[\begin{align*}
p(0 \vert Y,F) &amp;= \frac{p(Y \vert 0) p(F \vert 0) p(0)}{p(Y \vert 0) p(F \vert 0) p(0) + p(Y \vert 1) p(F \vert 1) p(1)} \\
&amp;= \frac{1/3 \times 1/3 \times 3/5}{1/3 \times 1/3 \times 3/5 + 1/2 \times 1/2 \times 2/5} \\
&amp;= \frac{1/15}{1/15 + 1/10} = \frac{2/30}{2/30+3/30} = \frac{2}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 27</li>
</ul>
<p>(a) The pdf is of the form
<span class="math display">\[\begin{align*}
k x^{\alpha-1} (1-x)^{\beta-1} \,,
\end{align*}\]</span>
with <span class="math inline">\(0 \leq x \leq 1\)</span>, so what we have is a beta distribution:
<span class="math inline">\(X \sim\)</span> Beta<span class="math inline">\((1,2)\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[X] = \frac{\alpha}{\alpha+\beta} = \frac{1}{3}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} + \left(\frac{1}{3}\right)^2 = \frac{2}{36} + \frac{4}{36} = \frac{1}{6} \,.
\end{align*}\]</span></p>
<p>(c) These expressions are straightforward to evaluate:
<span class="math display">\[\begin{align*}
E[C] = E[10X] = 10E[X] = \frac{10}{3} = 3.333
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
V[C] = E[C^2] - (E[C])^2 = E[100X^2] - \frac{100}{9} = 100E[X^2] - \frac{100}{9} = \frac{150}{9} - \frac{100}{9} = \frac{50}{9} = 5.556 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 28</li>
</ul>
<p>(a) <span class="math inline">\(f_X(x) = 12x - 24x^2 + 12x^3 = 12x(1-x)^2\)</span> for <span class="math inline">\(x \in [0,1]\)</span>…so this is
a Beta(2,3) distribution.</p>
<p>(b) <span class="math inline">\(X \sim {\rm Beta}(2,3) \Rightarrow E[X] = \alpha/(\alpha+\beta) = 2/(2+3) = 2/5 = 0.4\)</span> ,.</p>
<hr />
<ul>
<li>Problem 29</li>
</ul>
<p>One way to solve this problem is to utilize the shortcut formula:
<span class="math inline">\(E[X^2] = V[X] + E[X]^2\)</span>. With this in hand:
<span class="math display">\[\begin{align*}
V[X] = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} = \frac{6}{25 \cdot 6} = \frac{1}{25}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X] = \left(\frac{\alpha}{\alpha + \beta}\right)^2 = \left(\frac{2}{5}\right)^2 = \frac{4}{25} \,.
\end{align*}\]</span>
Therefore,
<span class="math display">\[\begin{align*}
E[X^2] = \frac{1}{25} +  \frac{4}{25} = \frac{1}{5} \,.
\end{align*}\]</span>
A second way to solve this problem is by brute-force integration:
<span class="math display">\[\begin{align*}
E[X^2] &amp;= \int_0^1 x^2 \frac{x(1-x)^2}{B(2,3)}dx = \int_0^1 \frac{x^3(1-x)^2}{B(2,3)}dx = \int_0^1 \frac{x^3(1-x)^2}{B(2,3)} \frac{B(4,3)}{B(4,3)}dx \\
&amp;= \frac{B(4,3)}{B(2,3)}\underbrace{\int_0^1 \frac{x^3(1-x)^2}{B(4,3)}dx}_{=1} \\
&amp;= \frac{B(4,3)}{B(2,3)} = \frac{\Gamma(4) \Gamma(3)}{\Gamma(7)}\frac{\Gamma(5)}{\Gamma(2)\Gamma(3)}\ =  \frac{\Gamma(4) \Gamma(5)}{\Gamma(2)\Gamma(7)} = \frac{3! 4!}{1!6!} = \frac{1}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 30</li>
</ul>
<p>(a) The median is the second sampled datum. The pdf for <span class="math inline">\(X\)</span> is
<span class="math inline">\(f_X(x) = 3x^2\)</span> and <span class="math inline">\(F_X(x) = x^3\)</span>, both for <span class="math inline">\(x \in [0,1]\)</span>. Thus
<span class="math display">\[\begin{align*}
f_{(2)}(x) = \frac{3!}{1!1!} [x^3]^1 [1 - x^3]^1 3x^2 = 18 x^5 (1 - x^3) \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>, and
<span class="math display">\[\begin{align*}
E[X_{(2)}] &amp;= \int_0^1 x 18 x^5 (1-x^3) dx = 18 \int_0^1 (x^6 - x^9) dx \\
&amp;= 18 \left( \left.\frac{x^7}{7}\right|_0^1 - \left.\frac{x^{10}}{10}\right|_0^1 \right) = 18 \left( \frac{1}{7}-\frac{1}{10} \right) = \frac{18 \cdot 3}{70} = \frac{27}{35} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 31</li>
</ul>
<p>(a) This is a Beta(2,2) distribution.</p>
<p>(b) We can determine <span class="math inline">\(c\)</span> via brute-force integration:
<span class="math display">\[\begin{align*}
c \int_0^1 x(1-x) dx = c\left[ \int_0^1 x dx - \int_0^1 x^2 dx \right] &amp;= c \left[ \left.\frac{x^2}{2}\right|_0^1 - \left.\frac{x^3}{3}\right|_0^1 \right] \\
&amp;= c \left[ \frac{1}{2} - \frac{1}{3} \right] \\
&amp;= c \frac{1}{6} = 1 ~~\Rightarrow~~ c = 6 \,.
\end{align*}\]</span>
Alternatively, we can recognize that
<span class="math display">\[\begin{align*}
c &amp;= \frac{1}{B(2,2)} = \frac{\Gamma(4)}{\Gamma(2) \Gamma(2)} = \frac{3!}{1! 1!} = 6 \,.
\end{align*}\]</span></p>
<p>(c) Since <span class="math inline">\(\alpha = \beta\)</span>, the distribution is symmetric around
<span class="math inline">\(x = 1/2\)</span>, which is its mean value.</p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
P(X \leq 1/4 \vert X \leq 1/2) &amp;= \frac{P(X \leq 1/4 \cap X \leq 1/2)}{P(X \leq 1/2)} = \frac{P(X \leq 1/4)}{P(X \leq 1/2)} = \frac{P(X \leq 1/4)}{1/2} \\
&amp;= 2P(X \leq 1/4) = 2 \int_0^{1/4} 6 x (1-x) dx = 12 \int_0^{1/4} x (1-x) dx \\
&amp;= 12 \left[ \left.\frac{x^2}{2}\right|_0^{1/4} - \left.\frac{x^3}{3}\right|_0^{1/4} \right] = 12 \left[ \frac{1}{32} - \frac{1}{192} \right] = 12 \frac{5}{192} = \frac{60}{192} = \frac{30}{96} = \frac{5}{16} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 32</li>
</ul>
<p>(a) We carry out a chi-square goodness-of-fit test:
<span class="math display">\[\begin{align*}
W = \sum_{i=1}^n \frac{(X_i - kp_i)^2}{kp_i} = \frac{1}{7}[(10-7)^2+(5-7)^2+(6-7)^2] = 2 \,.
\end{align*}\]</span></p>
<p>(b) There are <span class="math inline">\(m=3\)</span> outcomes, but we lose one degree of freedom
because of the constraint that
<span class="math inline">\(\sum_{i=1}^m X_i = 21\)</span>, so the number of degrees of freedom is 2.</p>
<p>(c) We are given that <span class="math inline">\(\alpha = 0.1\)</span>, so we want
<span class="math inline">\(F_W^{-1}(0.9) = 4.61\)</span>.</p>
<hr />
<ul>
<li>Problem 33</li>
</ul>
<p>(a) The correct answer is <em>homogeneity</em>, since the researcher is splitting the respondents into groups by giving them the different types of leaflets to read, and the goal is to determine the willingness to spend government funding is homogenous across the type of pamphlets.</p>
<p>(b) Since chi-square statistics involving summing squared differences over all combinations of leaflets and spending opinions, there are <span class="math inline">\(12 = 3 \times 4\)</span> terms.</p>
<p>(c) Since the all the information but one for each factor is sufficent, the test statistics will follow a chi-square distribution with <span class="math inline">\(6 = (3-1) \times (4-1)\)</span> degrees of freedom.</p>
<p>(d) The correct answer is (i) since <span class="math inline">\(p\)</span>-value is defined as a probability of events at least as extreme as the what actually observed under the null hypothesis, and larger values of the test statistic here correspond to larger differences between what we would expect under the null and what we observe.</p>
<hr />
<ul>
<li>Problem 34</li>
</ul>
<p>(a) The exponential distribution with mean 1 is <span class="math inline">\(e^{-x}\)</span>, for <span class="math inline">\(x \geq 0\)</span>. Therefore, the probabilities for arriving between 1 and 2 minutes after the previous person is given by
<span class="math display">\[\begin{equation*}
\int_1^2 e^{-x} dx =  -\left.e^{-x}\right|_1^2 = 0.233 \,.
\end{equation*}\]</span></p>
<p>(b) The probabilities for the other two “bins” are
<span class="math display">\[\begin{align*}
\int_0^1 e^{-x} dx &amp;= -\left.e^{-x}\right|_0^1 = 0.632 \\
\int_2^\infty e^{-x} dx &amp;= 1 - 0.632 - 0.233 = 0.135 \,.
\end{align*}\]</span>
We can now carry out a chi-square goodness-of-fit test:
<span class="math display">\[\begin{equation*}
W = \frac{(52-63.2)^2}{63.2} + \frac{(23-23.3)^2}{23.3} + \frac{(25-13.5)^2}{13.5} = 1.985 + 0.004 + 9.796 = 11.785 \,,
\end{equation*}\]</span>
and <span class="math inline">\(W \sim \chi_2^2\)</span>. The rejection region is <span class="math inline">\(W &gt; w_{\rm RR} = 5.991\)</span>
(i.e., <code>qchisq(0.95,2)</code>), and
the <span class="math inline">\(p\)</span>-value is 0.0028 (i.e., <code>1-pchisq(11.785,2)</code>).
We have sufficient evidence to reject
the null hypothesis and conclude that the time elapsed between people walking
through a particular door is <em>not</em> exponentially distributed with mean 1.</p>
<p>(c) If <span class="math inline">\(n = 10\)</span> and there are 3 bins, then there is no way that
<span class="math inline">\(np_i \geq 5\)</span> for all bins. Thus the chi-square goodness-of-fit test should
not be applied.</p>
<hr />
<ul>
<li>Problem 35</li>
</ul>
<p>(a) We have that <span class="math inline">\(p_{\rm out} = 8/9\)</span> and <span class="math inline">\(p_{\rm in} = 1/9\)</span>,
so <span class="math inline">\(kp_{\rm out} = 180 (8/9) = 160\)</span> and <span class="math inline">\(kp_{\rm in} = 180 (1/9)
= 20\)</span>.</p>
<p>(b) We perform a chi-square goodness-of-fit test:
<span class="math display">\[\begin{align*}
W = \frac{(150-160)^2}{160} + \frac{(30-20)^2}{20} = \frac{100}{160} + \frac{100}{20} = \frac58 + 5 = 5.625~(\mbox{or}~5~5/8) \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(W\)</span> is sampled from a chi-square distribution for <span class="math inline">\(2-1 = 1\)</span>
degree of freedom.</p>
<p>(d) The rejection region for a chi-square GoF test is
<span class="math inline">\(W &gt; w_{\rm RR}\)</span>, so, since 5.625 is greater than 3.841, we would
reject the null hypothesis.</p>
</div>
<div id="chapter-4" class="section level2 unnumbered hasAnchor">
<h2>Chapter 4<a href="chapter-exercises-solutions.html#chapter-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>(a) This is a Poisson problem: <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda = 2 \cdot 1/4 = 1/2\)</span>). So
<span class="math display">\[\begin{align*}
\mu = E[X] = \lambda = 1/2 ~~\text{and}~~ \sigma = \sqrt{V[X]} = \sqrt{\lambda} = \sqrt{1/2} \approx 0.707 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
P(1/2 \leq X \leq 1/2+1.414) = p(1) = \frac{\lambda^1}{1!} e^{-\lambda} = \frac12 e^{-1/2} = 0.303 \,.
\end{align*}\]</span></p>
<p>(b) We have that <span class="math inline">\(X \sim\)</span> Exp(<span class="math inline">\(\beta = 1/2\)</span>) (since there is a half-hour
on average between calls). By the memorylessness property,
<span class="math inline">\(P(X &gt; 1/2 \vert X &gt; 1/4) = P(X &gt; 1/4)\)</span>. Thus
<span class="math display">\[\begin{align*}
P(X &gt; 1/4) = \int_{1/4}^\infty \frac{1}{\beta} e^{-x/\beta} dx = \int_{1/4}^\infty 2 e^{-2x} dx = \left.-e^{-2x}\right|_{1/4}^\infty = e^{-1/2} = 0.607 \,.
\end{align*}\]</span></p>
<p>(c) The overall time <span class="math inline">\(T\)</span> is <span class="math inline">\(10X + (10-X)\)</span>, where <span class="math inline">\(X\)</span>, the number of calls
from the friend, is sampled from a binomial distribution with <span class="math inline">\(k = 10\)</span> and
<span class="math inline">\(p = 0.2\)</span>. Thus the average total number of minutes is
<span class="math display">\[\begin{align*}
E[T] = E[10X + (10-X)] = E[9X+10] = 9E[X] + 10 = 9kp + 10 = 28 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>(a) The number of (successful) shots can be infinite; only <em>on average</em> is
the number of shots in eight minutes going to be four. So we are working with
a Poisson distribution whose parameter <span class="math inline">\(\lambda\)</span> (the expected
number of <em>successful</em> shots) is
<span class="math display">\[\begin{align*}
\lambda = (1 \quad \frac{\text{shot}}{2\text{min}})(\frac{1}{2} \frac{\text{success}}{\text{shot}})(4 \quad 2\text{min}) = 2 \,.
\end{align*}\]</span>
Let <span class="math inline">\(X\)</span> be the number of successful shots. Then
<span class="math display">\[\begin{align*}
P(X \leq 1) = \frac{\lambda^0}{0!}e^{-\lambda} + \frac{\lambda^1}{1!}e^{-\lambda} = e^{-2}(1+2) = 3e^{-2} \,.
\end{align*}\]</span></p>
<p>(b) We know that <span class="math inline">\(E[X] = \lambda = 2\)</span>, <span class="math inline">\(V[X] = \lambda = 2\)</span>, and <span class="math inline">\(\sigma = \sqrt{\lambda} = \sqrt{2}\)</span>. Thus
<span class="math display">\[\begin{align*}
P(2 - \sqrt{2} &lt; X &lt; 2 + \sqrt{2}) &amp;= p_X(1) + p_X(2) + p_X(3) = e^{\lambda}\left(\lambda + \frac{\lambda^2}{2} + \frac{\lambda^3}{6} \right) \\
&amp;= e^{2}\left(2 + 2+ \frac{8}{6} \right) = \frac{16}{3} e^{-2} = 0.722 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>The particular event in question happens 8 times per year in the
three states, and thus the expected number of events in a two-year window
is 16. The appropriate distribution in this case is the
Poisson distribution. If the total number of observed events is denoted
<span class="math inline">\(X\)</span>, then <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span> = 16), and <span class="math inline">\(E[X] = V[X] = 16\)</span>.</p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) We utilize the shortcut formula:
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = 2\sigma^2 - \frac{\pi}{2}\sigma^2 + \frac{\pi}{2}\sigma^2 = 2\sigma^2 \,.
\end{align*}\]</span></p>
<p>(b) The first population moment is
<span class="math inline">\(\mu_1&#39; = E[X] = \sigma\sqrt{\pi/2}\)</span> and the first sample moment is
<span class="math inline">\(m_1&#39; = (1/n)\sum_{i=1}^n X_i = \bar{X}\)</span>. We set these equal and determine that
<span class="math display">\[\begin{align*}
\hat{\sigma}_{MoM} = \sqrt{\frac{2}{\pi}} \bar{X} \,.
\end{align*}\]</span></p>
<p>(c) The bias is <span class="math inline">\(E[\hat{\theta}-\theta] = E[\hat{\theta}] - \theta\)</span>,
or
<span class="math display">\[\begin{align*}
B[\hat{\theta}_{MoM}] &amp;= E\left[\sqrt{\frac{2}{\pi}} \bar{X}\right] - \sigma = \sqrt{\frac{2}{\pi}} E\left[\bar{X}\right] - \sigma = \sqrt{\frac{2}{\pi}} E\left[X\right] - \sigma = \sqrt{\frac{2}{\pi}} \sqrt{\frac{\pi}{2}} \sigma - \sigma = 0 \,.
\end{align*}\]</span></p>
<p>(d) The variance is
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MoM}] &amp;= V\left[\sqrt{\frac{2}{\pi}} \bar{X}\right] = \frac{2}{\pi} V\left[\bar{X}\right] = \frac{2}{\pi} \frac{V\left[X\right]}{n} = \frac{2}{n\pi} \frac{(4-\pi)\sigma^2}{2} = \frac{(4-\pi)\sigma^2}{n\pi} \,.
\end{align*}\]</span></p>
<p>(e) The second population moment is
<span class="math inline">\(\mu_2&#39; = E[X^2] = 2\sigma^2\)</span> (from part a) and the second sample moment is
<span class="math inline">\(m_2&#39; = (1/n)\sum_{i=1}^n X_i^2 = \overline{X^2}\)</span>.
We set these equal and determine that
<span class="math display">\[\begin{align*}
\widehat{\sigma^2}_{MoM} = \frac{\overline{X^2}}{2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>(a) We have that <span class="math inline">\(\mu_1&#39; = \frac{1}{p}\)</span> and <span class="math inline">\(m_1&#39; = X\)</span>. It follows from moment equation <span class="math inline">\(\mu_1&#39; = m_1&#39;\)</span> that <span class="math inline">\(\frac{1}{p} = X\)</span>, so <span class="math inline">\(\hat{p}_{MoM} = \frac{1}{X}\)</span>.</p>
<p>(b) We have that <span class="math inline">\(\mu_2&#39; = \frac{1 - p}{p^2} +  \frac{1}{p^2}  = \frac{2 - p}{p^2}\)</span> and <span class="math inline">\(m_2&#39; = X^2\)</span>. It follow from the second moment equation <span class="math inline">\(\mu_2&#39; = m_2&#39;\)</span> that
<span class="math display">\[\begin{align*}
\frac{2-p}{p^2} &amp; = X^2 \\
\Rightarrow ~~~ 2-p &amp; = p^2 X^2 \\
\Rightarrow ~~~ p^2 X^2 +p -2 &amp;= 0 \\
\Rightarrow ~~~ \hat{p}_{MoM} &amp; = \frac{-1 + \sqrt{1 + 8 X^2}}{2X^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) The expected value for a beta distribution is <span class="math inline">\(E[X] = \alpha/(\alpha+\beta)\)</span>, so, using the first sample moment, we get that
<span class="math display">\[\begin{align*}
E[X] &amp;= \bar{X} \\
\Rightarrow ~~~ \frac{\alpha}{\alpha+\beta} &amp;= \bar{X} \\
\Rightarrow ~~~ \frac{\alpha+\beta}{\alpha} = 1 + \frac{\beta}{\alpha} &amp;= \frac{1}{\bar{X}} \\
\Rightarrow ~~~ \frac{\beta}{\alpha} &amp;= \frac{1}{\bar{X}}-1 \\
\Rightarrow ~~~ \hat{\beta}_{MoM} &amp;= \alpha\left(\frac{1}{\bar{X}}-1\right) \,.
\end{align*}\]</span></p>
<p>(b) There is no invariance property for the method-of-moments estimator, so the answer is no.</p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>(a) The argument list is fine.</p>
<p>(b) We need to change <code>mean(X)</code> to <code>sum(X)</code> and <code>lambda</code> to <code>n*lambda</code>.</p>
<p>(c) The reference table tells us that a one-sided lower bound where <span class="math inline">\(E[U]\)</span> increases with <span class="math inline">\(\lambda\)</span> will have a value of <span class="math inline">\(q\)</span> equal to <span class="math inline">\(1-\alpha\)</span>. So we plug in 0.95.</p>
<p>(d) Confidence intervals derived from discrete sampling distributions will have coverages <span class="math inline">\(\geq 1-\alpha\)</span>, so, here, we would say “greater than or equal to 95%.”</p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>(a) The likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda_{LR} = \frac{\mbox{sup}_{\theta \in \Theta_o} \mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta} \mathcal{L}(\theta \vert \mathbf{x})}
\end{align*}\]</span>
Here, that becomes
<span class="math display">\[\begin{align*}
\lambda_{LR} &amp;= \frac{\mathcal{L}(p_o \vert \mathbf{x})}{\mathcal{L}(\hat{p}_{MLE} \vert \mathbf{x})} = \frac{p_o^{\sum_{i=1}^n X_i}(1-p_o)^{n-\sum_{i=1}^n X_i}}{\bar{X}^{\sum_{i=1}^n X_i}(1-\bar{X})^{n-\sum_{i=1}^n X_i}} = \frac{p_o^U(1-p_o)^{n-U}}{\bar{X}^U(1-\bar{X})^{n-U}} \,.
\end{align*}\]</span></p>
<p>(b) The test is a two-sided test, so we cannot proclaim it to be uniformly
most powerful. However, it very well may be…we just cannot say with the information we have at hand. So: “maybe.”</p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) The maximum-likelihood estimate is
<span class="math display">\[\begin{align*}
\ell(\lambda \vert x) &amp;= x \log \lambda - \log x! - \lambda \\
\Rightarrow ~~~ \ell&#39;(\lambda \vert x) &amp;= \frac{x}{\lambda} - 1 = 0\\
\Rightarrow ~~~ \hat{\lambda}_{MLE} &amp;= X \,.
\end{align*}\]</span>
which here takes on the value <span class="math inline">\(x_{\rm obs}\)</span>.</p>
<p>(b) The likelihood-ratio test statistic is
<span class="math display">\[\begin{align*}
\frac{\mbox{sup}_{\theta \in \Theta_o}\mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta}\mathcal{L}(\theta \vert \mathbf{x})} \,.
\end{align*}\]</span>
Here, that means that for the numerator, we insert the Poisson pmf (remember: one datum) with <span class="math inline">\(\lambda_o\)</span> plugged in, i.e.,
<span class="math display">\[\begin{align*}
\frac{\lambda_o^{x_{\rm obs}}}{x_{\rm obs}!} e^{-\lambda_o} \,,
\end{align*}\]</span>
while for the denominator, we plug in the MLE for <span class="math inline">\(\lambda\)</span>, i.e.,
<span class="math display">\[\begin{align*}
\frac{x_{\rm obs}^{x_{\rm obs}}}{x_{\rm obs}!} e^{-x_{\rm obs}} \,.
\end{align*}\]</span>
So the ratio is
<span class="math display">\[\begin{align*}
\left( \frac{\lambda_o}{x_{\rm obs}} \right)^{x_{\rm obs}} e^{-(\lambda_o-x_{\rm obs})} \,.
\end{align*}\]</span></p>
<p>(c) The expression is
<span class="math display">\[\begin{align*}
W = -2 \log \lambda_{LR} = -2 x_{\rm obs} \log \left( \frac{\lambda_o}{x_{\rm obs}} \right) + 2 (\lambda_o-x_{\rm obs}) \,.
\end{align*}\]</span></p>
<p>(d) <span class="math inline">\(W\)</span> is sampled from a chi-square distribution for 1 degree
of freedom.</p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) The factorized likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!}e^{-\lambda} = \left(\frac{1}{\prod_{i=1}^n x_i!}\right) \times \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda} = h(\mathbf{x}) \times g(\lambda,\mathbf{x}) \,.
\end{align*}\]</span>
The sufficient statistic is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<p>(b) The sum of <span class="math inline">\(n\)</span> iid Poisson random variables
is a Poisson random variable with parameter <span class="math inline">\(n\lambda\)</span>.
The moment-generating function for a Poisson random variable is
<span class="math inline">\(m_X(t) = \exp(\lambda(e^t-1))\)</span>, so the mgf for <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{eqnarray*}
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = \left[ m_{X_i}(t) \right]^n = \exp(n\lambda(e^t-1)) \,.
\end{eqnarray*}\]</span>
This is the mgf for a Poisson(<span class="math inline">\(n\lambda\)</span>) distribution.</p>
<p>(c) Recall that in an LRT context, <span class="math inline">\(\Theta = \Theta_o \cup \Theta_a\)</span>;
in other words, the null must contain <em>all</em> possible values of
<span class="math inline">\(\theta\)</span> that are not in the alternative. Hence: <span class="math inline">\(H_o : \theta \geq \theta_o\)</span>.
This inequality does not actually change how
the test is constructed, but does change how we interpret it: the true
<span class="math inline">\(\alpha\)</span> for this test will be less than or equal to the stated <span class="math inline">\(\alpha\)</span>.</p>
<p>(d) We are performing a lower-tail test, and we are on the “yes”
line (since <span class="math inline">\(E[Y] = n\lambda\)</span> increases with <span class="math inline">\(\lambda\)</span>). From the reference
table, that means <span class="math inline">\(y_{\rm RR}\)</span> is equal to <span class="math inline">\(F_Y^{-1}(\alpha \vert n\lambda_o)\)</span>,
which in code is {}. (Recall that there are no
discreteness corrections in rejection-region boundary computations.)</p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>(a) The likelihood function for the sample is:
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^{n} \frac{1}{\beta}e^{-x/\beta} = \frac{1}{\beta^n} e^{-\frac{\sum_{i=1}^{n}x_i}{\beta}} \,.
\end{align*}\]</span>
Under the null <span class="math inline">\(H_0 : \beta = 1\)</span>, the likelihood function becomes
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta_0 \vert \mathbf{x}) = e^{-\sum_{i=1}^{n}x_i} = e^{-n\bar x} \,,
\end{align*}\]</span>
while under the alternative,
<span class="math display">\[\begin{align*}
\sup_{\beta &gt; 0} \mathcal{L}(\beta \vert \mathbf{x}) &amp;= \mathcal{L}(\hat{\beta}_{MLE} \vert \mathbf{x})\\
&amp; = \frac{1}{\bar x^n}  e^{-\frac{\sum_{i=1}^{n}x_i}{\bar x}} = \frac{1}{\bar x^n}  e^{-n} \,.
\end{align*}\]</span>
So the likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda_{LR} &amp;= \frac{\mathcal{L}(\beta_o\vert \mathbf{x})}{\sup_{\beta &gt; 0} \mathcal{L}(\beta \vert \mathbf{x})}\\
&amp;= \frac{e^{-n\bar x}}{\frac{1}{\bar x^n} e^{-n}} \\
&amp;= \bar x^ne^{-n(\bar x-1)} \,.
\end{align*}\]</span></p>
<p>(b) Under the null hypothesis, the number of degrees of freedom is <span class="math inline">\(r_o = 0\)</span>, because <span class="math inline">\(\beta = 1\)</span> is set to a constant. Under the alternative hypothesis, the number of degrees of freedom is <span class="math inline">\(r = 1\)</span>, because we have one free parameter: <span class="math inline">\(\beta\)</span>. Therefore, according to Wilks’ theorem, the degree of freedom of the <span class="math inline">\(\chi^2\)</span> distribution is <span class="math inline">\(r - r_o = 1-0=1\)</span>.
Under the large-<span class="math inline">\(n\)</span> approximation, <span class="math inline">\(-2\log(\lambda_{LR}) \sim \chi^2(1)\)</span>.
Therefore, the rejection region corresponds to:
<span class="math display">\[\begin{align*}
-2\log(\lambda) &amp;&gt; \chi^2_{0.95, 1}\\
\Rightarrow ~~~ -2\log\left(\bar x^ne^{-n(\bar x-1)}\right) &amp;&gt; \chi^2_{0.95,1} = 3.84 \\
\Rightarrow ~~~ n\left(\log(\bar x) - \bar x+1\right) &amp;&lt; \frac{3.84}{2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) <span class="math inline">\(H_0: p = p_0 = 0.5\)</span> and <span class="math inline">\(H_a: p \neq 0.5\)</span></p>
<p>(b) <span class="math inline">\(\Theta_0 = \{p_0\}\)</span> and <span class="math inline">\(\Theta_a = \{p\, \vert\,  p \in [0,1] ~~\text{and}~~ p \neq p_0\}\)</span></p>
<p>(c) <span class="math inline">\(r_0 = 0\)</span> (<span class="math inline">\(p\)</span> is fixed) and <span class="math inline">\(r = 1\)</span></p>
<p>(d) The likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda = \frac{\mathcal{L}(p_0 \vert x)}{\mathcal{L}(\hat{p}_{MLE} \vert x)} = \frac{\frac{1000!}{550!450!} 0.5^{550} (1-0.5)^{450} }{ \frac{1000!}{550!450!} 0.55^{550} (1-0.55)^{450}}  = \frac{0.5^{1000}}{0.55^{550} \cdot 0.45^{450}} = 0.00668 \,,
\end{align*}\]</span>
where we make use of the fact that <span class="math inline">\(\hat{p}_{MLE} = x/n = 0.55\)</span>.</p>
<p>(e) We have that <span class="math inline">\(W_{\rm obs} = -2 \log(\lambda_{LR}) = 10.017\)</span>. According to Wilk’s theorem, the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[\begin{align*}
\int_{W_{\rm obs}}^\infty f_W(w) dw \,,
\end{align*}\]</span>
for 1 degree of freedom, or <code>1 - pchisq(10.017,1)</code> (= 0.00155).</p>
<p>(f) We have sufficient evidence to reject the null hypothesis and thus to
conclude that the coin is not a fair one.</p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>By inspection, <span class="math inline">\(X \sim\)</span> Gamma(3,2/3). Thus <span class="math inline">\(E[X] = \alpha \beta = 2\)</span> and <span class="math inline">\(V[X] = \alpha \beta^2 = 3 (2/3)^2 = 4/3\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) <span class="math inline">\(E[X] = \alpha \beta\)</span> and <span class="math inline">\(V[X] = \alpha \beta^2\)</span>, so <span class="math inline">\(V[X]/E[X] = \beta =
10/5 = 2\)</span>, and <span class="math inline">\(\alpha = 5/2 = 2.5\)</span>.</p>
<p>(b) <span class="math inline">\(\beta = 2\)</span> and <span class="math inline">\(\alpha = 2.5\)</span> <span class="math inline">\(\Rightarrow\)</span> chi-square distribution (for
5 degrees of freedom).</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X^{-1}] &amp;= \int_0^\infty \frac1x f_X(x) dx = \int_0^\infty \frac1x \frac{x^{\nu/2-1}}{2^{\nu/2}} \frac{e^{-x/2}}{\Gamma(\nu/2)} dx \\
&amp;= \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2}} \frac{e^{-x/2}}{\Gamma(\nu/2)} dx = \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2}} \frac{2^{-1}}{2^{-1}} \frac{e^{-x/2}}{\Gamma(\nu/2)} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2-1)} dx \\
&amp;= 2^{-1} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2)} \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2-1}} \frac{e^{-x/2}}{\Gamma(\nu/2-1)} dx = 2^{-1} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2)} \\
&amp;= \frac12 \frac{\Gamma(\nu/2-1)}{(\nu/2-1)\Gamma(\nu/2-1)} = \frac{1}{2(\nu/2-1)} = \frac{1}{\nu-2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X] = \int_0^\infty x f_X(x) dx &amp;= \int_0^\infty x \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x^{\alpha+1}} e^{-\beta/x} dx \\
&amp;= \int_0^\infty \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x^{\alpha}} e^{-\beta/x} dx \\
&amp;= \frac{\beta^{\alpha}}{\beta{\alpha-1}} \frac{\Gamma(\alpha-1)}{\Gamma(\alpha)} \int_0^\infty \frac{\beta^{\alpha-1}}{\Gamma(\alpha-1)} \frac{1}{x^{\alpha}} e^{-\beta/x} dx \\
&amp;= \frac{\beta^{\alpha}}{\beta{\alpha-1}} \frac{\Gamma(\alpha-1)}{\Gamma(\alpha)} \times 1 \\
&amp;= \beta \frac{\Gamma(\alpha-1)}{(\alpha-1)\Gamma(\alpha-1)} \\
&amp;= \frac{\beta}{\alpha-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>(a) If <span class="math inline">\(\beta = 2\)</span> and <span class="math inline">\(\alpha\)</span> is a half-integer or
an integer, then <span class="math inline">\(X\)</span> is sampled from a “chi-square” distribution.
(Note that because <span class="math inline">\(\alpha\)</span> is a half-integer here, you cannot answer
“Erlang” or “exponential.”)</p>
<p>(b) The gamma pdf with <span class="math inline">\(\alpha = 3/2\)</span> is
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x^{1/2}}{\beta^{3/2}} \frac{e^{-x/\beta}}{\Gamma(3/2)} \,,
\end{align*}\]</span>
so the likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x} = \prod_{i=1}^n \frac{x_i^{1/2}}{\beta^{3/2}} \frac{e^{-x_i/\beta}}{\Gamma(3/2)} = \frac{\sqrt{\prod_{i=1}^n x_i}}{[\Gamma(3/2)]^n} \times \frac{e^{-(\sum_{i=1}^n x_i)/\beta}}{\beta^{3/2}} = h(\mathbf{x}) \times g(\beta,\mathbf{x}) \,.
\end{align*}\]</span>
We can read off of the <span class="math inline">\(g(\cdot)\)</span> function that <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
a sufficient statistic for <span class="math inline">\(\beta\)</span>.</p>
<p>(c) We start by computing
<span class="math display">\[\begin{align*}
E[Y] = E\left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n E[X_i] = nE[X] = n \alpha \beta = \frac{3}{2}n\beta \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
E\left[\frac{2Y}{3n}\right] = \beta
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\hat{\beta}_{MVUE} = \frac{2Y}{3n} = \frac{2}{3}\bar{X} \,.
\end{align*}\]</span></p>
<p>(d) The first population moment is
<span class="math inline">\(\mu_1&#39; = E[X] = (3/2)\beta\)</span> and the first sample moment is
<span class="math inline">\(m_1&#39; = (1/n)\sum_{i=1}^n X_i = \bar{X}\)</span>.
We set these equal and find that
<span class="math display">\[\begin{align*}
\hat{\beta}_{MoM} = \frac{2}{3}\bar{X} \,.
\end{align*}\]</span></p>
<p>(e) Because the MoM is equivalent to the MVUE, we know
immediately that the bias of the MoM is 0.</p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X^{1/2}] = \int_0^\infty x^{1/2} f_X(x) dx &amp;= \int_0^\infty x^{1/2} \frac{x^{\alpha-1}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} \frac{\beta^{\alpha+1/2}}{\beta^{\alpha+1/2}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha+1/2)} dx \\
&amp;= \frac{\beta^{\alpha+1/2}}{\beta^{\alpha}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^{\alpha+1/2}}\frac{e^{-x/\beta}}{\Gamma(\alpha+1/2)} dx \\
&amp;= \frac{\beta^{\alpha+1/2}}{\beta^{\alpha}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \\
&amp;= \sqrt{\beta} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \,.
\end{align*}\]</span>
We note that in general,
<span class="math display">\[\begin{align*}
E[X^k] = \beta^k \frac{\Gamma(\alpha+k)}{\Gamma(\alpha)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) The overdispersion parameter in a negative
binomial regression is dubbed “Theta” and is thus 214,488.</p>
<p>(b) If the overdispersion parameter is <span class="math inline">\(\infty\)</span>, then
Poisson regression is recovered. The value here is sufficiently large that
we can say with confidence that there is no overdispersion.
(Backing up this conclusion are
the nearly identical results since when learning both regression models.)</p>
<p>(c) The answer is the Likelihood Ratio test. The
statistic is the difference in the deviance values, which we assume under
the null is chi-square distributed for the difference in the
numbers of degree of freedom.</p>
<p>(d) The null hypothesis in the LRT is <span class="math inline">\(\beta_1 = 0\)</span> and the
alternative is <span class="math inline">\(\beta_1 \neq 0\)</span>. The test statistic is <em>so large</em>
(<span class="math inline">\(\approx\)</span> 165), especially considering that the expected value for
a chi-square distribution is 1 for 1 degree of freedom, that we can
safely conclude that <span class="math inline">\(\beta_1 \neq 0\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bibliography.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

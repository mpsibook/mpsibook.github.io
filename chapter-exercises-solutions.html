<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter Exercises: Solutions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter Exercises: Solutions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bibliography.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.19" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#exercises"><i class="fa fa-check"></i><b>1.19</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-exponential-family-of-distributions"><i class="fa fa-check"></i><b>2.19</b> The Exponential Family of Distributions</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exponential-family-with-a-vector-of-parameters"><i class="fa fa-check"></i><b>2.19.1</b> Exponential Family with a Vector of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exercises-1"><i class="fa fa-check"></i><b>2.20</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.2.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-binomial-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>3.2.5</b> The Binomial Distribution as Part of the Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sufficiency-principle-examples-of-when-we-cannot-reduce-data"><i class="fa fa-check"></i><b>3.6.3</b> The Sufficiency Principle: Examples of When We Cannot Reduce Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.5</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#utilizing-simulations-when-data-reduction-is-not-possible"><i class="fa fa-check"></i><b>3.8.6</b> Utilizing Simulations When Data Reduction is Not Possible</a></li>
<li class="chapter" data-level="3.8.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.7</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises-2"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-poisson-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>4.2.1</b> The Poisson Distribution as Part of the Exponential Family</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.2</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
<li class="chapter" data-level="4.6.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation"><i class="fa fa-check"></i><b>4.6.5</b> Confidence Interval Estimation via Simulation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#exercises-3"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.3.1</b> Sufficient Statistics for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-using-an-order-statistic"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Using an Order Statistic</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-test-for-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> Hypothesis Test for the Uniform Distribution Upper Bound</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#using-numerical-integration-to-characterize-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Using Numerical Integration to Characterize a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.4</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
<li class="chapter" data-level="6.4.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#mutual-information"><i class="fa fa-check"></i><b>6.4.3</b> Mutual Information</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#multiple-comparisons"><i class="fa fa-check"></i><b>7.6</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>7.6.1</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><a href="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><i class="fa fa-check"></i>Appendix B: Confidence Interval and Hypothesis Test Reference Tables</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-1"><i class="fa fa-check"></i>Chapter 1</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-2"><i class="fa fa-check"></i>Chapter 2</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-6"><i class="fa fa-check"></i>Chapter 6</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-exercises-solutions" class="section level1 unnumbered hasAnchor">
<h1>Chapter Exercises: Solutions<a href="chapter-exercises-solutions.html#chapter-exercises-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="chapter-1" class="section level2 unnumbered hasAnchor">
<h2>Chapter 1<a href="chapter-exercises-solutions.html#chapter-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>We start with Bayes’ rule:
<span class="math display">\[\begin{align*}
P(A \vert B) &amp;= \frac{P(B \vert A)P(A)}{P(B)} \\
\Rightarrow ~~~ \frac{P(B \vert A)P(A)}{P(B)} &amp;= \frac{P(A \vert B)P(A)}{P(B)} \\
\Rightarrow ~~~ 1 &amp;= \frac{P(A)}{P(B)} \\
\Rightarrow ~~~ P(A) &amp;= P(B) \,.
\end{align*}\]</span>
Now let’s look at a 2 <span class="math inline">\(\times\)</span> 2 table:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(B\)</span></th>
<th align="center"><span class="math inline">\(\bar{B}\)</span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(A\)</span></td>
<td align="center"><span class="math inline">\(P(A \cap B)\)</span></td>
<td align="center"><span class="math inline">\(P(A)-P(A \cap B)\)</span></td>
<td align="center"><span class="math inline">\(P(A)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\bar{A}\)</span></td>
<td align="center"><span class="math inline">\(P(A)-P(A \cap B)\)</span></td>
<td align="center">0.2</td>
<td align="center"><span class="math inline">\(1-P(A)\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center"><span class="math inline">\(P(A)\)</span></td>
<td align="center"><span class="math inline">\(1-P(A)\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>So we have that
<span class="math display">\[\begin{align*}
P(A \cap B) + 2 [ P(A)-P(A \cap B) ] + 0.2 &amp;= 1 \\
\Rightarrow ~~~ P(A \cap B) + 2 [ P(A)-P(A \cap B) ] &amp;= 0.8 \\
\Rightarrow ~~~ 2 P(A) - P(A \cap B) &amp;= 0.8 \\
\Rightarrow ~~~ P(A \cap B) &amp;= 2 P(A) - 0.8 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>We are given that <span class="math inline">\(A \subset B\)</span>, so
<span class="math display">\[\begin{align*}
P(A \vert B)= \frac{P(B \cap A)}{P(B)} = \frac{P(A)}{P(B)} \neq P(A) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
P(A \cap B| A\cup B)  = \frac{P[(A \cap B) \cap (A \cup B)]}{P(A\cup B)} = \frac{P(A \cap B)}{P(A\cup B)} = \frac{1 - P(\bar{A} \cup \bar{B})}{1 - P(\bar{A} \cap \bar{B})} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
P(B|A) + P(\bar{B}|A) &amp;= 1 \\
\Rightarrow ~~~ P(B|A) &amp;= \frac{1}{4} = \frac{1}{3}P(\bar{B}|A) \\
\Rightarrow ~~~ \frac{P(A \cap B)}{P(A)} &amp;= \frac{1}{3} \frac{P(A \cap \bar{B})}{P[A]} \\
\Rightarrow ~~~ P(A \cap B) &amp;= x = \frac{1}{3} \frac{1}{6} = \frac{1}{18} \,.
\end{align*}\]</span></p>
<p>(b) Here, we utilize a <span class="math inline">\(2 \times 2\)</span> table:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(B\)</span></th>
<th align="center"><span class="math inline">\(\bar{\text{B}}\)</span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(A\)</span></td>
<td align="center">1/18</td>
<td align="center">1/6</td>
<td align="center">4/18</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\bar{\text{A}}\)</span></td>
<td align="center"></td>
<td align="center">1/3</td>
<td align="center">14/18</td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">1/2</td>
<td align="center">1/2</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Thus
<span class="math display">\[\begin{align*}
p(\bar{A} \cap B) = \frac{14}{18} - \frac{6}{18} = \frac{8}{18}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
P(B|\bar{A}) = \frac{\bar{A} \cap B}{P(\bar{A})} = \frac{8/18}{14/18} = \frac{8}{14} = \frac{4}{7} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>One way to approach this problem is to write
<span class="math display">\[\begin{align*}
P(\bar{A}|\bar{B}) = \frac{P(\bar{A}\cap\bar{B})}{P(\bar{B})} = \frac{1 - P(A \cup B)}{1 - P(B)} \,;
\end{align*}\]</span>
equivalently, we can write
<span class="math display">\[\begin{align*}
P(\bar{A}|\bar{B}) = 1 - P(A|\bar{B}) =  1 - \frac{P(\bar{B}|A)P(A)}{P(\bar{B})} = 1 - \frac{\left[1 - P(B|A)\right]P(A)}{1 - P(B)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>Using a decision tree:</p>
<ul>
<li><span class="math inline">\(P(W_1) = 2/5\)</span> and <span class="math inline">\(P(B_2 \vert W_1) = 3/4\)</span>, so <span class="math inline">\(P(B_2 \cap W_1) = 3/10\)</span>; and</li>
<li><span class="math inline">\(P(B_1) = 3/5\)</span> and <span class="math inline">\(P(B_2 \vert B_1) = 1/2\)</span>, so <span class="math inline">\(P(B_2 \cap B_1) = 3/10\)</span>.</li>
</ul>
<p>Thus, by adding the probabilities of these disjoint events, we determine that <span class="math inline">\(P(B_2) = P(B_2 \cap W_1) + P(B_2 \cap B_1) = 3/5\)</span>.</p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>If we denote the event of knowing the answer as <span class="math inline">\(K\)</span>, <span class="math inline">\(P(K) = 0.6\)</span> and <span class="math inline">\(P(\bar{K}) = 0.4\)</span>, and if we denote the event of getting the question correct as <span class="math inline">\(C\)</span>, we know that <span class="math inline">\(P(C \vert \bar{K}) = 0.2\)</span>. It is implicit in the wording of the question that <span class="math inline">\(P(C \vert K) = 1\)</span>. Ultimately, we want to determine <span class="math inline">\(P(K \vert C)\)</span>:
<span class="math display">\[\begin{align*}
P(K \vert C) = \frac{P(C \vert K)P(K)}{P(C)} = \frac{P(C \vert K)P(K)}{P(C \vert K)P(K)+P(C \vert \bar{K})P(\bar{K})}  = \frac{1 \cdot 0.6}{1 \cdot 0.6 + 0.2 \cdot 0.4} = \frac{60}{68} = \frac{15}{17} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>We are given that <span class="math inline">\(P(S) = 0.15\)</span>, <span class="math inline">\(P(D \vert S) = 10 \cdot P(D \vert \bar{S})\)</span>, and <span class="math inline">\(P(D) = 0.01\)</span>. So
<span class="math display">\[\begin{align*}
P(S \vert D) &amp;= \frac{P(D\vert S)P(S)}{P(D)} = \frac{P(D \vert S)P(S)}{P(D \vert S)P(S) + P(D \vert \bar{S})P(\bar{S})} \\
&amp;= \frac{P(D \vert S)P(S)}{P(D \vert S)P(S) + P(D \vert S)(1 - P(S))/10} \\
&amp;= \frac{P(S)}{P(S)+1/10-P(S)/10} = \cdots = 30/47 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>The compound event of winning is <span class="math inline">\(W = \{ 1\cap3, 1\cap1\cap2, 1\cap1\cap3, \ldots \}\)</span>. Let the event <span class="math inline">\(S\)</span> denote rolling a 1,
with <span class="math inline">\(P(S) = 1/3\)</span>, and the event <span class="math inline">\(F\)</span> denote rolling a 2 or a 3,
with <span class="math inline">\(P(F) = 2/3\)</span>. Then
<span class="math display">\[\begin{align*}
P(W) &amp;= P(1\cap3) + P(1\cap1\cap2) + \ldots = \\
&amp;= \frac{1}{2} P(S) P(F) + P(S)^2 P(F) +  P(S)^3 P(F) + \ldots \\
&amp;= \frac{1}{2}\frac{1}{3}\frac{2}{3} +  P(S)^2 P(F) \left(1 + P(S) + \ldots \right)\\
&amp; = \frac{1}{9} + P(S)^2 P(F) \frac{1}{1 - P(S)} = \frac{1}{9} + \frac{P(S)^2 P(F) }{P(F)} = \frac{1}{9} + P(S^2) = \frac{2}{9} \,.
\end{align*}\]</span>
Alternatively, we can define compound event of losing, <span class="math inline">\(L = \{2, 3, 1\cap2 \}\)</span>,
and write that
<span class="math display">\[\begin{align*}
P(L) &amp;= \frac{1}{3} + \frac{1}{3} + \left(\frac{1}{3}\right)^2 = \frac{7}{9} \\
\Rightarrow ~~~ P(W) &amp;= 1 - P(L) = 1 - \frac{7}{9} = \frac{2}{9} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>There are multiple ways to do this. Here is one:
<span class="math display">\[\begin{align*}
P(H_1 \cup T_2) &amp;= P(H_1) + P(T_2) - P(H_1 \cap T_2) = P(H_1) + P(T_2) - P(T_2 | H_1) P(H_1)\\
&amp;= P(H_1) + P(T_2|H_1)P(H_1) + P(T_2|T_1)P(T_1) - P(T_2 | H_1) P(H_1)\\
&amp;= P(H_1) + P(T_2|T_1)P(T_1) = 0.5 + 0.4 \cdot 0.5 = 0.7 \,.
\end{align*}\]</span>
Note: <span class="math inline">\(P(T_2|T_1) = 1 - P(H_2|T_1)= 0.4\)</span>, since <span class="math inline">\(H_2\)</span> is just <span class="math inline">\(\bar{T}_2\)</span>.</p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>Let <span class="math inline">\(N\)</span> be the event that a sample contains nitrates and <span class="math inline">\(R\)</span> be the event that the sample burns red. We are given that <span class="math inline">\(P(N) = 0.2\)</span>, <span class="math inline">\(P(R|N) = 0.9\)</span>, and <span class="math inline">\(P(R|\bar{N}) = 0.15\)</span>.</p>
<p>(a) We seek <span class="math inline">\(P(R)\)</span>, which is
<span class="math display">\[\begin{align*}
P(R|N)P(N) + P(R|\bar{N})P(\bar{N}) = 0.9 \cdot 0.2 + 0.15 \cdot (1-0.2) = 0.30 \,.
\end{align*}\]</span></p>
<p>(b) We seek <span class="math inline">\(P(N|R)\)</span>, which is
<span class="math display">\[\begin{align*}
\frac{P(R|N)P(N)}{P(R)} = \frac{0.9 \cdot 0.2}{0.3} = 0.60 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) Let <span class="math inline">\(M\)</span>, <span class="math inline">\(V\)</span>, and <span class="math inline">\(O\)</span> denote the event of flying on a major airline, private plane, or other aircraft, respectively, and let <span class="math inline">\(B\)</span> denote the event of being a business traveler. The Law of Total Probability tells us that
<span class="math display">\[\begin{align*}
P(B) &amp;= P(B \vert M) P(M) + P(B \vert V)P(V)+ P(B \vert O)P(O) = 0.5\cdot 0.6 + 0.6\cdot 0.3 + 0.9 \cdot 0.1 = 0.57 \,.
\end{align*}\]</span></p>
<p>(b) We use Bayes’ rule to determine that
<span class="math display">\[\begin{align*}
P(V \vert B) &amp;= \frac{P(B \vert V)P(V)}{P(B)} = \frac{0.6 \cdot 0.3}{0.57} = \frac{6}{19} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>We can use, e.g., a decision tree to determine that the probability mass function for <span class="math inline">\(X\)</span> is</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(p_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">1/2</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">1/2 <span class="math inline">\(\cdot\)</span> 2/3 = 1/3</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">1/2 <span class="math inline">\(\cdot\)</span> 1/3 = 1/6</td>
</tr>
</tbody>
</table>
<p>(a) The expected value is
<span class="math display">\[\begin{align*}
E[X] = \sum_{x=1}^3 x p_X(x) = 1 \cdot \frac{1}{2} + 2 \frac{1}{3} + 3 \frac{1}{6} = \frac{5}{3} \,.
\end{align*}\]</span></p>
<p>(b) We use the shortcut formula to find the variance:
<span class="math display">\[\begin{align*}
E[X^2] &amp;= \sum_{x=1}^3 x^2 p_X(x) = 1 \cdot \frac{1}{2} + 4 \frac{1}{3} + 9 \frac{1}{6} = \frac{10}{3} \\
\Rightarrow ~~~ V[X] &amp;= E[X^2] - E[X]^2 = \frac{10}{3}- \left(\frac{5}{3}\right)^2 = \frac{5}{9} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) Let <span class="math inline">\(W_2\)</span> denote the event of observing two white balls, and let 1, 2, and 3 denote the event of choosing Bowl 1, 2, and 3, respectively. We apply the Law of Total Probability here:
<span class="math display">\[\begin{align*}
P(W_2) &amp;= P(W_2 \vert 1)P(1) + P(W_2 \vert 2)P(2)+ P(W_2 \vert 3)P(3) \\
&amp;= 0 \cdot 1/3 + P(W_2 \vert 2) \cdot 1/3 + 1 \cdot 1/3 \,.
\end{align*}\]</span>
Bowl 2 has 2 white and 1 black balls. Thus <span class="math inline">\(P(W_2 \vert 2) = 2/3 \cdot 1/2 = 1/3\)</span>, since there’s a 2/3 chance of drawing a white ball first, and conditional on that a 1/2 chance of drawing a white ball second. So
<span class="math display">\[\begin{align*}
P(W_2) = 1/3 \cdot 1/3 + 1/3 = 4/9 \,.
\end{align*}\]</span></p>
<p>(b) We seek <span class="math inline">\(P(3 \vert W_2)\)</span>. We apply Bayes’ rule here:
<span class="math display">\[\begin{align*}
P(3 \vert W_2) = \frac{P(W_2 \vert 3)P(3)}{P(W_2)} = \frac{1 \cdot 1/3}{4/9} = 3/4 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>(a) We are given that <span class="math inline">\(P(A) = 0.8\)</span>, <span class="math inline">\(P(\bar{A}) = 0.2\)</span>, <span class="math inline">\(P(F \vert A) = 0.2\)</span>, and <span class="math inline">\(P(F \vert \bar{A}) = 0.1\)</span>. Using the Law of Total Probability, we find that
<span class="math display">\[\begin{align*}
P(F) = P(F \vert A)P(A) + P(F \vert \bar{A})P(\bar{A}) = 0.2 \cdot 0.8 + 0.1 \cdot 0.2 = 0.18 \,.
\end{align*}\]</span></p>
<p>(b) We seek <span class="math inline">\(P(A \vert F)\)</span>. Using Bayes’ rule,
<span class="math display">\[\begin{align*}
P(A \vert F) = \frac{P(F \vert A)P(A)}{P(F)} = \frac{0.2 \cdot 0.8}{0.18} = \frac{0.16}{0.18} = \frac{8}{9} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>We know that
<span class="math display">\[\begin{align*}
E[X] = \sum x p_X(x) = 0 \cdot x + 1 \cdot 0.5 + 2 \cdot x + 3 \cdot 0.3  
= 0.5 + 2x + 0.9 = 1.6 \,.
\end{align*}\]</span>
Thus <span class="math inline">\(p_X(2) = 0.1\)</span>. Since <span class="math inline">\(\sum p_X(x) = 1\)</span>, we also know <span class="math inline">\(p_X(0) = 0.1\)</span>. Next, we compute <span class="math inline">\(E[X^2]\)</span>:
<span class="math display">\[\begin{align*}
E[X^2] = \sum x^2 p_X(x) = 1 \cdot 0.5 + 4 \cdot 0.1 + 9 \cdot 0.3 = 3.6 \,.
\end{align*}\]</span>
So <span class="math inline">\(V[X] = 3.6 - (1.6)^2 = 3.6 - 2.56 = 1.04\)</span>, and <span class="math inline">\(\sigma \approx 1.02\)</span>. Thus
<span class="math display">\[\begin{align*}
P(\mu - \sigma \leq X \leq \mu+\sigma) &amp;= P(1.6-1.02 \leq X \leq 1.6+1.02)  
= P(0.58 \leq X \leq 2.62) \\
&amp;= p_X(1) + p_X(2) = 0.5 + 0.1 = 0.6 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>Note that one can derive the answer without ever determining the value of <span class="math inline">\(c\)</span>:
<span class="math display">\[\begin{align*}
P(X &gt; 1.25 | X &lt; 1.5) &amp;= \frac{P(X &gt; 1.25 \cap X &lt; 1.5)}{P(X &lt; 1.5)} = \frac{P(1.25 &lt; X &lt; 1.5)}{P(X &lt; 1.5)} \\
&amp;= \frac{\int_{5/4}^{3/2}\frac{c}{x^2}dx}{\int_{1}^{3/2}{\frac{c}{x^2}}dx} = \frac{-x^{-1}|_{5/4}^{3/2}}{-x^{-1}|_{1}^{3/2}} = \frac{(4/5 - 2/3)}{(1-2/3)} = \frac{12/15 - 10/15}{5/15} = \frac{2}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>(a) Later, we will recognize that this is a beta distribution and we thus would be able to use its properties to derive, e.g., a value for <span class="math inline">\(c\)</span>. In the meantime, we will apply brute-force integration:
<span class="math display">\[\begin{align*}
\frac{1}{c} &amp;= \left( \int_0^1 x^2 dx - \int_0^1 x^3 \right) = \left. \frac{x^3}{3} \right|_0^1 - \left. \frac{x^4}{4} \right|_0^1 = \frac13 - \frac14 = \frac{1}{12} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(c = 12\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
P(X &gt; 1/2) &amp;= 12 \int_{1/2}^1 x^2(1-x) dx = 12 \left[ \frac{x^3}{3}\bigg|_{1/2}^1 - \frac{x^4}{4}\bigg|_{1/2}^1 \right] = \left(4 - \frac{1}{2}\right) - \left(3 - \frac{3}{16}\right) = \frac{11}{16} \,.
\end{align*}\]</span></p>
<p>(c) The expected value is
<span class="math display">\[\begin{align*}
E[X] &amp;= 12 \left( \int_0^1 x^3 dx - \int_0^1 x^4 \right) = 12 \left. \frac{x^4}{4} \right|_0^1 - \left. \frac{x^5}{5} \right|_0^1 = 12 \left( \frac14 - \frac15 \right) = \frac{12}{20} = \frac35 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>The following 2 <span class="math inline">\(\times\)</span> 2 table shows the possible outcomes of the experiment, i.e., the values of <span class="math inline">\(X = \vert Y_2 - Y_1 \vert\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(Y_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(Y_2 = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(Y_1 = 0\)</span></td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(Y_1 = 1\)</span></td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Each outcome is equally likely. Hence <span class="math inline">\(P(X = 0) = 1/2\)</span> and <span class="math inline">\(P(X=1) = 1/2\)</span>.</p>
<p>(a) We seek <span class="math inline">\(\sigma = \sqrt{V[X]} = \sqrt{E[X^2]-(E[X])^2}\)</span>:
<span class="math display">\[\begin{align*}
E[X] &amp;= 0 \cdot 1/2 + 1 \cdot 1/2 = 1/2~~\mbox{and}\\
E[X^2] &amp;= 0^2 \cdot 1/2 + 1^2 \cdot 1/2 = 1/2 \,.
\end{align*}\]</span>
So <span class="math inline">\(\sigma = \sqrt{1/2-1/4} = 1/2\)</span>.</p>
<p>(b) The expected value is <span class="math inline">\(\mu = E[X] = 1/2\)</span>, while the standard deviation is <span class="math inline">\(\sigma = 1/2\)</span>, so ultimately we are asking for <span class="math inline">\(P(1/2-1/2 &lt; X &lt; 1/2+1/2) = P(0 &lt; X &lt; 1)\)</span>…which equals zero because there are no probability masses between 0 and 1 exclusive.</p>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) The mean of the distribution is
<span class="math display">\[\begin{align*}
E[X] = \int_0^1 x f_X(x) dx = \int_0^1 3x^3 dx = \left. \frac{3}{4}x^4 \right|_0^1 = \frac{3}{4} \,.
\end{align*}\]</span></p>
<p>(b) We first compute <span class="math inline">\(E[X^2]\)</span>:
<span class="math display">\[\begin{align*}
E[X^2] = \int_0^1 x^2 f_X(x) dx = \int_0^1 3x^4 dx = \left. \frac{3}{5}x^5 \right|_0^1 = \frac{3}{5} \,.
\end{align*}\]</span>
We then apply the shortcut formula to determine the variance:
<span class="math display">\[\begin{align*}
V[X] = E[X^2] - (E[X])^2 = \frac{3}{5} - \left(\frac34\right)^2 = \frac{3}{80} \,.
\end{align*}\]</span></p>
<p>(c) The expected value of the difference is
<span class="math display">\[\begin{align*}
E[2X_1 - 2X_2] = 2E[X_1] - 2E[X_2] = 2 \cdot \frac34 - 2 \cdot \frac34 = 0 \,.
\end{align*}\]</span></p>
<p>(d) The variance of the difference is
<span class="math display">\[\begin{align*}
V[2X_1 - 2X_2] = 4V[X_1] + 4V[X_2] = 4\left(\frac{3}{80} + \frac{3}{80}\right) = \frac{3}{10} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>(a) We are dealing with a continuous random variable. The pdf is thus
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx}F_X(x) = 3x^2 ~~~~~~ x \in [0,1] \,.
\end{align*}\]</span></p>
<p>(b) Given <span class="math inline">\(f_X(x) = 3x^2\)</span> from part (a), we can compute <span class="math inline">\(V[X]\)</span>:
<span class="math display">\[\begin{align*}
V[X] &amp;= E[X^2] - (E[X])^2 \\
&amp;= \int_0^1 x^2 f_X(x) dx - \left[\int_0^1 x f_X(x) dx\right]^2 = \int_0^1 3 x^4 dx - \left[\int_0^1 3 x^3 dx\right]^2  \\
&amp;= \left.\frac{3x^5}{5}\right|_0^1 - \left[ \left.\frac{3x^4}{4}\right|_0^1 \right]^2 = \frac{3}{5} - \left( \frac{3}{4} \right)^2 = \frac{48}{80} - \frac{45}{80} = \frac{3}{80} \,.
\end{align*}\]</span></p>
<p>(c) We seek <span class="math inline">\(P\left(X &lt; \frac{1}{2} \vert X &gt; \frac{1}{4}\right)\)</span>:
<span class="math display">\[\begin{align*}
P\left(X &lt; \frac{1}{2} \vert X &gt; \frac{1}{4}\right) &amp;= \frac{P\left(X &lt; \frac{1}{2} \cap X &gt; \frac{1}{4}\right)}{P\left(X &gt; \frac{1}{4}\right)} = \frac{P\left(\frac{1}{4} &lt; X &lt; \frac{1}{2}\right)}{P\left(X &gt; \frac{1}{4}\right)}  \\
&amp;= \frac{F_X\left(\frac{1}{2}\right)-F_X\left(\frac{1}{4}\right)}{1 - F_X\left(\frac{1}{4}\right)} = \frac{\left(\frac{1}{2}\right)^3 - \left(\frac{1}{4}\right)^3}{1 - \left(\frac{1}{4}\right)^3} = \frac{\frac{1}{8} - \frac{1}{64}}{1 - \frac{1}{64}} = \frac{\frac{7}{64}}{\frac{63}{64}} = \frac{1}{9} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>First, we write down the pmf:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_X(x)\)</span></td>
<td align="center">0.5</td>
<td align="center">0.25</td>
<td align="center">0.25</td>
</tr>
</tbody>
</table>
<p>Second, we compute <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span>:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=-a}^{a} x p_X(x) = 0 \cdot 0.5 + 1\cdot 0.25 + 3 \cdot 0.25 = 1 \,. \\
E[X^2] &amp;= \sum_{x=-a}^{a} x^2 p_X(x) = 0 \cdot 0.5 + 1\cdot 0.25 + 3^2 \cdot 0.25 = 2.5 \,.
\end{align*}\]</span>
Thus <span class="math inline">\(V[X] = E[X^2] - E[X]^2 = 2.5 - 1 = 1.5\)</span>.</p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>(a) We know that <span class="math inline">\(F_X(1) = 1\)</span>, so <span class="math inline">\(c(1^2) = 1 ~~~ \Rightarrow ~~~ c = 1\)</span>.</p>
<p>(b) Given that <span class="math inline">\(f_X(x) = 2x\)</span>, for <span class="math inline">\(0 \leq x \leq 1\)</span>,
<span class="math display">\[\begin{align*}
E[X] = \int_0^1 x 2x dx = \frac{2}{3}x^3\bigg|_0^1 = \frac{2}{3} \,.
\end{align*}\]</span></p>
<p>(c) We utilize the shortcut formula to determine the standard deviation:
<span class="math display">\[\begin{align*}
E[X^2] &amp;= \int_0^1 x^2 2x dx = \frac{2}{4}x^4\bigg|_0^1 = \frac{1}{2} \\
V[X] &amp;= E[X^2] - E[X]^2 = \frac{1}{2} - \frac{4}{9} = \frac{1}{18} \\
\Rightarrow ~~~ \sigma &amp;= \sqrt{V[X]} = \sqrt{\frac{1}{18}} = 0.236 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>(a) If <span class="math inline">\(b\)</span> is larger <span class="math inline">\(b\)</span>, then <span class="math inline">\(a\)</span> is smaller. Given that the minimum value of <span class="math inline">\(a\)</span> for a valid pdf is zero, we can determine that
<span class="math display">\[\begin{align*}
\int_1^2 bx dx = 1 = b \frac{x^2}{1}\bigg|_1^2 = b\left(\frac{4}{2} - \frac{1}{2} \right) ~~~ \Rightarrow ~~~ b = \frac{2}{3} \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
F_X(x) = \left\{ \begin{array}{cc} 0 &amp; x \leq 0 \\ \int_0^xa dz = az\bigg|_0^x = ax = \frac{1}{2}x &amp; x \in (0,1] \\ \int_0^1 a dz + \int_1^xbz dz = F_X(1) + b\frac{z^2}{2}\bigg|_0^x = \frac{1}{2} + \frac{1}{6}(x^2 - 1) &amp; x \in (1,2] \\ 1 &amp; x &gt; 2 \end{array} \right. \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) <span class="math inline">\(E[X] =  2 = 1(0.4) + y (0.2) ~~~ \Rightarrow ~~~ y = 8\)</span>.</p>
<p>(b) <span class="math inline">\(E[X] =  1(0.4) + 3 (0.2)  = 1\)</span> and <span class="math inline">\(E[X^2] =  1(0.4) + 9 (0.2)  = 2.2\)</span>. Therefore
<span class="math display">\[\begin{align*}
\sigma = \sqrt{E[X^2] - E[X]^2} = \sqrt{1.2} \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(P(\mu - \sigma \leq X \leq \mu + \sigma) = P(1 -\sqrt{1.2} \leq  X \leq 1 +\sqrt{1.2} ) = p_X(0) + p_X(1) = 0.8\)</span>.</p>
<hr />
<ul>
<li>Problem 26</li>
</ul>
<p>(a) <span class="math inline">\(\int_0^1 c \, dx = 1 - 0.2 - 0.2 = 0.6 = cx\bigg|_0^1 = c \, \Rightarrow \, c = 0.6\)</span>.</p>
<p>(b) <span class="math inline">\(E[X] = (-1)(0.2) + \int_0^1 0.6x dx + 2 (0.2) = 0.2 + 0.3x^2|_0^1 = 0.5\)</span>.</p>
<p>(c) The cdf is constantly <span class="math inline">\(0\)</span> before <span class="math inline">\(-1\)</span>; constantly <span class="math inline">\(0.2\)</span> between <span class="math inline">\([-1,0)\)</span>; a line that connects these two points <span class="math inline">\((0, 0.2) - (1,0.8)\)</span> between <span class="math inline">\([0,1)\)</span>; constantly <span class="math inline">\(0.8\)</span> between <span class="math inline">\([1,2)\)</span>; then constantly <span class="math inline">\(1\)</span> after <span class="math inline">\(2\)</span>.</p>
<hr />
<ul>
<li>Problem 27</li>
</ul>
<p>(a) <span class="math inline">\(F_X(x) = \int_0^x f_Z(z) dz =\int_0^x e^{-z} dz = -e^{-z} \bigg|_0^x = 1-e^{-x}\)</span>.</p>
<p>(b) <span class="math inline">\(P(X &gt; 1) = 1 - F_X(1) = 1 - (1 - e^{-1}) = e^{-1}\)</span>.</p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
P(X &lt; 1/2 \cup X&gt;2) &amp;= F_X(1/2) + (1 - F_X(2)) \\
&amp;= 1 - e^{-1/2} + (1  - (1 - e^{-2})) = 1 - e{-1/2} + e^{-2} \,.
\end{align*}\]</span></p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
P(X &lt; 2 | X&gt;1) = \frac{P(1 &lt; X &lt; 2)}{P(X&gt;1)} = \frac{F_X(2) - F_X(1)}{1 - F_X(1)} = \frac{(1 - e^{-2}) - (1 - e^{-1})}{1 - (1 - e^{-1})} =1 - e^{-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 28</li>
</ul>
<p>(a) The pdf is symmetric around zero <span class="math inline">\(\Rightarrow E[X] = 0\)</span>:
<span class="math display">\[\begin{align*}
E[X] = \int_{-1}^0 -x^2 dx + \int_0^1 x^2 dx = - \frac{x^3}{3}\bigg|_{-1}^0 + \frac{x^3}{3}\bigg|_{0}^1 = -\frac{1}{3} + \frac{1}{3} = 0 \,.
\end{align*}\]</span></p>
<p>(b) The variance is
<span class="math display">\[\begin{align*}
V[X] = E[X^2] - E[X]^2 =  \int_{-1}^0 -x^3 dx  + \int_0^1 x^3 dx = - \frac{x^4}{3}\bigg|_{-1}^0 + \frac{x^4}{4}\bigg|_{0}^1 + \frac{x^4}{4}\bigg|_{0}^1 = \frac{1}{4} + \frac{1}{4} = \frac{1}{2} \,.
\end{align*}\]</span></p>
<p>(c) We have that <span class="math inline">\(F_X(0) = \frac{1}{2}\)</span>…thus:
<span class="math display">\[\begin{align*}
F_X(x) \text{ for } x\in\left[0,1\right] = \frac{1}{2} + \int_0^x z dz = \frac{1}{2} + \frac{z^2}{2}\bigg|_0^x = \frac{1}{2} + \frac{x^2}{2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 29</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx}F_X(x) = \begin{cases} 0 &amp; \text{ if } x \leq 0\\ 2cx &amp; \text{ if } x \in (0, 1]\\ 0 &amp;\text{ if } x&gt;1 \end{cases} \,.
\end{align*}\]</span>
At <span class="math inline">\(x = 1\)</span>, <span class="math inline">\(cx^2 = 1\)</span>, and so <span class="math inline">\(c = 1\)</span>.</p>
<p>(b) <span class="math inline">\(E[X] = \int_0^1 2x^2 dx = \frac{2}{3}\bigg|_0^1 = \frac{2}{3}\)</span>.</p>
<hr />
<ul>
<li>Problem 30</li>
</ul>
<p>(a) A valid pdf has integral 1 over the domain. Thus
<span class="math display">\[\begin{align*}
\int_0^1 c dx + \int_1^\infty e^{-x} dx &amp;= 1 \\
c \int_0^1 dx &amp;= 1 - \int_1^\infty e^{-x} dx \\
c \cdot 1 &amp;= 1 + \left. e^{-x}\right|_1^\infty \\
c &amp;= 1 + (0 - e^{-1}) = 1 - e^{-1} \,.
\end{align*}\]</span></p>
<p>(b) The cdf for <span class="math inline">\(x \in [0,1)\)</span> is <span class="math inline">\(c \int_0^x dz = cx\)</span>. For <span class="math inline">\(x \in [1,\infty)\)</span>, we have
<span class="math display">\[\begin{align*}
\int_0^x f(z) dz &amp;= \int_0^1 c dz + \int_1^x e^{-z} dz \\
&amp;= c - \left. e^{-z}\right|_1^x = c - (e^{-x} - e^{-1}) = c + (e^{-1} - e^{-x}) \,.
\end{align*}\]</span>
The cdf is thus</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\((-\infty,0)\)</span></th>
<th align="center"><span class="math inline">\([0,1)\)</span></th>
<th align="center"><span class="math inline">\([1,\infty)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(F_X(x)\)</span></td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(cx\)</span></td>
<td align="center"><span class="math inline">\(c + (e^{-1} - e^{-x})\)</span></td>
</tr>
</tbody>
</table>
<hr />
<ul>
<li>Problem 31</li>
</ul>
<p>(a) The sum of the probability masses has to equal 1, so <span class="math inline">\(c = 1 - 1/6 - 1/6 = 2/3\)</span>.</p>
<p>(b) The cdf is 0 for <span class="math inline">\(x \in (-\infty,-1)\)</span>, then 1/6 for <span class="math inline">\(x \in [-1,0)\)</span>, then 1/6+2/3 = 5/6 for <span class="math inline">\(x \in [0,1)\)</span>…thus <span class="math inline">\(F_X(0.5) = 5/6\)</span>.</p>
<p>(c) The generalized inverse cdf is
<span class="math display">\[\begin{align*}
x = F_X^{-1}(q) = \mbox{inf}\{ x : F_X(x) \geq q \} \,,
\end{align*}\]</span>
i.e., the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q = 0.9\)</span>. Given that the cdf jumps from 5/6 (=0.833) to 1 at <span class="math inline">\(x=1\)</span>, the value that we want is <span class="math inline">\(x=1\)</span>.</p>
<p>(d) The variance is <span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x} x p_X(x) = -1 \cdot \frac16 + 0 \cdot \frac23 + 1 \cdot \frac16 = 0 \,. \\
E[X^2] &amp;= \sum_{x} x^2 p_X(x) = (-1)^2 \cdot \frac16 + (0)^2 \cdot \frac23 + (1)^2 \cdot \frac16 = \frac13 \,.
\end{align*}\]</span>
Thus <span class="math inline">\(V[X] = \frac13\)</span>.</p>
<hr />
<ul>
<li>Problem 32</li>
</ul>
<p>(a) In the range <span class="math inline">\([0,1]\)</span>, <span class="math inline">\(f_X(x)\)</span> is
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx} x^3 = 3x^2 \,.
\end{align*}\]</span></p>
<p>(b) The probability is
<span class="math display">\[\begin{align*}
P\left(\frac14 \leq X \leq \frac34\right) &amp;= F_X\left(\frac34\right) - F_X\left(\frac14\right) = \left(\frac34\right)^3 - \left(\frac14\right)^3 = \frac{27}{64} - \frac{1}{64} = \frac{26}{64} = \frac{13}{32} \,.
\end{align*}\]</span></p>
<p>(c) The conditional probability is
<span class="math display">\[\begin{align*}
P\left(X \leq \frac14 \vert X \leq \frac12\right) &amp;= \frac{P\left(X \leq \frac14 \cap X \leq \frac12\right)}{P\left(X \leq \frac12\right)} = \frac{P\left(X \leq \frac14\right)}{P\left(X \leq \frac12\right)} \\
&amp;= \frac{F_X\left(\frac14\right)}{F_X\left(\frac12\right)} = \frac{(1/4)^3}{(1/2)^3} = \frac{8}{64} = \frac18 \,.
\end{align*}\]</span></p>
<p>(d) The inverse cdf is given by
<span class="math display">\[\begin{align*}
q = x^3 ~~~ \Rightarrow ~~~ x = F_X^{-1}(q) = q^{1/3} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 33</li>
</ul>
<p>(a) The expected value is
<span class="math display">\[\begin{align*}
E[Y] = E[X_1+2X_2-3X_3-4] = E[X_1] + 2E[X_2] - 3E[X_3] - 4 = 1 + 2 - 3 - 4 = -4 \,.
\end{align*}\]</span></p>
<p>(b) The variance is
<span class="math display">\[\begin{align*}
V[Y] = V[X_1+2X_2-3X_3-4] = V[X_1] + 4V[X_2] + 9V[X_3] = 1 + 4 + 9 = 14 \,.
\end{align*}\]</span></p>
<p>(c) By “reversing” the shortcut formula, we find that
<span class="math display">\[\begin{align*}
E[Y^2] = V[Y] + (E[Y])^2 = 14 + (-4)^2 = 30 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 34</li>
</ul>
<p>(a) We know that <span class="math inline">\(F_X(2) = 1 = c \cdot 2^3\)</span>, so <span class="math inline">\(c = 1/8\)</span>.</p>
<p>(b) <span class="math inline">\(f_X(x) = (d/dx)F_X(x) = 3cx^2\)</span>.</p>
<p>(c) The conditional probability is
<span class="math display">\[\begin{align*}
P(X &lt; 1/2 \vert X &lt; 1) &amp;= \frac{P(X &lt; 1/2 \cap X &lt; 1)}{P(X &lt; 1)} = \frac{P(X &lt; 1/2)}{P(X &lt; 1)} = \frac{F_X(1/2)}{F_X(1)} = \frac{(1/2)^3}{1^3} = 1/8 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 35</li>
</ul>
<p>We are given that <span class="math inline">\(X \vert \theta\)</span> is a continuous random variable, and that <span class="math inline">\(\Theta\)</span> is a discrete random variable, so the appropriate Law of Total Probability expression is
<span class="math display">\[\begin{align*}
f_X(x) &amp;= \sum_{\theta} f_{X \vert \theta}(x \vert \theta) p_{\Theta}(\theta) \\
&amp;= \sum_{\theta} \theta x^{\theta-1} p_{\Theta}(\theta) \\
&amp;= 1 \cdot x^{1-1} \cdot p_{\Theta}(\theta = 1) + 2 \cdot x^{2-1} \cdot p_{\Theta}(\theta = 2) \\
&amp;= 1 \cdot x^{1-1} \cdot 1/2 + 2 \cdot x^{2-1} \cdot 1/2 \\
&amp;= x + 1/2 \,,
\end{align*}\]</span>
with <span class="math inline">\(x \in [0,1]\)</span>.</p>
<hr />
<ul>
<li>Problem 36</li>
</ul>
<p>(a) The expected value <span class="math inline">\(E[X]\)</span> is
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_0^1 x f_X(x) dx = \int_0^1 x 6 x (1-x) dx \\
&amp;= 6 \left[ \int_0^1 x^2 dx - \int_0^1 x^3 dx \right] = 6 \left[ \left.\frac{x^3}{3}\right|_0^1 - \left.\frac{x^4}{4}\right|_0^1 \right] = 6 \left( \frac{1}{3} - \frac{1}{4} \right) = 6 \frac{1}{12} = \frac{1}{2} \,.
\end{align*}\]</span></p>
<p>(b) We know that for any distribution, <span class="math inline">\(E[\bar{X}] = \mu = E[X]\)</span>. Hence <span class="math inline">\(E[\bar{X}] = 1/2\)</span>.</p>
<hr />
<ul>
<li>Problem 37</li>
</ul>
<p>(a) The biases are
<span class="math display">\[\begin{align*}
B[\hat{\mu}_1] &amp;= E[\hat{\mu}_1 - \mu] = E[\hat{\mu}_1] = E\left[X_1-X_2\right] = E[X_1] - E[X_2] = \mu - \mu = 0 \\
B[\hat{\mu}_2] &amp;= E[\hat{\mu}_2 - \mu] = E[\hat{\mu}_2] = E\left[\frac{X_1+X_2}{2}\right] = \frac12 \left(E[X_1] + E[X_2]\right) = \frac12 \left(\mu + \mu\right) = 0 \,.
\end{align*}\]</span>
Both estimators are unbiased.</p>
<p>(b) The variances are
<span class="math display">\[\begin{align*}
V[\hat{\mu}_1] &amp;= V\left[X_1-X_2\right] = V[X_1] + V[X_2] = \sigma^2 + \sigma^2 = 2\sigma^2 = \frac23\\
V[\hat{\mu}_2] &amp;= V\left[\frac{X_1+X_2}{2}\right] = \frac14 \left(V[X_1] + V[X_2]\right) = \frac14 \left(\sigma^2 + \sigma^2\right) = \frac{\sigma^2}{2} = \frac16\,.
\end{align*}\]</span></p>
<p>(c) The mean-squared errors are
<span class="math display">\[\begin{align*}
MSE[\hat{\mu}_1] &amp;= (B[\hat{\mu}_1])^2 + V[\hat{\mu}_1] = 0^2 + 2\sigma^2 = 2\sigma^2 = \frac23 \\
MSE[\hat{\mu}_2] &amp;= (B[\hat{\mu}_2])^2 + V[\hat{\mu}_2] = 0^2 + \frac{\sigma^2}{2} = \frac{\sigma^2}{2} = \frac16 \,.
\end{align*}\]</span></p>
<p>(d) <span class="math inline">\(\hat{\mu}_2\)</span> has the lower mean-squared error, so it is the better estimator.</p>
<hr />
<ul>
<li>Problem 38</li>
</ul>
<p>(a) The likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(a \vert \mathbf{x}) &amp;= \prod_{i=1}^n f_X(x_i) = \prod_{i=1}^n a (1+x_i)^{-(a+1)} = a^n [\prod_{i=1}^n (1+x_i)]^{-(a+1)} \,.
\end{align*}\]</span>
The log-likelihood function is thus
<span class="math display">\[\begin{align*}
\ell(a \vert \mathbf{x}) &amp;= \log \left[ a^n [\prod_{i=1}^n (1+x_i)]^{-(a+1)} \right] = \log a^n + \log \left[ [\prod_{i=1}^n (1+x_i)]^{-(a+1)} \right] \\
&amp;= n \log a - (a+1) \log [\prod_{i=1}^n (1+x_i)] = n \log a - (a+1) \sum_{i=1}^n \log (1+x_i) \,.
\end{align*}\]</span></p>
<p>(b) The maximum likelihood estimate is
<span class="math display">\[\begin{align*}
\frac{d}{da} \ell(a \vert \mathbf{x}) &amp;= \frac{n}{a} - \sum_{i=1}^n \log (1+x_i) = 0 \\
\Rightarrow ~~~ \frac{n}{a} &amp;= \sum_{i=1}^n \log (1+x_i) \\
\Rightarrow ~~~ a &amp;= \frac{n}{\sum_{i=1}^n \log (1+x_i)} \,.
\end{align*}\]</span>
We rewrite this into “probabilistic” notation:
<span class="math display">\[\begin{align*}
\hat{a}_{MLE} = \frac{n}{\sum_{i=1}^n \log (1+X_i)} \,.
\end{align*}\]</span></p>
<p>(c) We invoke the invariance property of MLEs to write that
<span class="math display">\[\begin{align*}
\hat{\mu}_{MLE} &amp;= \frac{1}{\hat{a}_{MLE}-1} = \frac{1}{[n/\sum_{i=1}^n \log (1+X_i)]-1} = \frac{\sum_{i=1}^n \log (1+X_i)}{n-\sum_{i=1}^n \log (1+X_i)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 39</li>
</ul>
<p>(a) The log-likelihood is
<span class="math display">\[\begin{align*}
\ell(\theta \vert \mathbf{x}) &amp;= \sum_{i=1}^n \log f_X(x_i \vert \theta) = \sum_{i=1}^n \log \theta + (\theta-1) \log(1-x_i) = n \log \theta + (\theta - 1) \sum_{i=1}^n \log(1-x_i) \,.
\end{align*}\]</span></p>
<p>(b) The first derivative of <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> is
<span class="math display">\[\begin{align*}
\frac{d}{d\theta} \left[ n \log \theta + (\theta - 1) \sum_{i=1}^n \log(1-x_i) \right] &amp;= \frac{n}{\theta} + \sum_{i=1}^n \log(1-x_i) \,.
\end{align*}\]</span>
We set this equal to zero and solve for <span class="math inline">\(\theta\)</span>:
<span class="math display">\[\begin{align*}
\hat{\theta}_{MLE} = -\frac{n}{\sum_{i=1}^n \log(1-X_i)} \,.
\end{align*}\]</span></p>
<p>(c) We utilize the invariance property of the MLE:
<span class="math display">\[\begin{align*}
\hat{\mu}_{MLE} = \frac{1}{1 + \hat{\theta}_{MLE}} = \frac{\sum_{i=1}^n \log(1-X_i)}{\left[ \sum_{i=1}^n \log(1-X_i) \right] - n} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 40</li>
</ul>
<p>We first derive the likelihood
<span class="math display">\[\begin{align*}
\ell(x_1, x_2, \dots, x_n | \alpha) &amp;= \sum_{i=1}^n \left[ \log \alpha + (\alpha-1) \log x_i \right] = n \log \alpha + (\alpha - 1) \sum_{i=1}^n \log x_i\\
\implies ~~~ \ell&#39;(x_1, x_2, \dots, x_n | \alpha) &amp;= \frac{n}{\alpha} + \sum_{i=1}^n \log x_i \,.
\end{align*}\]</span>
We set <span class="math inline">\(\frac{n}{\hat{\alpha}_{MLE}} + \sum_{i=1}^n \log x_i = 0\)</span> and find that
<span class="math display">\[\begin{align*}
\hat{\alpha}_{MLE} = \frac{n}{- \sum_{i=1}^n \log X_i)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 41</li>
</ul>
<p>(a) The log-likelihood and its derivative are
<span class="math display">\[\begin{align*}
\ell (x_1,\dots, x_n | p) &amp;= \log (1-p) \sum_{i=1}^n (x_i - 1)  + n \log p\\
\implies \ell&#39; (x_1,\dots, x_n | \hat{p}) &amp;= - \frac{\sum_{i=1}^n x_i - n}{1 - \hat{p}} + \frac{n}{\hat{p}} = 0 \,.
\end{align*}\]</span>
From
<span class="math display">\[\begin{align*}
\frac{\sum_{i=1}^n x_i - n}{1 - \hat{p}} = \frac{n}{\hat{p}} \quad \Rightarrow \quad \left(\sum_{i=1}^n x_i - n\right) \hat{p} = n (1 - \hat{p}) \quad \Rightarrow \quad \hat{p} \,\sum_{i=1}^n x_i = n \,,
\end{align*}\]</span>
it follows that <span class="math inline">\(\hat{p}_{MLE} = \frac{n}{\sum_{i=1}^n X_i}\)</span>. By the invariance property of the MLE, we find that
<span class="math display">\[\begin{align*}
\widehat{1/p}_{MLE} = \frac{\sum_{i=1}^n X_i}{n} \,.
\end{align*}\]</span></p>
<p>(b) The variance of this estimator is
<span class="math display">\[\begin{align*}
V\left[\widehat{1/p}_{MLE} \right] = V\left[\frac{\sum_{i=1}^n X_i}{n} \right] = \frac{V(X_1)}{n} = \frac{1-p}{np^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 42</li>
</ul>
<p>(a) The bias of the estimator is
<span class="math display">\[\begin{align*}
B[\hat{\theta}] = E[\hat{\theta}-\theta] = E[\hat{\theta}]-\theta = E\left[\frac{X_1-X_2}{2}\right] - \theta = \frac12 ( E[X_1] - E[X_2] ) - \theta  = \frac12 ( \theta - \theta ) - \theta = -\theta \,.
\end{align*}\]</span></p>
<p>(b) The variance of the estimator is
<span class="math display">\[\begin{align*}
V[\hat{\theta}] = V\left[\frac{X_1-X_2}{2}\right] = \frac14(V[X_1]+V[X_2]) = \frac14 \frac{2\theta^2}{12} = \frac{\theta^2}{6} \,.
\end{align*}\]</span></p>
<p>(c) The mean-squared error of the estimator is
<span class="math display">\[\begin{align*}
MSE[\hat{\theta}] = (B[\hat{\theta}])^2 + V[\hat{\theta}] = \theta^2 + \frac{\theta^2}{6} = \frac76\theta^2 \,.
\end{align*}\]</span></p>
<p>(d) Changing the sign does not change the variance! And “unbiased” means the bias is zero. So:
<span class="math display">\[\begin{align*}
MSE[\hat{\theta}&#39;] = V[\hat{\theta}] = \frac{\theta}{12} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 43</li>
</ul>
<p>(a) The bias of the estimator is
<span class="math display">\[\begin{align*}
B[\hat{\theta}] = E[\hat{\theta}-\theta] = E\left[X_1 - \frac{X_2}{n}\right] - \theta = E[X_1] - \frac{E[X_2]}{n} - \theta = \theta - \frac{\theta}{n} - \theta = -\frac{\theta}{n} \,.
\end{align*}\]</span></p>
<p>(b) The variance of the estimator is
<span class="math display">\[\begin{align*}
V[\hat{\theta}] = V\left[X_1 - \frac{X_2}{n}\right] = V[X_1] + \frac{1}{n^2}V[X_2] = \left(1+\frac{1}{n^2}\right)\theta^2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 44</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) &amp;= \prod_{i=1}^n f_X(x_i \vert \theta) = \prod_{i=1}^n c e^{-\theta x_i} \theta^{x_i-1} = c^n e^{-\theta \sum_{i=1}^n x_i} \theta^{\sum_{i=1}^n x_i - n} \,.
\end{align*}\]</span>
Thus the log-likelihood is
<span class="math display">\[\begin{align*}
\ell(\theta \vert \mathbf{x}) &amp;= n \log c - \theta \sum_{i=1}^n x_i + (\sum_{i=1}^n x_i - n) \log \theta \,.
\end{align*}\]</span></p>
<p>(b) The derivative of <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> with respect
to <span class="math inline">\(\theta\)</span> is
<span class="math display">\[\begin{align*}
\ell&#39;(\theta \vert \mathbf{x}) &amp;= - \sum_{i=1}^n x_i + \frac{\sum_{i=1}^n x_i - n}{\theta} \,.
\end{align*}\]</span>
Setting this to zero and solving for <span class="math inline">\(\theta\)</span>, we get
<span class="math display">\[\begin{align*}
\hat{\theta}_{MLE} = \frac{\sum_{i=1}^n X_i - n}{\sum_{i=1}^n X_i} = \frac{\bar{X}-1}{\bar{X}} \,.
\end{align*}\]</span></p>
<p>(c) We utilize the invariance property of the MLE:
<span class="math display">\[\begin{align*}
\hat{\mu}_{MLE} = \frac{1}{1-\hat{\theta}_{MLE}} = \frac{1}{1-(\bar{X}-1)/\bar{X})} = \frac{\bar{X}}{\bar{X}-(\bar{X}-1)} = \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 45</li>
</ul>
<p>(a) The cdf for the given pdf is
<span class="math display">\[\begin{align*}
F_Y(y) = \int_0^y a(1+v)^{-a-1} dv \,.
\end{align*}\]</span>
This is a <span class="math inline">\(u^n du\)</span>-style integral, meaning that
<span class="math display">\[\begin{align*}
F_Y(y) &amp;= \int_0^y a(1+v)^{-a-1} dv = a \frac{1}{-a} \left. (1+v)^{-a}\right|_0^y = -\left((1+y)^{-a} - 1^{-a}\right) = 1 - (1+y)^{-a} \,.
\end{align*}\]</span></p>
<p>(b) <span class="math inline">\(E[Y]\)</span> <em>decreases</em> with <span class="math inline">\(a\)</span>; combining that fact with our desire to determine a lower bound, we focus on the fourth line of the confidence interval reference table and solve
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert a) - \alpha &amp;= 0 \\
\Rightarrow ~~~ 1 - (1+y_{\rm obs})^{-a} - \alpha &amp;= 0 \\
\Rightarrow ~~~ (1+y_{\rm obs})^{-a} &amp;= 1 - \alpha \\
\Rightarrow ~~~ -a \log (1+y_{\rm obs}) &amp;= \log(1-\alpha) \\
\Rightarrow ~~~ \hat{a}_L &amp;= -\frac{\log(1-\alpha)}{\log(1+y_{\rm obs})} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 46</li>
</ul>
<p>(a) The fact that we wish to perform a lower-tail test, combined with the fact that <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\mu\)</span>, means that we will focus on the third line of the hypothesis test reference table and solve for
<span class="math display">\[\begin{align*}
y_{\rm RR} = F_Y^{-1}(\alpha \vert \mu_o) \,.
\end{align*}\]</span>
We have that
<span class="math display">\[\begin{align*}
F_Y(y_{\rm RR} \vert \mu_o) - \alpha &amp;= 0 \\
\Rightarrow ~~~ \frac{1}{1+\exp(-y_{\rm RR})} &amp;= \alpha \\
\Rightarrow ~~~ 1+\exp(-y_{\rm RR}) &amp;= \frac{1}{\alpha} \\
\Rightarrow ~~~ \exp(-y_{\rm RR}) &amp;= \frac{1}{\alpha}-1 \\
\Rightarrow ~~~ y_{\rm RR} &amp;= -\log\left(\frac{1}{\alpha}-1\right) \,.
\end{align*}\]</span>
Since <span class="math inline">\(1/0.05 = 20\)</span>, we find that <span class="math inline">\(y_{\rm RR} = -\log(19)\)</span>.</p>
<p>(b) As stated in the hypothesis test reference table, the rejection region is <span class="math inline">\(y_{\rm obs} &lt; y_{\rm RR}\)</span>. Thus if <span class="math inline">\(y_{\rm obs} \geq y_{\rm RR}\)</span>, we fail to reject the null hypothesis.</p>
<p>(c) If we go from a lower-tail test (with <span class="math inline">\(E[Y]\)</span> increasing with <span class="math inline">\(\mu\)</span>) to a two-tail test, then the lower rejection region boundary will shift because we put <span class="math inline">\(\alpha/2 = 0.025\)</span> into the equation rather than <span class="math inline">\(\alpha = 0.05\)</span>. This moves the boundary further from <span class="math inline">\(\mu_o = 0\)</span>. (Specifically, <span class="math inline">\(1/\alpha\)</span> would now be 40, so the new <span class="math inline">\(y_{\rm RR} = -\log(39)\)</span>, which is further from 0 than <span class="math inline">\(-\log(19)\)</span>.)</p>
</div>
<div id="chapter-2" class="section level2 unnumbered hasAnchor">
<h2>Chapter 2<a href="chapter-exercises-solutions.html#chapter-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>(a) We can rewrite the conditional expression as the ratio of two unconditional probabilities and work from there:
<span class="math display">\[\begin{align*}
P(X&gt;2 \vert X&gt;0) &amp;= \frac{P(X&gt;2 \cap X&gt;0)}{P(X&gt;0)} = \frac{P(X&gt;2)}{P(X&gt;0)} = \frac{P(Z&gt;\frac{2-4}{2})}{P(Z&gt;\frac{0-4}{2})} \\
&amp;= \frac{P(Z&gt;-1)}{P(Z&gt;-2)} = \frac{1-P(Z\leq-1)}{1-P(Z\leq-2)} = \frac{1-\Phi(-1)}{1- \Phi(-2)} = \frac{\Phi(1)}{\Phi(2)} \,.
\end{align*}\]</span>
In the last step we take advantage of the symmetry of the standard normal: <span class="math inline">\(\Phi(z) = 1 - \Phi(-z)\)</span>.</p>
<p>(b)
<span class="math display">\[\begin{align*}
P(X&lt;c \vert X&lt;4) &amp;= 0.5 = \frac{P(X&lt;c \cap X&lt;4)}{P(X&lt;4)}\\
&amp;= \frac{P(X&lt;c)}{P(X&lt;4)} = \frac{P(z&lt;\frac{c-4}{2})}{P(z&lt;\frac{4-4}{2})} = \frac{\Phi\left(\frac{c-4}{2}\right)}{\Phi(0)} = 2\Phi\left(\frac{c-4}{2}\right) \,.
\end{align*}\]</span>
Therefore
<span class="math display">\[\begin{align*}
\Phi\left(\frac{c-4}{2}\right) &amp;= \frac14 \\
\Rightarrow ~~~ \frac{c-4}{2} &amp;= \Phi^{-1}\left( \frac14 \right)\\
\Rightarrow ~~~ c &amp;= 4 + 2 \Phi^{-1}\left( \frac14 \right) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>We utilize the Law of the Unconscious Statistician here:
<span class="math display">\[\begin{align*}
E[e^{-Z^2/2}] &amp;= \int_{-\infty}^{\infty} e^{-z^2/2} f_Z(z) dz = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{\infty} e^{-z^2/2} e^{-z^2/2} dz = \frac{1}{\sqrt{2 \pi}}\int_{-\infty}^{\infty} e^{-z^2} dz \,.
\end{align*}\]</span>
We do not know how to do this integral by hand. However, if we make the substitution <span class="math inline">\(x = \sqrt{2}z\)</span> (with <span class="math inline">\(dx = \sqrt{2}dz\)</span>), then
<span class="math display">\[\begin{align*}
E[e^{-Z^2/2}] &amp;= \frac{1}{\sqrt{2}\sqrt{2 \pi}}\int_{-\infty}^{\infty} e^{-x^2/2} dx = \frac{1}{\sqrt{2}} \left( \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-x^2/2} dx \right) = \frac{1}{\sqrt{2}} \,.
\end{align*}\]</span>
In the last step, we take advantage of the fact that the integral is that of a normal distribution with mean <span class="math inline">\(\mu = 0\)</span> and variance <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>(a) Rejection occurs if <span class="math inline">\(M\)</span> is <span class="math inline">\(&lt; 4\)</span> or <span class="math inline">\(&gt; 6\)</span>:
<span class="math display">\[\begin{align*}
P(M &lt; 4 \cup M &gt; 6) &amp;= P\left(Z &lt; \frac{4-5}{2}\right) +  P\left(Z &gt; \frac{6-5}{2}\right) \\
&amp;= \Phi\left(-\frac12\right) + \left(1 - \Phi\left(\frac12\right)\right) \\
&amp;= \Phi\left(-\frac12\right) + \Phi\left(-\frac12\right) = 2 \Phi\left(-\frac12\right) \,.
\end{align*}\]</span></p>
<p>(b) We start with the expression
<span class="math display">\[\begin{align*}
\Phi\left(-\frac{1}{\sigma}\right)+ \left(1 - \Phi\left(-\frac{1}{\sigma}\right)\right) = 2 \Phi\left(-\frac{1}{\sigma}\right) = 0.02 \,.
\end{align*}\]</span>
We then solve for <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[\begin{align*}
\Phi\left(-\frac{1}{\sigma}\right) &amp;= 0.01\\
\Rightarrow ~~~ \frac{-1}{\sigma} &amp;= \Phi^{-1}(0.01)\\
\Rightarrow ~~~ \sigma &amp;= -\frac{1}{\Phi^{-1}(0.01)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>We want to determine the value of <span class="math inline">\(c\)</span> such that <span class="math inline">\(P(X_1 &lt; \mu-c \cap X_2 &gt; \mu) = 1/4\)</span>. Because <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, we know that
<span class="math display">\[\begin{align*}
P(X_1 &lt; \mu-c \cap X_2 &gt; \mu) = P(X_1 &lt; \mu-c)P(X_2 &gt; \mu) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
\frac14 &amp;= P(X_1 &lt; \mu-c)P(X_2 &gt; \mu) = P\left(Z_1 &lt; \frac{\mu-c-\mu}{\sigma}\right)P\left(Z_2 &gt; \frac{\mu-\mu}{\sigma}\right) \\
&amp;= P\left(Z_1 &lt; -\frac{c}{\sigma}\right)P(Z_2 &gt; 0) = \Phi\left(-\frac{c}{\sigma}\right)[1 - \Phi(0)] \\
&amp;= \frac12 \Phi\left(-\frac{c}{\sigma}\right) \\
\Rightarrow ~~~ \frac12 &amp;= \Phi\left(-\frac{c}{\sigma}\right) \\
\Rightarrow ~~~ \Phi^{-1}\left(\frac12\right) &amp;= -\frac{c}{\sigma} \\
\Rightarrow ~~~ c &amp;= -\sigma \Phi^{-1}\left(\frac12\right) = 0 \,.
\end{align*}\]</span>
Above, we take advantage of the fact that a normal with mean zero is symmetric about the <span class="math inline">\(y\)</span>-axis, so <span class="math inline">\(\Phi(0) = 1/2\)</span>.</p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>We can use the moment-generating function to generate moments, like the mean <span class="math inline">\(E[X]\)</span>:
<span class="math display">\[\begin{align*}
E[X] = \left. \frac{dm}{dt} \right|_{t=0} = \left. \frac{(1-b^2t^2)ae^{at} - e^{at}(-2b^2t)}{(1-b^2t^2)^2)} \right|_{t=0} = \frac{(1-0)ae^0 + e^0(0)}{1-0)^2} = a \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) We utilize the Law of the Unconscious Statistician and write down that
<span class="math display">\[\begin{align*}
m(t) = E[e^{tX}] = \sum_{x=0}^2 e^{tx} p_X(x) = e^0(0.2) + e^t(0.4) + e^{2t}(0.4) = 0.2 + 0.4e^t(1+e^t) \,.
\end{align*}\]</span></p>
<p>(b) The first derivative of the mgf, evaluated at <span class="math inline">\(t = 0\)</span>, is the expected value:
<span class="math display">\[\begin{align*}
E[X] = \left.\frac{d}{dt} m(t)\right|_{t=0} = \left.(0.4e^t + 0.8e^{2t})\right|_{t=0} = 0.4+0.8 = 1.2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>(a) We utilize the Law of the Unconscious Statistician and write down that
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tx}] = k \int_0^a e^{tx}e^{-x} dx = k \int_0^a e^{(t-1)x}dx = \frac{k}{(t-1)} e^{(t-1)x}\bigg|_0^a = \frac{k}{(t-1)}\left[ e^{(t-1)a} -1\right] \,.
\end{align*}\]</span></p>
<p>(b) We can determine the expected value by differentiating <span class="math inline">\(m_X(t)\)</span>…
<span class="math display">\[\begin{align*}
E[X]  = \frac{d m_{X}(t)}{dt}\bigg|_0 = \frac{(t-1)ke^{(t-1)a}a - (ke^{(t-1)a} - k)}{(t-1)^2}\bigg|_0 = k - ke^{-a}(a+1) = k(1-e^{-a}(a+1)) \,.
\end{align*}\]</span>
…or by brute-force, via integration by parts…
<span class="math display">\[\begin{align*}
E[X] = \int_0^a kxe^{-x}dx = k\left( -xe^{-x}\bigg|_0^a - e^{-x}\bigg|_0^a\right) = k \left( -ae^{-a} + 0 - e^{-a} + 1\right) = k(1-e^{-a}(a+1)) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>(a) The calculations of the two moment-generating functions are similar:
<span class="math display">\[\begin{align*}
m_{X_1}(t) &amp;= E[e^{tX_1}] = 0.4 \cdot e^{t \cdot 0} + 0.6 \cdot e^{t \cdot 1} = 0.4 + 0.6e^t \\
m_{X_2}(t) &amp;= E[e^{tX_2}] = 0.2 \cdot e^{t \cdot 0} + 0.8 \cdot e^{t \cdot 1} = 0.2 + 0.8e^t \,.
\end{align*}\]</span></p>
<p>(b) Given that <span class="math inline">\(Y = X_1 + 2X_2\)</span>, we have that
<span class="math display">\[\begin{align*}
m_Y(t) &amp;= m_{X_1}(t) m_{X_2}(2t) = (0.4 + 0.6e^t)(0.2 + 0.8e^{2t}) \\
&amp;= 0.08 + 0.12e^t + 0.32e^{2t} + 0.48e^{3t} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) The moment-generating function is
<span class="math display">\[\begin{align*}
E[e^{tX}] = \sum_x e^{tx} p_X(x) = \frac12 \left( e^{-t} + e^t \right) \,.
\end{align*}\]</span></p>
<p>(b) If <span class="math inline">\(\bar{X} = (\sum_i X_i)/n\)</span>, then
<span class="math display">\[\begin{align*}
m_{\bar{X}}(t) = \prod_{i=1}^n m_{X_i}\left(\frac{t}{n}\right) = \frac{1}{2^n} \left( e^{-t/n} + e^{t/n} \right)^n \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>The moment-generating function is
<span class="math display">\[\begin{align*}
E[e^{tX}] &amp;= \int_0^{1/2} \frac13 e^{tx} dx + \int_{1/2}^1 \frac23 e^{tx} dx \\
&amp;= \frac{1}{3t} \left. e^{tx} \right|_0^{1/2} + \frac{2}{3t} \left. e^{tx} \right|_{1/2}^1 \\
&amp;= \frac{1}{3t} \left(e^{t/2}-1\right) + \frac{2}{3t} \left(e^t - e^{t/2}\right) \,.
\end{align*}\]</span>
We can combine terms:
<span class="math display">\[\begin{align*}
E[e^{tX}] = \frac{1}{3t}\left(e^{t/2} - 1 + 2e^t - 2e^{t/2}\right) = \frac{1}{3t} \left(2e^t - e^{t/2} - 1\right) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>(a) Because the variance is unknown, we make inferences about <span class="math inline">\(\mu\)</span> utilizing the <span class="math inline">\(t\)</span> distribution:
<span class="math display">\[\begin{align*}
P(3 \leq \bar{X} \leq 6) &amp;= P\left( \frac{3-\mu}{S/\sqrt{n}} \leq \frac{\bar{X} -\mu}{S/\sqrt{n}} \leq \frac{6 - \mu}{S/\sqrt{n}} \right) \\
&amp;= P\left(-\frac{2}{3/4} \leq T \leq \frac{1}{3/4}\right) = F_T\left(\frac43\right) - F_T\left(-\frac83\right) \,,
\end{align*}\]</span>
where the number of degrees of freedom is <span class="math inline">\(n - 1 = 15\)</span>.</p>
<p>(b) As instructed, here we utilize the Central Limit Theorem:
<span class="math display">\[\begin{align*}
P(3 \leq \bar{X} \leq 6) &amp;= P\left( \frac{3-\mu}{\sigma/\sqrt{n}} \leq \frac{\bar{X} -\mu}{\sigma/\sqrt{n}} \leq \frac{6 - \mu}{\sigma/\sqrt{n}} \right) \\
&amp;= P\left(-\frac{2}{5/7} \leq Z \leq \frac{1}{5/7}\right) = \Phi\left(\frac75\right) - \Phi\left(-\frac{14}{5}\right) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) The moment-generating function for a normal distribution with mean 1 and variance 1 is
<span class="math display">\[\begin{align*}
m_X(t) = \exp\left( \mu t + \frac{\sigma^2}{2}t^2 \right) = \exp\left( t + \frac12 t^2 \right) \,.
\end{align*}\]</span>
Thus the mgf for <span class="math inline">\(Y = X_1 - X_2\)</span> is
<span class="math display">\[\begin{align*}
m_Y(t) = m_{X_1}(t) \cdot m_{X_2}(-t) = \exp\left( t + \frac12 t^2 \right) \exp\left( -t + \frac12 (-t)^2 \right) = \exp\left(t^2\right) \,.
\end{align*}\]</span>
This is the mgf for a normal distribution with mean 0 and variance 2.</p>
<p>(b) This is far simpler than it might first appear:
<span class="math display">\[\begin{align*}
P(X_1 \geq X_2) = P(X_1 - X_2 \geq 0) = \frac12 \,.
\end{align*}\]</span>
This follows from the fact that the distribution for <span class="math inline">\(X_1 - X_2\)</span> (derived
in part (a)) is symmetric around the coordinate <span class="math inline">\(x=0\)</span>.</p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
P(X_1 \leq 0 \cup X_2 \geq 2) &amp;= P(X_1 \leq 0) + P(X_1 \geq 2) \\
&amp;= P(X_1 \leq 0) + P(X_1 \leq 0) = 2P(X_1 \leq 0)\\
&amp;= 2P\left(\frac{X_1-\mu}{\sigma} \leq \frac{0-\mu}{\sigma}\right) \\
&amp;= 2P\left(Z \leq -\frac{1}{\sigma}\right) = 2\Phi\left(-\frac{1}{\sigma}\right) = 0.2 \\
\Rightarrow ~~~ \Phi\left(-\frac{1}{\sigma}\right) &amp;= 0.1 \\
\Rightarrow ~~~ -\frac{1}{\sigma} &amp;= \Phi^{-1}(0.1) \\
\Rightarrow ~~~ \sigma &amp;= -\frac{1}{\Phi^{-1}(0.1)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>We can answer this question using either moment-generating functions or a general transformation.</p>
<p>The mgf of <span class="math inline">\(\bar{X}\)</span> is
<span class="math display">\[\begin{align*}
m_{\bar{X}}(t) = \exp \left( \mu t + \frac{\sigma^2 t}{2n}\right) \,.
\end{align*}\]</span>
Let <span class="math inline">\(U = \bar{X} - \mu\)</span>. Then,
<span class="math display">\[\begin{align*}
m_U(t) = e^{-\mu t} m_{\bar{X}}(t) = \exp(-\mu t)\exp\left(\mu t + \frac{\sigma^2 t}{2n}\right) = \exp\left(\frac{\sigma^2 t}{2n}\right) \,.
\end{align*}\]</span>
This is the mgf for a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2/n\)</span>.</p>
<p>Alternatively, let <span class="math inline">\(U = \bar{X}-\mu\)</span>, so <span class="math inline">\(\bar{X} = U + \mu\)</span>, and
<span class="math display">\[\begin{align*}
F_U(u) = P(U \leq u) = P(g(\bar{X}) \leq u) = P(\bar{X} \leq g^{-1}(u)) &amp;= P(\bar{X} \leq u + \mu) \\
&amp;= \int_{-\infty}^{u+\mu} f_{\bar{X}}(\bar{x}) d\bar{x} \\
&amp;= F_{\bar{X}}(u+\mu) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du}F_U(u) = \frac{d}{du}F_{\bar{X}}(u+\mu) &amp;= f_{\bar{X}}(u+\mu) \\
&amp;= \frac{1}{\sqrt{2 \pi (\sigma^2/n)}} \exp \left( - \frac{(u + \mu - \mu)^2}{2(\sigma^2/n)} \right) \\
&amp;= \frac{1}{\sqrt{2 \pi (\sigma^2/n)}} \exp \left( - \frac{u^2}{2(\sigma^2/n)} \right) \,.
\end{align*}\]</span>
This is the pdf for a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2/n\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>We want to write down an evaluatable expression for <span class="math inline">\(P(\vert \bar{X} - \mu \vert \leq 1)\)</span>, meaning that we want to change the expression inside the parantheses so that the statistic matches one for which we know the sampling distribution:
<span class="math display">\[\begin{align*}
P(\vert \bar{X} - \mu \vert \leq 1) &amp;= P\left( \frac{\vert \bar{S} - \mu \vert}{S/\sqrt{n}} \leq \frac{1}{S/\sqrt{n}}\right) = P\left(\vert T \vert \leq \frac{\sqrt{n}}{S}\right) \\
&amp;= P\left(-\frac{4}{2} \leq T \leq \frac{4}{2}\right) = P(-2 \leq T \leq 2) \,,
\end{align*}\]</span>
where <span class="math inline">\(T\)</span> is a <span class="math inline">\(t\)</span>-distributed random variable for <span class="math inline">\(\nu = n-1 = 15\)</span> dof.</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>We are given that <span class="math inline">\(X \sim \mathcal{N}(30,\sigma^2)\)</span>, so
<span class="math display">\[\begin{align*}
P(X \geq 25) = P\left(Z \geq \frac{25-30}{\sigma}\right) = 1 - P\left(Z \leq -\frac{5}{\sigma}\right) = 1 - \Phi\left(-\frac{5}{\sigma}\right) = 0.9 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
\Phi\left( -\frac{5}{\sigma}\right) &amp;= 0.1 \\
\Rightarrow ~~~ -\frac{5}{\sigma} = \Phi^{-1}(0.1) \\
\Rightarrow ~~~ \sigma = -\frac{5}{\Phi^{-1}(0.1)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(X^3 \leq u) = P(X \leq u^{1/3}) = \int_0^{u^{1/3}} 3x^2 dx = \left. x^3 \right|_0^{u^{1/3}} = u \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du} F_U(u) = 1 \,,
\end{align*}\]</span>
for <span class="math inline">\(u \in [0,1]\)</span>.</p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_W(w) &amp;= P(W \leq w) = P(X^2 \leq w) = P(X \leq \sqrt{w}) = \int_0^{\sqrt{w}} dx = \left. x \right|_0^{\sqrt{w}} = \sqrt{w} \,.
\end{align*}\]</span>
(Note that we know <span class="math inline">\(x &gt; 0\)</span>, so we need not worry about the root
<span class="math inline">\(-\sqrt{w}\)</span>.) Thus
<span class="math display">\[\begin{align*}
f_W(w) = \frac{d}{dw} \sqrt{w} = \frac{1}{2\sqrt{w}} \,,
\end{align*}\]</span>
for <span class="math inline">\(w \in [0,1]\)</span>. The desired probability is thus
<span class="math display">\[\begin{align*}
P(1/4 \leq w \leq 1/2) = \int_{1/4}^{1/2} \frac12 w^{-1/2} dw = \left. w^{1/2} \right|_{1/4}^{1/2} = \frac{\sqrt{2}}{2} - \frac12 = 0.207 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(X^2 + 4 \leq u) = P(X \leq \sqrt{u-4}) = \int_0^{\sqrt{u-4}} e^{-x} dx \\
&amp;= -e^{-x}\bigg|_0^{\sqrt{u-4}} = 1 - e^{-\sqrt{u-4}} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du} F_U(u) = \frac{d}{du}(1 - e^{-\sqrt{u-4}}) = \frac{1}{2\sqrt{u-4}}e^{-\sqrt{u-4}} \,.
\end{align*}\]</span>
The domain can be found in this instance by plugging in the lower bound of
the domain of <span class="math inline">\(x\)</span>: <span class="math inline">\(x = 0 ~\Rightarrow~ u=4\)</span>. Thus <span class="math inline">\(u \in [4,\infty)\)</span>.</p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(-2X \leq u) =  P(X \geq -u/2) = \frac{1}{\beta} \int_{-u/2}^{\infty} e^{-x/\beta} dx = -e^{-x/\beta}\bigg|_{-u/2}^{\infty} = e^{u/2\beta} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du}e^{u/2\beta} = \frac{1}{2\beta}e^{u/2\beta} \,.
\end{align*}\]</span>
Since <span class="math inline">\(U = -2X\)</span>, the domain is <span class="math inline">\(u \in (-\infty,0]\)</span>.</p>
<p>(b) No, since the domain, <span class="math inline">\((-\infty,0]\)</span>, is not the domain of an exponential distribution.</p>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(X^2 - 1 \leq u)\\
&amp;= P(- \sqrt{u + 1} \leq X \leq  \sqrt{u + 1}) =  \int_{- \sqrt{u + 1}}^{\sqrt{u + 1}}\frac{dx}{2} = \sqrt{u+1} \,.
\end{align*}\]</span>
(Because the domain for <span class="math inline">\(f_X(x)\)</span> is <span class="math inline">\([-1,1]\)</span>, we have to be careful when setting the limits for the integral.) Thus
<span class="math display">\[\begin{align*}      
f_U(u) = \frac{d}{du} \sqrt{u+1} = \frac{1}{2 \sqrt{u+1}} \,.
\end{align*}\]</span>
When we plot <span class="math inline">\(u = x^2-1\)</span>, we would see that over the range <span class="math inline">\(x \in [-1,1]\)</span>, the values of <span class="math inline">\(u\)</span> range from <span class="math inline">\(-1\)</span> to 0. Hence the domain of <span class="math inline">\(f_U(u)\)</span> is <span class="math inline">\(u \in [-1,0]\)</span>.</p>
<p>(b) We ask for <span class="math inline">\(E[U+1]\)</span> because that makes for a more straightforward integral than <span class="math inline">\(E[U]\)</span>:
<span class="math display">\[\begin{align*}
E[U+1] = \int_{-1}^0 (u+1) \frac{du}{2\sqrt{u+1}} = \frac{1}{2}\int_{-1}^0 \sqrt{(u+1)} du = \frac{1}{3}(u+1)^{3/2}\bigg|_{-1}^0 = \frac{1}{3} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(e^{-X} \leq u) = P(-X \leq \log(u)) = P(X \geq -\log(u)) \\
&amp;= \int_{-\log(u)}^{1} 3x^2 dx = x^3\bigg|_{-\log(u)}^{1} = 1 + (\log(u))^3 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du} (1 + (\log(u))^3) = 3(\log(u))^2\frac{1}{u} \,.
\end{align*}\]</span>
As <span class="math inline">\(x\)</span> goes from 0 to 1, <span class="math inline">\(u\)</span> goes from 1 to <span class="math inline">\(e^{-1}\)</span>. Hence the domain of <span class="math inline">\(f_U(u)\)</span> is <span class="math inline">\(u \in [e^{-1},1]\)</span>.</p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>(a) We utilize the general transformation framework here:
<span class="math display">\[\begin{align*}
F_U(u) &amp;= P(U \leq u) = P(e^X \leq u) = P(X \leq \log u) = \int_0^{\log u} \theta x^{\theta-1} dx = \left. x^\theta \right|_0^{\log u} = (\log u)^\theta \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du}F_U(u) = \theta (\log u)^{\theta-1} \frac{1}{u} \,,
\end{align*}\]</span>
for <span class="math inline">\(u \in [1,\exp(1)]\)</span>.</p>
<p>(b) If <span class="math inline">\(U = 2X\)</span> and <span class="math inline">\(X = U/2\)</span>, then
<span class="math display">\[\begin{align*}
F_U(u) = \ldots = \int_0^{u/2} \theta x^{\theta-1} dx = \left. x^\theta \right|_0^{u/2} = \left(\frac{u}{2}\right)^\theta \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
P(U \geq 1) = 1 - P(U &lt; 1) = 1 - F_U(1) = 1 - \left(\frac12\right)^\theta \,.
\end{align*}\]</span>
There is no need to derive <span class="math inline">\(f_U(u)\)</span> here.</p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>We do not know (or cannot easily work with) the sampling distribution for <span class="math inline">\(S^2\)</span>, so we algebraically transform the quantities within the probability expression:
<span class="math display">\[\begin{align*}
P(1 \leq S^2 \leq 2) = P\left( \frac{(n-1) \cdot 1}{\sigma^2} \leq \frac{(n-1)S^2}{\sigma^2} \leq \frac{(n-1) \cdot 2}{\sigma^2} \right) \,.
\end{align*}\]</span>
So
<span class="math display">\[\begin{align*}
X &amp;= \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2 = \chi_{10}^2 \\
a &amp;= \frac{n-1}{\sigma^2} = 5 \\
b &amp;= \frac{2(n-1)}{\sigma^2} = 10 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>We do not know the distribution for <span class="math inline">\(\bar{X}\sqrt{n}/S\)</span>, but we do know that <span class="math inline">\(\frac{\bar{X} - \mu}{S/\sqrt{n}} \sim t(n-1)\)</span>. Hence:
<span class="math display">\[\begin{align*}
P\left(a \leq \frac{\bar{X}}{S/\sqrt{n}} \leq b\right) &amp;= P\left(a - \frac{\mu}{S/\sqrt{n}}\leq \frac{\bar{X} -\mu}{S/\sqrt{n}} \leq b -\frac{\mu}{S/\sqrt{n}} \right) \\
&amp;= F_T\left(b - \frac{\mu}{S/\sqrt{n}}\right) - F_T\left(a  - \frac{\mu}{S/\sqrt{n}}\right) \,,
\end{align*}\]</span>
where <span class="math inline">\(F_T\)</span> is the cdf for <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> dof.</p>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) We do not know the sampling distribution for <span class="math inline">\(\bar{X}/S\)</span>, so we algebraically transform the quantities within the probability expression:
<span class="math display">\[\begin{align*}
P\left(\frac{\bar{X}}{S} \leq a\right) &amp;= P\left(\frac{\bar{X}}{S} - \frac{\mu}{S} \leq a- \frac{\mu}{S} \right) \\
&amp;= P\left( \sqrt{n}\frac{\bar{X}-\mu}{S} \leq \sqrt{n}\left(a- \frac{\mu}{S} \right)\right) \\
&amp;= P\left(T \leq \sqrt{n}\left(a- \frac{\mu}{S} \right)\right) = F_T\left(\sqrt{n}\left(a- \frac{\mu}{S} \right)\right) \,.
\end{align*}\]</span>
where <span class="math inline">\(F_T\)</span> is the cdf for <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> dof.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
b = P\left(\frac{\bar{X}}{\sigma} \leq a\right) &amp;= P\left(\frac{\bar{X}}{\sigma} - \frac{\mu}{\sigma} \leq a- \frac{\mu}{\sigma} \right) \\
&amp;= P\left( \sqrt{n}\frac{\bar{X}-\mu}{\sigma} \leq \sqrt{n}\left(a- \frac{\mu}{\sigma} \right)\right) \\
&amp;= P\left(Z \leq \sqrt{n}\left(a- \frac{\mu}{\sigma} \right)\right) = \Phi\left(\sqrt{n}\left(a- \frac{\mu}{\sigma} \right)\right) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
\sqrt{n}\left(a- \frac{\mu}{\sigma} \right) &amp;= \Phi^{-1}(b) \\
\Rightarrow ~~~ n &amp;= \left(\frac{\Phi^{-1}(b)}{a - \mu/\sigma}\right)^2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 26</li>
</ul>
<p>(a) The Fisher information for a single datum, <span class="math inline">\(I(\theta)\)</span>, is
<span class="math display">\[\begin{align*}
I(\theta) = -E\left[ \frac{d^2}{d \theta^2} \log f_X(x \vert \theta) \right] \,.\end{align*}\]</span>
So we start by finding the log of <span class="math inline">\(f_X(x \vert \theta)\)</span>:
<span class="math display">\[\begin{align*}
\log f_X(x \vert \theta) = \log \theta + (\theta-1) \log(1-x) \,,
\end{align*}\]</span>
and then we derive the second derivative:
<span class="math display">\[\begin{align*}
\frac{d}{d\theta} f_X(x \vert \theta) &amp;= \frac{1}{\theta} + \log(1-x) \\
\frac{d^2}{d\theta^2} f_X(x \vert \theta) &amp;= -\frac{1}{\theta^2} \,.
\end{align*}\]</span>
Last, we find the expected value:
<span class="math display">\[\begin{align*}
I(\theta) = -E\left[ -\frac{1}{\theta^2} \right] = \frac{1}{\theta^2} \,.
\end{align*}\]</span></p>
<p>(b) The Cramer-Rao Lower Bound is
<span class="math display">\[\begin{align*}
\frac{1}{I_n(\theta)} = \frac{1}{nI(\theta)} = \frac{1}{n(1/\theta^2)} = \frac{\theta^2}{n} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 27</li>
</ul>
<p>(a) The log-likelihood is given by
<span class="math display">\[\begin{align*}
\ell(\theta \vert x) = -n \log \theta + \sum_{i=1}^n x_i + \frac{n}{\theta} - \frac{\sum_{i=1}^n e^{x_i}}{\theta} \,,
\end{align*}\]</span>
and its first derivative is given by
<span class="math display">\[\begin{align*}
\frac{d}{d\theta} \ell(\theta \vert x) = -\frac{n}{\theta} - \frac{n}{\theta^2} + \frac{\sum_{i=1}^n e^{x_i}}{\theta^2} \,.
\end{align*}\]</span>
If we set this expression to zero, we find that
<span class="math display">\[\begin{align*}
0 &amp;= -n - \frac{n}{\theta} + \frac{\sum_{i=1}^n e^{x_i}}{\theta} \\
\Rightarrow ~~~ \frac{\sum_{i=1}^n e^{x_i}-n}{\theta} &amp;= n \\
\Rightarrow ~~~ \theta &amp;= \frac{1}{n} \left( \sum_{i=1}^n e^{x_i}-n \right) \\
\Rightarrow ~~~ \hat{\theta}_{MLE} &amp;= \frac{1}{n} \left( \sum_{i=1}^n e^{X_i}-n \right) \,.
\end{align*}\]</span></p>
<p>(b) We find the Cramer-Rao Lower Bound as follows:
<span class="math display">\[\begin{align*}
\log f_X(x) &amp;=  -\log (\theta) + x + \frac{1}{\theta} -\frac{e^x}{\theta} \\
\Rightarrow ~~~ \frac{d}{d\theta} \log f_X(x) &amp;= -\frac{1}{\theta} - \frac{1}{\theta^2} + \frac{1}{\theta^2} e^x \\
\Rightarrow ~~~ \frac{d^2}{d\theta^2} \log f_X(x) &amp;= \frac{1}{\theta^2} + \frac{2}{\theta^3} - \frac{2e^x}{\theta^3} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
I(\theta) &amp;= -E\left[\frac{d^2}{d\theta^2} \log f(X)\right] = -\frac{1}{\theta^2}  -\frac{2}{\theta^3} + \frac{2}{\theta^3} E[e^X] \\
&amp;= -\frac{1}{\theta^2}  -\frac{2}{\theta^3} + \frac{2}{\theta^3}(\theta+1) = \frac{1}{\theta^2} \,,
\end{align*}\]</span>
and thus <span class="math inline">\(I_n(\theta) = n/\theta^2\)</span> and
<span class="math display">\[\begin{align*}
V[\hat\theta] \geq \frac{\theta^2}{n} \,.
\end{align*}\]</span>
The variance of the MLE is <span class="math inline">\(\theta^2 / n\)</span>, because
<span class="math display">\[\begin{align*}
V\left[ \frac{1}{n} \sum_{i=1}^n e^{X_i} - 1 \right] = V\left[ \frac{1}{n} \sum_{i=1}^n e^{X_i} \right] = \frac{1}{n^2} V\left[ \sum_{i=1}^n e^{X_i} \right] = \frac{n}{n^2} V\left[ e^X \right] = \frac{\theta^2}{n} \,.
\end{align*}\]</span>
Thus the MLE achieves the CRLB.</p>
<hr />
<ul>
<li>Problem 28</li>
</ul>
<p>The log-likelihood is
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) = \sum_{i = 1}^n [x_i \log(p) + (1 - x_i) \log (1-p)].
\end{align*}\]</span>
The first two derivatives are
<span class="math display">\[\begin{align*}
\frac{d}{dp} \ell(p \vert \mathbf{x}) &amp;= \sum_{i = 1}^n \bigg[ \frac{x_i}{p}  - \frac{(1 - x_i)}{1-p}\bigg] \\
\frac{d^2}{dp^2} \ell(p \vert \mathbf{x}) &amp;= \sum_{i = 1}^n \bigg[-\frac{x_i}{p^2}- \frac{(1- x_i)}{(1-p)^2}\bigg] \,.
\end{align*}\]</span>
The Fisher information <span class="math inline">\(I_n(p)\)</span> is therefore
<span class="math display">\[\begin{align*}
I_n(p) = E\bigg[-\frac{d^2}{dp^2} \ell(p \vert \mathbf{X} )\bigg] = \sum_{i = 1}^n \bigg[\frac{E[X_i]}{p^2} + \frac{E[(1- X_i)]}{(1-p)^2}\bigg] = \frac{n}{p(1-p)} \,,
\end{align*}\]</span>
and the asymptotic distribution of the maximum likelihood estimate <span class="math inline">\(\hat{p}_{MLE}\)</span> is thus <span class="math inline">\(\mathcal{N}(p,p(1-p)/n)\)</span>. (Note: we never had to explicitly derive the MLE to determine its asymptotic distribution.)</p>
<hr />
<ul>
<li>Problem 29</li>
</ul>
<p>(a) The Fisher information <span class="math inline">\(I(a)\)</span> is
<span class="math display">\[\begin{align*}       
I(a) = -E\left[\frac{\partial^2}{\partial a^2} \log f_X(x \vert a)\right] \,,
\end{align*}\]</span>
where<br />
<span class="math display">\[\begin{align*}
\log f_X(x \vert a) = \log a - (a+1) \log x \,.
\end{align*}\]</span><br />
Thus<br />
<span class="math display">\[\begin{align*}       
I(a) = -E\left[\frac{\partial^2}{\partial a^2} (\log a - (a+1) \log x)\right]
= -E\left[\frac{\partial}{\partial a} \left(\frac{1}{a} - \log x\right)\right]
= -E\left[-\left(\frac{1}{a^2}\right)\right] = \frac{1}{a^2} \,.
\end{align*}\]</span></p>
<p>(b) The Cramer-Rao Lower Bound is <span class="math inline">\(1/I_n(a)\)</span>, where <span class="math inline">\(I_n(a) = nI(a)\)</span>, and is thus <span class="math inline">\(a^2/n\)</span>.</p>
<p>(c) The maximum likelihood estimator for a distribution parameter converges in distribution to a normal random variable as <span class="math inline">\(n \rightarrow \infty\)</span>, with zero bias and variance given by the CRLB. Thus in the asymptotic limit,
<span class="math display">\[\begin{align*}
\hat{a}_{MLE} \sim \mathcal{N}\left(a,\frac{a^2}{n}\right) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 30</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
P(\vert \bar{X} - \mu \vert \geq 1) &amp;= P\left(\frac{\vert \bar{X} - \mu \vert}{\sigma/\sqrt{n}} \geq \frac{1}{\sigma/\sqrt{n}}\right)   
= P\left( \vert Z \vert \geq \frac{\sqrt{n}}{\sigma} \right)  \\
&amp;= P\left(Z \leq -\frac{\sqrt{n}}{\sigma} \cup Z \geq \frac{\sqrt{n}}{\sigma}\right) = P(Z \leq -2 \cup Z \geq 2) \\
&amp;= \Phi(-2) + (1-\Phi(2)) = 2\Phi(-2) \,.
\end{align*}\]</span></p>
<p>(b) We know that <span class="math inline">\(X_+ = n\bar{X}\)</span>, so since <span class="math inline">\(\bar{X}\)</span> is distributed normally via the CLT, <span class="math inline">\(X_+\)</span> is as well. We also know that <span class="math inline">\(E[X_+] = nE[\bar{X}] = 100 \cdot 10 = 1000\)</span>, and <span class="math inline">\(V[X_+] = n^2V[\bar{X}] = 10,000 \cdot \frac{25}{100} = 2500\)</span>. Hence <span class="math inline">\(X_+ \sim \mathcal{N}(1000,2500)\)</span>.</p>
<hr />
<ul>
<li>Problem 31</li>
</ul>
<p>Because the distribution for the time needed to complete the task is not given, we must fall back on the Central Limit Theorem:
<span class="math display">\[\begin{align*}
T_{\text{A}} \sim \mathcal{N}(\mu_{\text{A}}, \frac{\sigma^2_{\text{A}}}{n}) &amp;= \mathcal{N}(40,36/36=1) \\
T_{\text{B}} \sim \mathcal{N}(\mu_{\text{B}}, \frac{\sigma^2_{\text{B}}}{n}) &amp;= \mathcal{N}(38,36/36=1) \,.
\end{align*}\]</span>
We are interested in computing the probability <span class="math inline">\(P(T_{\text{A}} - T_{\text{B}} &lt; 0)\)</span>. Using the method of moment-generating functions, we know that
<span class="math display">\[\begin{align*}
m_{\Delta T}(t) &amp;= m_{T_{\text{A}}}(t) m_{T_{\text{B}}}(t) = \exp\left(\mu_{\text{A}}t + \frac{\sigma^2_{\text{A}}t^2}{2n}\right)\exp\left(-\mu_{\text{B}}t + \frac{\sigma^2_{\text{B}}t^2}{2n}\right)\\
&amp;= \exp\left((\mu_{\text{A}} -\mu_{\text{B}}) t + \left(\frac{\sigma^2_{\text{A}}+\sigma^2_{\text{B}}}{n}\right)\frac{t^2}{2}\right) \,.
\end{align*}\]</span>
and thus, approximately,
<span class="math display">\[\begin{align*}
T_{\text{A}} - T_{\text{B}} \sim \mathcal{N}(\mu_{\text{A}} - \mu_{\text{B}}, ((\sigma^2_{\text{A}} + \sigma^2_{\text{B}})/n) = \mathcal{N}(2,2) \,.
\end{align*}\]</span>
So, in the end, we have that
<span class="math display">\[\begin{align*}
P(T_{\text{A}} - T_{\text{B}} &lt; 0) = P\left(\frac{T_{\text{A}} - T_{\text{B}}-2}{\sqrt{2}} &lt; \frac{0-2}{\sqrt{2}}\right) = P(Z &lt; -\sqrt{2}) = \Phi(-\sqrt{2}) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 32</li>
</ul>
<p>Under the conditions of the Central Limit Theorem, we know that <span class="math inline">\(\bar{X} \sim \mathcal{N}(10,4^2/64 = 1/4)\)</span>. Therefore
<span class="math display">\[\begin{align*}
P(\bar{X} &gt; 9) = 1 - P(\bar{X} \leq 9) \approx 1 - P(Z \leq (9-10)\sqrt{2}) = 1 - \Phi(-2) = \Phi(2) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 33</li>
</ul>
<p>We have that <span class="math inline">\(n = 300\)</span> and that <span class="math inline">\(\sigma^2 = \frac{1}{12}(6^2 - 0^2) = 3\)</span>, so that <span class="math inline">\(\sigma = \sqrt{3}\)</span> and <span class="math inline">\(\sigma/\sqrt{n} = \sqrt{3/300} = 0.1\)</span>. Therefore, using the Central Limit Theorem, we find that
<span class="math display">\[\begin{align*}
P(2.8 \leq \bar{X} \leq 3.2) &amp;= P\left( \frac{2.8 - 3}{0.1} \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}\leq \frac{3.2 - 3}{0.1} \right) \\
&amp;\approx P(-2 \leq Z \leq 2) = \Phi(2) - \Phi(-2) = 2\Phi(2) - 1 = 1 - 2\Phi(-2) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 34</li>
</ul>
<p>By the Central Limit Theorem, we know that <span class="math inline">\(X_+ \sim \mathcal{N}(n\mu,n\sigma^2)\)</span>, so
<span class="math display">\[\begin{align*}
P(X_+ &lt; (n+1)\mu) &amp;= P\left( \frac{X_+ - n\mu}{\sqrt{n}\sigma} &lt; \frac{(n+1)\mu - n\mu}{\sqrt{n}\sigma} \right) \\
&amp;\approx P\left( Z &lt; \frac{\mu}{\sqrt{n}\sigma} \right) = \Phi\left(\frac{\mu}{\sqrt{n}\sigma}\right) = \Phi\left(\frac{1}{2}\right) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 35</li>
</ul>
<p>(a) This is a Central Limit Theorem question. Since <span class="math inline">\(n \geq 30\)</span>, we assume <span class="math inline">\(\bar{X} \sim \mathcal{N}(\mu,\sigma^2/n)\)</span>. Thus we have that
<span class="math display">\[\begin{align*}
P\left(\bar{X} \leq \mu-\sigma/10\right) = P\left(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \leq \frac{\mu-\sigma/10-\mu}{\sigma/\sqrt{n}}\right) \approx P(Z \leq \sqrt{n}/10 ) = \Phi(-1) = 1 - \Phi(1) \,.
\end{align*}\]</span></p>
<p>(b) If we don’t know <span class="math inline">\(\sigma^2\)</span>, we simply plug in <span class="math inline">\(S^2\)</span> and proceed as we did before…remember, we never use the <span class="math inline">\(t\)</span> distribution in CLT-related problems.
<span class="math display">\[\begin{align*}
P\left(\bar{X} \geq \mu+1 \right) &amp;= 1 - P\left(\frac{\bar{X}-\mu}{S/\sqrt{n}} \leq \frac{\mu + 1 - \mu}{S/\sqrt{n}}\right) \\
&amp;\approx 1 - P\left(Z \leq \frac{\sqrt{n}}{S}\right) = 1 - \Phi\left(\frac{10}{5}\right) = 1 - \Phi(2) = \Phi(-2) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 36</li>
</ul>
<p>(a) For a confidence interval, we need to solve the equation
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta) - q = 0
\end{align*}\]</span>
for <span class="math inline">\(\theta\)</span>, given <span class="math inline">\(q\)</span>. Here, we have the statistic <span class="math inline">\(w_{\rm obs} = (n-1)s_{\rm obs}^2/\sigma^2\)</span>, which is sampled from a chi-square distribution for <span class="math inline">\(n-1\)</span> degrees of freedom. Hence the function we will use is <code>pchisq()</code>.</p>
<p>(b) We are calling <code>pchisq()</code>, and the first argument would be the observed statistic value <code>(n-1)*s2.obs/sigma2</code>.</p>
<p>(c) We are solving for <span class="math inline">\(\sigma^2\)</span>, which has to be positive. So of the three choices, the only proper one is <code>c(0.001,1000)</code>.</p>
<hr />
<ul>
<li>Problem 37</li>
</ul>
<p>(a) To perform hypothesis tests about the variance of a normal distribution, the appropriate test statistic under the null is <span class="math inline">\(Y = (n-1)S^2/\sigma_o^2\)</span>, which is sampled from a chi-square distribution for <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<p>(b) When testing hypotheses about the normal population variance, it is the case that lower values of <span class="math inline">\(\sigma^2\)</span> lead to lower values of <span class="math inline">\(S^2\)</span>, hence we place the rejection region in the lower tail of the sampling distribution. Lower-tail rejection is <span class="math inline">\(c = F_W^{-1}(\alpha)\)</span>, with the number of degrees of freedom being <span class="math inline">\(n-1\)</span>.</p>
<p>(c) We start by noting that the power under the null is, by definition, <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[\begin{align*}       
P(Y &lt; c \vert \sigma^2 = \sigma_o^2) &amp;= \alpha \\
P(Y &lt; c) &amp;= \, ?\\
P\left(Y\frac{\sigma_o^2}{\sigma_a^2} &lt; c\frac{\sigma_o^2}{\sigma_a^2}\right) &amp;= \, ?\\                   
P\left(\frac{(n-1)S^2}{\sigma_a^2} &lt; c\frac{\sigma_o^2}{\sigma_a^2}\right) &amp;= \, ?\\                              
P\left(\frac{(n-1)S^2}{\sigma_a^2} &lt; c\frac{\sigma_o^2}{\sigma_a^2} \vert \sigma^2 = \sigma_a^2 \right) &amp;= 1 - \beta = power\\
P\left(W &lt; c\frac{\sigma_o^2}{\sigma_a^2} \vert \sigma^2 = \sigma_a^2 \right) &amp;= 1 - \beta = power\\
\Rightarrow ~~ power = F_W\left(c\frac{\sigma_o^2}{\sigma_a^2}\right)
\end{align*}\]</span></p>
<p>(d) We are performing a lower-tail test, so the power increases from <span class="math inline">\(\alpha\)</span> to 1 as <span class="math inline">\(\sigma_a^2\)</span> decreases from <span class="math inline">\(\sigma_o^2\)</span>, and decreases from <span class="math inline">\(\alpha\)</span> to 0 as <span class="math inline">\(\sigma_a^2\)</span> increases from <span class="math inline">\(\sigma_o^2\)</span>. Thus if <span class="math inline">\(\sigma_a^2 &gt; \sigma_o^2\)</span>, the power is <em>less than</em> <span class="math inline">\(\alpha\)</span>.</p>
<hr />
<ul>
<li>Problem 38</li>
</ul>
<p>(a) The <span class="math inline">\(p\)</span>-value can be written down directly by plugging <span class="math inline">\(x_{\rm obs}\)</span> and <span class="math inline">\(a_o\)</span> into the given cdf, which, because we sample a single datum, is the cdf for the sampling distribution:
<span class="math display">\[\begin{align*}                       
p = 1 - \left(\frac{1}{x_{\rm obs}}\right)^{a_o} = 1 - \left(\frac{4}{5}\right)^2 = 1 - \frac{16}{25} = \frac{9}{25} = 0.36 \,.
\end{align*}\]</span></p>
<p>(b) The <span class="math inline">\(p\)</span>-value is <span class="math inline">\(&gt; \alpha\)</span>, so we “fail to reject” the null hypothesis and conclude that we have insufficient data to rule out that <span class="math inline">\(a = 2\)</span>.<br />
(c) We solve this via the inverse cdf, utilizing an appropriate row of the hypothesis test reference table:
<span class="math display">\[\begin{align*}
&amp;\alpha = 1 - \left(\frac{1}{x_{\rm RR}}\right)^{a_o} \\
\Rightarrow ~~~ &amp;\left(\frac{1}{x_{\rm RR}}\right)^{a_o} = 1 - \alpha \\
\Rightarrow ~~~ &amp;\frac{1}{x_{\rm RR}} = (1-\alpha)^{1/a_o} \\
\Rightarrow ~~~ &amp;x_{\rm RR} = \frac{1}{(1-\alpha)^{1/a_o}} \\
\Rightarrow ~~~ &amp;x_{\rm RR} = \frac{1}{(0.95)^{1/2}} = 1.026 \,.
\end{align*}\]</span></p>
<p>(d) The test power is
<span class="math display">\[\begin{align*}
power(a=4) = 1 - \left(\frac{1}{x_{\rm RR}}\right)^{a} = 1 - \left((0.95)^{1/2}\right)^4 = 1 - (0.95)^2 = 0.0975 \,.
\end{align*}\]</span>
(The test power is low…you cannot easily reject a null with a single datum.)</p>
<hr />
<ul>
<li>Problem 39</li>
</ul>
<p>(a) The estimator for <span class="math inline">\(\hat{\beta}_1&#39;\)</span> is
<span class="math display">\[\begin{align*}
\hat{\beta}_1&#39; = \frac{(\sum_{i=1}^n Y_i a x_i) - n a \bar{x} \bar{Y}}{(\sum_{i=1}^n (ax_i)^2) - n a^2 \bar{x}^2} = \frac{a \left[(\sum_{i=1}^n Y_i x_i) - n \bar{x} \bar{Y}\right]}{a^2 \left[(\sum_{i=1}^n x_i^2) - n \bar{x}^2\right]} = \frac{\hat{\beta}_1}{a} \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[\hat{\beta}_1&#39;] = V\left[\frac{\hat{\beta}_1}{a}\right] = \frac{1}{a^2} V[\hat{\beta}_1] \,.
\end{align*}\]</span></p>
<p>(c) The test statistic is
<span class="math display">\[\begin{align*}
T&#39; = \frac{\hat{\beta}_1&#39; - 0}{\sqrt{V[\hat{\beta}_1&#39;]}} = \frac{(1/a) \hat{\beta}_1}{(1/a) \sqrt{V[\hat{\beta}_1]}} = \frac{\hat{\beta}_1}{\sqrt{V[\hat{\beta}_1]}} = T \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 40</li>
</ul>
<p>(a) Since <span class="math inline">\(x_i\)</span>s are fixed, <span class="math inline">\(E[Y_i]= \beta_0 + \beta_1 x_i\)</span> for all <span class="math inline">\(i = 1,\ldots,n\)</span> and <span class="math inline">\(E [\bar{Y}] = \beta_0 + \beta_1 \bar{x}\)</span>.
<span class="math display">\[\begin{align*}
E [\hat\beta_{1, OLS} ] &amp;= E \left[ \frac{\sum_{i=1}^n x_i Y_i - n \bar{x}\bar{Y}}{\sum_{i=1}^n x_i^2 - n \bar{x}^2} \right]\\
&amp;= \frac{E \left[\sum_{i=1}^n x_i Y_i - n \bar{x}\bar{Y}\right]}{\sum_{i=1}^n x_i^2 - n \bar{x}^2} = \frac{\sum_{i=1}^n x_i E [Y_i] - n \bar{x} E [\bar{Y}]}{\sum_{i=1}^n x_i^2 - n \bar{x}^2}\\
&amp;= \frac{\sum_{i=1}^n x_i (\beta_0 + \beta_1 x_i) - n \bar{x} (\beta_0 + \beta_1 \bar{x})}{\sum_{i=1}^n x_i^2 - n \bar{x}^2}\\
&amp;= \frac{\sum_{i=1}^n \beta_1 x_i^2 - \beta_1 n \bar{x}^2}{\sum_{i=1}^n x_i^2 - n \bar{x}^2}
= \beta_1 \,.
\end{align*}\]</span></p>
<p>(b) No. We only used the linearity of expectation and the assumption that <span class="math inline">\(E[\epsilon_i] = 0\)</span> for all <span class="math inline">\(i=1,\dots,n\)</span>.</p>
<hr />
<ul>
<li>Problem 41</li>
</ul>
<p>(a) The Shapiro-Wilk test result indicates that we would reject the null hypothesis that the residuals are normally distributed. In addition, the residuals shown in the summary imply that the residual values are highly skewed. So…“violated.”</p>
<p>(b) The estimated slope is the <span class="math inline">\(t\)</span> value times the standard error, so <span class="math inline">\(-4.624 \times 0.1189\)</span> (which is <span class="math inline">\(-0.550\)</span>).</p>
<p>(c) The number of degrees of freedom for a simple linear regression model is <span class="math inline">\(n-2\)</span>, so <span class="math inline">\(n = 28+2 = 30\)</span>.</p>
<p>(d) It is simply the square of the “Residual standard error,” or <span class="math inline">\(1.888^2\)</span> (which is <span class="math inline">\(3.565\)</span>).</p>
<p>(e) The slope is negative, so the data are negatively correlated, and the correlation estimated itself, <span class="math inline">\(R\)</span>, is the square root of “R-squared.” So the answer is <span class="math inline">\(-\sqrt{0.4329}\)</span> or <span class="math inline">\(-(0.4329^{1/2})\)</span> (which is <span class="math inline">\(-0.658\)</span>).</p>
</div>
<div id="chapter-3" class="section level2 unnumbered hasAnchor">
<h2>Chapter 3<a href="chapter-exercises-solutions.html#chapter-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>The median of a <span class="math inline">\(\mathcal{N}(2,4)\)</span> is <span class="math inline">\(\tilde{\mu} = 2\)</span>,
thus <span class="math inline">\(P(X_i &gt; 2) = 0.5\)</span> by inspection. Now, let <span class="math inline">\(Y\)</span> be the number of
values <span class="math inline">\(&gt; 2\)</span>. Then
<span class="math display">\[\begin{align*}
P(Y=m) = \binom{n}{m} \left(\frac12\right)^m \left(\frac12\right)^{n-m} = \binom{n}{m} \left(\frac12\right)^n \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>(a) We fix the number of successes to <span class="math inline">\(s=1\)</span>, with the
random variable being the number of tickets we need to buy <em>before</em> buying
the one that allows us to win the raffle. There are two correct answers here:
<span class="math inline">\(F\)</span> is sampled from
the geometric distribution, with <span class="math inline">\(p=0.4\)</span>, or from the negative binomial
distribution, with <span class="math inline">\(s=1\)</span> and <span class="math inline">\(p=0.4\)</span>.</p>
<p>(b) We have that <span class="math inline">\(W = 2-F\)</span>, so
<span class="math display">\[\begin{align*}
P(W &gt; 0) = P(2-F &gt; 0) &amp;= P(F &lt; 2) = p_X(0) + p_X(1) \\
&amp;= (0.4)^1(1-0.4)^0 + (0.4)^1(1-0.4)^1 = 0.4 + 0.24 = 0.64 \,.
\end{align*}\]</span></p>
<p>(c) Let <span class="math inline">\(X\)</span> be the number of winning tickets. Then
<span class="math display">\[\begin{align*}
P(X = 1 \vert X \geq 1) &amp;= \frac{P(X = 1 \cap X \geq 1)}{P(X \geq 1)} = \frac{P(X=1)}{1-P(X=0)} \\
&amp;= \frac{\binom{2}{1}(0.4)^1(0.6)^1}{1 - \binom{2}{0}(0.4)^0(0.6)^2} = \frac{2 \cdot 0.24}{1 - 0.36} = 0.48/0.64 = 0.75 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>The first step is to write down the probability mass function for the
number of insured drivers, given that the number of insured drivers is odd:
<span class="math display">\[\begin{align*}
P(X=1 \vert X=1 \cup X=3) &amp;= \frac{p_X(1)}{p_X(1)+p_X(3)} = \frac{\binom{3}{1}(1/2)^1(1/2)^2}{\binom{3}{1}(1/2)^1(1/2)^2 + \binom{3}{3}(1/2)^3(1/2)^0} \\
&amp;= \frac{3}{3 + 1} = \frac34 \\
P(X=3 \vert X=1 \cup X=3) &amp;= 1 - P(X=1 \vert X=1 \cup X=3) = \frac14 \,.
\end{align*}\]</span>
So the expected value is
<span class="math display">\[\begin{align*}
E[X \vert X=1 \cup X=3] = \sum_{1,3} x p_X(x \vert x=1 \cup x=3) = 1 \cdot 3/4 + 3 \cdot 1/4 = 3/2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) We are conducting a negative binomial experiment with <span class="math inline">\(s = 2\)</span>.
The random variable is the number of failures…here, <span class="math inline">\(X = 2\)</span>. So:
<span class="math display">\[\begin{align*}
p_X(2) = \binom{2+2-1}{2} \left(\frac{1}{2}\right)^2 \left(\frac{1}{2}\right)^2 = \frac{3!}{1!2!} \frac{1}{16} = \frac{3}{16} \,.
\end{align*}\]</span></p>
<p>(b) The sum of the data is negatively binomially distributed for
<span class="math inline">\(s=4\)</span> successes and probability of success <span class="math inline">\(p=1/2\)</span>.
The overall number of failures here is <span class="math inline">\(X = 1\)</span>. So
<span class="math display">\[\begin{align*}
p_X(1) = \binom{1+4-1}{1} \left(\frac{1}{2}\right)^4 \left(\frac{1}{2}\right)^1 = \frac{4!}{1!3!} \frac{1}{32} = \frac{4}{32} = \frac18 \,.
\end{align*}\]</span></p>
<p>(c) This is a negative binomial experiment, so
<span class="math inline">\(E[X] = s(1-p)/p\)</span>, which decreases as <span class="math inline">\(p\)</span> increases. Referring to the
confidence interval reference table, we see that <span class="math inline">\(q = 1-\alpha = 0.9\)</span>.</p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>(a) We have that <span class="math inline">\(f_X(x) = 3x^2\)</span> and thus that <span class="math inline">\(F_X(x) = x^3\)</span>. Plugging
these into the formula for <span class="math inline">\(f_{(j)}(x)\)</span> (along with <span class="math inline">\(j=1\)</span>) yields
<span class="math display">\[\begin{align*}
f_{(1)}(x) = \frac{4!}{3!0!}(1-x^3)^3 3 x^2 = 12(1-x^3)^3x^2 \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[X_{(4)}] = \int_0^1 x 12x^{11} = \int_0^1 12x^{12} = \left.\frac{12}{13}x^{13}\right|_0^1 = \frac{12}{13} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) The cdf is <span class="math inline">\(F_X(x) =  \int_0^x y dy = \frac{x^2}{2}\)</span>. Thus
<span class="math display">\[\begin{align*}
f_{(3)} = 3x\left[F_X(x)\right]^{2} = 3x\frac{x^4}{4} = \frac{3}{4} x^5
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,\sqrt{2}]\)</span>.</p>
<p>(b) The variance is <span class="math inline">\(V[X_{(3)}] = E[X_{(3)}^2] - (E[X_{(3)}])^2\)</span>, where
<span class="math display">\[\begin{align*}
E[X_{(3)}] &amp;= \int_0^{\sqrt{2}} x \left(\frac{3}{4} x^5\right)dx =\frac{3}{4} \frac{x^7}{7}\bigg|_0^{\sqrt{2}} = \frac{3\cdot 2^3}{28} \sqrt{2} = \frac{6}{7}\sqrt{2} = 1.212 \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X_{(3)}^2] &amp;= \int_0^{\sqrt{2}} x^2 \left(\frac{3}{4} x^5\right)dx =\frac{3}{4} \frac{x^8}{8}\bigg|_0^{\sqrt{2}} = \frac{3\cdot2^4}{32} = \frac{3}{2} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
V[X_{(3)}] = \frac{3}{2} - \frac{36\cdot 2}{49} = \frac{147 - 144}{98} = \frac{3}{98} = 0.031 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>We have that <span class="math inline">\(f_X(x) = 1\)</span>, <span class="math inline">\(F_X(x) = x\)</span>, and <span class="math inline">\(j = \frac{n+1}{2} = 2\)</span>, so
<span class="math display">\[\begin{align*}
f_{(2)}(x) = \frac{3!}{1!1!}x^1(1-x)^1(1) = 6x(1-x)
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. Therefore
<span class="math display">\[\begin{align*}
P\left(\frac{1}{3} \leq X_{(2)} \leq \frac{2}{3}\right) &amp;= \int_{1/3}^{2/3} 6x(1-x) dx = 6\left[ \frac{x^2}{2}\bigg|_{1/3}^{2/3} - \frac{x^3}{3}\bigg|_{1/3}^{2/3}\right]\\
&amp;= 6\left[ \frac{1}{2} \left( \frac{4}{9} - \frac{1}{9}\right) - \frac{1}{3} \left( \frac{8}{27} - \frac{1}{27}\right)  \right]\\
&amp;= 6\left[ \frac{3}{18} - \frac{7}{81}  \right] = 6\left[ \frac{27}{162} - \frac{14}{162}  \right] = \frac{13}{27} = 0.481 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>We have that <span class="math inline">\(f_X(x) = e^{-x}\)</span> for <span class="math inline">\(x \geq 0\)</span> and thus that
<span class="math inline">\(F_X(x) = 1-e^{-x}\)</span> over the same domain. Thus
<span class="math display">\[\begin{align*}
f_{(2)}(x) &amp;= \frac{3!}{1!1!}(1 - e^{-x})^1\left[1 - (1- e^{-x}) \right]^1 e^{-x} = 6(1 - e^{-x})e^{-x}e^{-x} \\
&amp;= 6(1 - e^{-x})e^{-2x} = 6e^{-2x}  - 6e^{-3x} \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X_{(2)}] &amp;= \underbrace{\int_0^{\infty} 6xe^{-2x} dx}_{\text{by } y=2x, \, dy/2 = dx} - \underbrace{\int_0^{\infty} 6xe^{-3x} dx}_{\text{by } y=3x, \, dy/3 = dx} \\
&amp;= \int_0^{\infty}  \frac{3}{2}y e^{-y} dy - \int_0^{\infty}  \frac{2}{3}y e^{-y} dy\\
&amp;= \frac{3}{2} \Gamma(2) - \frac{2}{3}\Gamma(2) = \frac{3}{2} - \frac{2}{3} =\frac{5}{6} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) A probability density function is the derivative of its associated
cumulative distribution function, so
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx} x^3 = 3x^2 \,.
\end{align*}\]</span></p>
<p>(b) The maximum order statistic has pdf
<span class="math display">\[\begin{align*}
f_{(n)}(x) &amp;= n f_X(x) [F_X(x)]^{n-1} \\
&amp;= n (3x^2) [x^3]^{n-1} = 3n x^2 x^{3n-3} = 3n x^{3n-1} \,.
\end{align*}\]</span></p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
F_{(n)}(x) = [F_X(x)]^n ~~ \Rightarrow ~~ F_{(n)}(x) = x^{3n} \,.
\end{align*}\]</span>
We can also show this via integration:
<span class="math display">\[\begin{align*}
F_{(n)}(x) = \int_0^x f_{(n)}(y) dy = \int_0^x 3n y^{3n-1} dy = \left. y^{3n}\right|_0^x = x^{3n} \,.
\end{align*}\]</span></p>
<p>(d) The expected value is
<span class="math display">\[\begin{align*}
E[X_{(n)}] = \int_0^1 x f_{(n)}(x) dx = \int_0^1 3n x^{3n} dx = \left. \frac{3n}{3n+1} x^{3n+1} \right|_0^1 = \frac{3n}{3n+1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) The cdf within the domain is
<span class="math display">\[\begin{align*}
F_X(x) = \int_0^x \frac12 y dy = \left. \frac14 y^2 \right|_0^x = \frac{x^2}{4} \,.
\end{align*}\]</span></p>
<p>(b) We plug <span class="math inline">\(n\)</span>, <span class="math inline">\(f_X(x)\)</span>, and <span class="math inline">\(F_X(x)\)</span> into the order statistic
pdf equation (where <span class="math inline">\(j = n = 2\)</span>):
<span class="math display">\[\begin{align*}
f_{(2)}(x) = 2 f_X(x) \left[ F_X(x) \right]^{2-1} \left[ 1 - F_X(x) \right]^{2-2} = 2 \left( \frac12 \right) x \left( \frac14 \right) x^2 = \frac14 x^3 \,.
\end{align*}\]</span></p>
<p>(c) The expected value is
<span class="math display">\[\begin{align*}
E[X_{(2)}] = \int_0^2 x f_{(2)}(x) dx = \int_0^2 \frac14 x^4 dx = \frac{1}{20} \left. x^5 \right|_0^2 = \frac{32}{20} = \frac85 = 1.6 \,.
\end{align*}\]</span></p>
<p>(d) They cannot be independent: given the value of one, the other
has to be either smaller (<span class="math inline">\(X_{(1)}\)</span>) or larger (<span class="math inline">\(X_{(2)}\)</span>).</p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>Let <span class="math inline">\(X_1, \ldots, X_n\)</span> denote the samples from the Bernoulli distribution.
The log-likelihood is
<span class="math display">\[\begin{align*}
\ell(X_1, \ldots, X_n | p) = \sum_{i = 1}^n [X_i \log(p) + (1 - X_i) \log (1-p)] \,.
\end{align*}\]</span>
The first two derivatives are
<span class="math display">\[\begin{align*}
\frac{d}{dp} \ell(X_1, \ldots, X_n | p) &amp;= \sum_{i = 1}^n\bigg[ \frac{X_i}{p}  - \frac{(1 - X_i)}{1-p}\bigg] \\
\frac{d^2}{dp^2} \ell(X_1,\ldots, X_n | p ) &amp;= \sum_{i = 1}^n \bigg[-\frac{X_i}{p^2}- \frac{(1- X_i)}{(1-p)^2}\bigg] \,.
\end{align*}\]</span>
The Fisher information is thus
<span class="math display">\[\begin{align*}
I_n(p) = E\bigg[-\frac{d^2}{dp^2} \ell(X_1,\ldots, X_n | p )\bigg] &amp;= \sum_{i=1}^n \bigg[\frac{E[X_i]}{p^2} + \frac{E[(1- X_i)]}{(1-p)^2}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{p}{p^2} + \frac{1-p}{(1-p)^2}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{1}{p} + \frac{1}{1-p}\bigg] \\
&amp;= \sum_{i=1}^n \bigg[\frac{1-p}{p(1-p)} + \frac{p}{p(1-p)}\bigg] \\
&amp;= \frac{n}{p(1-p)} \,,
\end{align*}\]</span>
and the asymptotic distribution of the MLE is <span class="math inline">\(\mathcal{N}(p,\frac{p(1-p)}{n})\)</span>.</p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) The log-likelihood and its derivative are
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) &amp;= \log (1-p) \sum_{i=1}^n (x_i - 1)  + n \log p\\
\ell&#39;(p \vert \mathbf{x}) &amp;= -\frac{\sum_{i=1}^n x_i - n}{1 - p} + \frac{n}{p} \,.
\end{align*}\]</span>
Setting the derivative to zero, we find that
<span class="math display">\[\begin{align*}
\frac{\sum_{i=1}^n x_i - n}{1 - p} &amp;= \frac{n}{p} \\
\Rightarrow ~~~ \left(\sum_{i=1}^n x_i - n\right) p &amp;= n (1 - p) \\
\Rightarrow ~~~ p \,\sum_{i=1}^n x_i &amp;= n \\
\Rightarrow ~~~ \hat{p} &amp;= \frac{1}{\bar{X}} \,.
\end{align*}\]</span>
Using the invariance property of the MLE, we find that
<span class="math inline">\(\widehat{1/p}_{MLE} = \bar{X}\)</span>.</p>
<p>(b) The variance of this estimator is
<span class="math display">\[\begin{align*}
V\left[\widehat{1/p}_{MLE}\right] = V \left[ \frac{\sum_{i=1}^n X_i}{n} \right] = \frac{V[X]}{n} = \frac{1 - p}{np^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>The likelihood for <span class="math inline">\(p\)</span> is
<span class="math display">\[\begin{align*}               
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n p_X(x_i \vert p) = \prod_{i=1}^n -\frac{1}{\log(1-p)} \frac{p^{x_i}}{x_i} = \underbrace{- \prod_{i=1}^n \frac{1}{x_i}}_{h(\mathbf{x})} \cdot \underbrace{\frac{1}{[\log(1-p)]^n} p^{\sum_{i=1}^n x_i}}_{g(p,\mathbf{x})} \,.
\end{align*}\]</span><br />
Given the expression for <span class="math inline">\(g(\cdot)\)</span>, we can see that a sufficient statistic
for <span class="math inline">\(p\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}           
\mathcal{L}(a,b \vert \mathbf{x}) &amp;= \prod_{i=1}^n a b x_i^{a-1} (1-x_i^a)^{b-1}\\
&amp;= a^n b^n \left(\prod_{i=1}^n  x_i\right)^{a-1} \left(\prod_{i=1}^n (1-x_i^a)\right)^{b-1}
\end{align*}\]</span>
At first glance, it seems that we can take
<span class="math display">\[\begin{align*}
\mathbf{Y} = \left\{ \prod_{i=1}^n x_i, \prod_{i=1}^n (1-x_i^a) \right\}
\end{align*}\]</span>
as the joint sufficient statistics. However, note that the parameter <span class="math inline">\(a\)</span>
occurs in the second statistic. Because this second statistic
includes a parameter value, it cannot be a sufficient statistic…and
thus we conclude that we cannot identify joint sufficient statistics for <span class="math inline">\(a\)</span>
and <span class="math inline">\(b\)</span>.</p>
<p>(b) With <span class="math inline">\(a=1\)</span> the density function becomes <span class="math inline">\(f_X(x) = b \cdot (1-x)^{b-1}\)</span>,
with resulting likelihood
<span class="math display">\[\begin{align*}
\mathcal{L}(b \vert \mathbf{x}) &amp;= \prod_{i=1}^n  b \cdot (1-x_i)^{b-1}\\
&amp;=  b^n \left(\prod_{i=1}^n (1-x_i)\right)^{b-1}\\
&amp;=  h(\mathbf{x}) g(b,\mathbf{x}) \,.
\end{align*}\]</span>
Hence, a sufficient statistic for <span class="math inline">\(b\)</span> is <span class="math inline">\(Y = \prod_{i=1}^n (1-X_i)\)</span>.</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \beta) &amp;= \prod_{i=1}^n \frac{x_i}{\beta^2} \exp\left(-\frac{x}{\beta}\right) \\
&amp;= \underbrace{\prod_{i=1}^n x_i}_{h(\mathbf{x})} \cdot \underbrace{\frac{1}{\beta^{2n}} \exp\left(-\frac{1}{\beta}\sum_{i=1}^n x_i\right)}_{g(\beta,\mathbf{x})} \,.
\end{align*}\]</span>
We can examine <span class="math inline">\(g(\cdot)\)</span> and immediately identify that a sufficient statistic
for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[Y] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n 2\beta = 2n\beta \,.
\end{align*}\]</span>
Hence
<span class="math display">\[\begin{align*}
E\left[\frac{Y}{2n}\right] = \beta
\end{align*}\]</span>
and <span class="math inline">\(\hat{\beta}_{MVUE} = Y/2n = \bar{X}/2\)</span>.</p>
<p>(c) Utilizing the general rule from 235:
<span class="math display">\[\begin{align*}
V[\hat{\beta}_{MVUE}] = V\left[\frac{\bar{X}}{2}\right] = \frac{V[\bar{X}]}{4} = \frac{V[X]}{4n} = \frac{2\beta^2}{4n} = \frac{\beta^2}{2n} \,.
\end{align*}\]</span></p>
<p>(d) The first step is to write down the log-likelihood for one datum:
<span class="math display">\[\begin{align*}
\ell(\beta \vert x) = \log f_X(x \vert \beta) = \log x - \frac{x}{\beta} - 2\log\beta \,.
\end{align*}\]</span>
We take the first two derivatives:
<span class="math display">\[\begin{align*}
\frac{d\ell}{d\beta} &amp;= \frac{x}{\beta^2} - \frac{2}{\beta} \\
\frac{d^2\ell}{d\beta^2} &amp;= -\frac{2x}{\beta^3} + \frac{2}{\beta^2} \,,
\end{align*}\]</span>
and then compute the expected value:
<span class="math display">\[\begin{align*}
I(\beta) = E\left[ \frac{2X}{\beta^3} - \frac{2}{\beta^2} \right] = \frac{2}{\beta^3}E[X] - \frac{2}{\beta^2} = \frac{2}{\beta^3}(2\beta) - \frac{2}{\beta^2} = \frac{2}{\beta^2} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(I_n(\beta) = (2n)/\beta^2\)</span> and the CRLB is <span class="math inline">\(1/I_n(\beta) = \beta^2/(2n)\)</span>.
The MVUE achieves the CRLB.</p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>(a) We can factorize the likelihood as follows:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \frac{1}{\theta} e^{x_i} e^{-e^{x_i}/\theta} = e^{\sum_{i=1}^n x_i} \theta^{-n} e^{-(\sum_{i=1}^n e^{x_i})/\theta} \,.
\end{align*}\]</span>
The first term does not contain <span class="math inline">\(\theta\)</span> and thus can be ignored. Thus
we identify <span class="math inline">\(Y = \sum_{i=1}^n e^{X_i}\)</span> as a sufficient statistic.</p>
<p>(b) We can determine <span class="math inline">\(E[Y]\)</span> by noticing that <span class="math inline">\(Y \sim\)</span> Gamma<span class="math inline">\((n,\theta)\)</span>, as stated in the question…so <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(E[Y/n] = \theta\)</span>. Thus the MVUE for <span class="math inline">\(\theta\)</span> is <span class="math inline">\((\sum_{i=1}^n e^{X_i})/n\)</span>.</p>
<p>(c) The MVUE will be a function of the sufficient statistic for <span class="math inline">\(\theta\)</span>, so let’s try <span class="math inline">\((\sum_{i=1}^n e^{X_i})^2\)</span>:
<span class="math display">\[\begin{align*}
E\left[\left(\sum_{i=1}^n e^{X_i}\right)^2\right] = V\left[\left(\sum_{i=1}^n e^{X_i}\right)\right] + E\left[\sum_{i=1}^n e^{X_i}\right]^2 = n \theta^2 + (n\theta)^2 = n(n+1)\theta^2 \,.
\end{align*}\]</span>
Therefore <span class="math inline">\((\sum_{i=1}^n e^{X_i})^2/(n(n+1))\)</span> is the MVUE for <span class="math inline">\(\theta^2\)</span>.</p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>(a) We factorize the likelihood:
<span class="math display">\[\begin{align*}
\mathcal{L}(a \vert \mathbf{x}) = \prod_{i=1}^n \sqrt{\frac{2}{\pi}} \frac{x_i^2}{a^3} e^{-x_i^2/(2a^2)} = \left[ \left(\frac{2}{\pi}\right)^{n/2} \left( \prod_{i=1}^n x_i^2 \right) \right] \cdot \left[ \frac{1}{a^{3n}} e^{-(\sum_{i=1}^n x_i^2)/(2a^2)} \right] = h(\mathbf{x}) \cdot g(a,\mathbf{x}) \,.
\end{align*}\]</span>
We can read off from the <span class="math inline">\(g(\cdot)\)</span> function term that
<span class="math inline">\(Y = \sum_{i=1}^n X_i^2\)</span>. (Including the minus sign, for instance,
is fine because a function of a
sufficient statistic is itself sufficent and we will get to the same
MVUE in the end.)</p>
<p>(b) We utilize the shortcut formula:
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = a^2 \frac{(3 \pi - 8)}{\pi} + (2a)^2 \frac{2}{\pi} = 3 a^2 + \frac{8 a^2}{\pi} - \frac{8 a^2}{\pi} = 3 a^2 \,.
\end{align*}\]</span></p>
<p>(c) We compute the expected value for <span class="math inline">\(Y\)</span>:
<span class="math display">\[\begin{align*}
E[Y] = E\left[\sum_{i=1}^n X_i^2\right] = \sum_{i=1}^n E[X_i^2] = n E[X^2] = 3 n a^2 \,.
\end{align*}\]</span>
Thus the expected value for <span class="math inline">\(Y/(3n)\)</span> is <span class="math inline">\(a^2\)</span>:
<span class="math display">\[\begin{align*}
\widehat{a^2}_{MVUE} = \frac{1}{3n} \sum_{i=1}^n X_i^2 \,.
\end{align*}\]</span></p>
<p>(d) There is no invariance principle for the MVUE. Maybe
the desired result holds and maybe it doesn’t, but we cannot simply
state that it does.</p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>We are constructing an upper-tail test where the test statistic is
trivially <span class="math inline">\(Y = X\)</span>. (So the NP Lemma does not <em>really</em> come into play here,
given the lack of choices for the test statistic.)
The expected value of <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{align*}
E[Y] = \int_0^2 y f_Y(y) dy = \int_0^2 \frac{\theta}{2^\theta} y^{\theta} dy = \left. \frac{\theta}{2^\theta} \frac{y^{\theta+1}}{\theta+1} \right|_0^2 = \frac{2\theta}{\theta+1} \,.
\end{align*}\]</span>
<span class="math inline">\(E[Y]\)</span> increases as <span class="math inline">\(\theta\)</span> increases, so we will be on the “yes” line of
the hypothesis test reference table. Hence the rejection region will be of
the form
<span class="math display">\[\begin{align*}
y_{\rm obs} &gt; F_Y^{-1}(1-\alpha \vert \theta_o) \,.
\end{align*}\]</span>
The cdf <span class="math inline">\(F_Y(y)\)</span> is
<span class="math display">\[\begin{align*}
F_Y(y) = \int_0^y \frac{\theta}{2^\theta} u^{\theta-1} du = \left. \frac{u^\theta}{2^\theta}\right|_0^y = \left(\frac{y}{2}\right)^\theta \,,
\end{align*}\]</span>
and the inverse cdf <span class="math inline">\(F_Y^{-1}(q)\)</span> is <span class="math inline">\(y = 2q^{1/\theta}\)</span>.
Hence the test we seek rejects the null hypothesis if
<span class="math display">\[\begin{align*}
y_{\rm obs} &gt; 2(1-\alpha)^{1/\theta_o} \,.
\end{align*}\]</span>
The rejection-region boundary does not
depend on <span class="math inline">\(\theta_a\)</span>, so we know that the test is the most powerful one
for all alternative values <span class="math inline">\(\theta_a &gt; \theta_o\)</span>…thus it is a
uniformly most powerful test.</p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) The likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^n \frac{\theta}{\beta}x_i^{\theta-1}\exp\left(-\frac{x_i^\theta}{\beta}\right) = \left[ \theta x_i^{\theta-1} \right] \cdot \left[ \frac{1}{\beta} \exp\left(-\frac{1}{\beta} x_i^\theta \right) \right] = h(\mathbf{x}) \cdot g(\beta,\mathbf{x}) \,,
\end{align*}\]</span>
thus a sufficient statistic for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i^\theta\)</span>.</p>
<p>(b) We are given that <span class="math inline">\(X^\theta \sim\)</span> Exp(<span class="math inline">\(\beta\)</span>). The mgf for an
exponential distribution is
<span class="math display">\[\begin{align*}
m_X(t) = (1 - \theta t)^{-1} \,,
\end{align*}\]</span>
and hence the mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> will be
<span class="math display">\[\begin{align*}
m_Y(t) = \prod_{i=1}^n (1 - \theta t)^{-1} = \left[ (1 - \theta t)^{-1} \right]^n = (1 - \theta t)^{-n} \,.
\end{align*}\]</span>
Following the hint given in the question, we find that <span class="math inline">\(Y\)</span> is a
gamma-distributed random variable with “shape” parameter <span class="math inline">\(n\)</span> and
“scale” parameter <span class="math inline">\(\theta\)</span>. (There are two common parameterizations of
the gamma distribution<span class="math inline">\(-\)</span>shape/scale and shape/rate<span class="math inline">\(-\)</span>and it is imperative
to determine the correct one! This will impact the answer to part (c).)</p>
<p>(c) The statistic <span class="math inline">\(Y\)</span> has expected value <span class="math inline">\(E[Y] = n\theta\)</span>, which increases
with <span class="math inline">\(\theta\)</span>. Hence we utilize the upper-tail/yes line of the hypothesis
test reference table: <span class="math inline">\(y_{\rm RR} = F_Y^{-1}(1 - \alpha \vert \theta_o)\)</span>,
or, in code,</p>
<pre><code>y.rr &lt;- qgamma(1-alpha,shape=n,scale=theta)</code></pre>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) The moment-generating function for the random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{align*}
m_X(t) = E\left[e^{tX}\right] &amp;= \int_b^\infty e^{tx} \frac{1}{\theta} e^{-(x-b)/\theta} dx \\
&amp;= e^{b/\theta} \frac{1}{\theta} \int_b^\infty e^{-x(1/\theta - t)} dx \\
&amp;= e^{b/\theta} \frac{1}{\theta} \frac{e^{-b(1/\theta - t)}}{(1/\theta-t)} \\
&amp;= e^{bt} (1-t\theta)^{-1} \,.
\end{align*}\]</span>
(Here we make the implicit assumption
that <span class="math inline">\(t &lt; 1/\theta\)</span>, so that the integral evaluated at <span class="math inline">\(\infty\)</span> is zero.)
This is the final answer, but
recall that when <span class="math inline">\(X = U+b\)</span>, <span class="math inline">\(m_X(t) = e^{bt} m_U(t)\)</span>. Since we recognize
that <span class="math inline">\((1-t\theta)^{-1}\)</span> is the mgf for an exponential distribution,
we can state that <span class="math inline">\(U = X-b\)</span> is an exponentially distributed random variable.</p>
<p>(b) The mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
<span class="math display">\[\begin{align*}
m_Y(t) = \prod_{i=1}^n e^{bt} (1-t\theta)^{-1} = \left[ e^{bt} (1-t\theta)^{-1} \right]^n = e^{nbt} (1-t\theta)^{-n} \,.
\end{align*}\]</span></p>
<p>(c) Going back to our answer for (a) (and our answer for the previous
problem), we recognize that the mgf for <span class="math inline">\(\sum_{i=1}^n U_i\)</span> is
<span class="math inline">\((1-t\theta)^{-n}\)</span>, which is the mgf for a gamma distribution with
shape parameter <span class="math inline">\(n\)</span> and scale parameter <span class="math inline">\(\theta\)</span>. Hence
<span class="math inline">\(Y&#39; = Y - nb \sim \text{Gamma}(n,\theta)\)</span>.</p>
<p>(d) We are on the lower-tail/yes line of the hypothesis test reference
table: <span class="math inline">\(y_{\rm RR}&#39; = F_Y^{-1}(\alpha \vert \theta_o)\)</span>, or, in code,</p>
<pre><code>y.rr.prime &lt;- qgamma(alpha,shape=n,scale=theta)</code></pre>
<p>We would reject the null hypothesis if <span class="math inline">\(y_{\rm obs} - nb &lt; y_{\rm RR}&#39;\)</span>.
Because this test is constructed using a sufficient statistic and because
no value of the alternative hypothesis appears in the definition of the
rejection region, we indeed have defined a uniformly most powerful test
of <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>.</p>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>(a) Let’s first find a sufficient statistic:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \theta e^{-\theta x_i} = \theta^n e^{-\theta \sum_{i=1}^n x_i} \,.
\end{align*}\]</span>
A sufficient statistic is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.
We are conducting a lower-tail test, and
since <span class="math inline">\(E[X] = 1/\theta\)</span> decreases as <span class="math inline">\(\theta\)</span> increases,
we are on the “no” line of the reference table.
We reject the null if <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i &gt; y_{\rm RR}\)</span>.</p>
<p>(b) <span class="math inline">\(\theta_o\)</span> is plugged in to compute the rejection-region boundary, but
<span class="math inline">\(\theta_a\)</span> does not appear at all. Hence the defined test is uniformly
most powerful, since it is most powerful for any value of <span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>(a) The sampling distribution is Binom(<span class="math inline">\(nk,p\)</span>). We can determine this
using the method of moment-generating functions, if necessary.</p>
<p>(b) <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, so we are on the upper-tail/“yes” line
of the hypothesis test reference tables. The rejection-region
boundary is given by <span class="math inline">\(F_Y^{-1}(1-\alpha \vert \theta_o)\)</span>, or, in
code, with <span class="math inline">\(p_o\)</span> in place of <span class="math inline">\(\theta_o\)</span>,</p>
<pre><code>qbinom(1-alpha,n*k,p.o)</code></pre>
<p>(c) For an upper-tail/yes test, the <span class="math inline">\(p\)</span>-value is
<span class="math inline">\(1 - F_Y(y_{\rm obs} \vert \theta_o)\)</span>. In code, with <span class="math inline">\(p_o\)</span> in
place of <span class="math inline">\(\theta_o\)</span>, the <span class="math inline">\(p\)</span>-value is</p>
<pre><code>1 - pbinom(y.obs,n*k,p.o)</code></pre>
<p>However, we have to apply a discreteness correction, because otherwise
we will not be summing over the correct range of <span class="math inline">\(y\)</span> values, i.e., our
<span class="math inline">\(p\)</span>-value will be wrong. Here, that factor is <span class="math inline">\(-1\)</span>, applied to the input.
So…</p>
<pre><code>1 - pbinom(y.obs-1,n*k,p.o)</code></pre>
<p>is the final answer.</p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>This is straightforward <em>if</em> we remember to set the link
function to the equation for the line:
<span class="math display">\[\begin{align*}
-(Y \vert x)^{-1} = \beta_0 + \beta_1 x ~~~ \Rightarrow ~~~ (Y \vert x)^{-1} = -\beta_0 - \beta_1 x ~~~ \Rightarrow ~~~ Y \vert x = (-\beta_0 - \beta_1 x)^{-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>(a) The degrees of freedom for the residual deviance
is <span class="math inline">\(n-p\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters (here, two: <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span>). Hence <span class="math inline">\(n = 32\)</span>.</p>
<p>(b) <span class="math inline">\(\beta_1\)</span> is set to zero to compute the null deviance.
So <span class="math inline">\(-2\log\mathcal{L}_{\rm max} = 43.230\)</span>.</p>
<p>(c) The odds are <span class="math inline">\(O(x) = \exp(\hat{\beta}_0 + \hat{\beta}_1 x)\)</span> or
just <span class="math inline">\(\exp(\hat{\beta}_0)\)</span> for <span class="math inline">\(x = 0\)</span>, meaning thet <span class="math inline">\(O(x=0) = \exp(12.040)\)</span>.</p>
<p>(d) The estimated slope <span class="math inline">\(\hat{\beta}_1\)</span> is negative, and we
know that <span class="math inline">\(O(x+1) = O(x) \exp(\hat{\beta}_1)\)</span>, so we know that
<span class="math inline">\(O(x+1) &lt; O(x)\)</span>…the odds <em>decrease</em> as <span class="math inline">\(x\)</span> increases.</p>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
O(x) = \frac{p \vert x}{1 - p \vert x} = \frac{0.1}{1-0.1} = \frac19 = 0.111 \,.
\end{align*}\]</span></p>
<p>(b) The new odds are
<span class="math display">\[\begin{align*}
O(589+100) = \exp(\hat{\beta}_0 + \hat{\beta_1}(589+100)) = O(589) \exp(100\hat{\beta}_1) = \frac19 \exp(0.14684) = 0.129 \,.
\end{align*}\]</span></p>
<p>(c) We have that <span class="math inline">\(Y_1 = 0\)</span> and <span class="math inline">\(\hat{Y}_i = 0.07\)</span>, so
<span class="math display">\[\begin{align*}
d_1 &amp;= \mbox{sign}(Y_1-\hat{Y}_1)\sqrt{-2[Y_1\log\hat{Y}_1+(1-Y_1)\log(1-\hat{Y}_1)]} = \mbox{sign}(-0.07) \sqrt{-2\log(0.93)} \\
&amp;= -\sqrt{-2\log(0.93)} = 0.381 \,.
\end{align*}\]</span></p>
<p>(d) The null deviance is computed assuming <span class="math inline">\(\beta_1 = 0\)</span>. This
model lies “farther” from the observed data than the model with
<span class="math inline">\(\hat{\beta}_1 = 0.00147\)</span>, meaning it deviates more from the data, meaning
that the deviance would be higher.</p>
<hr />
<ul>
<li>Problem 26</li>
</ul>
<p>Let’s start by collecting the basic pieces of information that
we would combine in a Naive Bayes regression model:
<span class="math display">\[\begin{align*}
p(0) = 3/5 ~~\mbox{and}~~ p(1) = 2/5 \,,
\end{align*}\]</span>
where <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> are the two response (i.e., <span class="math inline">\(Y\)</span>) values.
Next up, the conditionals:
<span class="math display">\[\begin{align*}
p(x1 = N \vert 0) = 2/3 ~~ &amp;\mbox{and}&amp; ~~ p(x1 = Y \vert 0) = 1/3 \\
p(N \vert 1) = 1/2 ~~ &amp;\mbox{and}&amp; ~~ P(Y \vert 1) = 1/2 \\
\\
p(x2 = T \vert 0) = 2/3 ~~ &amp;\mbox{and}&amp; ~~ p(x2 = F \vert 0) = 1/3 \\
p(T \vert 1) = 1/2 ~~ &amp;\mbox{and}&amp; ~~ P(F \vert 1) = 1/2 \,.
\end{align*}\]</span>
The estimated probability of observing a datum of Class 0 given <code>Y</code> and <code>F</code> is
thus
<span class="math display">\[\begin{align*}
p(0 \vert Y,F) &amp;= \frac{p(Y \vert 0) p(F \vert 0) p(0)}{p(Y \vert 0) p(F \vert 0) p(0) + p(Y \vert 1) p(F \vert 1) p(1)} \\
&amp;= \frac{1/3 \cdot 1/3 \cdot 3/5}{1/3 \cdot 1/3 \cdot 3/5 + 1/2 \cdot 1/2 \cdot 2/5} \\
&amp;= \frac{1/15}{1/15 + 1/10} = \frac{2/30}{2/30+3/30} = \frac{2}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 27</li>
</ul>
<p>(a) The pdf is of the form
<span class="math display">\[\begin{align*}
k x^{\alpha-1} (1-x)^{\beta-1} \,,
\end{align*}\]</span>
with <span class="math inline">\(0 \leq x \leq 1\)</span>, so what we have is a beta distribution:
<span class="math inline">\(X \sim\)</span> Beta<span class="math inline">\((1,2)\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[X] = \frac{\alpha}{\alpha+\beta} = \frac{1}{3}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)} + \left(\frac{1}{3}\right)^2 = \frac{2}{36} + \frac{4}{36} = \frac{1}{6} \,.
\end{align*}\]</span></p>
<p>(c) These expressions are straightforward to evaluate:
<span class="math display">\[\begin{align*}
E[C] = E[10X] = 10E[X] = \frac{10}{3} = 3.333
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
V[C] = E[C^2] - (E[C])^2 = E[100X^2] - \frac{100}{9} = 100E[X^2] - \frac{100}{9} = \frac{150}{9} - \frac{100}{9} = \frac{50}{9} = 5.556 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 28</li>
</ul>
<p>(a) <span class="math inline">\(f_X(x) = 12x - 24x^2 + 12x^3 = 12x(1-x)^2\)</span> for <span class="math inline">\(x \in [0,1]\)</span>…so this is
a Beta(2,3) distribution.</p>
<p>(b) <span class="math inline">\(X \sim {\rm Beta}(2,3) \Rightarrow E[X] = \alpha/(\alpha+\beta) = 2/(2+3) = 2/5 = 0.4\)</span> ,.</p>
<hr />
<ul>
<li>Problem 29</li>
</ul>
<p>One way to solve this problem is to utilize the shortcut formula:
<span class="math inline">\(E[X^2] = V[X] + E[X]^2\)</span>. With this in hand:
<span class="math display">\[\begin{align*}
V[X] = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} = \frac{6}{25 \cdot 6} = \frac{1}{25}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E[X] = \left(\frac{\alpha}{\alpha + \beta}\right)^2 = \left(\frac{2}{5}\right)^2 = \frac{4}{25} \,.
\end{align*}\]</span>
Therefore,
<span class="math display">\[\begin{align*}
E[X^2] = \frac{1}{25} +  \frac{4}{25} = \frac{1}{5} \,.
\end{align*}\]</span>
A second way to solve this problem is by brute-force integration:
<span class="math display">\[\begin{align*}
E[X^2] &amp;= \int_0^1 x^2 \frac{x(1-x)^2}{B(2,3)}dx = \int_0^1 \frac{x^3(1-x)^2}{B(2,3)}dx = \int_0^1 \frac{x^3(1-x)^2}{B(2,3)} \frac{B(4,3)}{B(4,3)}dx \\
&amp;= \frac{B(4,3)}{B(2,3)}\underbrace{\int_0^1 \frac{x^3(1-x)^2}{B(4,3)}dx}_{=1} \\
&amp;= \frac{B(4,3)}{B(2,3)} = \frac{\Gamma(4) \Gamma(3)}{\Gamma(7)}\frac{\Gamma(5)}{\Gamma(2)\Gamma(3)}\ =  \frac{\Gamma(4) \Gamma(5)}{\Gamma(2)\Gamma(7)} = \frac{3! 4!}{1!6!} = \frac{1}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 30</li>
</ul>
<p>(a) The median is the second sampled datum. The pdf for <span class="math inline">\(X\)</span> is
<span class="math inline">\(f_X(x) = 3x^2\)</span> and <span class="math inline">\(F_X(x) = x^3\)</span>, both for <span class="math inline">\(x \in [0,1]\)</span>. Thus
<span class="math display">\[\begin{align*}
f_{(2)}(x) = \frac{3!}{1!1!} [x^3]^1 [1 - x^3]^1 3x^2 = 18 x^5 (1 - x^3) \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>, and
<span class="math display">\[\begin{align*}
E[X_{(2)}] &amp;= \int_0^1 x 18 x^5 (1-x^3) dx = 18 \int_0^1 (x^6 - x^9) dx \\
&amp;= 18 \left( \left.\frac{x^7}{7}\right|_0^1 - \left.\frac{x^{10}}{10}\right|_0^1 \right) = 18 \left( \frac{1}{7}-\frac{1}{10} \right) = \frac{18 \cdot 3}{70} = \frac{27}{35} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 31</li>
</ul>
<p>(a) This is a Beta(2,2) distribution.</p>
<p>(b) We can determine <span class="math inline">\(c\)</span> via brute-force integration:
<span class="math display">\[\begin{align*}
c \int_0^1 x(1-x) dx = c\left[ \int_0^1 x dx - \int_0^1 x^2 dx \right] &amp;= c \left[ \left.\frac{x^2}{2}\right|_0^1 - \left.\frac{x^3}{3}\right|_0^1 \right] \\
&amp;= c \left[ \frac{1}{2} - \frac{1}{3} \right] \\
&amp;= c \frac{1}{6} = 1 ~~\Rightarrow~~ c = 6 \,.
\end{align*}\]</span>
Alternatively, we can recognize that
<span class="math display">\[\begin{align*}
c &amp;= \frac{1}{B(2,2)} = \frac{\Gamma(4)}{\Gamma(2) \Gamma(2)} = \frac{3!}{1! 1!} = 6 \,.
\end{align*}\]</span></p>
<p>(c) Since <span class="math inline">\(\alpha = \beta\)</span>, the distribution is symmetric around
<span class="math inline">\(x = 1/2\)</span>, which is its mean value.</p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
P(X \leq 1/4 \vert X \leq 1/2) &amp;= \frac{P(X \leq 1/4 \cap X \leq 1/2)}{P(X \leq 1/2)} = \frac{P(X \leq 1/4)}{P(X \leq 1/2)} = \frac{P(X \leq 1/4)}{1/2} \\
&amp;= 2P(X \leq 1/4) = 2 \int_0^{1/4} 6 x (1-x) dx = 12 \int_0^{1/4} x (1-x) dx \\
&amp;= 12 \left[ \left.\frac{x^2}{2}\right|_0^{1/4} - \left.\frac{x^3}{3}\right|_0^{1/4} \right] = 12 \left[ \frac{1}{32} - \frac{1}{192} \right] = 12 \frac{5}{192} = \frac{60}{192} = \frac{30}{96} = \frac{5}{16} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 32</li>
</ul>
<p>(a) We carry out a chi-square goodness-of-fit test:
<span class="math display">\[\begin{align*}
W = \sum_{i=1}^n \frac{(X_i - kp_i)^2}{kp_i} = \frac{1}{7}[(10-7)^2+(5-7)^2+(6-7)^2] = 2 \,.
\end{align*}\]</span></p>
<p>(b) There are <span class="math inline">\(m=3\)</span> outcomes, but we lose one degree of freedom
because of the constraint that
<span class="math inline">\(\sum_{i=1}^m X_i = 21\)</span>, so the number of degrees of freedom is 2.</p>
<p>(c) We are given that <span class="math inline">\(\alpha = 0.1\)</span>, so we want
<span class="math inline">\(F_W^{-1}(0.9) = 4.61\)</span>.</p>
<hr />
<ul>
<li>Problem 33</li>
</ul>
<p>(a) The correct answer is <em>homogeneity</em>, since the researcher is splitting the respondents into groups by giving them the different types of leaflets to read, and the goal is to determine the willingness to spend government funding is homogenous across the type of pamphlets.</p>
<p>(b) Since chi-square statistics involving summing squared differences over all combinations of leaflets and spending opinions, there are <span class="math inline">\(12 = 3 \cdot 4\)</span> terms.</p>
<p>(c) Since the all the information but one for each factor is sufficent, the test statistics will follow a chi-square distribution with <span class="math inline">\(6 = (3-1) \cdot (4-1)\)</span> degrees of freedom.</p>
<p>(d) The correct answer is (i) since <span class="math inline">\(p\)</span>-value is defined as a probability of events at least as extreme as the what actually observed under the null hypothesis, and larger values of the test statistic here correspond to larger differences between what we would expect under the null and what we observe.</p>
<hr />
<ul>
<li>Problem 34</li>
</ul>
<p>(a) The exponential distribution with mean 1 is <span class="math inline">\(e^{-x}\)</span>, for <span class="math inline">\(x \geq 0\)</span>. Therefore, the probabilities for arriving between 1 and 2 minutes after the previous person is given by
<span class="math display">\[\begin{align*}
\int_1^2 e^{-x} dx =  -\left.e^{-x}\right|_1^2 = 0.233 \,.
\end{align*}\]</span></p>
<p>(b) The probabilities for the other two “bins” are
<span class="math display">\[\begin{align*}
\int_0^1 e^{-x} dx &amp;= -\left.e^{-x}\right|_0^1 = 0.632 \\
\int_2^\infty e^{-x} dx &amp;= 1 - 0.632 - 0.233 = 0.135 \,.
\end{align*}\]</span>
We can now carry out a chi-square goodness-of-fit test:
<span class="math display">\[\begin{align*}
W = \frac{(52-63.2)^2}{63.2} + \frac{(23-23.3)^2}{23.3} + \frac{(25-13.5)^2}{13.5} = 1.985 + 0.004 + 9.796 = 11.785 \,,
\end{align*}\]</span>
and <span class="math inline">\(W \sim \chi_2^2\)</span>. The rejection region is <span class="math inline">\(W &gt; w_{\rm RR} = 5.991\)</span>
(i.e., <code>qchisq(0.95,2)</code>), and
the <span class="math inline">\(p\)</span>-value is 0.0028 (i.e., <code>1-pchisq(11.785,2)</code>).
We have sufficient evidence to reject
the null hypothesis and conclude that the time elapsed between people walking
through a particular door is <em>not</em> exponentially distributed with mean 1.</p>
<p>(c) If <span class="math inline">\(n = 10\)</span> and there are 3 bins, then there is no way that
<span class="math inline">\(np_i \geq 5\)</span> for all bins. Thus the chi-square goodness-of-fit test should
not be applied.</p>
<hr />
<ul>
<li>Problem 35</li>
</ul>
<p>(a) We have that <span class="math inline">\(p_{\rm out} = 8/9\)</span> and <span class="math inline">\(p_{\rm in} = 1/9\)</span>,
so <span class="math inline">\(kp_{\rm out} = 180 (8/9) = 160\)</span> and <span class="math inline">\(kp_{\rm in} = 180 (1/9)
= 20\)</span>.</p>
<p>(b) We perform a chi-square goodness-of-fit test:
<span class="math display">\[\begin{align*}
W = \frac{(150-160)^2}{160} + \frac{(30-20)^2}{20} = \frac{100}{160} + \frac{100}{20} = \frac58 + 5 = 5.625~(\mbox{or}~5~5/8) \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(W\)</span> is sampled from a chi-square distribution for <span class="math inline">\(2-1 = 1\)</span>
degree of freedom.</p>
<p>(d) The rejection region for a chi-square GoF test is
<span class="math inline">\(W &gt; w_{\rm RR}\)</span>, so, since 5.625 is greater than 3.841, we would
reject the null hypothesis.</p>
<hr />
<ul>
<li>Problem 36</li>
</ul>
<p>When the null hypothesis is true, the <span class="math inline">\(p\)</span>-value is sampled uniformly between 0 and 1. Hence the probability of rejecting the null is <span class="math inline">\(P(p \leq \alpha) = \alpha\)</span>. When we collect <span class="math inline">\(k\)</span> <span class="math inline">\(p\)</span>-values, we are fixing the number of trials, so <span class="math inline">\(X\)</span>, the number of <span class="math inline">\(p\)</span>-values observed to be <span class="math inline">\(\leq \alpha\)</span>, will be binomially distributed, and thus
<span class="math display">\[\begin{align*}
P(X = x) = p_X(x) = {k \choose x} \alpha^x (1-\alpha)^{k-x} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 37</li>
</ul>
<p>This is an moment-generating function problem. For a geometric distribution,
<span class="math display">\[\begin{align*}
m_{X_i}(t) = \frac{p}{[1 - (1-p)e^t]} \,.
\end{align*}\]</span>
As for the sum,
<span class="math display">\[\begin{align*}
m_{Y}(t) = \prod_{i=1}^3 m_{X_i}(t) = \left(\frac{p}{[1 - (1-p)e^t]}\right)^3 \,,
\end{align*}\]</span>
which is the mgf for a negative binomial distribution; specifically, <span class="math inline">\(Y \sim\)</span> NBinom(<span class="math inline">\(3,p\)</span>).</p>
<hr />
<ul>
<li>Problem 38</li>
</ul>
<p>The moment-generating functions for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are
<span class="math display">\[\begin{align*}
m_{X_1}(t) &amp;= E[e^{tX}] = e^{ta}p(a) = e^{at} \\
m_{X_2}(t) &amp;= (1-p) + pe^t \,.
\end{align*}\]</span>
As for the sum,
<span class="math display">\[\begin{align*}
m_Y(t) = m_{X_1}(t) m_{X_2}(t) = e^{at}((1-p) + pe^t) = p e^{(a+1)t} + (1-p)e^{at} \,.
\end{align*}\]</span>
Given this,
<span class="math display">\[\begin{align*}
E[Y] &amp;= \frac{d m_Y(t)}{dt}\bigg|_0 = p(a+1)e^{(a+1)t} + (1-p)ae^{at} \bigg|_0 = p(a+1) + (1-p)a = a+p \\
E[Y^2] &amp;= \frac{d^2 m_{S_n}(t)}{dt^2}\bigg|_0 = p(a+1)^2 e^{(a+1)t} + (1-p)a^2e^{at}\bigg|_0 = p(a+1)^2 + (1-p)a^2 = a^2+2ap+p \\
\Rightarrow V[Y] &amp;= E[Y^2] - (E[Y])^2 = a^2+2ap+p - (a^2+2ap + p^2) = p(1-p) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 39</li>
</ul>
<p>(a) The outcomes are discrete, we sample with replacement, there are two outcomes (either <span class="math inline">\(\leq 3\)</span> or <span class="math inline">\(&gt; 3\)</span>), and the number of trials is fixed…so we are dealing with the binomial distribution:
<span class="math display">\[\begin{align*}
P(Z &gt; 3) &amp;= 1 - P(Z \leq 3) = 1 - \Phi(3) = p \\
\Rightarrow P(X = 1) &amp;= {100 \choose 1}(1 - \Phi(3))^1(1-1+\Phi(3))^{99} \\
&amp;= 100(1-\Phi(3))\Phi(3)^{99} ~~{\rm or}~~ 100\Phi(-3)\Phi(3)^{99} \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
T \sim {\rm Geom}(p=1-\Phi(3)) ~~~ \Rightarrow ~~~ E[T] = 1/p = 1/(1-\Phi(3)) = 1/\Phi(-3) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 40</li>
</ul>
<p>(a) This is an order statistics problem. We know that
<span class="math display">\[\begin{align*}
f_X(x) = e^{-x} ~~~\mbox{and}~~~ F_X(x) = 1 - e^{-x}
\end{align*}\]</span>
for <span class="math inline">\(x \geq 0\)</span> and that <span class="math inline">\(n = 2\)</span>. We plug this information into the formula for the pdf of the minimum value:
<span class="math display">\[\begin{align*}
f_{(1)}(x) = nf_X(x)[1-F_X(x)]^{n-1} = 2e^{-x}[e^{-x}]^1  = 2e^{-2x}
\end{align*}\]</span>
for <span class="math inline">\(x \geq 0\)</span>.</p>
<p>(b) The moment-generating function is
<span class="math display">\[\begin{align*}
m_{X_{(1)}}(t) = E[e^{tX}] = 2 \int_0^\infty e^{tx} e^{-2x} dx = 2\int_0^\infty e^{-(2-t)x} dx = \frac{2}{2-t} = \frac{1}{1-t/2} \,.
\end{align*}\]</span></p>
<p>(c) The form of the pdf is that of an exponential with <span class="math inline">\(\beta = 1/2\)</span>; you could also infer this from the form of the mgf (which for an exponential is <span class="math inline">\(1/(1-\beta t)\)</span>). So: <span class="math inline">\(X_{(1)} \sim\)</span> Exponential(1/2).</p>
<hr />
<ul>
<li>Problem 41</li>
</ul>
<p>It is simpler to do this problem if we compute <span class="math inline">\(E[1+X]\)</span>:
<span class="math display">\[\begin{align*}
E[1+X] &amp;= \int_0^1 (1+x) \frac{x^{\alpha-1}(1+x)^{-\alpha-\beta}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha-1}(1+x)^{-\alpha-(\beta-1)}}{B(\alpha,\beta)} dx \,.
\end{align*}\]</span>
In the numerator, <span class="math inline">\(\beta \rightarrow (\beta-1)\)</span>…so we need to change <span class="math inline">\(B(\alpha,\beta) \rightarrow B(\alpha,\beta-1)\)</span> in the denominator:
<span class="math display">\[\begin{align*}
E[1+X] &amp;= \int_0^1 \frac{B(\alpha,\beta-1)}{B(\alpha,\beta-1)} \frac{x^{\alpha-1}(1+x)^{-\alpha-(\beta-1)}}{B(\alpha,\beta)} dx \\
&amp;= \frac{B(\alpha,\beta-1)}{B(\alpha,\beta)} \int_0^1 \frac{x^{\alpha-1}(1+x)^{-\alpha-(\beta-1)}}{B(\alpha,\beta-1)} dx \\
&amp;= \frac{B(\alpha,\beta-1)}{B(\alpha,\beta)} \\
&amp;= \frac{\Gamma(\alpha)\Gamma(\beta-1)}{\Gamma(\alpha+\beta-1)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
&amp;= \frac{\Gamma(\beta-1)}{\Gamma(\alpha+\beta-1)} \frac{(\alpha-\beta-1)\Gamma(\alpha+\beta-1)}{(\beta-1)\Gamma(\beta-1)} \\
&amp;= \frac{\alpha-\beta-1}{\beta-1} = \frac{\alpha}{\beta-1} + 1 \,.
\end{align*}\]</span>
Hence <span class="math inline">\(E[X] = E[1+X] - 1 = \alpha/(\beta-1)\)</span>.</p>
<hr />
<ul>
<li>Problem 42</li>
</ul>
<p>(a) We are conducting a chi-square goodness-of-fit test with expected counts <span class="math inline">\(kp_1 = kp_2 = 24/4 = 6\)</span> and <span class="math inline">\(kp_3 = 12\)</span>:
<span class="math display">\[\begin{align*}
W = 2 \times \frac{(8-6)^2}{6} + \frac{(8-12)^2}{12} = \frac{4}{3} + \frac{4}{3} = \frac{8}{3} \,.
\end{align*}\]</span></p>
<p>(b) The number of degrees of freedom is <span class="math inline">\(m-1 = 2\)</span> and the rejection region boundary is given by</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="chapter-exercises-solutions.html#cb370-1" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="fl">0.95</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 5.991465</code></pre>
<p>(c) The <span class="math inline">\(p\)</span>-value is given by</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="chapter-exercises-solutions.html#cb372-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="dv">8</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.2635971</code></pre>
<p>(d) The data <span class="math inline">\(\{8,8,8\}\)</span> are sampled according to a <em>multinomial</em> distribution.</p>
</div>
<div id="chapter-4" class="section level2 unnumbered hasAnchor">
<h2>Chapter 4<a href="chapter-exercises-solutions.html#chapter-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>(a) This is a Poisson problem: <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda = 2 \cdot 1/4 = 1/2\)</span>). So
<span class="math display">\[\begin{align*}
\mu = E[X] = \lambda = 1/2 ~~\text{and}~~ \sigma = \sqrt{V[X]} = \sqrt{\lambda} = \sqrt{1/2} \approx 0.707 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
P(1/2 \leq X \leq 1/2+1.414) = p(1) = \frac{\lambda^1}{1!} e^{-\lambda} = \frac12 e^{-1/2} = 0.303 \,.
\end{align*}\]</span></p>
<p>(b) We have that <span class="math inline">\(X \sim\)</span> Exp(<span class="math inline">\(\beta = 1/2\)</span>) (since there is a half-hour
on average between calls). By the memorylessness property,
<span class="math inline">\(P(X &gt; 1/2 \vert X &gt; 1/4) = P(X &gt; 1/4)\)</span>. Thus
<span class="math display">\[\begin{align*}
P(X &gt; 1/4) = \int_{1/4}^\infty \frac{1}{\beta} e^{-x/\beta} dx = \int_{1/4}^\infty 2 e^{-2x} dx = \left.-e^{-2x}\right|_{1/4}^\infty = e^{-1/2} = 0.607 \,.
\end{align*}\]</span></p>
<p>(c) The overall time <span class="math inline">\(T\)</span> is <span class="math inline">\(10X + (10-X)\)</span>, where <span class="math inline">\(X\)</span>, the number of calls
from the friend, is sampled from a binomial distribution with <span class="math inline">\(k = 10\)</span> and
<span class="math inline">\(p = 0.2\)</span>. Thus the average total number of minutes is
<span class="math display">\[\begin{align*}
E[T] = E[10X + (10-X)] = E[9X+10] = 9E[X] + 10 = 9kp + 10 = 28 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>(a) The number of (successful) shots can be infinite; only <em>on average</em> is
the number of shots in eight minutes going to be four. So we are working with
a Poisson distribution whose parameter <span class="math inline">\(\lambda\)</span> (the expected
number of <em>successful</em> shots) is
<span class="math display">\[\begin{align*}
\lambda = (1 \quad \frac{\text{shot}}{2\text{min}})(\frac{1}{2} \frac{\text{success}}{\text{shot}})(4 \quad 2\text{min}) = 2 \,.
\end{align*}\]</span>
Let <span class="math inline">\(X\)</span> be the number of successful shots. Then
<span class="math display">\[\begin{align*}
P(X \leq 1) = \frac{\lambda^0}{0!}e^{-\lambda} + \frac{\lambda^1}{1!}e^{-\lambda} = e^{-2}(1+2) = 3e^{-2} \,.
\end{align*}\]</span></p>
<p>(b) We know that <span class="math inline">\(E[X] = \lambda = 2\)</span>, <span class="math inline">\(V[X] = \lambda = 2\)</span>, and <span class="math inline">\(\sigma = \sqrt{\lambda} = \sqrt{2}\)</span>. Thus
<span class="math display">\[\begin{align*}
P(2 - \sqrt{2} &lt; X &lt; 2 + \sqrt{2}) &amp;= p_X(1) + p_X(2) + p_X(3) = e^{\lambda}\left(\lambda + \frac{\lambda^2}{2} + \frac{\lambda^3}{6} \right) \\
&amp;= e^{2}\left(2 + 2+ \frac{8}{6} \right) = \frac{16}{3} e^{-2} = 0.722 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>The particular event in question happens 8 times per year in the
three states, and thus the expected number of events in a two-year window
is 16. The appropriate distribution in this case is the
Poisson distribution. If the total number of observed events is denoted
<span class="math inline">\(X\)</span>, then <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span> = 16), and <span class="math inline">\(E[X] = V[X] = 16\)</span>.</p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) We utilize the shortcut formula:
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = 2\sigma^2 - \frac{\pi}{2}\sigma^2 + \frac{\pi}{2}\sigma^2 = 2\sigma^2 \,.
\end{align*}\]</span></p>
<p>(b) The first population moment is
<span class="math inline">\(\mu_1&#39; = E[X] = \sigma\sqrt{\pi/2}\)</span> and the first sample moment is
<span class="math inline">\(m_1&#39; = (1/n)\sum_{i=1}^n X_i = \bar{X}\)</span>. We set these equal and determine that
<span class="math display">\[\begin{align*}
\hat{\sigma}_{MoM} = \sqrt{\frac{2}{\pi}} \bar{X} \,.
\end{align*}\]</span></p>
<p>(c) The bias is <span class="math inline">\(E[\hat{\theta}-\theta] = E[\hat{\theta}] - \theta\)</span>,
or
<span class="math display">\[\begin{align*}
B[\hat{\theta}_{MoM}] &amp;= E\left[\sqrt{\frac{2}{\pi}} \bar{X}\right] - \sigma = \sqrt{\frac{2}{\pi}} E\left[\bar{X}\right] - \sigma = \sqrt{\frac{2}{\pi}} E\left[X\right] - \sigma = \sqrt{\frac{2}{\pi}} \sqrt{\frac{\pi}{2}} \sigma - \sigma = 0 \,.
\end{align*}\]</span></p>
<p>(d) The variance is
<span class="math display">\[\begin{align*}
V[\hat{\theta}_{MoM}] &amp;= V\left[\sqrt{\frac{2}{\pi}} \bar{X}\right] = \frac{2}{\pi} V\left[\bar{X}\right] = \frac{2}{\pi} \frac{V\left[X\right]}{n} = \frac{2}{n\pi} \frac{(4-\pi)\sigma^2}{2} = \frac{(4-\pi)\sigma^2}{n\pi} \,.
\end{align*}\]</span></p>
<p>(e) The second population moment is
<span class="math inline">\(\mu_2&#39; = E[X^2] = 2\sigma^2\)</span> (from part a) and the second sample moment is
<span class="math inline">\(m_2&#39; = (1/n)\sum_{i=1}^n X_i^2 = \overline{X^2}\)</span>.
We set these equal and determine that
<span class="math display">\[\begin{align*}
\widehat{\sigma^2}_{MoM} = \frac{\overline{X^2}}{2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>(a) We have that <span class="math inline">\(\mu_1&#39; = \frac{1}{p}\)</span> and <span class="math inline">\(m_1&#39; = X\)</span>. It follows from moment equation <span class="math inline">\(\mu_1&#39; = m_1&#39;\)</span> that <span class="math inline">\(\frac{1}{p} = X\)</span>, so <span class="math inline">\(\hat{p}_{MoM} = \frac{1}{X}\)</span>.</p>
<p>(b) We have that <span class="math inline">\(\mu_2&#39; = \frac{1 - p}{p^2} +  \frac{1}{p^2}  = \frac{2 - p}{p^2}\)</span> and <span class="math inline">\(m_2&#39; = X^2\)</span>. It follow from the second moment equation <span class="math inline">\(\mu_2&#39; = m_2&#39;\)</span> that
<span class="math display">\[\begin{align*}
\frac{2-p}{p^2} &amp; = X^2 \\
\Rightarrow ~~~ 2-p &amp; = p^2 X^2 \\
\Rightarrow ~~~ p^2 X^2 +p -2 &amp;= 0 \\
\Rightarrow ~~~ \hat{p}_{MoM} &amp; = \frac{-1 + \sqrt{1 + 8 X^2}}{2X^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) The expected value for a beta distribution is <span class="math inline">\(E[X] = \alpha/(\alpha+\beta)\)</span>, so, using the first sample moment, we get that
<span class="math display">\[\begin{align*}
E[X] &amp;= \bar{X} \\
\Rightarrow ~~~ \frac{\alpha}{\alpha+\beta} &amp;= \bar{X} \\
\Rightarrow ~~~ \frac{\alpha+\beta}{\alpha} = 1 + \frac{\beta}{\alpha} &amp;= \frac{1}{\bar{X}} \\
\Rightarrow ~~~ \frac{\beta}{\alpha} &amp;= \frac{1}{\bar{X}}-1 \\
\Rightarrow ~~~ \hat{\beta}_{MoM} &amp;= \alpha\left(\frac{1}{\bar{X}}-1\right) \,.
\end{align*}\]</span></p>
<p>(b) There is no invariance property for the method-of-moments estimator, so the answer is no.</p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>(a) The argument list is fine.</p>
<p>(b) We need to change <code>mean(X)</code> to <code>sum(X)</code> and <code>lambda</code> to <code>n*lambda</code>.</p>
<p>(c) The reference table tells us that a one-sided lower bound where <span class="math inline">\(E[U]\)</span> increases with <span class="math inline">\(\lambda\)</span> will have a value of <span class="math inline">\(q\)</span> equal to <span class="math inline">\(1-\alpha\)</span>. So we plug in 0.95.</p>
<p>(d) Confidence intervals derived from discrete sampling distributions will have coverages <span class="math inline">\(\geq 1-\alpha\)</span>, so, here, we would say “greater than or equal to 95%.”</p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>(a) The likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda_{LR} = \frac{\mbox{sup}_{\theta \in \Theta_o} \mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta} \mathcal{L}(\theta \vert \mathbf{x})}
\end{align*}\]</span>
Here, that becomes
<span class="math display">\[\begin{align*}
\lambda_{LR} &amp;= \frac{\mathcal{L}(p_o \vert \mathbf{x})}{\mathcal{L}(\hat{p}_{MLE} \vert \mathbf{x})} = \frac{p_o^{\sum_{i=1}^n X_i}(1-p_o)^{n-\sum_{i=1}^n X_i}}{\bar{X}^{\sum_{i=1}^n X_i}(1-\bar{X})^{n-\sum_{i=1}^n X_i}} = \frac{p_o^U(1-p_o)^{n-U}}{\bar{X}^U(1-\bar{X})^{n-U}} \,.
\end{align*}\]</span></p>
<p>(b) The test is a two-sided test, so we cannot proclaim it to be uniformly
most powerful. However, it very well may be…we just cannot say with the information we have at hand. So: “maybe.”</p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) The maximum-likelihood estimate is
<span class="math display">\[\begin{align*}
\ell(\lambda \vert x) &amp;= x \log \lambda - \log x! - \lambda \\
\Rightarrow ~~~ \ell&#39;(\lambda \vert x) &amp;= \frac{x}{\lambda} - 1 = 0\\
\Rightarrow ~~~ \hat{\lambda}_{MLE} &amp;= X \,.
\end{align*}\]</span>
which here takes on the value <span class="math inline">\(x_{\rm obs}\)</span>.</p>
<p>(b) The likelihood-ratio test statistic is
<span class="math display">\[\begin{align*}
\frac{\mbox{sup}_{\theta \in \Theta_o}\mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta}\mathcal{L}(\theta \vert \mathbf{x})} \,.
\end{align*}\]</span>
Here, that means that for the numerator, we insert the Poisson pmf (remember: one datum) with <span class="math inline">\(\lambda_o\)</span> plugged in, i.e.,
<span class="math display">\[\begin{align*}
\frac{\lambda_o^{x_{\rm obs}}}{x_{\rm obs}!} e^{-\lambda_o} \,,
\end{align*}\]</span>
while for the denominator, we plug in the MLE for <span class="math inline">\(\lambda\)</span>, i.e.,
<span class="math display">\[\begin{align*}
\frac{x_{\rm obs}^{x_{\rm obs}}}{x_{\rm obs}!} e^{-x_{\rm obs}} \,.
\end{align*}\]</span>
So the ratio is
<span class="math display">\[\begin{align*}
\left( \frac{\lambda_o}{x_{\rm obs}} \right)^{x_{\rm obs}} e^{-(\lambda_o-x_{\rm obs})} \,.
\end{align*}\]</span></p>
<p>(c) The expression is
<span class="math display">\[\begin{align*}
W = -2 \log \lambda_{LR} = -2 x_{\rm obs} \log \left( \frac{\lambda_o}{x_{\rm obs}} \right) + 2 (\lambda_o-x_{\rm obs}) \,.
\end{align*}\]</span></p>
<p>(d) <span class="math inline">\(W\)</span> is sampled from a chi-square distribution for 1 degree
of freedom.</p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) The factorized likelihood is
<span class="math display">\[\begin{align*}
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!}e^{-\lambda} = \left(\frac{1}{\prod_{i=1}^n x_i!}\right) \cdot \lambda^{\sum_{i=1}^n x_i} e^{-n\lambda} = h(\mathbf{x}) \cdot g(\lambda,\mathbf{x}) \,.
\end{align*}\]</span>
The sufficient statistic is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.</p>
<p>(b) The sum of <span class="math inline">\(n\)</span> iid Poisson random variables
is a Poisson random variable with parameter <span class="math inline">\(n\lambda\)</span>.
The moment-generating function for a Poisson random variable is
<span class="math inline">\(m_X(t) = \exp(\lambda(e^t-1))\)</span>, so the mgf for <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{align*}
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = \left[ m_{X_i}(t) \right]^n = \exp(n\lambda(e^t-1)) \,.
\end{align*}\]</span>
This is the mgf for a Poisson(<span class="math inline">\(n\lambda\)</span>) distribution.</p>
<p>(c) Recall that in an LRT context, <span class="math inline">\(\Theta = \Theta_o \cup \Theta_a\)</span>;
in other words, the null must contain <em>all</em> possible values of
<span class="math inline">\(\theta\)</span> that are not in the alternative. Hence: <span class="math inline">\(H_o : \theta \geq \theta_o\)</span>.
This inequality does not actually change how
the test is constructed, but does change how we interpret it: the true
<span class="math inline">\(\alpha\)</span> for this test will be less than or equal to the stated <span class="math inline">\(\alpha\)</span>.</p>
<p>(d) We are performing a lower-tail test, and we are on the “yes”
line (since <span class="math inline">\(E[Y] = n\lambda\)</span> increases with <span class="math inline">\(\lambda\)</span>). From the reference
table, that means <span class="math inline">\(y_{\rm RR}\)</span> is equal to <span class="math inline">\(F_Y^{-1}(\alpha \vert n\lambda_o)\)</span>,
which in code is <code>qpois(0.05,n*lambda.o)</code>. (Recall that there are no
discreteness corrections in rejection-region boundary computations.)</p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>(a) The likelihood function for the sample is:
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x}) = \prod_{i=1}^{n} \frac{1}{\beta}e^{-x/\beta} = \frac{1}{\beta^n} e^{-\frac{\sum_{i=1}^{n}x_i}{\beta}} \,.
\end{align*}\]</span>
Under the null <span class="math inline">\(H_0 : \beta = 1\)</span>, the likelihood function becomes
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta_0 \vert \mathbf{x}) = e^{-\sum_{i=1}^{n}x_i} = e^{-n\bar x} \,,
\end{align*}\]</span>
while under the alternative,
<span class="math display">\[\begin{align*}
\sup_{\beta &gt; 0} \mathcal{L}(\beta \vert \mathbf{x}) &amp;= \mathcal{L}(\hat{\beta}_{MLE} \vert \mathbf{x})\\
&amp; = \frac{1}{\bar x^n}  e^{-\frac{\sum_{i=1}^{n}x_i}{\bar x}} = \frac{1}{\bar x^n}  e^{-n} \,.
\end{align*}\]</span>
So the likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda_{LR} &amp;= \frac{\mathcal{L}(\beta_o\vert \mathbf{x})}{\sup_{\beta &gt; 0} \mathcal{L}(\beta \vert \mathbf{x})}\\
&amp;= \frac{e^{-n\bar x}}{\frac{1}{\bar x^n} e^{-n}} \\
&amp;= \bar x^ne^{-n(\bar x-1)} \,.
\end{align*}\]</span></p>
<p>(b) Under the null hypothesis, the number of degrees of freedom is <span class="math inline">\(r_o = 0\)</span>, because <span class="math inline">\(\beta = 1\)</span> is set to a constant. Under the alternative hypothesis, the number of degrees of freedom is <span class="math inline">\(r = 1\)</span>, because we have one free parameter: <span class="math inline">\(\beta\)</span>. Therefore, according to Wilks’ theorem, the degree of freedom of the <span class="math inline">\(\chi^2\)</span> distribution is <span class="math inline">\(r - r_o = 1-0=1\)</span>.
Under the large-<span class="math inline">\(n\)</span> approximation, <span class="math inline">\(-2\log(\lambda_{LR}) \sim \chi^2(1)\)</span>.
Therefore, the rejection region corresponds to:
<span class="math display">\[\begin{align*}
-2\log(\lambda) &amp;&gt; \chi^2_{0.95, 1}\\
\Rightarrow ~~~ -2\log\left(\bar x^ne^{-n(\bar x-1)}\right) &amp;&gt; \chi^2_{0.95,1} = 3.84 \\
\Rightarrow ~~~ n\left(\log(\bar x) - \bar x+1\right) &amp;&lt; \frac{3.84}{2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) <span class="math inline">\(H_0: p = p_0 = 0.5\)</span> and <span class="math inline">\(H_a: p \neq 0.5\)</span></p>
<p>(b) <span class="math inline">\(\Theta_0 = \{p_0\}\)</span> and <span class="math inline">\(\Theta_a = \{p\, \vert\,  p \in [0,1] ~~\text{and}~~ p \neq p_0\}\)</span></p>
<p>(c) <span class="math inline">\(r_0 = 0\)</span> (<span class="math inline">\(p\)</span> is fixed) and <span class="math inline">\(r = 1\)</span></p>
<p>(d) The likelihood ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda = \frac{\mathcal{L}(p_0 \vert x)}{\mathcal{L}(\hat{p}_{MLE} \vert x)} = \frac{\frac{1000!}{550!450!} 0.5^{550} (1-0.5)^{450} }{ \frac{1000!}{550!450!} 0.55^{550} (1-0.55)^{450}}  = \frac{0.5^{1000}}{0.55^{550} \cdot 0.45^{450}} = 0.00668 \,,
\end{align*}\]</span>
where we make use of the fact that <span class="math inline">\(\hat{p}_{MLE} = x/n = 0.55\)</span>.</p>
<p>(e) We have that <span class="math inline">\(W_{\rm obs} = -2 \log(\lambda_{LR}) = 10.017\)</span>. According to Wilk’s theorem, the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[\begin{align*}
\int_{W_{\rm obs}}^\infty f_W(w) dw \,,
\end{align*}\]</span>
for 1 degree of freedom, or <code>1 - pchisq(10.017,1)</code> (= 0.00155).</p>
<p>(f) We have sufficient evidence to reject the null hypothesis and thus to
conclude that the coin is not a fair one.</p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>By inspection, <span class="math inline">\(X \sim\)</span> Gamma(3,2/3). Thus <span class="math inline">\(E[X] = \alpha \beta = 2\)</span> and <span class="math inline">\(V[X] = \alpha \beta^2 = 3 (2/3)^2 = 4/3\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) <span class="math inline">\(E[X] = \alpha \beta\)</span> and <span class="math inline">\(V[X] = \alpha \beta^2\)</span>, so <span class="math inline">\(V[X]/E[X] = \beta =
10/5 = 2\)</span>, and <span class="math inline">\(\alpha = 5/2 = 2.5\)</span>.</p>
<p>(b) <span class="math inline">\(\beta = 2\)</span> and <span class="math inline">\(\alpha = 2.5\)</span> <span class="math inline">\(\Rightarrow\)</span> chi-square distribution (for
5 degrees of freedom).</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X^{-1}] &amp;= \int_0^\infty \frac1x f_X(x) dx = \int_0^\infty \frac1x \frac{x^{\nu/2-1}}{2^{\nu/2}} \frac{e^{-x/2}}{\Gamma(\nu/2)} dx \\
&amp;= \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2}} \frac{e^{-x/2}}{\Gamma(\nu/2)} dx = \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2}} \frac{2^{-1}}{2^{-1}} \frac{e^{-x/2}}{\Gamma(\nu/2)} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2-1)} dx \\
&amp;= 2^{-1} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2)} \int_0^\infty \frac{x^{\nu/2-2}}{2^{\nu/2-1}} \frac{e^{-x/2}}{\Gamma(\nu/2-1)} dx = 2^{-1} \frac{\Gamma(\nu/2-1)}{\Gamma(\nu/2)} \\
&amp;= \frac12 \frac{\Gamma(\nu/2-1)}{(\nu/2-1)\Gamma(\nu/2-1)} = \frac{1}{2(\nu/2-1)} = \frac{1}{\nu-2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X] = \int_0^\infty x f_X(x) dx &amp;= \int_0^\infty x \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x^{\alpha+1}} e^{-\beta/x} dx \\
&amp;= \int_0^\infty \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x^{\alpha}} e^{-\beta/x} dx \\
&amp;= \frac{\beta^{\alpha}}{\beta{\alpha-1}} \frac{\Gamma(\alpha-1)}{\Gamma(\alpha)} \int_0^\infty \frac{\beta^{\alpha-1}}{\Gamma(\alpha-1)} \frac{1}{x^{\alpha}} e^{-\beta/x} dx \\
&amp;= \frac{\beta^{\alpha}}{\beta{\alpha-1}} \frac{\Gamma(\alpha-1)}{\Gamma(\alpha)} \cdot 1 \\
&amp;= \beta \frac{\Gamma(\alpha-1)}{(\alpha-1)\Gamma(\alpha-1)} \\
&amp;= \frac{\beta}{\alpha-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>(a) If <span class="math inline">\(\beta = 2\)</span> and <span class="math inline">\(\alpha\)</span> is a half-integer or
an integer, then <span class="math inline">\(X\)</span> is sampled from a “chi-square” distribution.
(Note that because <span class="math inline">\(\alpha\)</span> is a half-integer here, we cannot answer
“Erlang” or “exponential.”)</p>
<p>(b) The gamma pdf with <span class="math inline">\(\alpha = 3/2\)</span> is
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x^{1/2}}{\beta^{3/2}} \frac{e^{-x/\beta}}{\Gamma(3/2)} \,,
\end{align*}\]</span>
so the likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(\beta \vert \mathbf{x} = \prod_{i=1}^n \frac{x_i^{1/2}}{\beta^{3/2}} \frac{e^{-x_i/\beta}}{\Gamma(3/2)} = \frac{\sqrt{\prod_{i=1}^n x_i}}{[\Gamma(3/2)]^n} \cdot \frac{e^{-(\sum_{i=1}^n x_i)/\beta}}{\beta^{3/2}} = h(\mathbf{x}) \cdot g(\beta,\mathbf{x}) \,.
\end{align*}\]</span>
We can read off of the <span class="math inline">\(g(\cdot)\)</span> function that <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
a sufficient statistic for <span class="math inline">\(\beta\)</span>.</p>
<p>(c) We start by computing
<span class="math display">\[\begin{align*}
E[Y] = E\left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n E[X_i] = nE[X] = n \alpha \beta = \frac{3}{2}n\beta \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
E\left[\frac{2Y}{3n}\right] = \beta
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
\hat{\beta}_{MVUE} = \frac{2Y}{3n} = \frac{2}{3}\bar{X} \,.
\end{align*}\]</span></p>
<p>(d) The first population moment is
<span class="math inline">\(\mu_1&#39; = E[X] = (3/2)\beta\)</span> and the first sample moment is
<span class="math inline">\(m_1&#39; = (1/n)\sum_{i=1}^n X_i = \bar{X}\)</span>.
We set these equal and find that
<span class="math display">\[\begin{align*}
\hat{\beta}_{MoM} = \frac{2}{3}\bar{X} \,.
\end{align*}\]</span></p>
<p>(e) Because the MoM is equivalent to the MVUE, we know
immediately that the bias of the MoM is 0.</p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
E[X^{1/2}] = \int_0^\infty x^{1/2} f_X(x) dx &amp;= \int_0^\infty x^{1/2} \frac{x^{\alpha-1}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^\alpha}\frac{e^{-x/\beta}}{\Gamma(\alpha)} \frac{\beta^{\alpha+1/2}}{\beta^{\alpha+1/2}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha+1/2)} dx \\
&amp;= \frac{\beta^{\alpha+1/2}}{\beta^{\alpha}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \int_0^\infty \frac{x^{\alpha-1/2}}{\beta^{\alpha+1/2}}\frac{e^{-x/\beta}}{\Gamma(\alpha+1/2)} dx \\
&amp;= \frac{\beta^{\alpha+1/2}}{\beta^{\alpha}} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \\
&amp;= \sqrt{\beta} \frac{\Gamma(\alpha+1/2)}{\Gamma(\alpha)} \,.
\end{align*}\]</span>
We note that in general,
<span class="math display">\[\begin{align*}
E[X^k] = \beta^k \frac{\Gamma(\alpha+k)}{\Gamma(\alpha)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) The overdispersion parameter in a negative
binomial regression is dubbed “Theta” and is thus 214,488.</p>
<p>(b) If the overdispersion parameter is <span class="math inline">\(\infty\)</span>, then
Poisson regression is recovered. The value here is sufficiently large that
we can say with confidence that there is no overdispersion.
(Backing up this conclusion are
the nearly identical results since when learning both regression models.)</p>
<p>(c) The answer is the Likelihood Ratio test. The
statistic is the difference in the deviance values, which we assume under
the null is chi-square distributed for the difference in the
numbers of degree of freedom.</p>
<p>(d) The null hypothesis in the LRT is <span class="math inline">\(\beta_1 = 0\)</span> and the
alternative is <span class="math inline">\(\beta_1 \neq 0\)</span>. The test statistic is <em>so large</em>
(<span class="math inline">\(\approx\)</span> 165), especially considering that the expected value for
a chi-square distribution is 1 for 1 degree of freedom, that we can
safely conclude that <span class="math inline">\(\beta_1 \neq 0\)</span>.</p>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) The moment-generating function for a Gamma(1,1) distribution is
<span class="math display">\[\begin{align*}
m_X(t) = (1-\beta t)^{-\alpha} = (1-t)^{-1} \,.
\end{align*}\]</span>
Thus the mgf for <span class="math inline">\(\bar{X}\)</span> is
<span class="math display">\[\begin{align*}
m_{\bar{X}}(t) = \prod_{i=1}^{100} m_X\left(\frac{t}{n}\right) = \left(1-\frac{t}{100}\right)^{-100} \,,
\end{align*}\]</span>
which we recognize as a Gamma(100,1/100) distribution.</p>
<p>(b) The expected value of <span class="math inline">\(X\)</span> is <span class="math inline">\(E[X] = 1 \cdot 1 = 1\)</span>, so the quantity we seek is
<span class="math display">\[\begin{align*}
P(\bar{X} \geq E[X]) = 1 - P(\bar{X} &lt; 1) = 1 - F_{\bar{X}}(1) \,,
\end{align*}\]</span>
which rendered in <code>R</code> is</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="chapter-exercises-solutions.html#cb374-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pgamma</span>(<span class="dv">1</span>,<span class="at">shape=</span><span class="dv">100</span>,<span class="at">scale=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">100</span>)</span></code></pre></div>
<pre><code>## [1] 0.4867012</code></pre>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>(a) The entropy is
<span class="math display">\[\begin{align*}
E[-\log p_X(x)] &amp;= E\left[-X \log \lambda + \lambda + \log X! \right] = - E[X] \log \lambda + \lambda + E[\log X!] \\
&amp;= \lambda (1 - \log \lambda) + E[\log X!] \,.
\end{align*}\]</span></p>
<p>(b) The probability-generating function is
<span class="math display">\[\begin{align*}
E[z^X] &amp;= \sum_{x=0}^\infty z^x \frac{\lambda^x}{x!} e^{-\lambda} = \sum_{x=0}^\infty \frac{(z\lambda)^x}{x!} e^{-\lambda} = \sum_{x=0}^\infty \frac{(z\lambda)^x}{x!} e^{-\lambda} \frac{e^{-z\lambda}}{e^{-z\lambda}} = \frac{e^{-\lambda}}{e^{-z\lambda}} \sum_{x=0}^\infty \frac{(z\lambda)^x}{x!} e^{-z\lambda} = \frac{e^{-\lambda}}{e^{-z\lambda}} = e^{(z-1)\lambda} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>(a) The functional form and domain specify: <em>gamma</em> (or <em>Erlang</em>).</p>
<p>(b) We can read off of <span class="math inline">\(e^{-2x} = e^{-x/(1/2)}\)</span> that <span class="math inline">\(\beta = 1/2\)</span>, and we can read off of <span class="math inline">\(x^2 = x^{\alpha-1}\)</span> that <span class="math inline">\(\alpha = 3\)</span>.</p>
<p>(c) The easy way: <span class="math inline">\(E[X] = \alpha\beta = 3/2\)</span>. The hard way:
<span class="math display">\[\begin{align*}
E[X] = \int_0^\infty x f_X(x) dx &amp;= \int_0^\infty 4x^3 e^{-2x} dx \,.
\end{align*}\]</span>
We set <span class="math inline">\(u = 2x\)</span> and <span class="math inline">\(du = 2dx\)</span>; the bounds of integration do not change. Thus
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_0^\infty 4x^3 e^{-2x} dx \\
&amp;= \int_0^\infty 4 \left(\frac{u}{2}\right)^3 e^{-u} \frac{du}{2} \\
&amp;= \frac14 \int_0^\infty u^3 e^{-u} du \\
&amp;= \frac14 \Gamma(4) = \frac14 3! = \frac64 = \frac32 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>(a) The first population moment is <span class="math inline">\(E[X] = \sqrt{2/\pi} \sigma\)</span> and the first sample moment is <span class="math inline">\(\bar{X}\)</span>…hence:
<span class="math display">\[\begin{align*}
\hat{\sigma}_{MoM} = \sqrt{\frac{\pi}{2}} \bar{X} \,.
\end{align*}\]</span></p>
<p>(b) The second population moment is
<span class="math display">\[\begin{align*}
E[X^2] = V[X] + (E[X])^2 = \sigma^2\left(1-\frac{2}{\pi}\right) + \sigma^2\frac{2}{\pi} = \sigma^2 \,.
\end{align*}\]</span>
The second sample moment is <span class="math inline">\(\overline{X^2}\)</span>. Hence:
<span class="math display">\[\begin{align*}
\hat{\sigma^2}_{MoM} = \overline{X^2} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>(a) The likelihood is <span class="math inline">\(f_X(x \vert \theta)\)</span> and thus
<span class="math display">\[\begin{align*}
\ell(x \vert \theta) &amp;= \log \theta + (\theta-1) \log x - \theta \log 3 \\
\Rightarrow ~~~ \ell&#39;(x \vert \theta) &amp;= \frac{1}{\theta} + (\log x - \log 3) \\
\Rightarrow ~~~ &amp;= \frac{1}{\theta} + \log (x/3) = 0 \\
\Rightarrow ~~~ \hat{\theta}_{MLE} &amp;= -\frac{1}{\log (X/3)} \,.
\end{align*}\]</span></p>
<p>(b) The likelihood-ratio test statistic is
<span class="math display">\[\begin{align*}
\lambda_{\rm LR} = \frac{ {\rm sup}_{\theta \in \Theta_o} \mathcal{L}(\theta \vert \mathbf{x}) }{ {\rm sup}_{\theta \in \Theta} \mathcal{L}(\theta \vert \mathbf{x}) } \,,
\end{align*}\]</span>
where <span class="math inline">\(\Theta_o = \{ \theta : \theta = \theta_o \}\)</span> and <span class="math inline">\(\Theta = \{ \theta : \theta &gt; 0 \}\)</span>. Thus in the numerator we plug in <span class="math inline">\(\theta_o\)</span> and in the denominator we plug in <span class="math inline">\(\hat{\theta}_{MLE}\)</span>:
<span class="math display">\[\begin{align*}
\lambda_{\rm LR} &amp;= \frac{ (\theta_o/3^{\theta_o}) X^{\theta_o-1} }{ (\hat{\theta}_{MLE}/3^{\hat{\theta}_{MLE}}) X^{\hat{\theta}_{MLE}-1} } = \frac{\theta_o}{\hat{\theta}_{MLE}} \left(\frac{X}{3}\right)^{\theta_o-\hat{\theta}_{MLE}} \,.
\end{align*}\]</span></p>
<p>(c) The test statistic is <span class="math inline">\(W = -2\log \lambda_{\rm LR}\)</span>:
<span class="math display">\[\begin{align*}
W &amp;= -2 \log \frac{\theta_o}{\hat{\theta}_{MLE}} \left(\frac{X}{3}\right)^{\theta_o-\hat{\theta}_{MLE}} = -2 \left[ \log \frac{\theta_o}{\hat{\theta}_{MLE}} + (\theta_o-\hat{\theta}_{MLE}) \log \frac{X}{3} \right] \,.
\end{align*}\]</span></p>
<p>(d) For the constrained model, <span class="math inline">\(r_o = 0\)</span>. (<span class="math inline">\(\theta\)</span> is fixed.) For the unconstrained model, <span class="math inline">\(r = 1\)</span>. Thus <span class="math inline">\(\Delta r = r - r_o = 1\)</span> is the number of degrees of freedom for the chi-square distribution that <code>stat</code> is sampled from under the null:</p>
<pre><code>1 - pchisq(stat,1)</code></pre>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) The model to the right is a <em>negative binomial</em> regression model.</p>
<p>(b) The AIC is lower to the <em>left</em>.</p>
<p>(c) To determine whether the model to the left is a viable representation of the data-generating process, we assume that under the null hypothesis that it is, the residual deviance is sampled from a chi-square distribution for its associated number of degrees of freedom. Hence the <span class="math inline">\(p\)</span>-value here is</p>
<pre><code>1 - pchisq(41.94,28)</code></pre>
<p>That’s the final answer…but in real-life we can take the additional step of computing the value: 0.044. We could decide to reject the null (and say that the Poisson model is not a good representation of the data-generating process), but given that the <span class="math inline">\(p\)</span>-value is only approximate (because the residual deviance is only approximately chi-square-distributed under the null), and its value is <span class="math inline">\(\approx\)</span> 0.05, this decision would be marginal at best.</p>
</div>
<div id="chapter-5" class="section level2 unnumbered hasAnchor">
<h2>Chapter 5<a href="chapter-exercises-solutions.html#chapter-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>Here, the first population moment is <span class="math inline">\(\mu_1&#39; = E[X] = \frac{3}{2} \theta\)</span> and the first sample moment is <span class="math inline">\(m_1&#39; = \frac{1}{n} \sum X_i = \bar{X}\)</span>. So the MoM estimator for <span class="math inline">\(\theta\)</span>, following from setting <span class="math inline">\(\mu_1&#39; = m_1&#39;\)</span>, is <span class="math inline">\(\hat{\theta}_{MoM} = \frac{2}{3} \bar{X}\)</span>.</p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
P(X &gt; a+b \vert X &gt; b) = \frac{P(X &gt; a+b \cap X &gt; b)}{P(X &gt; b)} = \frac{P(X &gt; a+b)}{P(X &gt; b)} \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
P(X &gt; a+b) = \int_{a+b}^1 dx = 1-(a+b) ~~~ P(X &gt; b) = \int_b^1 dx = 1-b \,.
\end{align*}\]</span>
So
<span class="math display">\[\begin{align*}
\frac{P(X &gt; a+b)}{P(X &gt; b)} = \frac{1 - (a+b)}{1-b} \,.
\end{align*}\]</span>
The uniform distribution does <em>not</em> exhibit the memoryless property, as the ratio above does not depend just on <span class="math inline">\(a\)</span>. (If <span class="math inline">\(b\)</span> cancelled out top and bottom, then the distribution would exhibit memorylessness.)</p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>(a) It doesn’t matter when she arrives:
<span class="math display">\[\begin{align*}
P(x_0 \leq X \leq x_0 + 10) = \int_{x_0}^{x_0 + 10} \frac{1}{70} dx = \frac{x_0 + 10}{70} - \frac{x_0}{70} = \frac{1}{7} \,.
\end{align*}\]</span></p>
<p>(b) We have that <span class="math inline">\(Y =\)</span> Binomial<span class="math inline">\((n=5,p=1/7)\)</span>, so
<span class="math display">\[\begin{align*}
P(Y\geq 1) = 1 - P(Y = 0) = 1 - {5 \choose 0}\frac{1}{7}^0\left(1 - \frac{1}{7}\right) = 1 - \left( \frac{6}{7}\right)^5.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
P(X \leq 2u | X \geq u) = \frac{P(X \leq 2u \cap X \geq u)}{P(X \geq u)} = \frac{\int_u^{2u}dx}{\int_u^{1}dx} = \frac{2u - u}{1 - u} = \frac{u}{1-u} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
P(X_1 &lt; 2X_2 \vert X_2 &lt; 1/2) = \frac{P(X_2 &gt; X_1/2 \cap X_2 &lt; 1/2)}{P(X_2 &lt; 1/2)} \,.
\end{align*}\]</span>
We can approach this geometrically. The denominator is, by inspection, 1/2, so
<span class="math display">\[\begin{align*}
P(X_1 &lt; 2X_2 \vert X_2 &lt; 1/2) = 2P(X_2 &gt; X_1/2 \cap X_2 &lt; 1/2) \,.
\end{align*}\]</span>
The remaining expression evaluates as the area of the triangle with vertices (0,0), (1,1/2), and (0,1/2), which is (1/2)(1)(1/2) = 1/4. Thus
<span class="math display">\[\begin{align*}
P(X_1 &lt; 2X_2 \vert X_2 &lt; 1/2) = 2 \cdot 1/4 = 1/2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>(a) Since <span class="math inline">\(\theta\)</span> is a lower bound, the sufficient statistic is <span class="math inline">\(X_{(1)}\)</span>, the minimum observed datum, by inspection.</p>
<p>(b) The cdf <span class="math inline">\(F_{(1)}(x)\)</span> is
<span class="math display">\[\begin{align*}
F_{(1)}(x) = 1 - [1 - F_X(x)]^n \,.
\end{align*}\]</span>
The cdf <span class="math inline">\(F_X(x)\)</span> is
<span class="math display">\[\begin{align*}
F_X(x) = - \int_{\theta}^x \frac{1}{\theta} dy = - \frac{1}{\theta} \int_{\theta}^x dy = - \frac{1}{\theta} (x - \theta) = 1 - x/\theta \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
F_{(1)}(x) = 1 - [1 - (1 - x/\theta)]^n = 1 - \left(\frac{x}{\theta}\right)^n \,,
\end{align*}\]</span></p>
<p>(c) If the null is true, we cannot observe a value of <span class="math inline">\(X_{(1)}\)</span> that is smaller than <span class="math inline">\(\theta_o\)</span>. So the “trivial rejection region” is <span class="math inline">\(X_{(1)} &lt; \theta_o\)</span>. This is “trivial” because we can write it down via inspection (and it does not depend on <span class="math inline">\(\alpha\)</span>).</p>
<p>(d) We can only reject the null if <span class="math inline">\(X_{(1)} &gt; \theta_o\)</span>, so we have to “all the <span class="math inline">\(\alpha\)</span>” on that side of <span class="math inline">\(\theta_o\)</span>: <span class="math inline">\(1 - \alpha\)</span>.</p>
<p>(e) We have that
<span class="math display">\[\begin{align*}
1 - \left(\frac{x_{RR}}{\theta_o}\right)^n &amp;= 1 - \alpha ~~~ \Rightarrow ~~~ \left(\frac{x_{RR}}{\theta_o}\right)^n = \alpha ~~~ \Rightarrow ~~~ x_{RR} = \theta_o \alpha^{1/n} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>(a) By inspection, <span class="math inline">\(\hat{\theta}_{MLE} = X_{(n)}\)</span>.</p>
<p>(b) The cdf for <span class="math inline">\(X_{(n)}\)</span> is <span class="math inline">\([F_X(x)]^n = (x/\theta)^{2n}\)</span>, and the pdf is thus
<span class="math display">\[\begin{align*}
f_{(n)}(x) = \frac{d}{dx} F_{(n)}(x) = \frac{2n}{\theta^{2n}} x^{2n-1} \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
E[X_{(n)}] &amp;= \int_0^\theta x f_{(n)}(x) dx = \frac{2n}{\theta^{2n}} \int_0^\theta x^{2n} dx = \frac{2n}{\theta^{2n}} \left. \frac{x^{2n+1}}{2n+1}\right|_0^{\theta} = \frac{2n}{2n+1}\theta \,.
\end{align*}\]</span></p>
<p>(c) Since
<span class="math display">\[\begin{align*}
E[X_{(n)}] &amp;= \frac{2n}{2n+1}\theta \,,
\end{align*}\]</span>
we have that
<span class="math display">\[\begin{align*}
E\left[\frac{2n+1}{2n}X_{(n)}\right] &amp;= \theta
\end{align*}\]</span>
and thus
<span class="math display">\[\begin{align*}
\hat{\theta}_{MVUE} = \frac{2n+1}{2n}X_{(n)} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>(a) As <span class="math inline">\(\theta\)</span> is a lower bound, <span class="math inline">\(X_{(1)}\)</span> is a sufficient statistic.</p>
<p>(b) The MLE <em>is</em> the sufficient statistic in (a): <span class="math inline">\(\hat{\theta}_{MLE} = X_{(1)}\)</span>.</p>
<p>(c) The pdf for the minimum datum is
<span class="math display">\[\begin{align*}
f_{(1)}(x) = n f_X(x) [1-F_X(x)]^{n-1} = n e^{-(x-\theta)} \left(e^{-(x-\theta)}\right)^{n-1} = n e^{-n(x-\theta)} \,.
\end{align*}\]</span></p>
<p>(d) The expected value of <span class="math inline">\(X_{(1)}\)</span> is
<span class="math display">\[\begin{align*}
E[X_{(1)}] = \int_{\theta}^\infty x n e^{-n(x-\theta)} dx = n \int_{\theta}^\infty x n e^{-n(x-\theta)} dx \,.
\end{align*}\]</span>
Let <span class="math inline">\(u = n(x-\theta)\)</span>. Then <span class="math inline">\(du = n dx\)</span>, and if <span class="math inline">\(x = \theta\)</span>, <span class="math inline">\(u = 0\)</span>, and if
<span class="math inline">\(x = \infty\)</span>, <span class="math inline">\(u = \infty\)</span>. Thus
<span class="math display">\[\begin{align*}
E[X_{(1)}] &amp;= n \int_0^\infty \left(\frac{u}{n}+\theta\right) e^{-u} \frac{du}{n} = \int_0^\infty \frac{u}{n} e^{-u} du + \int_0^\infty \theta e^{-u} du = \frac{1}{n} \int_0^\infty u e^{-u} du + \theta \int_0^\infty e^{-u} du \\
&amp;= \frac{1}{n} \Gamma(2) + \theta \Gamma(1) = \frac{1}{n} \cdot 1! + \theta \cdot 0! = \theta + \frac{1}{n} \,.
\end{align*}\]</span>
Hence <span class="math inline">\(\hat{\theta}_{MVUE} = X_{(1)} - 1/n\)</span>.</p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, <span class="math inline">\(P(X_1 &gt; 1/2 \vert X_2 &lt; 1/2) = P(X_1 &gt; 1/2) = 1/2\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
P\left(X_1 &gt; \frac12 \vert X_1 &lt; \frac34\right) = \frac{P(X_1 &gt; 1/2 \cap X_1 &lt; 3/4)}{P(X_1 &lt; 3/4)} = \frac{P(1/2 &lt; X_1 &lt; 3/4)}{P(X_1 &lt; 3/4)} = \frac{0.25}{0.75} = \frac13 \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(X_1 &lt; 3X_2\)</span> is equivalent to <span class="math inline">\(X_2 &gt; \frac13 X_1\)</span>, i.e., <span class="math inline">\(X_2\)</span> lies above the line with intercept 0 and slope 1/3. The area of this region is 1 minus the area of the triangle with vertices (0,0), (1,0), and (1,1/3) or <span class="math inline">\(1 - 1/6\)</span> = 5/6.</p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
P\left(X_2 &lt; X_1 \vert X_2 &lt; \frac12\right) = \frac{P(X_2 &lt; X_1 \cap X_2 &lt; 1/2)}{P(X_1 &lt; 1/2)} = 2P(X_2 &lt; X_1 \cap X_2 &lt; 1/2) \,.
\end{align*}\]</span>
The probability is the area of the polygon with vertices (0,0), (1,0), (1,1/2), and (1/2,1/2), or 3/8. So <span class="math inline">\(P\left(X_2 &lt; X_1 \vert X_2 &lt; \frac12\right) = 6/8\)</span> or 3/4.</p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) The expected value indicates that we are on the “yes” line of the confidence interval reference table, hence we want to solve
<span class="math display">\[\begin{align*}
1 - e^{-n(y_{\rm obs}-\theta)} - (1-\alpha) = 0
\end{align*}\]</span>
for <span class="math inline">\(\theta\)</span>:
<span class="math display">\[\begin{align*}
e^{-n(y_{\rm obs}-\theta)} &amp;= \alpha \\
\Rightarrow ~~~ -n(y_{\rm obs}-\theta) &amp;= \log(\alpha) \\
\Rightarrow ~~~ \theta - y_{\rm obs} &amp;= \frac{1}{n}\log(\alpha) \\
\Rightarrow ~~~ \hat{\theta}_L &amp;= y_{\rm obs} + \frac{1}{n}\log(\alpha) \,.
\end{align*}\]</span></p>
<p>(b) Note that although this is a two-tailed test, it is impossible to sample a statistic value less than <span class="math inline">\(\theta\)</span>, so we derive the rejection region boundary as if we are performing an upper-tail test. We are on the “yes” line of the hypothesis test reference table, hence we want to solve
<span class="math display">\[\begin{align*}
1 - e^{-n(y_{\rm RR}-\theta_o)} - (1-\alpha) = 0
\end{align*}\]</span>
for <span class="math inline">\(y_{\rm RR}\)</span>:
<span class="math display">\[\begin{align*}
e^{-n(y_{\rm RR}-\theta_o)} &amp;= \alpha ~~~ \Rightarrow ~~~ -n(y_{\rm RR}-\theta_o) &amp;= \log(\alpha) ~~~ \Rightarrow ~~~ y_{\rm RR} - \theta_o = -\frac{1}{n}\log(\alpha) ~~~ \Rightarrow ~~~ y_{\rm RR} = \theta_o - \frac{1}{n}\log(\alpha) \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>(a) We utilize the Law of the Unconscious Statistician to derive the moment-generating function:
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tx}] = k \sum_x e^{tx}p_X(x) =  \frac{1}{2}\left[ e^{t} + e^{2t}\right] = \frac{e}{2}\left[ 1 + e^{t}\right] \,.
\end{align*}\]</span></p>
<p>(b) To derive the variance, we take the first two derivatives of <span class="math inline">\(m_X(t)\)</span>, set <span class="math inline">\(t\)</span> to zero in each, and apply the shortcut formula:
<span class="math display">\[\begin{align*}
E[X] &amp;= \frac{d m_{X}(t)}{dt}\bigg|_0 = \frac{e^t}{2} e^t + \frac{e^t}{2}\left[ 1 + e^{t}\right]\bigg|_0 = \frac{1}{2}\cdot 1 + \frac{1}{2}(1+1) = \frac{3}{2} \\
E[X^2] &amp;= \frac{d^2 m_{X}(t)}{dt^2}\bigg|_0 = \frac{2e^{2t}}{2}  + \frac{e^te^t}{2} + e^t\left[ 1 + e^{t}\right] \bigg|_0 = 1 + \frac{1}{2} + 1= \frac{5}{2} \\
\Rightarrow ~~~ V[X] &amp;= E[X^2] - E[X]^2 = \frac{5}{2}- \left(\frac{3}{2}\right)^2 = \frac{1}{4} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>(a) We utilize a general transformation here:
<span class="math display">\[\begin{align*}
F_U(u) = P(U \leq u) = P(\sqrt{X} \leq u) = P(X \leq u^2) = \int_0^{u^2} f(x) dx = u^2 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du}F_U(u) = 2u
\end{align*}\]</span>
for <span class="math inline">\(u \in [0,1]\)</span>.</p>
<p>(b) <span class="math inline">\(f_U(u) = 2u\)</span> for <span class="math inline">\(0 \leq u \leq 1\)</span> is a beta distribution: <span class="math inline">\(U \sim\)</span>
Beta(2,1).</p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>(a) If <span class="math inline">\(x = 0\)</span>, then <span class="math inline">\(u = x^2 = 0\)</span>; if <span class="math inline">\(x = 1\)</span>, then <span class="math inline">\(u = x^2 = 1\)</span>. Also, the function is one-to-one, so we know that <span class="math inline">\(u\)</span> does not stray outside these endpoints. Thus <span class="math inline">\(u \in [0,1]\)</span>.</p>
<p>(b) We utilize a general transformation to determine that
<span class="math display">\[\begin{align*}
F_U(u) = P(U \leq u) = P(X^2 \leq u) = P(X \leq \sqrt{u}) = \int_0^{\sqrt{u}} 1 dx = \sqrt{u} \,.
\end{align*}\]</span></p>
<p>(c) The probability density function is the derivative of the cumulative distribution function:
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du} F_U(u) = \frac{1}{2} u^{-1/2} \,.
\end{align*}\]</span></p>
<p>(d) The expected value is
<span class="math display">\[\begin{align*}
E[U] = \int_0^1 u f_U(u) du = \frac{1}{2} \int_0^1 u^{1/2} du = \frac12 \frac23 \left. u^{3/2} \right|_0^1 = \frac13 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>(a) A probability density function for a continuous distribution is the derivative of its cumulative distribution function, so
<span class="math display">\[\begin{align*}
f_X(x) = \frac{d}{dx} F_X(x) &amp;= \frac{d}{dx} c\left(1-e^{-x/\theta}\right) = -ce^{-x/\theta}\left(-\frac{1}{\theta}\right) = \frac{c}{\theta}e^{-x/\theta} \,.
\end{align*}\]</span>
This is a truncated exponential distribution.</p>
<p>(b) We sample a single datum, so our statistic is <span class="math inline">\(X\)</span>. We are constructing an upper-tail test where <span class="math inline">\(E[X]\)</span> <em>decreases</em> with <span class="math inline">\(c\)</span> (as <span class="math inline">\(c \rightarrow \infty\)</span>, <span class="math inline">\(-\theta\log(1-1/c) \rightarrow 0\)</span>, from the right). So we are on the “no” line of the hypothesis test reference table: <span class="math inline">\(q = \alpha\)</span>.
<span class="math display">\[\begin{align*}
c_o\left(1-e^{-x_{\rm RR}/\theta}\right) - \alpha &amp;= 0 \\
\Rightarrow ~~~ \left(1-e^{-x_{\rm RR}/\theta}\right) &amp;= \frac{\alpha}{c_o} \\
\Rightarrow ~~~ e^{-x_{\rm RR}/\theta} &amp;= 1-\frac{\alpha}{c_o} \\
\Rightarrow ~~~ -\frac{x_{\rm RR}}{\theta} &amp;= \log\left(1-\frac{\alpha}{c_o}\right) \\
\Rightarrow ~~~ x_{\rm RR} &amp;= -\theta\log\left(1-\frac{\alpha}{c_o}\right) \,.
\end{align*}\]</span></p>
<p>(c) The power can be (essentially) read directly off of the hypothesis test reference table:
<span class="math display">\[\begin{align*}
power(\theta) = F_Y(y_{\rm RR} \vert \theta) ~~~ \Rightarrow ~~~ power(c) = c\left(1 - e^{-x_{\rm RR}/\theta}\right) \,.
\end{align*}\]</span>
There is no “trivial” rejection here; that could only arise if <span class="math inline">\(c &lt; c_o\)</span>.</p>
<p>(d) If we observed a value of <span class="math inline">\(X\)</span> in the range <span class="math inline">\((-\theta\log(1-1/c_o),\infty)\)</span> (or <span class="math inline">\((x_c,\infty)\)</span>), we would trivially reject the null: it is impossible to sample a value of <span class="math inline">\(X\)</span> in this range if the null is correct.</p>
<p>(e) <span class="math inline">\(x_c = -\theta \log(1-1/c)\)</span> must be larger than the datum with the largest value, so the sufficient statistic is <span class="math inline">\(X_{(n)}\)</span>.</p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>(a) The maximum likelihood estimate is the sufficient statistic <span class="math inline">\(X\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_0^\theta x \frac{2}{\theta} \left( 1 - \frac{x}{\theta} \right) dx = \frac{1}{\theta} \left. x^2 \right|_0^\theta - \frac{1}{\theta^2} \left. \frac{2x^3}{3} \right|_0^\theta = \frac{1}{\theta} \theta^2 - \frac{1}{\theta^2} \frac{2\theta^3}{3} = \theta - \frac23 \theta = \frac13 \theta \,.
\end{align*}\]</span></p>
<p>(c) The bias is <span class="math inline">\(E[\hat{\theta}_{MLE}-\theta] = E[X] - \theta = -2\theta/3\)</span>.</p>
<p>(d) Since <span class="math inline">\(E[X] = \theta/3\)</span>, <span class="math inline">\(E[3X] = \theta\)</span>…and thus <span class="math inline">\(\hat{\theta}_{MVUE} = 3X\)</span>.</p>
<p>(e) One can compute this using the MLE result, or using the MVUE result:
<span class="math display">\[\begin{align*}
{\rm MLE}&amp;: (B[\hat{\theta}_{MLE}])^2 + V[\hat{\theta}_{MLE}] = \frac49\theta^2 + \frac{1}{18}\theta^2 = \frac12 \theta^2 \\
{\rm MVUE}&amp;: (B[\hat{\theta}_{MVUE}])^2 + V[\hat{\theta}_{MVUE}] = 0 + V[3X] = 9V[X] = \frac{9}{18}\theta^2 = \frac12 \theta^2 \,.
\end{align*}\]</span></p>
</div>
<div id="chapter-6" class="section level2 unnumbered hasAnchor">
<h2>Chapter 6<a href="chapter-exercises-solutions.html#chapter-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Problem 1</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
\int_0^1 \left( \int_0^1 dx_2 k (x_1 + x_2^2) \right) dx_1 &amp;= 1 = k \left[ \int_0^1 x_1 \left( \int_0^1 dx_2 \right) dx_1 + \int_0^1 \left( \int_0^1 x_2^2 dx_2 \right) dx_1 \right] \\
&amp;= k \left[ \int_0^1 x_1 dx_1 + \int_0^1 \frac13 dx_1 \right] = k \left[ \left. \frac{x_1^2}{2} \right|_0^1 + \left. \frac{x_1}{3} \right|_0^1 \right] \\
&amp;= k \left( \frac{1}{2} + \frac{1}{3} \right) = k \frac{5}{6} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(k = 6/5\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \frac{k (x_1 + x_2^2)}{f_{X_2}(x_2)} \,.
\end{align*}\]</span>
So we need to compute the marginal density:
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) = k \int_0^1 dx_1 (x_1+x_2^2) = k \left[ \int_0^1 x_1 dx_1 + \int_0^1 x_2^2 dx_1 \right] = k \left[ \left. \frac{x_1^2}{2} \right|_0^1 + x_2^2 (\left. x_1\right|_0^1) \right] = k \left( \frac{1}{2} + x_2^2 \right) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{k (x_1 + x_2^2)}{k ( \frac{1}{2} + x_2^2 )} = \frac{2x_1 + 2x_2^2}{1 + 2x_2^2} \,.
\end{align*}\]</span>
This may initially appear strange (in that if <span class="math inline">\(x_1 &gt; 1/2\)</span>, <span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2) &gt; 1\)</span>), but we simply need to remind ourselves that <span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2)\)</span> is a conditional probability density function, not a probability itself.</p>
<hr />
<ul>
<li>Problem 2</li>
</ul>
<p>(a) Cov(<span class="math inline">\(X_1,X_2\)</span>) = <span class="math inline">\(E[X_1X_2] - E[X_1]E[X_2]\)</span> = <span class="math inline">\(1 \cdot 1 \cdot 0.1 - (1 \cdot 0.4 + 1 \cdot 0.1)^2 = 0.1 - 0.25 = -0.15\)</span>.</p>
<p>(b) <span class="math inline">\(\rho\)</span> = Cov(<span class="math inline">\(X_1,X_2\)</span>)/(<span class="math inline">\(\sigma_1\sigma_2\)</span>), where
<span class="math display">\[\begin{align*}
\sigma_1 = \sqrt{E[X_1^2] - (E[X_1])^2} = \sqrt{0.5 - (0.5)^2} = \sqrt{0.25} = 0.5 = \sigma_2 \,.
\end{align*}\]</span>
So <span class="math inline">\(\rho = -0.15/0.5/0.5 = -0.15/0.25 = -0.6\)</span>.</p>
<p>(c) <span class="math inline">\(E[X_1 \vert X_2 &lt; 1]\)</span> is equivalent to <span class="math inline">\(E[X_1 \vert X_2 = 0]\)</span>, i.e., the expected value for data drawn from the first row of the given table.
<span class="math display">\[\begin{align*}
E[X_1 \vert X_2 = 0] &amp;= \sum_{x_1=0}^1 x_1 p(x_1 \vert x_2=0) = \sum_{x_1=0}^1 x_1 \frac{p(x_1,x_2=0)}{p(p_2{x_2}=0)} \\
&amp;= \frac{0 \cdot p(x_1=0,x_2=0) + 1 \cdot p(x_1=1,x_2=0)}{p(x_1=0,x_2=0)+p(x_1=1,x_2=0)} = \frac{0.4}{0.5} = 0.8 \,.
\end{align*}\]</span>
This answer could be reasoned out by inspecting the table.</p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
V[X_2 \vert X_1=1] &amp;= E[X_2^2 \vert X_1=1] - (E[X_2\vert X_1=1])^2 \\
&amp;= \sum_{x_2=0}^1 x_2^2 p(x_2 \vert x_1=1) - \left[\sum_{x_2=0}^1 x_2 p(x_2 \vert x_1=1)\right]^2 \\
&amp;= \sum_{x_2=0}^1 x_2^2 \frac{p(x_1=1,x_2)}{p(p_1{x_1=1})} - \left[\sum_{x_2=0}^1 x_2 \frac{P(x_1=1,x_2)}{p(p_1{x_1=1})}\right]^2 \\
&amp;= 1 \cdot \frac{p(x_1=1,x_2=1)}{p(x_1=1,x_2=0)+p(x_1=1,x_2=1)} -
&amp;~~~~~\left[1 \cdot \frac{p(x_1=1,x_2=1)}{p(x_1=1,x_2=0)+p(x_1=1,x_2=1)}\right]^2 \\
&amp;= \frac{0.1}{0.5} - \left(\frac{0.1}{0.5}\right)^2 = 0.2 - 0.04 = 0.16 \,.
\end{align*}\]</span>
This answer could also be reasoned out by inspecting the table.</p>
<hr />
<ul>
<li>Problem 3</li>
</ul>
<p>(a) We have that <span class="math inline">\(X \vert p\)</span> <span class="math inline">\(\sim\)</span> Bin(<span class="math inline">\(n,p\)</span>) and that <span class="math inline">\(p \sim\)</span> Uniform(0,0.1). The expected value of <span class="math inline">\(X \vert p\)</span> is <span class="math inline">\(np\)</span> and the expected value of <span class="math inline">\(p\)</span> is <span class="math inline">\((0+0.1)/2 = 0.05\)</span>. Thus
<span class="math display">\[\begin{align*}
E[X] = E[E[X \vert p]] = E[np] = nE[p] = 0.05n \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[X] &amp;= E[V[X \vert p]] + V[E[X \vert p]] = E[np(1-p)] + V[np] = n(E[p] - E[p^2]) + n^2V[p] \\
&amp;= n[E[p] - (V[p]+E[p]^2)] + n^2V[p] = n(n-1)V[p] + nE[p] - n(E[p])^2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 4</li>
</ul>
<p>(a) The area of integration lies between the <span class="math inline">\(x_1\)</span> axis, the <span class="math inline">\(x_2\)</span> axis, and the line <span class="math inline">\(x_2 = 1-x_1\)</span>, in the first quadrant.
<span class="math display">\[\begin{align*}
1 &amp;= \int_0^1 \int_0^{1-x_1} k x_1^2 x_2 dx_2 dx_1 = k \int_0^1 x_1^2 \int_0^{1-x_1} x_2 dx_2 dx_1 = k \int_0^1 x_1^2 \left( \left. \frac{x_2^2}{2}\right|_0^{1-x_1}\right) dx_1 \\
&amp;= \frac{k}{2} \int_0^1 x_1^2 (1-x_1)^2 dx_1 = \frac{k}{2} B(3,3) = \frac{k \Gamma(3) \Gamma(3)}{2 \Gamma(6)} = \frac{4k}{240} \,.
\end{align*}\]</span>
So <span class="math inline">\(k = 60\)</span>.</p>
<p>(b) <span class="math inline">\(P(X_1 &gt; 0.25 \vert X_2 = 0.5) = \int_{0.25}^{0.5} f(x_1 \vert x_2=0.5)dx_1\)</span>. <span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2) = f_{X_1,X_2}(x_1,x_2)/f_{X_2}(x_2)\)</span>, and
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) = \int_0^{1-x_2} 60 x_1^2 x_2 dx_1 = 60 x_2 \left( \left.\frac{x_1^3}{3}\right|_0^{1-x_2}\right) = 20x_2(1-x_2)^3 \,.
\end{align*}\]</span>
So:
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{60x_1^2y_2}{20x_2(1-y_2)^3} = \frac{3x_1^2}{(1-x_2)^3} \,.
\end{align*}\]</span>
Plugging in <span class="math inline">\(x_2 = 0.5\)</span>, we get <span class="math inline">\(24x_1^2\)</span>. Finally:
<span class="math display">\[\begin{align*}
\int_{1/4}^{1/2} 24 x_1^2 dx_1 = \left.8 x_1^3\right|_{1/4}^{1/2} = 7/8 \,.
\end{align*}\]</span></p>
<p>(c) The region over which <span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2)\)</span> is non-zero is not rectangular: <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are dependent random variables.</p>
<hr />
<ul>
<li>Problem 5</li>
</ul>
<p>(a) We sum over rows:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_2\)</span></th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_{x_2}(x_2)\)</span></td>
<td align="center">0.4</td>
<td align="center">0.4</td>
<td align="center">0.2</td>
</tr>
</tbody>
</table>
<p>(b) We have that</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_2\)</span></th>
<th align="center"><span class="math inline">\((-\infty,0)\)</span></th>
<th align="center">[0,1)</th>
<th align="center">[1,2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(F_{X_2}(x_2)\)</span></td>
<td align="center">0</td>
<td align="center">0.4</td>
<td align="center">0.8</td>
</tr>
</tbody>
</table>
<p>(c) We have that
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_x x p_X(x) = 0 \cdot 0.4 + 1 \cdot 0.4 + 2 \cdot 0.2 = 0.8 \\
E[Y^2] &amp;= \sum_x x^2 p_X(x) = 0^2 \cdot 0.4 + 1^2 \cdot 0.4 + 2^2 \cdot 0.2 = 1.2 \\
\end{align*}\]</span>
so <span class="math inline">\(V[X] = E[X^2] - (E[X])^2 = 0.56\)</span> and <span class="math inline">\(\sigma = \sqrt{0.56} = 0.748\)</span>.</p>
<hr />
<ul>
<li>Problem 6</li>
</ul>
<p>The region of integration lies between the <span class="math inline">\(x\)</span> axis, the <span class="math inline">\(y\)</span> axis, and the line <span class="math inline">\(x_2 = 1-x_1\)</span>, in the first quadrant.</p>
<p>(a) <span class="math inline">\({\rm Cov}(X_1,X_2) = E[X_1X_2] - E[X_1]E[X_2]\)</span>…so we need to compute
<span class="math inline">\(E[X_1X_2]\)</span>:
<span class="math display">\[\begin{align*}
E[X_1X_2] &amp;= \int_0^1 \int_0^{1-x_1} x_1 x_2 60 x_1^2 x_2 dx_2 dx_1 = 60 \int_0^1 x_1^3 \int_0^{1-x_1} x_2^2 dx_2 dx_1 \\
&amp;= 60 \int_0^1 x_1^3 \frac{(1-x_1)^3}{3} dx_1 = 20 B(4,4) = 20 \frac{3! 3!}{7!} = 1/7 \,.
\end{align*}\]</span>
Hence Cov(<span class="math inline">\(X_1,X_2\)</span>) = 1/7 - 1/2(1/3) = <span class="math inline">\(-1/42\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
{\rm Corr}(X_1,X_2) = \frac{{\rm Cov}(X_1,X_2)}{\sigma_{X_1}\sigma_{X_2}} = \frac{-1/42}{\sqrt{2/7-1/4} \sqrt{1/7-1/9}} = \cdots = -1/\sqrt{2} \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(V[X_1-2X_2] = V[X_1] + 4V[X_2] - 2 \cdot 2 \cdot {\rm Cov}(X_1,X_2) = 1/28 + 8/63 + 4/42 = \cdots = 65/252\)</span>.</p>
<hr />
<ul>
<li>Problem 7</li>
</ul>
<p>Let <span class="math inline">\(N\)</span> be the number of laid egges: <span class="math inline">\(N \sim\)</span> Poi(<span class="math inline">\(\lambda\)</span>), and <span class="math inline">\(E[N] = V[N] = \lambda\)</span>. Let <span class="math inline">\(X\)</span> be the number of hatched eggs: <span class="math inline">\(X \vert N \sim\)</span> Bin(<span class="math inline">\(N,p\)</span>), and <span class="math inline">\(E[X \vert N] = Np\)</span> and <span class="math inline">\(V[X \vert N] = Np(1-p)\)</span>.</p>
<p>(a) <span class="math inline">\(E[X] = E[E[X\vert N]] = E[Np] = pE[N] = \lambda p\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[X] &amp;= V[E[X\vert N]] + E[V[X \vert N]] = V[Np] + E[Np(1-p)] \\
&amp;= p^2V[N] + p(1-p)E[N] = \lambda p^2 + \lambda p - \lambda p^2 = \lambda p \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 8</li>
</ul>
<p>(a) <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are independent <span class="math inline">\(\Rightarrow f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2) = \left[ k_1 x_1 e^{-x_1/2}\right]\left[k_2x_2(1-x_2) \right]\)</span>. <span class="math inline">\(X_1 \sim \text{Gamma}(2,2) \left[ \text{or } \chi^2_4\right]\)</span>, and <span class="math inline">\(X_2 \sim \text{Beta}(2,2)\)</span>, thus
<span class="math display">\[\begin{align*}
k = k_1 k_2 = \frac{1}{\beta_1^{\alpha_1}\Gamma(\alpha_1)}\underbrace{\frac{\Gamma(\alpha_2 + \beta_2)}{\Gamma(\alpha_2)\Gamma(\beta_2)}}_{1/B(\alpha_2,\beta_2)} = \frac{1}{2^21!}\frac{3!}{1!1!} = \frac{3}{2} \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[X_1 - X_2] &amp;=  V[X_1] + V[X_2] = \underbrace{\alpha_1 \beta_1^2}_{\text{Gamma}} + \underbrace{\alpha\beta/\left[(\alpha_2 + \beta_2)^2(\alpha_2 + \beta_2 + 1)\right]}_{\text{Beta}}\\
&amp;= 8 + 4/(16 \cdot 5) = 8 + \frac{1}{20} = \frac{161}{20} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 9</li>
</ul>
<p>(a) <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are uniformly distributed. In a plane described by <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, the region where <span class="math inline">\(f_{X_1,X_2}(x_1, x_2)&gt;0\)</span> corresponds to a rectangular trapezoid, with long base going from the point with coordinate <span class="math inline">\((0,0)\)</span> to the point with coordinate <span class="math inline">\((2,0)\)</span>, and short base going from the point with coordinate <span class="math inline">\((0,1)\)</span> to the point with coordinate <span class="math inline">\((1,1)\)</span>. Therefore the segment connecting the two points <span class="math inline">\((1,1)-(2,0)\)</span> corresponds to the line <span class="math inline">\(X_2 = -X_1 + 2\)</span>, or <span class="math inline">\(X_1 = 2 - X_2\)</span>. Thus
<span class="math display">\[\begin{align*}
f_{X_1,X_2}(x_1, x_2) = \frac{1}{\text{area of the region for which }f_{X_1,X_2}(x_1, x_2)&gt;0 }= \frac{1}{3/2} = \frac{2}{3} \,.
\end{align*}\]</span></p>
<p>(b) <span class="math inline">\(f_{X_2}(x_2) = \int_{x_1 = 0}^{x_1 = 2-x_2} k dx_1 = k \left[ x_1|_0^{2-x_2}\right] = k(2-x_2)\)</span>,i if <span class="math inline">\(0\leq x_2 \leq 1\)</span>. Therefore,
<span class="math inline">\(f_{X_2}(x_2) = k(2-x_2)\)</span> for <span class="math inline">\(x_2 \in [0,1]\)</span>.</p>
<p>(c) <span class="math inline">\(f_{X_1 | X_2}(x_1 | x_2) = f_{X_1,X_2}(x_1, x_2)/f_{X_2}(x_2) = 1/(2-x_2)\)</span>, for <span class="math inline">\(x_1 \in [0,2-x_2]\)</span> and <span class="math inline">\(x_2 \in [0,1]\)</span>.</p>
<p>(d) We have that
<span class="math display">\[\begin{align*}
E[X_1] =&amp; \int_0^1 \int_{x_1 = 0}^{x_1 = 2-x_2} x_1 k dx_1 dx_2 = k \int_0^1 \left[\frac{x_1^2}{2}\bigg|_{0}^{2-x_2} \right] dx_2 \\
=&amp; k \int_0^1 \frac{1}{2} (2-x_2)^2 dx_2 = \frac{k}{2} \int_0^1 (4 - 4x_2 + x_2^2) dx_2\\
=&amp; \frac{k}{2} \left[ 4x_2\bigg|_0^1 - 2x_2^2\bigg|_0^1 + \frac{x_2^3}{3}\bigg|_0^1\right] = \frac{k}{2}\left(4 -2 +\frac{1}{3}\right) = \frac{7}{6}k \,.
\end{align*}\]</span></p>
<p>(e) They are dependent since the region over which <span class="math inline">\(f_{X_1,X_2}(x_1, x_2) &gt;0\)</span> is non-rectangular.</p>
<hr />
<ul>
<li>Problem 10</li>
</ul>
<p>(a) <span class="math inline">\(p \sim {\rm Beta}(2,2)\)</span>, and <span class="math inline">\(X|p \sim {\rm Bin}(5,p)\)</span>, thus
<span class="math display">\[\begin{align*}
E\left[ E\left[ X|p\right] \right]  = E[5p] = 5E[p] = 5\frac{\alpha}{\alpha + \beta} = 2.5 \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[X] &amp;= V\left[ E\left[ X|p\right] \right] +  E\left[ V\left[ X|p\right] \right] = V[5p] + E[5p(1-p)] = 25V[p] + 5\left[E[p] - E[p^2] \right]\\
&amp;= 25V[p] + 5E[p] -5\left[ V[p] + (E[p])^2 \right] = 20V[p] + 5E[p] -5(E[p])^2\\
&amp;= 20 \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} + 5\frac{\alpha}{\alpha + \beta} - 5\left(\frac{\alpha}{\alpha + \beta}\right)^2\\
&amp;= 20 \frac{4}{16 \cdot 5} + 2.5 -1.25 = 1 + 2.5 - 1.25 = 2.25 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 11</li>
</ul>
<p>The region where <span class="math inline">\(f_{X_1,X_2}(x_1, x_2)&gt;0\)</span> can be represented by a triangle with vertices (0,0), (1,0), and (1,1). Thus:
<span class="math display">\[\begin{align*}
E[X_1 X_2] &amp;= \int_0^1 \left( \int_0^{x_1} x_1 x_2 (3 x_1) dx_2 \right) dx_1
= \int_0^1 3x_1^2 \left( \int_0^{x_1} x_2 dx_2\right) dx_1 \\
&amp;=  \int_0^1 3x_1^2 \frac{x_1^2}{2} dx_1 = \frac{3}{2} \int_0^1x_1^4 dx_1 = \frac{3}{10} x_1^5\bigg|_0^1 = \frac{3}{10} \,.
\end{align*}\]</span>
Therefore
<span class="math display">\[\begin{align*}
\rho = \frac{{\rm Cov}(X_1,X_2)}{\sqrt{V[X_1] V[X_2]}} = \frac{E[X_1X_2] - E[X_1] E[X_2]}{\sqrt{V[X_1] V[X_2]}}  = \frac{\frac{3}{10} - \frac{9}{32}}{\sqrt{\frac{3}{80}\frac{19}{320}}} = 0.397 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 12</li>
</ul>
<p>The region of integration lies in the first quadrant, below the line <span class="math inline">\(x_2 = x_1\)</span>.</p>
<p>(a) We have that
<span class="math display">\[\begin{align*}
\int_0^\infty \left[ \int_0^{x_1} k e^{-x_1} dx_2 \right] dx_1 = k \int_0^\infty e^{-x_1} \int_0^{x_1} dx_2 dx_1 = k \int_0^\infty x_1 e^{-x_1} dx_1 = k \Gamma(2) = 1 \Rightarrow k = 1 \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
P(X_2 &lt; 1) &amp;= \int_0^1 \left[ \int_{x_2}^\infty e^{-x_1} dx_1 \right] dx_2  = \int_0^1 \left(-\left.e^{-x_1}\right|_{x_2}^\infty \right) dx_2 \\
&amp;= \int_0^1 e^{-x_2} dx_2 = -\left.e^{-x_2}\right|_0^1 = -(e^{-1}-1) = 1-e^{-1} \,.
\end{align*}\]</span></p>
<p>(c) The marginal distribution is
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) = \int_{x_2}^\infty e^{-x_1} dx_1 = -\left.e^{-x_1}\right|_{x_2}^\infty = -(0-e^{-x_2}) = e^{-x_2} \,,
\end{align*}\]</span>
or, in full,
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) = \left\{ \begin{array}{cl} e^{-x_2} &amp; x_2 \geq 0 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{align*}\]</span></p>
<p>(d) The conditional pdf is
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \frac{e^{-x_1}}{e^{-x_2}} = e^{x_2-x_1} \,,
\end{align*}\]</span>
or, in full,
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) = \left\{ \begin{array}{cl} e^{x_2-x_1} &amp; x_1,x_2 \geq 0 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{align*}\]</span></p>
<p>(e) We have that
<span class="math display">\[\begin{align*}
P(X_1 &gt; 2 \vert X_1 = 1) &amp;= \int_2^\infty f_{X_1 \vert X_2=1}(x_1 \vert x_2=1) dx_1 = \int_2^\infty e^{1-x_1} dx_1 \\
&amp;= e^1 \left(-\left.e^{-x_1}\right|_2^\infty\right) = e^1(-(0-e^{-2})) = e^{-1} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 13</li>
</ul>
<p>We are given that <span class="math inline">\(X|\lambda \sim\)</span> Poisson<span class="math inline">\((\lambda)\)</span>, and <span class="math inline">\(\lambda \sim\)</span> NegBinom<span class="math inline">\((4,1/2)\)</span>. Thus <span class="math inline">\(E[X|\lambda] = \lambda\)</span> and <span class="math inline">\(E[\lambda] = \frac{r(1-p)}{p} = 4\)</span>, and <span class="math inline">\(V[X|\lambda] = \lambda\)</span> and <span class="math inline">\(V[\lambda] = \frac{r(1-p)}{p^2} = \frac{2}{1/4} = 8\)</span>.</p>
<p>(a) <span class="math inline">\(E[X] = E\left[ E\left[ X|\lambda\right] \right] = E[\lambda] = 4\)</span>.</p>
<p>(b) <span class="math inline">\(V[X] = E\left[ V\left[ X|\lambda\right] \right] + V\left[ E\left[ X|\lambda\right] \right] = E[\lambda] + V[\lambda] = 4 + 8 = 12\)</span>.</p>
<hr />
<ul>
<li>Problem 14</li>
</ul>
<p>The region where <span class="math inline">\(f_{X_1,X_2}(x_1, x_2)\)</span> is positive can be described as the union of two rectangular triangle. The first one with vertexes <span class="math inline">\((-2,0) - (-2, 2) - (0,0)\)</span>. The second one with vertexes <span class="math inline">\((2,0) - (2, 2) - (0,0)\)</span>.</p>
<p>(a) The region is not rectangular: <span class="math inline">\(X_1, X_2\)</span> are not independent.</p>
<p>(b) Uniformity <span class="math inline">\(\Rightarrow k = \frac{1}{\text{geometric area}} = \frac{1}{4}\)</span>.</p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
E[X_2] &amp;= \int_0^2 \left[\int_{-2}^{-x_2} \frac{x_2}{4} dx_1 + \int_{x_2}^{2} \frac{x_2}{4} dx_1  \right] dx_2 = 2\int_0^2 \left[\int_{x_2}^{2} \frac{x_2}{4} dx_1 \right] dx_2\\
&amp;=\frac{1}{2}\int_0^2 x_2 \left[\int_{x_2}^{2} dx_1 \right] dx_2 =\frac{1}{2}\int_0^2 x_2(2-x_2) dx_2 = \frac{1}{2} \left[ x_2^2\bigg|_0^2 - \frac{x_2^3}{3}\bigg|_0^2\right] \\
&amp;= \frac{1}{2} \left[ 4 - \frac{8}{3}\right]=  \frac{2}{3} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 15</li>
</ul>
<p>(a) We have that
<span class="math display">\[\begin{align*}
\int_0^1 \int_0^1 f_{X_1,X_2}(x_1, x_2) dx_1 dx_2 = 1 = k \left[ \int_0^1 x_1 dx_1 \int_0^1 x_2 dx_2 \right] =  k \left[  \frac{x_1^2}{2}\bigg|_0^1 \frac{x_2^2}{2}\bigg|_0^1 \right]  = \frac{k}{4} \,.
\end{align*}\]</span>
Therefore <span class="math inline">\(k = 4\)</span>.</p>
<p>(b) <span class="math inline">\(X_1, X_2\)</span> are independent, so
<span class="math display">\[\begin{align*}
P(X_2 &gt; 1/2|X_1 = 1/2) &amp;= P(X_2 &gt; 1/2) = \int_{1/2}^1 \left[ \int_0^1 4 x_1 x_2 dx_1\right] dx_2\\
&amp;=  \int_{1/2}^1 x_2 \left[ \int_0^1 4 x_1 dx_1\right] dx_2 =  \int_{1/2}^1 x_2 \frac{1}{2} dx_2 = 2 \left( \frac{x_2^2}{2}\bigg|_{1/2}^1\right) \\
&amp;= 1^2 - (1/2)^2 = \frac{3}{4} \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(X_1, X_2\)</span> are independent, so Cov<span class="math inline">\((X_1, X_2) = 0\)</span>.</p>
<hr />
<ul>
<li>Problem 16</li>
</ul>
<p>We have that
<span class="math display">\[\begin{align*}
V[U] = a^T\Sigma a =
\begin{bmatrix}
2 &amp; -1
\end{bmatrix}
\begin{bmatrix}
3 &amp; 2\\
2 &amp; 3
\end{bmatrix}
\begin{bmatrix}
2 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
2 &amp; -1
\end{bmatrix}
\begin{bmatrix}
4 \\
1
\end{bmatrix}
= 8 - 1 =7 \,.
\end{align*}\]</span>
Alternatively,
<span class="math display">\[\begin{align*}
V[U] = a_1^2V[X_1] + a_2^2 V[X_2] + 2a_1a_2 \text{Cov}(X_1, X_2) = 4 \cdot 3 + 1 \cdot 3 + 2 (2)(-1)(2) = 15-8 = 7 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 17</li>
</ul>
<p>Before we begin, we can write down that <span class="math inline">\(X \sim \text{Bernoulli}(1/2)\)</span>, so <span class="math inline">\(E[X] = \frac{1}{2}\)</span> and <span class="math inline">\(V[X] = \frac{1}{2}\left( 1 - \frac{1}{2}\right) = \frac{1}{4}\)</span>, and that <span class="math inline">\(U \vert X \sim \text{Uniform}(X,2)\)</span>, so <span class="math inline">\(E[U \vert X] = \frac{X+2}{2}\)</span> and <span class="math inline">\(V[U \vert X] = \frac{1}{12}\left( 2 - X^2 \right)\)</span>.</p>
<p>We have that
<span class="math display">\[\begin{align*}
V[U] &amp;= E\left[V \left[ U|X \right] \right] + V\left[E \left[ U|X \right] \right] \\
&amp;= E\left[\frac{1}{12}\left( 2 - X^2 \right) \right] + V\left[\frac{X+2}{2} \right] \,,
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
V\left[\frac{X+2}{2}\right] = V\left[\frac{X}{2}\right] = \frac{1}{4}V[X] = \frac{1}{16}
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
E\left[\frac{1}{12}\left( 2 - X^2 \right) \right] &amp;= \frac{1}{12} E[4 - 4X + X^2] = \frac{1}{3} - \frac{1}{6} + \frac{1}{12}\left[V[X] + (E[X])^2 \right]\\
&amp;= \frac{1}{6} + \frac{1}{12} \left[ \frac{1}{4} \frac{1}{4}\right]  = \frac{4}{24} + \frac{1}{24} = \frac{5}{24} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(V[U] = 5/24 + 1/16 = 13/48\)</span>.</p>
<hr />
<ul>
<li>Problem 18</li>
</ul>
<p>(a) Is the region rectangular? Yes. And <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) f_{X_2}(x_2) = f_{X_1}(x_1)\)</span>, with <span class="math inline">\(f_{X_2}(x_2) = 1\)</span>, so <span class="math inline">\(X_1, X_2\)</span> are independent random variables.</p>
<p>(b) <span class="math inline">\(f_{X_1,X_2}(x_1, x_2) =f_{X_1}(x_1) f_{X_2}(x_2) = 12 x_1^2(1-x_1) \cdot 1\)</span>, therefore <span class="math inline">\(f_{X_2}(x_2) = 1\)</span> for <span class="math inline">\(x_2 \in [0,1]\)</span>, or <span class="math inline">\(X_2 \sim \text{Uniform}(0,1)\)</span>. Other ways to derive this result include
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) &amp;= \underbrace{\int_0^1 12 x_1^2(1-x_1) dx_1}_{\text{Beta}(3,2)} = 1 \\
&amp;= \int_0^1 12 x_1^2(1-x_1) dx_1 = 12 \left[ \frac{x_1^3}{3}\bigg|_0^1 - \frac{x_1^4}{4}\bigg|_0^1 \right] = \frac{12}{12} = 1 \,.
\end{align*}\]</span></p>
<p>(c) <span class="math inline">\(X_1 \sim \text{Beta}(3,2)\)</span>, hence <span class="math inline">\(E[X_1] = \alpha/(\alpha + \beta) = \frac{3}{5}\)</span>. Another way to derive this result is
<span class="math display">\[\begin{align*}
f_{X_1}(x_1) &amp;= \int_0^1 12 x_1^2(1-x_1) dx_1 =  12 x_1^2(1-x_1) \\
E[X_1] &amp;= \int_0^1 x_1 f_{X_1}(x_1) dx_1 = \int_0^1 12 x_1^3(1-x_1) dx_1 =  12 \left[ \frac{x_1^4}{4}\bigg|_0^1 - \frac{x_1^5}{5}\bigg|_0^1 \right] = \frac{12}{20} = \frac{3}{5} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 19</li>
</ul>
<p>(a) The area where <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)&gt;0\)</span> is a triangle with vertices at (0,0), (1,1), and (2,0). For a bivariate uniform, <span class="math inline">\(c = 1/\)</span>(the area of the region), so <span class="math inline">\(c = 1\)</span>. Another way to derive this is via brute force:
<span class="math display">\[\begin{align*}
\int_0^1 \left[\int_{x_2}^{2 - x_2} c dx_1\right] dx_2 &amp;= c \int_0^1 (2- x_2) - x_2 dx_2\\
&amp;= c \int_0^1 2(1- x_2) dx_2 = 2c \left[ x_2\bigg|_0^1 - \frac{x_2^2}{2}\bigg|_0^1 \right]  = c  = 1 \,.
\end{align*}\]</span></p>
<p>(b) <span class="math inline">\(f_{X_2}(x_2) = \int_{x_2}^{2 - x_2} dx_1 = 2(1- x_2)\)</span> for <span class="math inline">\(x_2 \in [0,1]\)</span>.</p>
<p>(c) <span class="math inline">\(f_{X_1|X_2}(x_1|x_2) =\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = 1/[2(1-x_2)]\)</span> for <span class="math inline">\(x_1 \in [0,2]\)</span>, and <span class="math inline">\(x_2 \in [0,x_1]\)</span> and <span class="math inline">\(x_2 \in [0,2-x_1]\)</span>.</p>
<hr />
<ul>
<li>Problem 20</li>
</ul>
<p>(a) We can recognize immediately that <span class="math inline">\(X_1,X_2\)</span> are independent, thus <span class="math inline">\(f_{X_1}(x_1) = 2x_1\)</span> for <span class="math inline">\(x_1 \in [0,1]\)</span>. We can also derive this result via brute force:
<span class="math display">\[\begin{align*}
f_{X_1}(x_1) = \int_0^1 4 x_1 x_2 dx_2 = 4x_1 \frac{x_2^2}{2}\bigg|_0^1 = 2x_1 \,.
\end{align*}\]</span></p>
<p>(b) We have that <span class="math inline">\(f_{X_2|X_1}(x_2|x_1) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \frac{4 x_1 x_2}{2x_1} = 2x_2\)</span> for <span class="math inline">\(x_1 \in [0,1]\)</span> and <span class="math inline">\(x_2 \in [0,1]\)</span>.</p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
P(X_2 &lt; 1/2 |X_1 = x_1) = \int_0^{1/2} f_{X_2|X_1}(x_2|x_1) dx_2 =  \int_0^{1/2}2x_2dx_2 = x_2^2\bigg|_0^{1/2} = \frac{1}{4} \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 21</li>
</ul>
<p>We begin by noting that <span class="math inline">\(V[X_1] = E[X_1^2] - E[X_1]^2\)</span>, and that
<span class="math display">\[\begin{align*}
E[X_1] &amp;= \int_0^1 \int_0^{1-x_1} 2x_1 dx_2 dx_1 = 2 \int_0^1 x_1(1-x_1) dx_1 = 2B(2,2) = 2 \frac{\Gamma(2)\Gamma(2)}{\Gamma(4)} = \frac{1!1!}{3!} = \frac{1}{3} \,. \\
E[X_1^2] &amp;= \int_0^1 \int_0^{1-x_1} 2x_1 x_1 dx_2 dx_1 = 2 \int_0^1 x_1^2(1-x_1) dx_1 = 2B(3,2) = 2 \frac{2!1!}{4!} = \frac{1}{6} \,.
\end{align*}\]</span>
Thus <span class="math inline">\(V[X_1] = 1/6 -  \left( 1/3 \right)^2 = 1/18\)</span>.</p>
<hr />
<ul>
<li>Problem 22</li>
</ul>
<p>We have that <span class="math inline">\(V[X_1 - X_2] = V[X_1] + V[X_2] - 2\)</span>Cov<span class="math inline">\((X_1,X_2)\)</span>. So we need to compute every part of the formula above:
<span class="math display">\[\begin{align*}
E[X_1] &amp;= \sum \sum x_1 p_{X_1,X_2}(x_1,x_2) = 1 \cdot \frac{2}{9} + 1 \cdot \frac{2}{9} + 2 \cdot \frac{1}{9} = \frac{6}{9}\\
V[X_1] &amp;= E[X_1^2] - E[X_1]^2 =  \frac{8}{9} -  \frac{36}{81} = \frac{4}{9}\\
E[X_2] &amp;= \sum \sum x_2 p_{X_1,X_2}(x_1,x_2) = 1 \cdot \frac{3}{9} + 1 \cdot \frac{2}{9} + 1 \cdot \frac{1}{9} = \frac{6}{9}\\
V[X_1] &amp;= E[X_1^2] - E[X_1]^2 =  \frac{6}{9} -  \frac{36}{81} = \frac{2}{9}\\
E[X_1X_2] &amp;=  \sum \sum x_1 x_2 p_{X_1,X_2}(x_1,x_2) = 1 \cdot 1 \cdot \frac{2}{9} + 2 \cdot 1 \cdot \frac{1}{9} = \frac{4}{9}\\
\mbox{Cov}(X_1, X_2) &amp;= \frac{4}{9} - \left( \frac{6}{9}\right)^2 = 0 \,.
\end{align*}\]</span>
Thus <span class="math inline">\(V[X_1 - X_2] = 6/9 = 2/3\)</span>.</p>
<hr />
<ul>
<li>Problem 23</li>
</ul>
<p>We recognize that <span class="math inline">\(X|\beta \sim \text{Gamma}(\alpha, \beta)\)</span> and that <span class="math inline">\(\beta \sim \text{Exp}(\gamma)\)</span>, with <span class="math inline">\(E[\beta] = \gamma, V[\beta] =  \gamma^2\)</span>, and <span class="math inline">\(E[X|\beta] = \alpha \beta\)</span>, <span class="math inline">\(V[X|\beta] = \alpha \beta^2\)</span>.</p>
<p>(a) <span class="math inline">\(E[X] = E\left[ E\left[ X|p\right] \right] = E[\alpha \beta] = \alpha E[\beta] = \alpha \gamma\)</span>.</p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
V[X] &amp;= V\left[ E\left[ X|p\right] \right] +  E\left[ V\left[ X|p\right] \right] = E[\alpha \beta^2] + V[\alpha \beta]\\
&amp;= \alpha E[\beta^2] + \alpha^2V[\beta]= \alpha \left[ V[\beta] + E[\beta]^2\right] + \alpha^2 \gamma^2\\
&amp;= \alpha \left[ \gamma^2+\gamma^2\right] + \alpha^2 \gamma^2  = (2\alpha + \alpha^2) \gamma^2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 24</li>
</ul>
<p>The region of integration is a triangle with vertices at (0,0), (0,1), and (1,1).</p>
<p>(a) The area of the triangle is 1/2, so <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = 2\)</span>.</p>
<p>(b) Geometry will not directly help us here; we still have to integrate. The expected value <span class="math inline">\(E[X_1]\)</span> is
<span class="math display">\[\begin{align*}
E[X_1] &amp;= \int_0^1 \left[ \int_0^{x_1} 2 x_1 dx_2 \right] dx_1 = 2 \int_0^1 x_1 \left[ \int_0^{x_1} dx_2 \right] dx_1 \\
&amp;= 2 \int_0^1 x_1 x_1 dx_1 = 2 \left.\frac{x_1^3}{3}\right|_0^1 = \frac23 \,.
\end{align*}\]</span></p>
<p>(c) We have that Cov[<span class="math inline">\(X_1,X_2\)</span>] = <span class="math inline">\(E[X_1X_2] - E[X_1]E[X_2]\)</span>. So we need to compute <span class="math inline">\(E[X_1X_2]\)</span>:
<span class="math display">\[\begin{align*}
E[X_1X_2] &amp;= \int_0^1 \left[ \int_0^{x_1} 2 x_1 x_2 dx_2 \right] dx_1 = 2 \int_0^1 x_1 \left[ \int_0^{x_1} x_2 dx_2 \right] dx_1 \\
&amp;= 2 \int_0^1 x_1 \left[ \left.\frac{x_2^2}{2}\right|_0^{x_1} \right] dx_1 = \int_0^1 x_1 x_1^2 dx_1 = \left.\frac{x_1^4}{4}\right|_0^1 = \frac14 \,.
\end{align*}\]</span>
So the covariance is <span class="math inline">\(1/4 - (2/3)(1/3) = 9/36 - 8/36 = 1/36\)</span>.</p>
<hr />
<ul>
<li>Problem 25</li>
</ul>
<p>(a) The long way to do this involves integration. The short way is to see by inspection that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent (the region of integration is “rectangular,” and <span class="math inline">\(x_2\)</span> doesn’t directly appear in <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span>, so <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> can be trivially split into two functions <span class="math inline">\(g(x_1)h(x_2)\)</span>), and to see that
<span class="math display">\[\begin{align*}
X_1 \sim {\rm Exp}(\beta = 2) ~{\rm and}~ X_2 \sim {\rm Unif}(0,2) \,.
\end{align*}\]</span>
We know <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2) = (1/\beta){\rm exp}(-x_1/2) \cdot (1/2)\)</span>, so <span class="math inline">\(k = 1/(2\beta) = 1/4\)</span>.</p>
<p>(b) The key here is to realize that since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, <span class="math inline">\(E[X_1]\)</span> is just the expected value of <span class="math inline">\(f_{X_1}(x_1)\)</span>, i.e., it is the expected value of an exponential distribution with mean 2. So: <span class="math inline">\(E[X_1] = \beta = 2\)</span>.</p>
<hr />
<ul>
<li>Problem 26</li>
</ul>
<p>The region of integration is a triangle with vertices (0,0), (2,0), and (1,1). The area of the region of integration is 1, so <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = 1\)</span>.</p>
<p>(a) We have that
<span class="math display">\[\begin{align*}
f_{X_2}(x_2) &amp;= \int_{x_2}^{2-x_2} dx_1 = 2(1-x_2) ~~~ x_2 \in [0,1] \,.
\end{align*}\]</span></p>
<p>(b) We have that
<span class="math display">\[\begin{align*}
f_{X_1 \vert X_2}(x_1 \vert x_2) &amp;= \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \frac{1}{2(1-x_2)} ~~~ x_1 \in [x_2,2-x_2] ~~~ x_2 \in [0,1] \,.
\end{align*}\]</span></p>
<p>(c) The new region of integration is a triangle with vertices at (0,0), (1,0), and (1/2,1/2). Since we are dealing with a bivariate uniform, we know that the probability of sampling data from this triangle is the ratio of its area to the area of the total region of integration for the pdf. Thus <span class="math inline">\(P(X_1 + X_2 \leq 1) = (1/2 \cdot 1 \cdot 1/2)/1 = 1/4\)</span>.</p>
<hr />
<ul>
<li>Problem 27</li>
</ul>
<p>We are given that <span class="math inline">\(X \vert \mu \sim \mathcal{N}(\mu,1)\)</span> and that <span class="math inline">\(\mu \sim \mathcal{N}(0,1)\)</span>. We thus know that <span class="math inline">\(E[X \vert \mu] = \mu\)</span> and <span class="math inline">\(V[X \vert \mu] = 1\)</span>, while <span class="math inline">\(E[\mu] = 0\)</span> and <span class="math inline">\(V[\mu] = 1\)</span>. Thus
<span class="math display">\[\begin{align*}
V[X] &amp;= V\left[E[X \vert \mu]\right] + E\left[V[X \vert \mu]\right] = V[\mu] + E[1] = 1 + 1 = 2 \,.
\end{align*}\]</span></p>
<hr />
<ul>
<li>Problem 28</li>
</ul>
<p>(a) The conditional expected value is
<span class="math display">\[\begin{align*}
E[X_1 \vert X_2=1] &amp;= (x_1 = 0) \cdot p(x_1=0 \vert x_2=1) + (x_1 = 1) \cdot p(x_1=1 \vert x_2=1) \\
&amp;= 1 \cdot \frac{p(1,1)}{p_2(1)} = 1 \cdot \frac{0.1}{0.3+0.1} = 0.25 \,.
\end{align*}\]</span></p>
<p>(b) The conditional variance is
<span class="math display">\[\begin{align*}
V[X_1 \vert X_2=1] &amp;= E[X_1^2 \vert X_2=1] - (E[X_1 \vert X_2=1])^2 \,.
\end{align*}\]</span>
We know the second term. The first term can be derived in a manner similar to above:
<span class="math display">\[\begin{align*}
E[X_1^2 \vert X_2=1] &amp;= \ldots = 1^2 \cdot \frac{p(1,1)}{p_2(1)} = 1 \cdot \frac{0.1}{0.3+0.1} = 0.25 \,.
\end{align*}\]</span>
Thus the conditional variance is <span class="math inline">\(V[X_1 \vert X_2=1] = 0.25 - 0.25^2 = 1/4 - 1/16 = 3/16\)</span>.</p>
<hr />
<ul>
<li>Problem 29</li>
</ul>
<p>(a) Because the boundary of the domain is not rectangular (see <span class="math inline">\(x_1 + x_2 \leq 2\)</span>), <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are <em>not</em> independent.</p>
<p>(b) We need the distribution to integrate to 1:
<span class="math display">\[\begin{align*}
1 &amp;= \int_{x_1} \int_{x_2} c x_1^2 dx_2 dx_1 \,.
\end{align*}\]</span>
The domain is a triangle with vertices (0,0), (2,0), and (0,2), and thus there is no real advantage gained by utilizing either order of integration, so we’ll keep the integral over <span class="math inline">\(x_2\)</span> as our “inner” integral:
<span class="math display">\[\begin{align*}
\int_0^2 \int_0^{2-x_1} c x_1^2 dx_2 dx_1 &amp;= c \int_0^2 x_1^2 \left( \int_0^{2-x_1} dx_2 \right) dx_1 \\
&amp;= c \int_0^2 x_1^2 (2 - x_1) dx_1 \\
&amp;= c \left( 2 \int_0^2 x_1^2 dx_1 - \int_0^2 x_1^3 dx_1 \right) \\
&amp;= c \left( 2 \left. \frac{x_1^3}{3}\right|_0^2 - \left. \frac{x_1^4}{4}\right|_0^2 \right) \\
&amp;= c \left( \frac{16}{3} - 4 \right) = c\frac{4}{3} \,.
\end{align*}\]</span>
Hence <span class="math inline">\(c = 3/4\)</span>.</p>
<p>(c) We actually already derived the marginal distribution above:
<span class="math display">\[\begin{align*}
f_{X_1}(x_1) &amp;= c x_1^2 \int_0^{2-x_1} dx_2 \\
&amp;= c x_1^2 (2 - x_1) ~~~ x_1 \in [0,2] \,.
\end{align*}\]</span></p>
<p>(d) The conditional distribution is
<span class="math display">\[\begin{align*}
f_{X_2 \vert X_1}(x_2 \vert x_1) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \frac{cx_1^2}{cx_1^2(2-x_1)} = \frac{1}{2-x_1} ~~~ x_2 \in [0,2-x_1] \,.
\end{align*}\]</span></p>
<p>(e) Given the domain and the flat pdf, we know that <span class="math inline">\(f_{X_2 \vert X_1}(x_2 \vert x_1)\)</span> is a “uniform” distribution.</p>
<hr />
<ul>
<li>Problem 30</li>
</ul>
<p>(a) We need to determine <span class="math inline">\(E[X_1]\)</span>, <span class="math inline">\(E[X_2]\)</span>, and <span class="math inline">\(E[X_1X_2]\)</span>:
<span class="math display">\[\begin{align*}
E[X_1] &amp;= 1 \cdot (0.1 + 0.5) = 0.6 \\
E[X_2] &amp;= 1 \cdot (0.1 + 0.5) = 0.6 \\
E[X_1X_2] &amp;= 1 \cdot 1 \cdot 0.5 = 0.5 \,.
\end{align*}\]</span>
Thus Cov(<span class="math inline">\(X_1,X_2\)</span>) = <span class="math inline">\(E[X_1X_2] - E[X_1]E[X_2] = 0.5 - 0.6^2 = 0.14\)</span>.</p>
<p>(b) Now we need <span class="math inline">\(E[X_1^2]\)</span> and <span class="math inline">\(E[X_2^2]\)</span>:
<span class="math display">\[\begin{align*}
E[X_1^2] &amp;= 1^2 \cdot (0.1 + 0.5) = 0.6 \\
E[X_2^2] &amp;= 1^2 \cdot (0.1 + 0.5) = 0.6 \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
V[X_1] &amp;= E[X_1^2] - (E[X_1])^2 = 0.6 - 0.6^2 = 0.24 \\
V[X_2] &amp;= E[X_2^2] - (E[X_2])^2 = 0.6 - 0.6^2 = 0.24 \,,
\end{align*}\]</span>
and <span class="math inline">\(\sigma_1\sigma_2 = \sqrt{V[X_1]V[X_2]} = 0.24\)</span>, and <span class="math inline">\(\rho_{X_1,X_2} = 0.14/0.24 = 7/12\)</span>.</p>
<p>(c) We have that
<span class="math display">\[\begin{align*}
V[Y] &amp;= a_1^2 V[X_1] + a_2^2 V[X_2] + 2 a_1 a_2 \mbox{Cov}(X_1,X_2) \\
&amp;= 4 \cdot 0.24 + 1 \cdot 0.24 - 2 \cdot 2 \cdot 1 \cdot 0.14 \\
&amp;= 1.2 - 4 \cdot 0.14 = 0.64 \,.
\end{align*}\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bibliography.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

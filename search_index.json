[["preface.html", "Modern Probability and Statistical Inference Illustrated with R Preface", " Modern Probability and Statistical Inference Illustrated with R Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University) January 2025 Preface I have developed this textbook for a new mathematical statistics course sequence for statistics students at Carnegie Mellon University (numbered 36-235 and 36-236), first piloted in Fall 2022 and Spring 2023. The primary difference between it and others that are commonly used in math-stat classes is that I explicitly implement a spiral-learning framework, with concepts that are usually covered in relative isolation elsewhere being repeatedly covered here (such as, e.g., point estimation). My use of a spiral-learning framework is motivated by the observation that many students fixate on mathematics and coding and pay less attention to statistical concepts, with the details of concepts that are seen once often quickly forgotten. The spiral approach is meant to result in enhanced conceptual retention. To build the spiral structure, I have rather radically revised the order in which I cover topics, relative to the order one usually sees in a classic math-stat textbook. That textbook might have one chapter that illustrates the properties of discrete probability distributions using, e.g., the binomial, geometric, and Poisson distributions, then a second chapter that illustrates the properties of continuous distributions such as, e.g., the normal and gamma distributions, with neither chapter showing how these distributions are applied in statistical inference. Here, major distributions are broken out into their own chapters, and within each I revisit fundamental concepts: probability mass and density functions, cumulative distribution functions, statistics, sampling distibutions, point estimation, interval estimation, and hypothesis testing, etc. Then, as I move from chapter to chapter, I cover concepts at greater depth. As a concrete example: when discussing point estimation in Chapter 1, I introduce the concepts of bias, variance, and using the likelihood function to define estimators; when I return to point estimation in Chapter 2, I review these concepts (and derive the MLEs for normal distribution parameters), then add the concepts of consistency, Fisher information, the Cramer-Rao lower bound, and the asymptotic distribution of maximum likelihood estimates. Then in Chapter 3, I add in sufficient statistics and likelihood factorization, along with the minimum variance unbiased estimator. Etc. Another important difference between this textbook and older, more established math-stat textbooks is that I utilize R for coding visualizations, analyses, and simulations. It is expected that this will help students understand concepts more readily; it also allows me to broaden the “problem space” beyond typically used, analytically tractable distributions. (But I note that the newest generation of textbooks often employ enhanced computation…so it is really the spiral structure that makes this textbook fundamentally different.) If you are an instructor, feel free to utilize aspects of this book for your own class(es), but please do not share this document without my expressed consent. If you have comments or questions (or you want an updated version), please send email to the address given below. Peter Freeman pfreeman@cmu.edu January 2025 "],["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to thank Eileen Xiao, who converted the original base-R graphics in the book to ggplot()-based graphics during the summer of 2023. "],["the-basics-of-probability-and-statistical-inference.html", "1 The Basics of Probability and Statistical Inference 1.1 Data and Statistical Populations 1.2 Sample Spaces and the Axioms of Probability 1.3 Conditional Probability and the Independence of Events 1.4 Further Laws of Probability 1.5 Random Variables 1.6 Probability Distributions 1.7 Characterizing Probability Distributions 1.8 Working With R: Probability Distributions 1.9 Cumulative Distribution Functions 1.10 The Law of Total Probability 1.11 Working With R: Data Sampling 1.12 Statistics and Sampling Distributions 1.13 The Likelihood Function 1.14 Point Estimation 1.15 Statistical Inference with Sampling Distributions 1.16 Confidence Intervals 1.17 Hypothesis Testing 1.18 Working With R: Simulating Statistical Inference 1.19 Exercises", " 1 The Basics of Probability and Statistical Inference 1.1 Data and Statistical Populations Data surround us, in the form of numbers, texts, images, and more that are collected and analyzed across disciplines. Tweets contain data about user sentiments. Receipts contain data about people’s buying habits. Pictures help us differentiate between, e.g., goldfish and dogs. These data\\(-\\)tweets, receipts, pictures\\(-\\)are unstructured data, so-called because we generally cannot visualize or analyze them directly. So what can we do? We can provide structure: to determine if a tweet indicates that a film was liked or disliked, we can extract counts of words indicating sentiments (e.g., “good” and “bad”). To determine the whether an image is that of a goldfish or a dog, it can be passed through appropriate filters that break down the images to a series of analyzable numbers. Etc. The result of all this “pre-processing” is generation of structured data, data in the form of a table in which the columns represent particular measurements (e.g., the number of instances of the word “good”) and the rows representing the objects of study (e.g., individual films). Let’s focus on a single table column. Perhaps its data look like this: 34.1 28.6 37.7 52.1 26.6 28.9 ... Such data are dubbed quantitative data. Quantitative data are numbers that might be discretely valued (e.g., 1, 2, and 3) or continuously valued and measured to arbitrary precision (e.g., 15.4959735). Data may also look like this: Heads Heads Tails Heads ... These data are categorical data; each outcome is one element from a set of categories. Here that set is {Heads,Tails}. An experiment is the act of measuring and recording a datum (like when after each flip of a coin we record \\(H\\) for heads and \\(T\\) for tails). The data we generate from experiments are drawn from populations, the sets of all possible experimental outcomes. A population can be an existing group of objects (e.g., 52 cards in a deck, eight socks of different colors in a drawer), but it can also be hypothetical (e.g., a mathematical function, like a bell curve, which indicates the relative rates at which we would draw samples with particular values). To boil down the discipline of statistics to its essence, our goal is to use the data we have drawn from a population to say something (i.e., to infer something) about the population itself. If we record the heights of 100 people, we would like to say something about the average height of humans. If we record the ice-cream flavor preferences of 500 people, we would like to infer the proportion of humans that prefer chocolate to vanilla. Etc. Data surround us and the possibilities for inference are plentiful. We pictorially summarize what we write above in Figure 1.1. One might immediately notice the word “statistic,” which we have yet to define. As we see later in this chapter, a statistic is simply a function of data (such as their average value) that helps reduce data volume while (hopefully!) retaining sufficient information to allow useful inferences to happen. Defining and understanding useful statistics is a major part of this course! Figure 1.1: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, and use statistics, or functions of the data, to infer population properties. But, the reader says: the course is called Modern Probability and Statistical Inference. Where is probability in all of this? Probability is the so-called “language of statistics,” and it provides the mathematical framework upon which we can build statistical inference. Remember how above we say that a population might be a mathematical function indicating the relative rates of observing experimental outcomes? Those relative rates are probabilities (or at least probability densities). Thus the structure of this chapter (and “mathemtical statistics” courses as a whole): we discuss probability first, and then use our newfound knowledge to show how the enterprise of statistical inference works, both algorithmically and mathematically. 1.2 Sample Spaces and the Axioms of Probability Probability is the long-term frequency of the occurrence of an event. For instance, what is the probability of flipping a coin and observing heads? (Intuitively, this probability is 1/2, if the coin is fair.) Or: what is the probability that a student finishes a particular test in between 30 and 40 minutes? Etc. To build up an understanding of probability, it is conventional to start with the concept of a sample space. A sample space is the set of all possible outcomes of an “experiment” (or “trial”), which is simply some process that can, in theory, be repeated an infinite number of times. (For instance, the flipping of a coin.) For instance, if our experiment is to flip a single coin twice, the sample space would be \\[ \\Omega = \\{HH,HT,TH,TT\\} \\,, \\] where \\(H\\) and \\(T\\) represent observing heads and tails, respectively. (The Greek letter \\(\\Omega\\) is a capital “omega,” or “oh-MAY-gah.”) See Figure 1.2. Figure 1.2: This is an example of a sample space \\(\\Omega\\), representing the experimental outcomes of flipping a single coin twice and recording the observed side of the coin. For purposes of intuition, it is common to associate the area shown for each outcome with that outcome’s probability of occurrence, so here we may view the coin as an unfair one that favors tails. The members of the set \\(\\Omega\\) are dubbed events and they come in two varieties: simple events: specific experimental outcomes (e.g., \\(HH\\)); any two simple events in \\(\\Omega\\) are mutually exclusive, or disjoint, as they cannot be observed simultaneously in a single experiment. compound events: sets of two or more simple events (e.g., \\(\\{HH,HT,TH\\}\\), which represents the set of outcomes where heads was observed at least once). As stated above, a sample space is a set of possible experimental outcomes; thus we can apply set notation to, e.g., define specific events as functions of others: term notation intuitive terminology superset \\(A \\supset B\\) “encompasses” subset \\(A \\subset B\\) “within” union \\(A \\cup B\\) “or” intersection \\(A \\cap B\\) “and” complement \\(\\bar{A}\\) “not” We show examples of how we use set notation in the context of samples spaces below. Here are a few more things to keep in mind regarding sample spaces: The number of simple events in \\(\\Omega\\) (i.e., the set’s cardinality) may be finite (as in the example above) or either countably or uncountably infinite (e.g., the set of all non-negative integers versus the set of real numbers). (For instance, the simple events in the experiment of repeating a task until one fails are \\(\\{F,SF,SSF,SSSF,\\ldots\\}\\), where \\(S\\) denotes success and \\(F\\) denotes failure.) The definition of a sample space can depend upon whether the order of outcomes matters. For instance, if the order of outcomes does not matter, we could rewrite our two-coin-flip sample space as \\(\\Omega = \\{HH,HT,TT\\}\\) (or \\(\\{HH,TH,TT\\}\\)). At no point thus far have we indicated the probability of observing any simple event. It is not the case in general that each experimental outcome is equally likely! Regarding the last point above: while we may not know the probability of observing any simple event, there are some things we can say about its long-term relative frequency of occurrence: it must be \\(&gt; 0\\) and \\(\\leq 1\\); the relative frequencies of all simple events in \\(\\Omega\\) must sum to 1; and the relative frequency of a compound event must equal the sum of the relative frequencies of its component simple events. These statements appear to be self-evident, and as thus may be dubbed axiomatic. (A mathematical axiom is a statement accepted without proof.) In probability theory, these statements were recast as the so-called Kolmogorov axioms, introduced by Andrey Kolmogorov in 1933. Let \\(A\\) denote an event within \\(\\Omega\\) (i.e., \\(A \\subset S\\)), either simple or compound. A probability measure on \\(\\Omega\\) is a function \\(P\\) from subsets of \\(\\Omega\\) to \\(\\mathbb{R}^n\\) that satisfies the following: \\(P(A) \\in [0,1]\\); \\(P(\\Omega) = 1\\); and if \\(\\{B_1,\\ldots,B_k\\}\\) is a set of mutually exclusive simple or compound events, then \\(P(\\bigcup_{i=1}^k B_i) = \\sum_{i=1}^k P(B_i)\\), where the symbol \\(\\bigcup\\) refers to the union of the set of events, i.e., the combination of all the events in the set into a single compound event. 1.2.1 Utilizing Set Notation Let’s suppose that we have in our hand one six-sided die, with faces numbered 1 through 6. We roll it once, and observe the value on the uppermost face. The sample space is \\[ \\Omega = \\{1,2,3,4,5,6\\} \\,. \\] Let the event \\(A\\) be all odd-numbered outcomes, and let the event \\(B\\) be all outcomes less than 4. Thus \\(A = \\{1,3,5\\}\\) and \\(B = \\{1,2,3\\}\\). What is \\(\\bar{A}\\), the complement of \\(A\\)? It is the set of all outcomes not in \\(A\\), i.e., the set of all even-numbered faces: \\(\\bar{A} = \\{2,4,6\\}\\). What is \\(A \\cup B\\), the union of \\(A\\) and \\(B\\)? It is the set of all outcomes observed in either \\(A\\) or \\(B\\), without double counting: \\(A \\cup B = \\{1,2,3,5\\}\\) (and not \\(A \\cup B = \\{1,1,2,3,3,5\\}\\)…it is meaningless to write out the same experimental outcome twice). What is \\(A \\cap B\\), the intersection of \\(A\\) and \\(B\\)? It is the set of all outcomes observed in both \\(A\\) and \\(B\\), again without double counting: \\(A \\cap B = \\{1,3\\}\\). We note that we can combine unions, intersections, and complements in, e.g., the distributive law, \\[ A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C) \\,, \\] the associative law, \\[ A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) \\,, \\] and De Morgan’s laws, \\[ \\overline{A \\cup B} = \\bar{A} \\cap \\bar{B} ~~\\mbox{and}~~ \\overline{A \\cap B} = \\bar{A} \\cup \\bar{B} \\,. \\] 1.2.2 Working With Contingency Tables Let’s assume that we are given the following information about two events \\(A\\) and \\(B\\): \\(A\\) \\(\\bar{A}\\) \\(B\\) 0.45 0.12 \\(\\bar{B}\\) 0.21 0.22 This is dubbed a contingency table (or, more specifically here, a two-by-two contigency table). The numbers in each cell represent probabilities; for instance, \\(P(A \\cap B) = 0.45\\). A contingency table is appropriate to work with if the probabilities associated with each event do not change from experiment to experiment. What is \\(P(A)\\)? We can determine this by summing down the \\(A\\) column: \\(P(A) = P[A \\cap \\Omega] = P[A \\cap (B \\cup \\bar{B})] = P[(A \\cap B) \\cup (A \\cap \\bar{B})]\\); since \\(A \\cap B\\) and \\(A \\cap \\bar{B}\\) are disjoint, we can view the \\(\\cup\\) as addition, and so \\(P(A) = P(A \\cap B) + P(A \\cap \\bar{B}) = 0.45 + 0.21 = 0.66\\). What is \\(P(A \\cup B)\\)? Utilizing De Morgan’s laws, this would be \\(1 - P(\\overline{A \\cup B}) = 1 - P(\\bar{A} \\cap \\bar{B}) = 1 - 0.22 = 0.78\\). What is the probability of observing \\(A\\) or \\(B\\), but not both? This would be \\(P(A \\cup B) - P(A \\cap B)\\), which is \\(0.78 - 0.45 = 0.33\\). It is well worth taking the time to see how one could derive each of these answers through visual inspection of the table. For instance, \\(P(A \\cup B)\\) is 1 minus the value in the cell at lower right, which does not lie in the row for \\(B\\) or the column for \\(A\\). 1.3 Conditional Probability and the Independence of Events Intuitively, we can picture a sample space \\(\\Omega\\) and two of its constituent events as looking something like what we show in Figure 1.3. Figure 1.3: A sample space with non-disjoint events \\(A\\) and \\(B\\). We can imagine that the geometric areas of each region represent probability, with \\(P(\\Omega) = 1\\) (given the second Kolmogorov axiom) and \\(P(A \\cap B) &gt; 0\\) being the probability that both \\(A\\) and \\(B\\) occur during an experiment. (Perhaps \\(A\\) is the event of speaking French and \\(B\\) is the event of living is Brussels. The symbol \\(\\cap\\) denotes the intersection or overlap between two sets, whereas the analogous symbol \\(\\cup\\) represents the union of two sets.) We can use this intuitive picture to illustrate the concept of conditional probability. A stated event probability, such as \\(P(A)\\), is an unconditional probability: its occurrence does not depend on whether or not other events occur. To denote a conditional probability, we add a vertical bar and place the conditions to the right of it. For instance, \\(P(A \\vert B)\\) denotes the probability that the event \\(A\\) is observed, given that the event \\(B\\) is observed. (Note that there is no implied causality: it is not necessarily the case that \\(B\\) occurring is “causing” changes to the probability that \\(A\\) will occur.) To illustrate why \\(P(A)\\) may not equal \\(P(A \\vert B)\\), we first point out that \\(P(A) = P(A \\vert \\Omega)\\), which we may think of as “the probability of observing the event \\(A\\) if we observe the event \\(\\Omega\\),” which is the ratio of geometric areas of \\(A \\cap \\Omega\\) and \\(\\Omega\\): \\[ P(A) = P(A \\vert \\Omega) = \\frac{P(A \\cap \\Omega)}{P(\\Omega)} \\,. \\] When we condition on the event \\(B\\), we are reducing the set of possible outcomes from the full sample space \\(\\Omega\\) to \\(B\\), i.e., we are replacing \\(\\Omega\\) in the expression above with \\(B\\): \\[ P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)} \\,, \\] In the context of our intuitive picture, we are changing the one shown above to the one we show in Figure 1.4. Figure 1.4: The new sample space that arises when we condition on the event \\(B\\). \\(B\\) thus defines a new “sample space.” Two events \\(A\\) and \\(B\\) are independent if the probability of observing one does not depend on the probability of observing the other. The intuitive picture many have of independence is the one shown in Figure 1.5. Figure 1.5: To many, \\(A\\) and \\(B\\) appear to be independent events…but they are simply disjoint. The events \\(A\\) and \\(B\\) do not overlap…hence they are independent events, right? No: they are simply disjoint events. Also, with reflection, we realize that if, e.g., the event \\(A\\) is observed in a given experiment, then we know that \\(B\\) cannot be observed. So these events are very much dependent! Figure 1.6 shows how we can actually represent \\(A\\) and \\(B\\) as independent events. In this figure, the ratio of the geometric area associated with the event \\(A\\) to the geometric area of \\(\\Omega\\) is equal to the ratio of the areas of \\(A \\cap B\\) and \\(B\\). Thus we can write that \\[ P(A) = P(A \\vert \\Omega) = \\frac{P(A \\cap \\Omega)}{P(\\Omega)} = \\frac{P(A \\cap B)}{P(B)} = P(A \\vert B) \\,. \\] The probability of observing the event \\(A\\) is unchanged if the event \\(B\\) occurs: \\(A\\) and \\(B\\) are independent events. Figure 1.6: \\(A\\) and \\(B\\) are independent events. 1.3.1 Visualizing Conditional Probabilities: Contingency Tables Let’s recall the two-by-two contingency table we defined in the previous section, but with some additional information added: \\(A\\) \\(\\bar{A}\\) \\(B\\) 0.45 0.12 0.57 \\(\\bar{B}\\) 0.21 0.22 0.43 0.67 0.33 The numbers in the so-called “margins” are the row and column sums, so, for instance, \\(P(A) = 0.67\\). This information is useful to have when computing conditional probabilities. In analogy with what was stated above about imposing conditions and what that does to the sample space, here we can say that imposing a condition will restrict us to a given row or a given column. For instance, what is \\(P(\\bar{A} \\vert B)\\)? The condition restricts us to the top row, and within that row, the probability of observing the event \\(\\bar{A}\\) is 0.12/0.57 = 0.21. So \\(P(\\bar{A} \\vert B) = P(\\bar{A} \\cap B) / P(B) = 0.21\\). Now, are \\(A\\) and \\(B\\) independent events? The easy way to visually infer this given a two-by-two table is to see if the rows (or columns) are multiples of each other…meaning, here, is there a number \\(a\\) such that \\(0.45 = 0.21 a\\) and \\(0.12 = 0.22 a\\)? The answer here is no…so the events \\(A\\) and \\(B\\) are dependent events. (The conventional, yet longer way to determine independence is to see if, e.g., \\(P(A \\vert B) = P(A)\\); if so, \\(A\\) and \\(B\\) are independent.) 1.3.2 Conditional Independence If \\(A\\) and \\(B\\) are independent events, is it automatically the case that the events \\(A \\vert C\\) and \\(B \\vert C\\) are also independent events? Recall the figure above that shows how independent events appear in a Venn diagram. Recall also that if we impose a condition \\(C\\), we effectively change the sample space from \\(\\Omega\\) to \\(C\\). Imagine \\(C\\) as an arbitrarily shaped region superimposed on the last figure, so that now we have a situation like the one in Figure 1.7. Figure 1.7: \\(A\\) and \\(B\\) are not necessarily independent events, given \\(C\\). The events \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if \\(P(C) &gt; 0\\) and \\(P(A \\cap B \\vert C) = P(A \\vert C) P(B \\vert C)\\). As we can see in Figure 1.7, \\(C\\) can be made to overlap \\(B\\), \\(A\\), and \\(A \\cap B\\) in any number of ways such that \\(P(A \\cap B \\vert C) \\neq P(A \\vert C) P(B \\vert C)\\)…so it is not the case that if \\(A\\) and \\(B\\) are independent, \\(A \\vert C\\) and \\(B \\vert C\\) are always independent. (If \\(C\\) had a rectangular shape with a horizontal base and top and vertical sides, conditional independence would hold. Think through why this would be true…) 1.4 Further Laws of Probability Now that we have learned about the concepts of conditional probabilities and independence, we can write down some useful laws that one can use to solve an array of probability-based problems. Multiplicative Law. This follows simply from rearranging the definition of conditional probability: \\[ P(A \\cap B) = P(A) P(B \\vert A) = P(B) P(A \\vert B) \\] We can generalize this law given an arbitrary number of events \\(k\\): \\[\\begin{align*} P(A_1 \\cap A_2 \\cap \\cdots \\cap A_k) &amp;= P(A_1 \\vert A_2 \\cap \\cdots \\cap A_k) P(A_2 \\cap \\cdots \\cap A_k) = \\cdots \\\\ &amp;= P(A_k) \\prod_{i=1}^{k-1} P(A_i \\vert A_{i+1} \\cap \\cdots \\cap A_k) \\,, \\end{align*}\\] where \\(\\prod\\) is the product symbol, the multiplicative analogue to the summation symbol \\(\\sum\\). Additive Law. The probability of the union of two events \\(A\\) and \\(B\\) is \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\,. \\] If the events \\(A\\) and \\(B\\) are not disjoint, then if we add their probabilities, we count the probability of \\(A \\cap B\\) twice…hence the subtracted term. Law of Total Probability (LoTP). Assume that we partition the sample space \\(\\Omega\\) into \\(k\\) disjoint (simple or compound) events \\(\\{B_1,\\ldots,B_k\\}\\), all of which have non-zero probability of occurring. Then, given any event \\(A\\), we can write \\[ P(A) = \\sum_{i=1}^k P(A \\vert B_i) P(B_i) \\,. \\] Bayes’ Rule. Continue to assume that the sample space is partitioned into the events \\(\\{B_1,\\ldots,B_k\\}\\). The conditional probability of each of these events, given that \\(A\\) occurs, is \\[ P(B_i \\vert A) = \\frac{P(A \\vert B_i) P(B_i)}{\\sum_{j=1}^k P(A \\vert B_j)P(B_j)} = \\frac{P(A \\vert B_i)P(B_i)}{P(A)} \\,. \\] 1.4.1 The Additive Law for Independent Events Let’s assume that for a given experiment, we can define the independent events \\(A\\) and \\(B\\), with \\(P(A) = 0.6\\) and \\(P(B) = 0.4\\). What is \\(P(A \\cup B)\\)? In general, when solving probability problems, we look at all the rules and relationships at our disposal and see which one (or more!) contains the probabilities we know and the one we don’t know, and we use that rule or relationship to derive the solution. Here, there is nothing that directly relates \\(A\\) and \\(B\\) to \\(A \\cup B\\)…the events may overlap when represented on a Venn diagram, and we don’t know by how much. Except…we are given the word “independent.” That allows us to say that \\(P(A \\cap B) = P(A)P(B)\\)…and now we know that the additive law is in play: \\[\\begin{align*} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;= P(A) + P(B) - P(A \\vert B)P(B) \\\\ &amp;= P(A) + P(B) - P(A)P(B) = 0.6 + 0.4 - 0.6 \\cdot 0.4 = 0.76 \\,. \\end{align*}\\] Is this the only way to solve the problem? No…we know from De Morgan’s laws that \\(\\overline{A \\cup B} = \\bar{A} \\cap \\bar{B}\\), and thus \\[\\begin{align*} P(A \\cup B) &amp;= 1 - P(\\overline{A \\cup B}) = 1 - P(\\bar{A} \\cap \\bar{B}) = 1 - P(\\bar{A} \\vert \\bar{B})P(\\bar{B}) \\\\ &amp;= 1 - P(\\bar{A})P(\\bar{B}) = 1 - (1-0.6)(1-0.4) = 0.76 \\,. \\end{align*}\\] There is no “right” way to solve a probability problem…just correct ones. 1.4.2 The Monty Hall Problem Let’s Make a Deal is a game show that has appeared on television at various times since 1963. During one part of the show, contestants are brought on stage and presented with three closed doors; behind one is an expensize prize (say, a car or an around-the-world cruise), and behind the other two are inexpensive prizes (like a year’s supply of Turtle Wax). The contestant is asked to pick a door (say, Door #1), at which point the show’s host will open another door (say, Door #3) and show the inexpensive prize behind that door (thereby taking that door out of play). The contestant is then asked if they want to stick with the door they’ve chosen (here, Door #1), or switch their choice to the other unopened door (here, Door #2). What should we advise the constestant to do? The original, and most famous, host of Let’s Make a Deal was Monty Hall. Hence: the Monty Hall Problem. (Note that the problem is often stated such that there is a car being behind one door and goats behind the other two. The author is old enough to have seen the show in its heyday and he recalls seeing no goats. Or maybe they made no impression at the time…) Assume, without loss of generality, that Door #1 is chosen. Then, let \\(O_i\\) = “Monty Hall opens Door #\\(i\\)” \\(C_i\\) = “The car is behind Door #\\(i\\)” and assume that \\(P(C_i) = 1/3\\) for all \\(i\\). (The car could have been placed behind any door before the show was filmed.) The sample space of experimental outcomes is \\[ \\Omega = \\{ O_2 \\cap C_1 , O_2 \\cap C_3 , O_3 \\cap C_1 , O_3 \\cap C_2\\} \\,. \\] Why not \\(O_2 \\cap C_2\\) and \\(O_3 \\cap C_3\\)? Monty is not stupid: he won’t open the door the car is behind. (He knows where it is!) Let’s assume, again without any loss of generality, that Monty opens Door #3. The probability we want to compute is \\(P(C_2 \\vert O_3)\\): what is the probability that the car is actually behind Door #2? (Note that this is \\(1 - P(C_1 \\vert O_3)\\)…again, \\(P(C_3 \\vert O_3) = 0\\), as Monty is not stupid.) Is this probability 1/2? We utilize Bayes’ rule and the LoTP to write \\[ P(C_2 \\vert O_3) = \\frac{P(O_3 \\vert C_2) P(C_2)}{P(O_3)} = \\frac{P(O_3 \\vert C_2) P(C_2)}{P(O_3 \\vert C_2) P(C_2) + P(O_3 \\vert C_1) P(C_1)} = \\frac{P(O_3 \\vert C_2)}{P(O_3 \\vert C_2) + P(O_3 \\vert C_1)}\\,. \\] What do we know? \\(P(O_3 \\vert C_2) = 1\\): if the car is behind Door #2, Monty has to open Door #3 \\(P(O_3 \\vert C_1) = 1/2\\): Monty can open either Door #2 or #3 if the car is behind Door #1 Hence \\[ P(C_2 \\vert O_3) = \\frac{1}{1 + 1/2} = \\frac{2}{3} \\,. \\] We should advise the contestant to open Door #2! Confused? Think about the solution this way: the contestant has a one-third chance of correctly picking the door the car is behind, and a two-thirds chance of being wrong. Opening one of the other doors (while knowing there is no car behind it) doesn’t change these conditions at all: the contestant still has a one-third chance of having initially picked the correct door. Thus the contestant should change their pick to the other unopened door. 1.4.3 Visualizing Conditional Probabilities: Tree Diagrams In the previous sections, we show how one can use contingency tables to aid the visualization of probabilities (and to solve for probabilities of simple and/or compound events). Here we show another, somewhat more general probability visualizer: the tree diagram. Why “somewhat more general”? First, a tree in a tree diagram can have arbitrary depth: if we have events \\(A\\), \\(B\\), and \\(C\\), the table would be three-dimensional, with the axes representing the experimental outcome in terms of \\(A\\) and \\(\\bar{A}\\), \\(B\\) and \\(\\bar{B}\\), and \\(C\\) and \\(\\bar{C}\\). A table is not an optimal means to represent probabilities. And second, a tree is arguably a more natural means to represent probabilities when an experiment represents sequential outcomes, particularly when we sample without replacement. Let’s elaborate on that second point. Let’s say we have a drawer with five socks, three of which are red and two of which are blue. We plan to draw three socks in succession from the drawer without placing the socks back into the drawer, but we will stop early if we draw two socks of the same color on the first two draws. What is the probability that our final sample contains two blue socks? We can write out the following: if \\(B_i\\) and \\(R_i\\) are the probabilities of drawing a blue and red sock from the drawer when taking out the \\(i^{\\rm th}\\) sock, then \\(P(B_1) = 2/5\\) and \\(P(R_1) = 3/5\\)…and \\(P(B_2 \\vert B_1) = 1/4\\) because there is one less blue sock in the drawer, and… Actually, this gets tiring quickly. Let’s use a tree diagram instead. Figure 1.8: An example of visualizing probabilities using a decision tree. In Figure 1.8, we show the tree diagram for this problem. We note some aspects of this diagram: the tree can be truncated along some branches (here, that’s because we stop removing socks from the drawer if we remove two of the same color in the first two draws); at any branching point, the (conditional) probabilities of going down each branch sum to one; and the probability of ending up at a particular leaf (where the leaves collectively represent the simple events of the experiment) is the product of all the branch probabilities leading to that leaf. So, now, what is the probability of drawing two blue socks in this experiment? To find that, we determine which leaves are associated with drawing two blue socks; from the top, that would be leaves 1, 2, and 4, with probabilities 1/10, 1/10, and 1/10. Because simple events are disjoint by definition, the probability of the compound event is simply the sum of the probabilities of the simple events, which here is 3/10. In any given replication of this experiment, we have a 30% chance of ending up with two blue socks. 1.5 Random Variables Let’s say that we perform an experiment in which we flip a fair coin three times. Let \\(H\\) denote observing heads, and \\(T\\) tails. The sample space of outcomes \\(\\Omega\\) is \\[ \\{ HHH,HHT,HTH,THH,HTT,THT,TTH,TTT\\} \\] and each outcome is observed with probability 1/8. What is the probability of observing exactly one tail? We can determine this by laboriously generating a table of probabilities, like so: \\[\\begin{align*} P(\\mbox{``no tails&#39;&#39;}) &amp;= P(HHH) = 1/8 \\\\ P(\\mbox{``one tail&#39;&#39;}) &amp;= P(HHT \\cup HTH \\cup THH) = 3/8 \\\\ P(\\mbox{``two tails&#39;&#39;}) &amp;= P(HTT \\cup THT \\cup TTH) = 3/8 \\\\ P(\\mbox{``three tails&#39;&#39;}) &amp;= P(TTT) = 1/8 \\,. \\end{align*}\\] One can easily imagine how, if we were to flip a coin 50 times, or 500 times, the generation of tables would be onerous. A better way to portray the information in a sample space is to use a random variable. In probability theory, a random variable \\(X\\) is a measurable function mapping from a set of outcomes (here, \\(\\Omega\\)) to a measurable space (here, \\(\\mathbb{R}^n\\), where \\(\\mathbb{R}^1 = \\mathbb{R}\\) is the real-number line). (See Figure 1.9.) While \\(X\\) is a function, it is natural in an undergraduate context to think of it as a variable whose value is an experimental outcome. For instance, if we define \\(X\\) as being “the number of tails observed in three flips of a fair coin,” then \\(P(X=1) = 3/8\\). (Below, we will complete our transition away from laboriously built probability tables by introducing mathematical functions\\(-\\)probability mass functions or probability density functions associated with distributions\\(-\\)that allow us to compute probabilities more generally, as a function of an arbitrary observed value \\(X=x\\) or a range of observed values \\(X \\in [a,b]\\).) Figure 1.9: A random variable is a function that maps events in \\(\\Omega\\) to the real-number line \\(\\mathbb{R}\\). There are a few initial things to note about random variables. First, they are conventionally denoted with capital Latin letters (e.g., \\(X\\), \\(Y\\), \\(Z\\)). Second, note the words “if we define” above. There is no unique random variable associated with a sample space. In addition to \\(X\\), we could just as easily have defined \\(Y\\) as the number of heads observed, or \\(Z\\) as having value 0 if at least one head and at least one tail are observed, and 1 otherwise, etc. Third, and most important, is that random variables come in two types, discrete and continuous: A discrete random variable \\(X\\) maps the sample space \\(\\Omega\\) to countably finite (e.g., \\(\\{0,1\\}\\)) or infinite (e.g., \\(\\{0,1,\\ldots,\\}\\)) outcomes. A continuous random variable \\(X\\) maps the sample space \\(\\Omega\\) to an outcome that is uncountably infinite (e.g., \\([0,1]\\) or \\([0,\\infty)\\)). 1.6 Probability Distributions A probability distribution is a mapping \\(P: \\Omega \\rightarrow \\mathbb{R}^n\\) that describes how probabilities are distributed across the values of a random variable. (A random variable simply maps events in a sample space to a measurable space like the real-number line, without regard to the probability of the event. A distribution adds this additional layer of information.) There are different ways to mathematically define a distribution; here, we concentrate upon the probability mass function (or pmf): if \\(X\\) is a discrete random variable, this represents the probability that \\(X\\) takes on a particular value \\(x\\), i.e., \\(p_X(x) = P(X = x)\\); or the probability density function (or pdf): if \\(X\\) is a continuous random variable, this represents the probability density (think of this as the “probability per unit interval”) at the value \\(x\\), i.e., \\(f_X(x)\\). To be clear, we can represent a given distribution with a pmf or a pdf, but not both simultaneously; the choice is dictated by whether \\(X\\) is discretely or continuously valued. (It is possible to mix probability masses and densities into a single distribution, however. See the example below.) Later, we introduce two alternatives to pmfs/pdfs: the cumulative distribution function (cdf), and the moment-generating function (mgf). Probability mass and density functions have two fundamental constraints: (a) they are non-negative; and (b) they sum or integrate to 1: pmf pdf \\(p_X(x) \\in [0,1]\\) \\(f_X(x) \\in [0,\\infty)\\) \\(\\sum_x p_X(x) = 1\\) \\(\\int_x f_X(x) dx = 1\\) Before continuing on to discussing properties of distributions, we reiterate the point that one cannot interpret a pdf \\(f_X(x)\\) as the probability of sampling the value \\(x\\)! It is, again, a probability density function and not a probability itself; to determine a probability, we utilize integration: \\[ P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx \\,. \\] To drive home the point that a pdf does not itself represent probability, we note that for any value \\(a\\), \\[ P(X = a) = \\int_a^a f_X(x) dx = 0 \\,. \\] 1.6.1 A Simple Probability Density Function Let’s assume that we have defined the following pdf: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} 2x &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We visualize this pdf in Figure 1.10. Figure 1.10: The probability density function \\(f_X(x) = 2x\\), for \\(0 \\leq x \\leq 1\\). This pdf helps to illustrate many of the points made above. Note that it is (a) non-negative and although its maximum value is \\(&gt; 1\\), (b) it integrates &gt; to 1. (We need not actually integrate here, as geometry is sufficient: the area under the curve is 1/2 \\(\\times\\) 1 \\(\\times\\) 2 = 1.) How would one interpret this pdf? Where its value is larger, we are more likely to sample data. Full stop. What is the probability of sampling a datum between 0 and 1/2? Again, we can use geometry and see that the area under the curve is 1/2 \\(\\times\\) 1/2 \\(\\times\\) 1 = 1/4. (Which means the probability of sampling a datum between 1/2 and 1 must be \\(1 - 1/4 = 3/4\\).) Let’s extend this example a bit by adding a condition. For instance, what is the probability of sampling a datum between 1/4 and 1/2, given that we sample a datum between 0 and 3/4? In analogy with how we worked with conditional probabilities above, we can write that \\[ P(1/4 \\leq X \\leq 1/2 \\, \\vert \\, 0 \\leq X \\leq 3/4) = \\frac{P(1/4 \\leq X \\leq 1/2 \\cap 0 \\leq X \\leq 3/4)}{P(0 \\leq X \\leq 3/4)} = \\frac{P(1/4 \\leq X \\leq 1/2)}{P(0 \\leq X \\leq 3/4)} \\,. \\] (How does this differ from computing the unconditional probability \\(P(1/4 \\leq X \\leq 1/2)\\)? Technically, it does not…we could write out a similar expression to the one above. But we note that the denominator would be \\(P(0 \\leq X \\leq 1) = 1\\) and thus it would “go away.”) Using geometrical arguments, we should be able to convince ourselves that the answer we seek is 1/3. One last point we will make here is that for a continuous distribution, it is meaningless to compute \\(P(X = a)\\). For instance: \\[ P\\left(X = \\frac{1}{2}\\right) = \\int_{1/2}^{1/2} 2 x dx = \\left. x^2 \\right|_{1/2}^{1/2} = \\frac{1}{4} - \\frac{1}{4} = 0 \\,. \\] What are we to make of this? Recall that a pdf is a probability density function, and that one can think of it as having units of probability per unit interval…so one has to integrate the pdf over an interval of length greater than zero to derive a non-zero probability value. 1.6.2 Shape Parameters and Families of Distributions In the previous example, the stated pdf was the stated pdf: there was no means by which to change its shape. We can generalize it by utilizing a shape parameter: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] where in the previous example, \\(\\theta = 2\\). It is conventional to denote a population parameter or a set of such parameters with the Greek letter \\(\\theta\\) (theta, pronounced “thay-tah”). Here, \\(\\theta\\) represents a single, constant parameter whose value is \\(&gt; 0\\). (If \\(\\theta\\) were negative, for instance, \\(f_X(x \\vert \\theta)\\) would be \\(&lt; 0\\), which is not allowed!) \\(f_X(x \\vert \\theta)\\), with \\(\\theta \\in \\Theta = (0,\\infty)\\), is perhaps confusingly dubbed a family of distributions. (One might think that a family would refer to a set of different mathematical forms for pdfs, like \\(\\theta x^{\\theta-1}\\) and \\(e^{-x/\\theta}/\\theta\\), etc., but it actually refers to the fact that \\(\\theta\\) can take on more than one value, yielding a family of shapes as illustrated in Figure 1.11.) Figure 1.11: Examples of the family of pdfs \\(f_X(x \\vert \\theta) = \\theta x^{\\theta-1}\\) for \\(0 \\leq x \\leq 1\\), with parameters \\(\\theta =\\) 1/2 (solid red line), 1 (dashed green line), and 2 (dotted blue line). 1.6.3 A Simple Probability Mass Function Let’s play a game: we throw a dart at a board that has ten numbers on it, 1 through 10. Assume that we are guaranteed to hit the board, and that the regions associated with each number have the exact same size. If we hit an even number, we get 0 points, while if we hit an odd number, we get 2 points. Furthermore, if we hit a prime number, we get a bonus of 1 point. What is the probability mass function for the number of points we will score given a single throw of the dart? If we hit the 4, 6, 8, or 10, we get 0 points. (2 is prime, so we’d get a bonus of 1 point by hitting that.) If we hit the 9, we get 2 points, and if we hit the 1, 3, 5, or 7, we get 3 points. Hence the probability mass function is \\(x\\) \\(p_X(x)\\) 0 4/10 1 1/10 2 1/10 3 4/10 We see that this pmf is (a) non-negative and (b) has values \\(p_X(x)\\) that lie between 0 and 1. Because we are dealing with masses and not densities, probability calculations involve summations (while taking care to note whether one or both limits of summation lie at a mass, and if so, whether or not the inequality is, e.g., \\(&gt;\\) or \\(\\geq\\)). For instance, what is the probability of achieving a score greater than 1 point? \\(P(X &gt; 1) = p_X(2) + p_X(3) = 1/2\\). What about a score of 3 points, given a score greater than 0 points? \\[ P(X = 3 \\vert X &gt; 0) = \\frac{P(X = 3 \\cap X &gt; 0)}{P(X &gt; 0)} = \\frac{P(X = 3)}{P(X &gt; 0)} = \\frac{p_X(3)}{p_X(1)+p_X(2)+p_X(3)} = \\frac{4}{1+1+4} = \\frac{2}{3} \\,. \\] 1.6.4 A More Complex Example Involving Both Masses and Densities There is no reason why masses and densities cannot be combined into a single probability distribution. For instance, perhaps we have the following: \\[ h_X(x) = \\left\\{ \\begin{array}{cc} 1/2 &amp; x \\in [0,1] \\\\ 1/2 &amp; x = 2 \\end{array} \\right. \\,. \\] There is nothing special about this function; the mathematics of probability calculations is just a tad more complicated than before. For instance, what is the probability of sampling a value greater than 3/4? \\[ P(X &gt; 3/4) = \\int_{3/4}^1 h_X(x) dx + h_X(2) = \\frac{1}{2} \\left. x \\right|_{3/4}^1 + \\frac{1}{2} = \\frac{1}{8} + \\frac{1}{2} = \\frac{5}{8} \\,. \\] Integrate over the domain(s) where densities are defined and sum over the domain(s) where masses are defined. Done! 1.7 Characterizing Probability Distributions A probability distribution represents the rates of occurrence of different experimental outcomes. Can we determine an “average” outcome? In other words, can we determine what value to expect when we next run the experiment? The answer is yes: this is the expected value of a random variable (or expectation) and it is the weighted average of all possible experimental outcomes: \\[\\begin{align*} E[X] &amp;= \\frac{\\sum_x x p_X(x)}{\\sum_x p_X(x)} = \\sum_x x p_X(x) ~~ \\mbox{(discrete r.v.)} \\\\ &amp;= \\frac{\\int_x x f_X(x) dx}{\\int_x f_X(x) dx} = \\int_x x f_X(x) dx ~~ \\mbox{(continuous r.v.)} \\,. \\end{align*}\\] In each case, the denominator disappears because it equals 1, by definition. Note that Greek letter \\(\\mu\\) (mu, pronounced “myoo”), which conventionally denotes the mean value of a pdf or pmf, is also sometimes used interchangeably with \\(E[X]\\). See Figure 1.12. It is important here to note the following: The input to the expected value operator is (usually!) a random variable, so that input is capitalized. In other words, we always write \\(E[X]\\) and not \\(E[x]\\). (\\(x\\) is just a coordinate on the real-number line…its expected value is simply \\(x\\) itself. See “Expected Value Tricks” in the examples below.) The expected value is a constant; it is not random! For any given pmf or pdf, the average value of a sampled datum does not change from experiment to experiment…there is no randomness. Now, because the expected value is simply a weighted average, we can write down a more general expression for it: \\[\\begin{align*} E[g(X)] &amp;= \\frac{\\sum_x g(x)p_X(x)}{\\sum_x p_X(x)} = \\sum_x g(x) p_X(x) ~~ \\mbox{(discrete r.v.)} \\\\ &amp;= \\frac{\\int_x g(x)f_X(x) dx}{\\int_x f_X(x) dx} = \\int_x g(x) f_X(x) dx ~~ \\mbox{(continuous r.v.)} \\,. \\end{align*}\\] This has been dubbed the “Law of the Unconscious Statistician” (e.g., Ross 1988, as noted by Casella &amp; Berger 2002) due to the fact that we all think of it a definition…and not the result of a theorem. A probability distribution may have an extended domain (e.g., \\([0,\\infty)\\)) but often the probability mass or density is concentrated in a relatively small interval. A metric that represents the square of the “width” of that interval is the variance, which is defined as \\[ V[X] = \\sigma^2 = E[(X-\\mu)^2] = E[X^2] - (E[X])^2 \\,. \\] The “width” itself\\(-\\)the square root of the variance\\(-\\)is dubbed the standard deviation and is denoted with the Greek letter \\(\\sigma\\) (“sigma,” pronounced “SIG-muh”). Note that because the variance is the expected value of a squared quantity, it is always non-negative. (And like the expected value, it is a constant.) See Figure 1.12. &lt;img src=“_main_files/figure-html/exvx-1.png” alt=“Examples of a probability mass function (left) and a probability density function (right), with the expected values \\(E[X]\\) indicated by the vertical lines and the distribution”widths” (here, \\(E[X]-\\sqrt{V[X]}\\) to \\(E[X]+\\sqrt{V[X]}\\)) indicated by the horizontal lines.” width=“45%” /&gt;&lt;img src=“_main_files/figure-html/exvx-2.png” alt=“Examples of a probability mass function (left) and a probability density function (right), with the expected values \\(E[X]\\) indicated by the vertical lines and the distribution”widths” (here, \\(E[X]-\\sqrt{V[X]}\\) to \\(E[X]+\\sqrt{V[X]}\\)) indicated by the horizontal lines.” width=“45%” /&gt; Figure 1.12: Examples of a probability mass function (left) and a probability density function (right), with the expected values \\(E[X]\\) indicated by the vertical lines and the distribution “widths” (here, \\(E[X]-\\sqrt{V[X]}\\) to \\(E[X]+\\sqrt{V[X]}\\)) indicated by the horizontal lines. Both the expected value and variance are examples of moments of probability distributions. Moments represent elements of a distribution’s location and shape. In the end, moments are “just” expected values computed via the Law of the Unconscious Statistician, ones that are defined around the coordinate origin (\\(E[X^k]\\)), and ones that are defined around the distribution’s mean, \\(\\mu\\) (\\(E[(X-\\mu)^k]\\)). Other metrics used to describe a probability distribution, such as its skewness, are also related to moments. (One definition of skewness is Fisher’s moment coefficient: \\(E[(X-\\mu)^3]/\\sigma^3\\).) 1.7.1 Expected Value Tricks The expected value operator \\(E[X]\\) has the following properties. If we multiply \\(X\\) by a constant \\(a\\), that constant can be moved out of the operator, i.e., \\[ E[aX] = aE[X] \\,. \\] The expected value of a constant is simply that constant, i.e., \\[ E[b] = b \\,. \\] The expected value operator is a linear operator, which means that we can split it at \\(+\\)’s and \\(-\\)’s, with the sign being preserved: \\[ E[aX - b] = E[aX] - E[b] = aE[X] - b \\,. \\] If, for example, we define a random variable \\(Y = 10X - 5\\) and we know that \\(E[X] = 4\\), then we can write that \\(E[Y] = 10E[X] - 5 = 35\\). Note that we have said nothing about \\(E[XY]\\) here. In general, we cannot simplify this expression at all, unless \\(X\\) and \\(Y\\) are independent random variables (a concept we haven’t discussed yet), in which case \\(E[XY] = E[X]E[Y]\\). 1.7.2 Variance Tricks The variance operator \\(V[X]\\) has the following properties. If we multiply \\(X\\) by a constant \\(a\\), that constant can be moved out of the operator, but it is then squared, i.e., \\[ V[aX] = a^2V[X] \\,. \\] The variance of a constant is zero: \\[ V[b] = 0 \\,. \\] The variance operator is a linear operator, which means that we can split it at \\(+\\)’s and \\(-\\)’s, with all signs becoming positive: \\[ V[aX - b] = V[aX] + V[b] = a^2V[X] + 0 = a^2V[X] \\,. \\] If, again, \\(Y = 10X - 5\\), and if \\(V[X] = 2\\), then \\(V[Y] = 100V[X] = 200\\). 1.7.3 The Shortcut Formula for Variance Above, we indicate that \\[ V[X] = E[(X-\\mu)^2] = E[X^2] - (E[X])^2 \\,. \\] This is the so-called shortcut formula for determining the variance of a distribution. We can derive it as follows, making use of the “tricks” we show above: \\[\\begin{align*} V[X] = E[(X-\\mu)^2] &amp;= E[X^2 - 2X\\mu + \\mu^2] ~~\\mbox{(expand)} \\\\ &amp;= E[X^2] - E[2X\\mu] + E[\\mu^2] ~~\\mbox{(split on + and -)} \\\\ &amp;= E[X^2] - 2\\mu E[X] + \\mu^2 ~~\\mbox{(slide constants out)}\\\\ &amp;= E[X^2] - 2(E[X])^2 + (E[X])^2 = E[X^2] - (E[X])^2 \\,, \\end{align*}\\] where in the last line we make use of the fact that \\(E[X] = \\mu\\). Note what this shortcut formula means: it means that to compute a variance, it is sufficient to compute both \\(E[X]\\) and \\(E[X^2]\\) and combine the results. It also means that if we are given any two of the quantities \\(E[X]\\), \\(E[X^2]\\), and \\(V[X]\\), we can immediately derive the third one. 1.7.4 The Expected Value and Variance of a Probability Density Function In the last section, we define the pdf \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] For this pdf, the expected value is \\[ E[X] = \\int_0^1 x f_X(x) dx = \\int_0^1 \\theta x^\\theta dx = \\frac{\\theta}{\\theta+1} \\left. x^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,. \\] As for the variance, we utilize the shortcut formula, which means that we compute \\(E[X^2]\\) first: \\[ E[X^2] = \\int_0^1 x^2 f_X(x) dx = \\int_0^1 \\theta x^{\\theta+1} dx = \\frac{\\theta}{\\theta+2} \\left. x^{\\theta+2} \\right|_0^1 = \\frac{\\theta}{\\theta+2} \\,. \\] Hence the variance is \\[ V[X] = E[X^2] - (E[X])^2 = \\frac{\\theta}{\\theta+2} - \\frac{\\theta^2}{(\\theta+1)^2} = \\frac{\\theta}{(\\theta+2)(\\theta+1)^2} \\,. \\] We see that the value for our new pdf is similar: 0.643. 1.8 Working With R: Probability Distributions In this section, we introduce R as a tool with which to, e.g., visualize and numerically manipulate probability distributions. We start with the concept of the vector: x &lt;- c(&quot;Hello, world!&quot;) (The reader should feel free to open R and type in these lines at the Console prompt.) In this example, we define a vector of character strings which we name x; here, x has length 1: length(x) ## [1] 1 c() is an R function whose arguments (e.g., \"Hello, world!\") are what are to be the constituents of the vector. The arrow is an assignment operator; = is equally valid. We can create a numeric vector as follows: x &lt;- c(1,2,4,8) print(x) ## [1] 1 2 4 8 length(x) ## [1] 4 but when the numbers follow a (long) sequence, it can be easier to utilize seq(): x &lt;- seq(0,pi,by=0.01) # 0, 0.01, 0.02, ..., 3.14 (but not 3.15) # pi and Inf are built-in constants length(x) ## [1] 315 When it comes to probability distributions, what might we want to do first? Let’s suppose that our data are sampled from this pdf: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} c x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] \\(c\\) is a normalization constant, meaning it has some value (to be determined) such that the integral of \\(f_X(x)\\) from 0 to \\(\\pi\\) is 1. Below, we will show how we can determine the value of \\(c\\) using R code. But first, we will assume \\(c = 1\\) and determine if \\(f_X(x)\\) is non-negative (as it should be!): x &lt;- seq(0,pi,by=pi/100) f.x &lt;- x*sin(x) min(f.x) ## [1] 0 Note how we do not have to use a for-loop here, as one of the hallmarks of R is vectorization: if R sees that x is a vector, it will work with the vector directly and thus f.x will itself be a vector with the same length as x (and with the first element of x corresponding to the first element of f.x, etc.). We see that the minimum value is 0. If we want to go further, we can make a simple plot (see Figure 1.13): x &lt;- seq(0,pi,by=pi/100) f.x &lt;- x*sin(x) df &lt;- data.frame(x=x,f.x=f.x) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_hline(yintercept=0,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.13: The function \\(x \\sin x\\). The ggplot() function puts x on the \\(x\\)-axis and f.x on the \\(y\\)-axis. We then connect each point with a line (geom_line()), make the line blue (col=\"blue\"), overlay a horizontal red line at \\(y = 0\\) (geom_hline(), with yintercept=0), and change the default \\(y\\)-axis label to one that includes the subscript “X” (labs()). The next step is to determine the normalization constant. Let’s suppose that we have forgotten integration by parts and thus we are not sure how to integrate \\(f_X(x)\\). We can code numerical integration in R using a combination of a function that evaluates \\(f_X(x)\\) and a call to the built-in function integrate(), which performs numerical integration: f &lt;- function(x) { return(x*sin(x)) } integrate(f,0,pi) # integrate the function f between 0 and pi ## 3.141593 with absolute error &lt; 3.5e-14 We see that the integral is \\(\\pi\\), so to make the pdf valid, we have to set \\(c\\) to \\(1/\\pi\\): \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] Let’s suppose we sample data from this distribution. What is the probability that the next observed datum will have a value between 1 and 2? We can use integrate to figure that out: f &lt;- function(x) { return(x*sin(x)/pi) # we now include the normalization constant } integrate(f,1,2) # integrate the function f between 1 and 2 ## 0.4585007 with absolute error &lt; 5.1e-15 The answer is 0.4585…there is a 45.85% chance that the next datum will have a value between 1 and 2. What is the expected value, \\(E[X]\\), of \\(f_X(x)\\)? f &lt;- function(x) { return(x^2*sin(x)/pi) # add an additional x } integrate(f,0,pi) ## 1.868353 with absolute error &lt; 2.1e-14 The expected value is 1.868. Given the appearance of the pdf, this number makes sense. 1.8.1 Numerical Integration and Conditional Probability Let’s suppose that we would like to numerically evaluate \\[ P(1 \\leq X \\leq 2 \\vert X &gt; 0.5) = \\frac{P(1 \\leq X \\leq 2 \\cap X &gt; 0.5)}{P(X &gt; 0.5} = \\frac{P(1 \\leq X \\leq 2)}{P(X &gt; 0.5} \\,. \\] As we have already defined At first, it would appear that all we have to do is to call integrate() twice f &lt;- function(x) { return(x*sin(x)/pi) } integrate(f,1,2) / integrate(f,0.5,pi) However, this will not work, since integrate() returns a list, not a single numerical value. So we have to figure out where the value of the integral value is stored: names(integrate(f,1,2)) # return the names of each list element ## [1] &quot;value&quot; &quot;abs.error&quot; &quot;subdivisions&quot; &quot;message&quot; &quot;call&quot; What we want is value. To reference the value directly, we use a dollar sign, as shown here: integrate(f,1,2)$value / integrate(f,0.5,pi)$value ## [1] 0.3836833 Done. Our conditional probability is 0.4645. 1.8.2 Numerical Integration and Variance Above, we compute the expected value of \\(f_X(x)\\). For the variance, we adapt the same code to compute \\(E[X^2]\\), then utilize the shortcut formula: f &lt;- function(x) { return(x^2*sin(x)/pi) # same code as above } E.X &lt;- integrate(f,0,pi)$value f &lt;- function(x) { return(x^3*sin(x)/pi) # add one more power of x } V.X &lt;- integrate(f,0,pi)$value - E.X^2 V.X ## [1] 0.3788611 sqrt(V.X) ## [1] 0.6155169 The variance is 0.379 and the standard deviation is 0.616. We interpret these numbers as saying that the majority of the observed data will lie between \\(1.868 - 0.616 = 1.252\\) and \\(1.868 + 0.616 = 2.484\\). If we recall introductory statistics, the proportion of values within one standard deviation of the mean for a normal distribution (i.e., a bell curve) is 0.683…but that value changes from distribution to distribution. What is the value here? f &lt;- function(x) { return(x*sin(x)/pi) # back to the original pdf } integrate(f,E.X-sqrt(V.X),E.X+sqrt(V.X))$value ## [1] 0.642609 We see that the value for our new pdf is similar: 0.643. 1.9 Cumulative Distribution Functions A cumulative distribution function (a cdf) is another means by which to mathematically express a probability distribution, which is to say, if we have a cdf, we can derive the associated pmf/pdf and vice-versa. A cdf is, in the discrete case, a sum of probability masses that lie to the left of a chosen coordinate \\(x\\) on the real-number line\\(-\\) \\[ F_X(x) = \\sum_{y \\leq x} p_Y(y) \\] \\(-\\)while in the continuous case it is an integral of the probability density that lies to the left of \\(x\\)\\(-\\) \\[ F_X(x) = \\int_{y \\leq x} f_Y(y) dy \\,. \\] In both cases, we utilize a dummy variable for the pmf/pdf itself because \\(x\\) is the upper limit of summation/integration. See Figure 1.14, which illustrates how a cdf “collects” all the probability masses or density “to the left” of a given value of \\(x\\). Given this figure, it should be clear that \\(F_X(-\\infty) = 0\\) (there is nothing to collect “to the left” of \\(-\\infty\\)) and \\(F_X(\\infty) = 1\\) (since, by the time we reach \\(x = \\infty\\), all masses or density have been collected). Another thing to keep in mind is that even if a random variable is discrete, its associated cdf \\(F_X(x)\\) is continuously valued, because it is defined at all values of \\(x\\) (although it is technically not “mathematically continuous” due to the steps that \\(F_X(x)\\) takes at each value of \\(x\\) where there is a probability mass). Figure 1.14: Illustration of the relationship between a probability mass function (left) and a probability density function (right) and its associated cdf (evaluated here at \\(x = 2.5\\)). For the pmf, the cdf is the sum of the probability masses to the left of \\(x = 2.5\\) (the masses marked in green), while for the pdf, the cdf is the integral over \\(x \\in [0,2.5]\\) (the area under curve shown in green). Figure 1.15: Examples of the cdfs \\(F_X(x)\\) for the probability mass function (left) and the probability density function (right) shown in Figure 1.14. A cdf is useful to have when our goal is to compute the probability of that the value of a sampled random variable lies between \\(x = a\\) and \\(x = b\\). For the case of a continuous random variable, \\[ P(a &lt; X &lt; b) = F_X(b) - F_X(a) \\,. \\] As we can see, if we have the cdf, we do not need to perform integration to compute the probability…we just plug in coordinate values. (Note that the form of the inequality, i.e., whether we have \\(&lt;\\) or \\(\\leq\\), does not matter.) However, when we are dealing with a discrete random variable, we need to tread far more carefully, because the form of the inequality can matter. Let’s suppose we have a pmf with masses given at \\(x = \\{0,1\\}\\). Then, e.g., \\[\\begin{align*} P(0 \\leq X \\leq 1) &amp;= \\sum_{x \\in [0,1]} p_X(x) = p_X(0) + p_X(1) = F_X(1) \\\\ P(0 &lt; X \\leq 1) &amp;= \\sum_{x \\in (0,1]} p_X(x) = p_X(1) = F_X(1) - F_X(0) \\\\ P(0 &lt; X &lt; 1) &amp;= \\sum_{x \\in (0,1)} p_X(x) = 0 \\,. \\end{align*}\\] Figure 1.16: An illustration of the relationship between a cdf and probability. The probability \\(P(1 &lt; X &lt; 3)\\) is given by the distance between the two red lines (i.e., \\(F_X(3)-F_X(1)\\)). We will make two final points here about cdfs. First, as indicated above, given a cdf, we can find the associated pmf/pdf. If a pmf has non-zero masses at values \\(x - \\Delta x\\) and \\(x\\), and none in between, then \\[ p_X(x) = F_X(x) - F_X(x-\\Delta x) \\,, \\] while in the continuous case, \\[ f_X(x) = \\frac{d}{dx}F_X(x) \\,, \\] assuming \\(F_X(x)\\) is differentiable at \\(x\\). Second, we can define an inverse cumulative distribution function, or inverse cdf. The inverse cdf takes as input the total probability collected to the left of \\(x\\) (e.g., the green region shown in the right panel of Figure 1.14) and returns the associated value of \\(x\\). In other words, if \\(q = F_X(x)\\), then \\(x = F_X^{-1}(q)\\). One issue that arises with the inverse cdf is that if \\(F_X(x)\\) is not strictly monotonically increasing (i.e., if for some range of values, \\(\\frac{d}{dx}F_X(x) = 0\\)) then there is no unique inverse. For instance, see the left panel of Figure 1.15: if we input \\(F_X(x) = 0.35\\), then \\(x \\in [2,3)\\). We can circumvent this issue by utilizing the generalized inverse cdf instead, for which \\[ x = F_X^{-1}(q) = \\mbox{inf}\\{ x : F_X(x) \\geq q \\} \\,. \\] The symbol “inf” indicates that we are finding the infimum, or smallest value, of the indicated set of values. Here, the output \\(x\\) is the smallest value for which \\(F_X(x) \\geq q\\) holds. For our given example, \\(x = 2\\). On the other hand, if we pick a value of \\(F_X(x)\\) that lies between the steps, we would choose the smallest \\(x\\) value associated with the next higher step. For instance, if for our example we want the inverse cdf for \\(F_X(x) = 0.5\\), which lies between the steps at 0.35 and 0.6, we would take the smallest value of \\(x\\) associated with \\(F_X(x) = 0.6\\), which is \\(x = 3\\). (Note that R utilizes the generalized form of the inverse cdf.) 1.9.1 The Cumulative Distribution Function for a Probability Density Function We work again with our simple parameterized pdf: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] The cdf for this function is simply the integral of the pdf “to the left” of the coordinate \\(x\\): \\[ F_X(x \\vert \\theta) = \\int_0^x f_Y(y \\vert \\theta) dy \\,. \\] Because the upper bound of the integral is \\(x\\), we replace \\(x\\) in the integrand with a dummy variable. (Here, \\(y\\) was chosen arbitrarily.) Thus \\[ F_X(x \\vert \\theta) = \\int_0^x \\theta y^{\\theta-1} dy = \\left. y^\\theta \\right|_0^x = x^\\theta \\,. \\] We can answer a variety of questions given this cdf. For example… What is the median of this distribution? The median \\(\\tilde{x}\\) is the point on the real-number line where \\[ P(X \\leq \\tilde{x}) = \\frac{1}{2} \\,. \\] For our distribution, \\[ \\tilde{x}^\\theta = \\frac{1}{2} ~\\Rightarrow~ \\tilde{x} = \\left( \\frac{1}{2} \\right)^{1/\\theta} \\,. \\] Now let \\(\\theta = 3\\). What is the probability of sampling a datum between \\(x = 1/4\\) and \\(x = 3/4\\)? \\[ P\\left(\\frac{1}{4} \\leq X \\leq \\frac{3}{4}\\right) = F_X\\left(\\frac{3}{4} \\vert \\theta=3\\right) - F_X\\left(\\frac{1}{4} \\vert \\theta=3\\right) = \\left(\\frac{3}{4}\\right)^3 - \\left(\\frac{1}{4}\\right)^3 = \\frac{26}{64} = \\frac{13}{32} \\,. \\] 1.9.2 Visualizing the Cumulative Distribution Function in R We continue with the pdf we use above, with \\(\\theta = 3\\). To show the region being integrated over to compute a cdf value, for say \\(x = 0.6\\), we utilize R’s polygon() function. (See Figure 1.17.) x &lt;- seq(0,1,by=0.01) f.x &lt;- 3*x^2 x.o &lt;- 0.6 df &lt;- data.frame(x=x,f.x=f.x) df.shade &lt;- subset(df,x&lt;=x.o) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_area(data = df.shade,aes(x,y=f.x),fill=&quot;green&quot;,col=&quot;blue&quot;,outline.type=&quot;full&quot;) + geom_vline(xintercept=x.o,col=&quot;red&quot;,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.17: The cdf for \\(f_X(x) = 3x^2\\) at \\(x = 0.6\\) is the area represented in green. What is happening in this code chunk? We first define a sequence of values for x (via seq()), then compute the pdf for each x value (f.x). We then define a data frame with x and f.x as columns, and determine which rows correspond to values of x that are less than or equal to 0.6 (via subset()). To create the polygon, we pass the subset data frame df.shade to the function geom_area(). If we wish to visualize the full cdf, we can do the following. (See Figure 1.18.) x &lt;- seq(0,1,by=0.01) F.x &lt;- x^3 df &lt;- data.frame(x=x,F.x=F.x) ggplot(data=df,aes(x=x,y=F.x)) + geom_hline(yintercept=0,lty=2,col=&quot;red&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;red&quot;) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_segment(x=-1,xend=0,y=0,yend=0,col=&quot;blue&quot;,lwd=1) + geom_segment(x=1,xend=2,y=1,yend=1,col=&quot;blue&quot;,lwd=1) + labs(y = expression(F[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.18: The cdf for \\(f_X(x) = 3x^2\\). 1.9.3 The CDF for a Mathematically Discontinuous Distribution Assume that we are handed the following pdf: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} 1/2 &amp; 0 \\leq x \\leq 1 \\\\ 2-x &amp; 1 \\leq x \\leq 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] which we display in Figure 1.19. df &lt;- data.frame(x=c(-0.25,0,0,1,1,2,2.25),f.x=c(0,0,0.5,0.5,1,0,0)) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_hline(yintercept=0,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.19: A continuous probability density function that is mathematically discontinuous at \\(x=1\\). This is a completely valid, “continuous” pdf that has a mathematical discontinuity at \\(x = 1\\). What is the cdf for this function? The key insight is that we should not try to evaluate the integral of \\(f_X(x)\\) from 0 to \\(x\\) when \\(x &gt; 1\\) with a single integral…this will not work! We simply have to break the problem up so as to define the cdf over the domain [0,1), and then over the domain [1,2]. \\[\\begin{align*} F_X(x \\vert x &lt; 1) &amp;= \\int_0^x f_Y(y) dy = \\frac{1}{2} \\int_0^y dy = \\frac{x}{2} \\\\ F_X(x \\vert x \\geq 1) &amp;= \\int_0^1 f_Y(y) dy + \\int_1^x f_Y(y) dy = \\left. \\frac{y}{2} \\right|_0^1 + \\int_1^x (2-y) dy = \\frac{1}{2} - \\left. \\frac{(2-y)^2}{2} \\right|_1^x \\\\ &amp;= \\frac{1}{2} - \\left( \\frac{(2-x)^2}{2} - \\frac{1}{2} \\right) = 1 - \\frac{(2-x)^2}{2} \\,. \\end{align*}\\] (Not sure if this is right? We can at the very least do sanity checking, as we know \\(F_X(1) = 1/2\\) and \\(F_X(2) = 1\\)…and our formula produces these results! Alternatively, we can take the derivative of \\(F_X(x)\\) and see if it matches \\(f_X(x)\\).) We display the cdf in Figure 1.20. x.seq &lt;- seq(1.01,2,by=0.01) x &lt;- c(0,1,x.seq) F.x &lt;- c(0,0.5,1-(2-x.seq)^2/2) df &lt;- data.frame(x=x,F.x=F.x) ggplot(data=df,aes(x=x,y=F.x)) + geom_hline(yintercept=0,lty=2,col=&quot;red&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;red&quot;) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_segment(x=-1,xend=0,y=0,yend=0,col=&quot;blue&quot;,lwd=1) + geom_segment(x=2,xend=3,y=1,yend=1,col=&quot;blue&quot;,lwd=1) + labs(y = expression(F[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.20: The cdf for our mathematically discontinuous pdf. 1.10 The Law of Total Probability One of the laws of probability that we introduce earlier in this chapter is the Law of Total Probability, or LoTP: if we partition a sample space \\(\\Omega\\) into \\(k\\) disjoint events \\(\\{B_1,\\ldots,B_k\\}\\), then for any event \\(A\\) we can write \\[ P(A) = \\sum_{i=1}^k P(A \\vert B_i) P(B_i) \\,. \\] We are in a position now, having introduced random variables and probability distributions, to update how we think of this law: it can express the probability of a random variable \\(X\\) when it is sampled from a discrete distribution with parameter \\(\\theta\\)…and when \\(\\theta\\) itself is not a fixed constant (as it has been up until now), but is itself a discrete random variable. To see this, let’s rewrite the LoTP given this description: \\[ p_X(x) = \\sum_\\theta p_{X \\vert \\theta}(x \\vert \\theta) p_{\\Theta}(\\theta) \\,. \\] This equation is saying that the probability mass associated with the coordinate \\(x\\) is the value of the mass for \\(x\\), given the value \\(\\theta\\), weighted by the probability that we would even observe the value \\(\\theta\\) in the first place. Or, that \\(p_X(x)\\) is a weighted average of the values of the conditional distribution \\(p_{X \\vert \\theta}(x \\vert \\theta)\\), where the weights are given by \\(p_{\\Theta}(\\theta)\\). What if \\(\\theta\\) is actually a continuous random variable? We can extend the LoTP to handle that possibility by replacing the summation over a discrete random variable with an integral over a continuous one: \\[ p_X(x) = \\int_\\theta p_{X \\vert \\theta}(x \\vert \\theta) f_{\\Theta}(\\theta) d\\theta \\,. \\] And what if the distribution of \\(X \\vert \\theta\\) is continuous? We would just replace the \\(p_X\\) and the \\(p_{X \\vert \\theta}\\) in the equations above with \\(f_X\\) and \\(f_{X \\vert \\theta}\\), i.e., we would use the LoTP to define a probability density instead of a probability mass. 1.10.1 The LoTP With Two Simple Discrete Distributions Let’s suppose we have two random variables \\(X\\) and \\(Y\\), where the probability mass function for \\(Y\\) is \\(y\\) \\(p_Y(y)\\) 0 2/3 1 1/3 and where, if \\(Y = 0\\), the pmf for \\(X\\) is \\(x \\vert y=0\\) \\(p_{X \\vert Y}(x \\vert y=0)\\) 0 4/5 1 1/5 and if \\(Y = 1\\) the pmf for \\(X\\) is \\(x \\vert y=1\\) \\(p_{X \\vert Y}(x \\vert y=1)\\) 0 3/5 1 2/5 What is the pmf \\(p_X(x)\\)? The Law of Total Probability tells us that \\[ p_X(x) = \\sum_y p_{X \\vert Y}(x \\vert y) p_{Y}(y) \\,, \\] so \\[\\begin{align*} p_X(0) &amp;= p_{X \\vert Y}(0 \\vert 0) p_{Y}(0) + p_{X \\vert Y}(0 \\vert 1) p_{Y}(1) = \\frac{4}{5} \\cdot \\frac{2}{3} + \\frac{3}{5} \\cdot \\frac{1}{3} = \\frac{11}{15} \\\\ p_X(1) &amp;= p_{X \\vert Y}(1 \\vert 0) p_{Y}(0) + p_{X \\vert Y}(1 \\vert 1) p_{Y}(1) = \\frac{1}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{3} = \\frac{4}{15} \\,. \\end{align*}\\] The pmf is thus \\(x\\) \\(p_X(x)\\) 0 11/15 1 4/15 The masses sum to 1, so indeed this is a proper pmf. 1.10.2 The Law of Total Expectation If we inspect the tables above, we see that, e.g., \\[\\begin{align*} E[X \\vert Y=0] &amp;= 0 \\cdot \\frac{4}{5} + 1 \\cdot \\frac{1}{5} = \\frac{1}{5} \\,. \\end{align*}\\] A similar calculation yields \\(E[X \\vert Y=1] = 2/5\\). What then is the expected value of \\(X\\) itself? A result related to the Law of Total Probability is the Law of Total Expectation (LoTE), which states that when \\(Y\\) is finite and countable, \\[ E[X] = E[E[X \\vert Y]] = \\sum_y E[X \\vert Y=y] ~ p_Y(y) \\,, \\] i.e., the overall expected value is a weighted average of the individual values \\(E[X \\vert Y=y]\\). Here, the LoTE yields \\[ E[X] = \\frac{1}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{3} = \\frac{4}{15} \\,. \\] 1.10.3 The LoTP With Two Continuous Distributions Let’s suppose that we have two random variables, \\(X\\) and \\(\\theta\\), such that \\[\\begin{align*} f_{X \\vert \\Theta}(x \\vert \\theta) &amp;= \\theta \\exp(-\\theta x) \\\\ f_{\\Theta}(\\theta) &amp;= \\exp(-\\theta) \\,, \\end{align*}\\] for \\(x \\in [0,\\infty)\\) and \\(\\theta &gt; 0\\). What is \\(f_X(x)\\)? As mentioned above, the primary change to the LoTP would be that we use integrate over all possible values of \\(\\theta\\), rather than sum, so the LoTP looks like this: \\[ f_X(x) = \\int_0^\\infty f_{X \\vert \\Theta}(x \\vert \\theta) f_{\\Theta}(\\theta) d\\theta \\,. \\] Now that we’ve established this equation, the rest is math…except as we’ll see, we need to use integration by parts. \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\theta \\exp(-\\theta x) \\exp(-\\theta) d\\theta \\\\ &amp;= \\int_0^\\infty \\theta \\exp(-\\theta (x+1)) d\\theta \\,. \\end{align*}\\] We set up the integration as follows: \\[\\begin{align*} u = \\theta ~~~ &amp; ~~~ dv = \\exp(-\\theta (x+1)) d\\theta \\\\ du = d\\theta ~~~ &amp; ~~~ v = -\\frac{1}{x+1}\\exp(-\\theta (x+1)) \\,. \\end{align*}\\] Then \\[\\begin{align*} f_X(x) &amp;= \\left.(u v)\\right|_0^\\infty - \\int_0^\\infty v du \\\\ &amp;= -\\left.\\frac{\\theta}{x+1}\\exp(-\\theta (x+1))\\right|_0^\\infty + \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\\\ &amp;= 0 + \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\,. \\end{align*}\\] (We will stop here momentarily to remind the reader that when we evaluate an expression of the form \\(x e^{-x}\\), the result as \\(x \\rightarrow \\infty\\) is zero because \\(e^{-x} \\rightarrow 0\\) faster than \\(x \\rightarrow \\infty\\). We now carry on…) \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\\\ &amp;= \\left. -\\frac{1}{(x+1)^2} \\exp(-\\theta (x+1)) \\right|_0^\\infty \\\\ &amp;= \\frac{1}{(x+1)^2} \\,, \\end{align*}\\] for \\(x \\in [0,\\infty)\\). Done. We will leave it as an exercise to the reader to confirm that \\(f_X(x)\\) is a valid pdf that integrates to one. Above, we say that “we need to use integration by parts.” This is not quite true. A handy result that we will utilize as the book goes on is that \\[ \\Gamma(t) = \\int_0^\\infty u^{t-1} \\exp(-u) du \\,. \\] This is the gamma function. (The symbol \\(\\Gamma\\) represents a capital gamma.) One of the properties that makes this function useful is that when \\(x\\) is a non-negative integer, the gamma function is related to the factorial function: \\(\\Gamma(x) = (x-1)! = (x-1) (x-2) \\cdots 1\\). But the reason why the gamma function is useful here is that we can use it to avoid integration by parts. Our integral is \\[ f_X(x) = \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\,. \\] To solve this, we implement variable substitution. The three steps of variable substitution are to write down a viable substitution \\(u = g(\\theta)\\); to then derive \\(du = h(u,\\theta) d\\theta\\); and finally to use \\(u = g(\\theta)\\) to transform the bounds of the integral. For our integral \\[ (1) ~~ u = (x+1)\\theta ~~ \\implies ~~ (2) ~~ du = (x+1)d\\theta \\] and \\[ (3) ~~ \\theta = 0 ~\\implies~ u = 0 ~~~ \\mbox{and} ~~~ \\theta = \\infty ~\\implies~ u = \\infty \\,, \\] We see from point (3) that making the variable substitution will not affect the bounds of the integral. Thus we have that \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\frac{1}{x+1}\\exp(-u) \\frac{du}{x+1} \\\\ f_X(x) &amp;= \\frac{1}{(x+1)^2} \\int_0^\\infty u^0 \\exp(-u) du \\\\ f_X(x) &amp;= \\frac{1}{(x+1)^2} \\Gamma(1) = \\frac{1}{(x+1)^2} 0! = \\frac{1}{(x+1)^2} \\,. \\end{align*}\\] (Here, we utilize the fact that zero factorial is one.) 1.11 Working With R: Data Sampling One of the primary uses of R is to perform simulations in which we repeatedly create mock datasets and analyze them. But: how do we create such datasets? Below, we will describe two methods for randomly sampling data given a probability distribution. The first, rejection sampling, is appropriate to use when we cannot work with the cumulative distribution function of the assumed distribution analytically (i.e., with pencil and paper). As we will see, rejection sampling is (relatively) computationally inefficient, but it does have the benefit that we can apply it in just about any sampling situation. The second method, inverse transform sampling, is efficient and should always be our first choice when the cdf is tractable. To head off a question the reader may have: no, we do not always have to hand-code samplers when working in R…for commonly used distributions, R supplies “wrapper functions” that effectively abstract away the details of inverse transform sampling. However, knowing how to code a sampler is a good skill to have! Let’s suppose we are working with one of the pdfs that we define above: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] The cdf for this distribution is \\[ F_X(x) = \\frac{1}{\\pi}\\left( \\sin x - x \\cos x \\right) \\,, \\] which is not easily inverted. Thus to sample data from this distribution, we utilize the following algorithmic steps. Determine the range of values over which we will sample data values: \\([x_{lo},x_{hi}]\\). Nominally this will be the domain of the distribution, but sometimes that’s not viable, such as when the domain is semi- or fully infinite. (Here, the range is easily specified: \\([0,\\pi]\\).) Within \\([x_{lo},x_{hi}]\\), determine the maximum value of \\(f_X(x)\\). (For our assumed distributione, this is not necessarily a simple calculation, as the derivative of \\(f_X(x)\\) is \\((\\sin x + x \\cos x)/\\pi\\). We can solve for the root using, e.g., R’s uniroot() function: f &lt;- function(x) { (sin(x) + x*cos(x))/pi } uniroot(f,interval=c(0.01,pi))$root ## [1] 2.028758 This looks for the root of the given function within the stated interval; since there is a root at 0, corresponding to a functional minimum, we exclude that point by setting the interval lower bound to 0.01. uniroot() is an extremely useful function and we will see it again throughout the rest of this book. The root is \\(x_{max} = 2.0288\\) and \\(f_X(x_{max}) = 0.5792\\).) 3. We repeat the following steps until we reach our target sample size \\(n\\): (a) sample a random number \\(u\\) assuming uniform weighting between \\(x_{lo}\\) and \\(x_{hi}\\); (b) sample another random number \\(v\\) assuming uniform weighting between 0 and \\(f_X(x_{max})\\); and (c) keep \\(u\\) as part of our sample if \\(v \\leq f_X(u)\\). (a) and (b) are summed up by the statement “draw a rectangle whose vertices are \\((x_{lo},0)\\), \\((x_{hi},0)\\), \\(x_{hi},f_X(x_{max}))\\), and \\(x_{lo},f_X(x_{max}))\\) and pick a random point inside the rectangle,” while (c) is summed up by saying “keep the random point if it lies below \\(f_X(x)\\).” Note that we will assume that at the very least, we can use an R wrapper function to sample a numbers with uniform weighting; without this assumption, we would have to wade into the quagmire that is random number generation, which is well beyond the scope of this book! In a code chunk and in Figure 1.21 we show how we sample \\(n = 1000\\) data sampled from our distribution, and the final result. (We dub the observed distribution the empirical distribution of the data, where “empirical” simply means “what we actually observe.”) Rejection sampling seems quick and easy…should we always use it when we are not already provided a sampling function for our pmf or pdf? No, not necessarily, because as noted above it is computationally inefficient: we might have to sample \\(m \\gg n\\) points in order to populate a sample of size \\(n\\). (We will also note here that this is the first time that we are running across the R function set.seed(). This initializes the underlying random number generator such that we generate the same numerical results every time we run the subsequent code…which is useful when doing analyses that we want to be reproducible. If we leave out set.seed(), then every time we run the subsequent code, we get a different data sample. The number that we pass to set.seed() can be anything…we adopt 101 here, but it can any real number.) set.seed(101) n &lt;- 1000 x.lo &lt;- 0 x.hi &lt;- pi f.x.hi &lt;- 0.58 # rounding up is OK, it just decreases algorithm efficiency X.sample &lt;- rep(NA,n) # rejection sampling ii &lt;- 0 while ( ii &lt; n ) { u &lt;- runif(1,min=x.lo,max=x.hi) v &lt;- runif(1,min=0,max=f.x.hi) if ( v &lt; u*sin(u)/pi ) { ii &lt;- ii+1 X.sample[ii] &lt;- u } } empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,pi,by=0.01) f.x &lt;- x*sin(x)/pi true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,3.2,by=0.2)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.21: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = (x \\sin x)/\\pi\\) via the rejection sampling algorithm. We observe that our empirical distribution follows the true distribution well. A primary alternative to rejection sampling is inverse transform sampling, in which we utilize the inverse cdf function to generate appropriately distributed data. Inverse transform sampling is efficient in that every proposal point is kept. Let’s suppose we are working with the pdf: \\[ f_X(x) = \\theta x^{\\theta-1} \\,, \\] where \\(\\theta &gt; 0\\) and \\(x \\in [0,1]\\). The inverse cdf, as derived in an example above, is \\(F_X^{-1}(q) = q^{1/\\theta}\\). Inverse transform sampling utilizes the following algorithmic steps. Pick the target sample size \\(n\\). Sample \\(n\\) data with uniform weighting between 0 and 1. These are the cdf bounds. Call these data \\(q\\). Transform the data \\(q\\) to be \\(x = F_X^{-1}(q)\\). In a code chunk and in Figure 1.22 we display our inverse-transform sampling code as well as the empirical distribution of \\(n = 1000\\) data sampled from our distribution (assuming \\(\\theta = 3\\)). We note that the code to generate our sample is much simpler than the code needed to perform rejection sampling! set.seed(101) theta &lt;- 3 n &lt;- 1000 q &lt;- runif(n,min=0,max=1) X.sample &lt;- q^(1/theta) empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,1,by=0.01) f.x &lt;- theta*x^(theta-1) true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.1)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.22: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = 3x^2\\) via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well. 1.11.1 More Inverse-Transform Sampling Let’s suppose that we are to sample \\(n\\) data from the following distribution: \\[ f_X(x) = 2(1-x) ~~~ x \\in [0,1] \\,. \\] Can we do this via inverse-transform sampling? The answer is yes, if (a) we can derive the cdf \\(F_X(x)\\), and (b) we can invert it. Here, \\[ F_X(x) = \\int_0^x f_V(v) dv = \\int_0^x 2(1-v) dv = \\left. -(1-v)^2 \\right|_0^x = -(1-x)^2 - (-1) = 1 - (1-x)^2 \\,. \\] To invert the cdf, we set it equal to \\(q\\) and solve for \\(x\\): \\[\\begin{align*} q &amp;= 1 - (1-x)^2 \\\\ \\Rightarrow ~~~ 1 - q &amp;= (1-x)^2 \\\\ \\Rightarrow ~~~ \\sqrt{1 - q} &amp;= 1-x \\\\ \\Rightarrow ~~~ x &amp;= 1 - \\sqrt{1 - q} \\., \\end{align*}\\] To check for the correctness of our inversion, we utilize a code like the one in the main body of the section above and compare our sampled data against the pdf. See Figure 1.23. set.seed(101) n &lt;- 1000 q &lt;- runif(n,min=0,max=1) X.sample &lt;- 1 - sqrt(1-q) empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,1,by=0.01) f.x &lt;- 2*(1-x) true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.1)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.23: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = 2(1-x)\\) via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well. 1.12 Statistics and Sampling Distributions Let’s say that we run an experiment in which we randomly sample many data from some distribution \\(P\\): \\[ \\mathbf{X} = \\{X_1,X_2,\\ldots,X_n\\} \\overset{iid}{\\sim} P \\,. \\] The expected value for this distribution is \\(E[X] = \\mu\\), while the variance is \\(V[X] = \\sigma^2\\) (assumed to be finite). So we have data…now what? Figure 1.24: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, assume that the population can be represented by some family of distributions parameterized by \\(\\theta\\), and compute and use statistics to infer the value(s) of \\(\\theta\\). The answer, typically, is that we would use these data to infer the (unknown) properties of the population from which they are drawn. A simple picture of the experiment-and-infer cycle is given in Figure 1.24. Notice how in this figure we use the term “statistical inference.” (Fitting, as it is part of the name of this course!) This is an appropriate term to use because we utilize statistics when trying to infer the properties of the unknown underlying population, like its true mean \\(\\mu\\). But this motivates a next question… What is a statistic? A statistic is simply a function of the data we observe. It can be any function of the data\\(-\\)\\(X_1\\), sin\\((X_1) + \\pi X_2\\), etc.\\(-\\)and it provides a useful means by which to summarize data (i.e., reduce \\(n\\) numbers to a single number). But it should be intuitively obvious (we would hope!) that some statistics are going to be more informative than others: for instance, if we are trying to infer what \\(\\mu\\) might be, \\(X_1\\) is probably going to be more useful to us than sin\\((X_1) + \\pi X_2\\). But it may not (nay, will not) be the most useful quantity when the sample size \\(n &gt; 1\\). We could say that much of what we do as statisticians is to pick appropriate (and optimal!) statistics to perform inference. What are some common statistics? The sample mean: \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\,. \\] This is always useful for inferring a population mean. Why it is better than, e.g., \\(X_1\\) when \\(n &gt; 1\\) will become more clear below. (Foreshadowing: there are metrics we can compute that provide numerical assessments of the usefulness of a statistic for performing inference. We introduce some of these metrics in the next section.) The sample variance: \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_1 - \\bar{X})^2 \\,. \\] As we might guess, this one helps us infer population variances, and thus helps us make sense of the “width” of the pmf or pdf from which the data are sampled. The square root of \\(S^2\\) is the sample standard deviation. (One might ask: why \\(n-1\\)? Metrics, again…we will return to this point below when we discuss point estimation.) The sample range: \\[ R = X_{(n)} - X_{(1)} \\,. \\] Here, we introduce a notational wrinkle: \\(X_{(i)}\\) represents the \\(i^{\\rm th}\\) smallest datum. So \\(X_{(1)}\\) is the observed datum with the smallest value (but not necessarily the first one to be recorded in our experiment), and \\(X_{(n)}\\) is the one with the largest value. \\(X_{(\\cdot)}\\) is dubbed an order statistic and we will illustrate its use as we go on, beginning in Chapter 3. There are a myriad of others: the interquartile range, the median, etc. We have stated what a statistic is: it is a function of the observed data. But what does this imply? It implies that statistics, which are functions of random variables, are themselves random variables, and thus are sampled from a pmf or pdf. It is convention to call the pmf or pdf associated with a given statistic the sampling distribution, but we are not necessarily fans of the term: it makes it sound like something new and different, when in reality a sampling distribution is just another pmf or pdf, with properties equivalent to those discussed earlier in the chapter (e.g., a sampling distribution has an expected value, a variance, etc.). As we will see in the first example below, if the statistic is a linear function of random variables (like the sample mean), we can derive its expected value, variance, and standard error now given the tools we already have at our disposal. The term standard error simply refers to the standard deviation of a sampling distribution, i.e., \\[ se[Y] = \\sqrt{V[Y]} \\,, \\] where \\(Y\\) is our statistic. Now, can we go beyond this and derive the mathematical form of a statistic’s pmf or pdf now? The short answer is no…we have not yet introduced methods for deriving the functional forms of sampling distributions. In Chapter 2, we will introduce moment-generating functions, which can help us derive sampling distributions for linear functions of random variables (an example of which is the sample mean). In Chapter 3, we will show how one can write down the pmf or pdf for order statistics (like the sample median); once the sampling distribution is known, then we can derive its expected value and variance. (As an aside, we should mention here the empirical rule. While nominally about the normal distribution, we will think of it as stating that nearly all statistics should be observed as laying within three standard errors of their population means. For instance, as we show below in an example, \\(E[\\bar{X}] = \\mu\\), so virtually all values of \\(\\bar{X}\\), as observed over repetitions of an experiment, should lie in the range \\([\\mu - 3 \\cdot se(\\bar{X}),\\mu + 3 \\cdot se(\\bar{X})]\\), a range that gets smaller and smaller as the sample size \\(n\\) goes to infinity.) There are, however, two paths that one could follow that allow us to build up an empirical sampling distribution for a statistic. The first assumes that we know (or are willing to assume) the pmf or pdf for the individual data: we would repeatedly simulate data from the distribution and record the values for the statistic. This builds off of the material in the last section above. (See the example below, as well as the last section of this chapter.) The other is useful for situations where we do not know nor are willing to assume the form of the pmf or pdf for the individual data…this is the bootstrap. We discuss the bootstrap technique in Chapter 4. Many important results in statistical inference assume that we have collected a sample of \\(n\\) iid random variables, so over the course of the rest of this chapter and for the next several, we will assume that when we have sampled two or more random variables, they will be iid random variables. (We will discuss concepts related to simultaneously sampling values for two or more dependent random variables in Chapter 6.) 1.12.1 Expected Value and Variance of the Sample Mean (For All Distributions) Given \\(n\\) iid data from some distribution, we can use the results from earlier in this chapter to immediately show that \\[ E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{1}{n} n \\mu = \\mu \\,, \\] and \\[ V[\\bar{X}] = V\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n^2} V\\left[\\sum_{i=1}^n X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n V[X_i] = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\,. \\] The standard error for the sample mean is thus \\(\\sqrt{\\bar{X}} = \\sigma/\\sqrt{n}\\). There are two important conclusions to take away from this simple example. First, we never state the distribution from which we sample the initial data, so this is a general result that holds for all distributions. Second, we see that the width of the sampling distribution for the sample mean decreases as we collect more and more data, as \\(1/\\sqrt{n}\\), meaning that any inferences that we make about the population mean will become more and more accurate as the sample size increases. 1.12.2 Visualizing the Distribution of the Sample Mean In an example in the last section above, we use inverse-transform sampling to sample data from the pdf \\(f_X(x) = 2(1-x)\\) for \\(x \\in [0,1]\\). Here, we extend our R code so as to visualize the distribution of the sample mean of \\(n = 10\\) data drawn from this distribution. We note that some of the material below foreshadows that which we cover in the last section of this chapter, when we discuss numerical simulation. set.seed(101) n &lt;- 10 num.sim &lt;- 1000 X.bar &lt;- rep(NA,num.sim) # set aside storage for X.bar # NA == Not Available - this is overwritten for ( ii in 1:num.sim ) { q &lt;- runif(n,min=0,max=1) X.sample &lt;- 1 - sqrt(1-q) X.bar[ii] &lt;- mean(X.sample) } empirical.mean &lt;- data.frame(X.bar=X.bar) x &lt;- seq(0,1,by=0.01) f.x &lt;- 2*(1-x) pdf &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.mean,aes(x=X.bar)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.05)) + geom_line(data=pdf,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + geom_vline(xintercept=1/3,col=&quot;green&quot;,lwd=1) + geom_segment(x=1/3-0.0745,xend=1/3+0.0745,y=2.5,yend=2.5,col=&quot;green&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.25: The empirical distribution of the sample mean of \\(n = 10\\) data sampled from the distribution \\(f_X(x) = 2(1-x)\\) for \\(x \\in [0,1]\\). The red line indicates \\(f_X(x)\\), while the vertical and horizontal green lines indicate \\(E[\\bar{X}] = \\mu = 1/3\\) and the range \\([\\mu-se(\\bar{X}),\\mu+se(\\bar{X})] = [0.2588,0.4078]\\). The shape of the empirical distribution is approaching that of a normal distribution, a result that we will discuss in Chapter 2 when introducing the Central Limit Theorem. See Figure 1.25. The mean of our pdf is \\[ E[X] = \\int_0^1 2x(1-x) dx = \\int_0^1 2xdx - \\int_0^1 2x^2dx = \\left. x^2 \\right|_0^1 - \\left. \\frac{2}{3}x^3 \\right|_0^1 = 1 - \\frac{2}{3} = \\frac{1}{3} \\,. \\] This is indicated via the green vertical line in the figure. The variance is \\[\\begin{align*} V[X] &amp;= E[X^2] - (E[X])^2 \\\\ &amp;= \\left[ \\int_0^1 2x^2dx - \\int_0^1 2x^3dx \\right] - \\left(\\frac{1}{3}\\right)^2 \\\\ &amp;= \\left(\\frac{2}{3} - \\frac{1}{2}\\right) - \\frac{1}{9} \\\\ &amp;= \\frac{1}{6} - \\frac{1}{9} = \\frac{1}{18}\\\\ \\end{align*}\\] The standard error for \\(\\bar{X}\\) is thus \\[ se(\\bar{X}) = \\sqrt{\\frac{V[X]}{n}} = \\sqrt{\\frac{1}{180}} = 0.0745 \\,. \\] The range from the mean minus one standard error to the mean plus one standard error is indicated via the green horizontal line segment in the figure. We observe that the distribution of the sample mean values matches the expected mean and standard error well, and is definitely different from the distribution of the individual data values (which is shown as the red line in the figure). The sample mean distribution almost looks like a normal distribution, but it isn’t one exactly…and for now we will have to content ourselves with only knowing the mean and the standard error of the distribution, and not its mathematical details. However, as we’ll see in Chapter 2, the sample mean distribution will look more and more like a normal distribution as the sample size \\(n\\) goes to infinity, in a result dubbed the Central Limit Theorem. 1.13 The Likelihood Function Assume we are given iid data \\(\\mathbf{X} = \\{X_1,\\ldots,X_n\\}\\), with each datum sampled from a continuous distribution \\(f_X(x \\vert \\theta)\\). (Recall that \\(\\theta\\) is the conventionally used symbol for a population parameter or set of parameters. Here, without loss of generality, we will assume that \\(\\theta\\) represents one parameter.) The likelihood function for the entire sample is defined as \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = f_X(\\mathbf{x} \\vert \\theta) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\,. \\] The second equality holds because the data are assumed to be iid. Note that the same definition holds for discrete distributions, with the notational change \\(f \\rightarrow p\\). Additionally, recall that \\(\\prod\\) is the product symbol, the multiplicative analogue of the summation symbol \\(\\sum\\): \\(\\prod_{i=1}^n f_X(x_i \\vert \\theta) = f_X(x_1 \\vert \\theta) \\cdot f_X(x_2 \\vert \\theta) \\cdot \\cdots \\cdot f_X(x_n \\vert \\theta)\\). As a last comment, we will find that we often work not with the likelihood function itself, but with the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\); given iid data, the log-likelihood is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\left[ \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\right] = \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta) \\,. \\] Let’s step back for an instant here, and assume our sample size is \\(n = 1\\). At first blush, it would appear that a likelihood is the same as a probability density function, because, after all, \\(\\mathcal{L}(\\theta \\vert x) = f_X(x \\vert \\theta)\\). But note what is being conditioned upon in both functions: for the likelihood, we consider that the datum is fixed to its observed value (i.e., \\(X = x\\)) and that we are free to vary the value of \\(\\theta\\). To show the difference between a probability distribution and the likelihood function, let’s take a look at a simple example where we flip a potentially unfair coin twice, with \\(p\\) being the probability of observing heads in any single flip. The probability mass function for \\(X\\), the random variable denoting the number of heads observed, is \\[ p_X(x \\vert p) = \\left\\{ \\begin{array}{cl} 2 &amp; p^2 \\\\ 1 &amp; 2p(1-p) \\\\ 0 &amp; (1-p)^2 \\end{array} \\right. \\,. \\] The probability mass functions that arise when we set \\(p\\) to, e.g., 0.3, 0.5, and 0.8 are shown in Figure 1.26, while the likelihood functions for each observable value of \\(x\\) are shown in Figure 1.27. None of the plots in Figure 1.27 shows the pmf \\(p_X(x \\vert p)\\); rather, they each show the relative plausibility of a particular probability \\(p\\) given the number of observed heads. If we observe zero (or two) heads, then a probability of \\(p=0\\) (or \\(p=1\\)) is the most plausible value…but only \\(p=1\\) (or \\(p=0\\)) is impossible. On the other hand, when we observe one head and one tail, the most plausible value for \\(p\\) is 0.5, although all other values (save \\(p=0\\) and \\(p=1\\)) are possible as well. Figure 1.26: From left to right, the probability mass functions \\(p_X(x \\vert p)\\) for probabilities \\(p =\\) 0.3, 0.5, and 0.8. Figure 1.27: From left to right, the likelihood function \\(\\mathcal{L}(p \\vert x)\\) for the probability parameter \\(p\\) given that we observe \\(x=0\\) heads, \\(x=1\\) head, and \\(x=2\\) heads, respectively. One might question at this point why the likelihood function is important, as it is indeed not a pmf or pdf. We will see below that it is often used when trying to uncover the truth about a population: e.g., given a set of data, randomly sampled from some family of distributions parameterized by \\(\\theta\\), we can utilize the likelihood to infer, or estimate, \\(\\theta\\). Before we talk about estimation, however, we need to discuss how one might summarize a set of data so as to make it mathematically more easy to work with…in other words, we need to talk about statistics. 1.13.1 Examples of Likelihood Functions for IID Data (For more information on the product-symbol manipulations utilized below, see the material on useful product symbol tricks in Chapter 8.) We are given \\(n\\) iid samples from \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\theta x_i^{\\theta-1} = \\theta^n \\left(\\prod_{i=1}^n x_i\\right)^{\\theta-1} \\,, \\] while the log-likelihood function is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x}) = n \\log \\theta + (\\theta-1) \\log \\prod_{i=1}^n x_i = n \\log \\theta + (\\theta-1) \\sum_{i=1}^n \\log x_i \\,. \\] We are given \\(n\\) iid samples from \\[ p_X(x \\vert p) = \\left\\{ \\begin{array}{cl} p &amp; x=1 \\\\ 1-p &amp; x=0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We observe \\(k\\) values of 1, and \\(n-k\\) values of 0. Thus the likelihood function is \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n p_X(x_i \\vert p) = \\prod_{i=1}^k p \\times \\prod_{i=1}^{n-k} (1-p) = p^k(1-p)^{n-k} \\,, \\] and the log-likelihood function is \\[ \\ell(p \\vert \\mathbf{x}) = \\log \\mathcal{L}(p \\vert \\mathbf{x}) = k \\log p + (n-k) \\log (1-p) \\,. \\] We are given \\(n\\) iid samples from the mixed distribution \\[ h_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\frac{x}{\\theta} &amp; x \\in [0,1] \\\\ 1-\\frac{1}{2\\theta} &amp; x=2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We observe \\(l\\) values between 0 and 1, along with \\(n-l\\) values of 2. Let \\(j\\) denote the indices of those data with values between 0 and 1, and \\(k\\) denote the indices of those data with value 2. The likelihood function is then \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n h_X(x_i \\vert p) = \\prod_{j=1}^l \\frac{x_j}{\\theta} \\times \\prod_{k=1}^{n-l} \\left(1-\\frac{1}{2\\theta}\\right) = \\left( \\frac{1}{\\theta^l} \\prod_{j=1}^l x_j \\right) \\left(1-\\frac{1}{2\\theta}\\right)^{n-l} \\] 1.13.2 Coding the Likelihood Function in R Let’s code and display the likelihood function for the mixed distribution we introduce immediately above. The first thing we will do, however, is generate data from this distribution using inverse transform sampling. The cdf for this distribution is \\[ H_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} 0 &amp; x &lt; 0 \\\\ x^2/2\\theta &amp; 0 \\leq x \\leq 1 \\\\ 1/2\\theta &amp; 1 &lt; x &lt; 2 \\\\ 1 &amp; x \\geq 2 \\end{array} \\right. \\,. \\] For cdf values \\(q &lt; 1/(2\\theta)\\), the inverse function is \\(x = \\sqrt{2 \\theta q}\\). We thus code the inverse transform sampler as follows… set.seed(101) n &lt;- 100 theta &lt;- 1.5 q &lt;- runif(n,min=0,max=1) w &lt;- which(q &lt; 1/(2*theta)) X.sample &lt;- rep(2,n) X.sample[w] &lt;- sqrt(2*theta*q[w]) Given our dataset, we can compute and visualize the likelihood function \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\)…but…this will be problematic. If we examine the likelihood function, we see that its values will be (a) tiny, and (b) spread over a large dynamic range. (In fact, for a sufficiently large dataset, the likelihood function will have a value too small to be recordable as a floating-point number on a computer!) Thus in practice it is often best to visualize the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x})\\) as a function of \\(\\theta\\). We do this below in Figure 1.28. (We will leave the derivation of the log-likelihood as an exercise to the reader.) loglike &lt;- function(theta,x) { w &lt;- which(x &lt; 1) n &lt;- length(x) l &lt;- length(w) return(log(prod(x[w])) - l*log(theta) + (n-l)*log(1-1/2/theta)) } theta &lt;- seq(0.51,10.0,by=0.01) llike &lt;- rep(NA,length(theta)) for ( ii in 1:length(theta) ) { llike[ii] &lt;- loglike(theta[ii],X.sample) } w &lt;- which.max(llike) df &lt;- data.frame(theta=theta,llike=llike) ggplot(data=df,aes(x=theta,y=llike)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_vline(xintercept=theta[w],col=&quot;red&quot;,lwd=1) + labs(x=expression(theta),y=&quot;Log-Likelihood&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.28: The log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\) for the pdf \\(h_X(x \\vert \\theta)\\) defined in the text. The red line indicates the value of \\(\\theta\\) (1.56) for which \\(\\ell(\\theta \\vert \\mathbf{x})\\) is maximized. 1.14 Point Estimation Let’s suppose we are given a sample of iid data \\(\\{X_1,\\ldots,X_n\\}\\), sampled from some distribution with mean \\(E[X]\\) and finite variance \\(V[X]\\). As we started to discuss above, we can use functions of these data, or statistics, to make inferences about population properties: what are plausible values for the population mean \\(\\mu\\)? or the population variance \\(\\sigma^2\\)? or, more generally, any population parameter \\(\\theta\\)? In point estimation, the statistic that we choose is an estimator of \\(\\theta\\). We dub the estimate \\(\\hat{\\theta}\\): this is a number that is our best guess for what the true value of \\(\\theta\\) is, given the data we have observed thus far. Now…how do we define an estimator? Well, to start, we can guess what might be good estimators, and compare their properties. For instance, here, let’s propose two estimators for \\(\\mu\\): \\(\\hat{\\mu} = X_1\\) and \\(\\hat{\\mu} = \\bar{X}\\). Which is better? It may seem intuitively obvious that \\(\\bar{X}\\) is better, because it incorporates more data…but how do we quantify “better”? In the last section, we indicated that we can assess the utility of a statistic used as an estimator by computing metrics, quantities that allow us to directly compare estimators. Here, we will highlight two of them, the bias and the variance; later, we will highlight others. (Recall that an estimator is a statistic, and thus it is a random variable that is drawn from a sampling distribution with some mean and some variance.) Bias: does the estimator yield the true value, on average? In other words, is \\(B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta] = E[\\hat{\\theta}] - \\theta = 0\\)? If so, we say that our estimator is unbiased. Here, both estimators are unbiased: \\(E[X_1-\\mu] = E[X_1]-\\mu = \\mu-\\mu = 0\\), and \\(E[\\bar{X}-\\mu] = E[\\bar{X}] - \\mu = \\mu-\\mu = 0\\). Variance: is the spread of values that the estimator generates from experiment to experiment relatively small, or relatively large? (Recall that it cannot be zero, due to the randomness inherent in the data-generating process!) Here, the first estimator has variance \\(V[X_1] = \\sigma^2\\), while the second has variance \\(V[\\bar{X}] = \\sigma^2/n\\). (Recall that we derive the latter result in the previous section!) If \\(n &gt; 1\\), the second estimator, with its smaller variance, is the better one to use. (If \\(n = 1\\), then \\(\\bar{X} = X_1\\), so the two estimators are identical anyway.) See Figure 1.29 for a graphical representation of bias and variance. Figure 1.29: A graphical representation of the concepts of bias (how far on average an estimate is from the truth…represented here as an offset from the bullseye) and variance (the spread of estimate values…represented here as the spatial spread of the plotted points). Given a choice, we generally prefer unbiased estimators with smaller variances; however, it is theoretically possible that we might be better off with an estimator with a small bias if it has an even lower variance than the best unbiased one. We can start making sense of this statement now by stating that we can combine the information about bias and variance together into a single metric, the mean-squared error (or MSE). The MSE is defined as \\[ MSE[\\hat{\\theta}] = B[\\hat{\\theta}]^2 + V[\\hat{\\theta}] \\,, \\] and smaller values are better. For \\(\\hat{\\theta} = X_1\\), the MSE is \\(0^2 + \\sigma^2 = \\sigma^2\\), while for \\(\\hat{\\theta} = \\bar{X}\\), the MSE is \\(0^2 + \\sigma^2/n = \\sigma^2/n\\); \\(\\bar{X}\\) is still the better estimator. At this point, one might be thinking that guessing estimators would be a sub-optimal approach. And one would be correct. Over the remainder of the book, we introduce different algorithmic approaches for defining estimators with (presumably) good properties. Here we examine a first one: maximum likelihood estimation (or MLE). Above, in the section introducing the likelihood, we discussed how the likelihood function \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) encapsulates the relative plausibilities of different values of \\(\\theta\\) given the data that are observed. Maximum likelihood estimation takes this idea to its natural conclusion: the most plausible value of \\(\\theta\\), the one that maximizes the likelihood function, is indeed a good way to estimate the true value of \\(\\theta\\). Assuming that the likelihood function achieves a maximum away from \\(\\theta_{\\rm lo}\\) and \\(\\theta_{\\rm hi}\\), the parameter bounds, then the steps to find \\(\\hat{\\theta}_{MLE}\\) involve straightforward calculus, albeit with a simplifying twist: Write down the likelihood function: \\(\\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta)\\). Take the natural logarithm of \\(\\mathcal{L}\\): \\(\\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta)\\). Compute the first derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) with respect to \\(\\theta\\). If \\(\\theta\\) represents more than one parameter, take the first partial derivative with respect to the parameter of interest. Set \\(\\ell&#39;(\\theta \\vert \\mathbf{x}) = 0\\). Solve for \\(\\theta\\). The solution is \\(\\hat{\\theta}_{MLE}\\), assuming that the second derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) is negative (i.e., concave down); otherwise, we have actually found a local minimum of the likelihood function. Note that the “simplifying twist” is transforming the likelihood \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) to the log-likelihood \\(\\ell(\\theta \\vert \\mathbf{x})\\); differentiating the latter is often, if not always, easier than differentiating the former. But even if we skip step 2 and compute the first derivative of \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) directly, we will eventually get the same expression for \\(\\hat{\\theta}_{MLE}\\). 1.14.1 Comparing Two Estimators Let’s assume that we are given \\(n\\) iid data sampled from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta} \\] for \\(0 \\leq x \\leq \\theta\\), with \\(\\theta\\) unknown. The expected value and variance of this distribution are \\(\\mu = E[X] = \\theta/2\\) and \\(\\sigma^2 = V[X] = \\theta^2/12\\), respectively. We propose two estimators for \\(\\theta\\): \\(2\\bar{X}\\), and \\(X_1+X_2\\). Which is better? Intuitively, we know the answer, but can we quantify it, i.e., can we determine which of the two estimators has a smaller mean-squared error? For \\(\\hat{\\theta} = 2\\bar{X}\\), the expected value is \\[ E[2\\bar{X}] = 2E[\\bar{X}] = 2\\mu = \\theta \\,, \\] and thus we can see that \\(2\\bar{X}\\) is unbiased. (Here, we utilize the general result that \\(E[\\bar{X}] = \\mu\\).) Thus the MSE will simply be the variance of this estimator: \\[ MSE[\\hat{\\theta}] = V[2\\bar{X}] = 4V[\\bar{X}] = 4\\frac{\\sigma^2}{n} = \\frac{\\theta^2}{3n} \\,. \\] (Here, we utilize the general result that \\(V[\\bar{X}] = \\frac{\\sigma^2}{n}\\).) For \\(\\hat{\\theta} = X_1 + X_2\\), the expected value is \\[ E[X_1+X_2] = E[X_1] + E[X_2] = \\frac{\\theta}{2} + \\frac{\\theta}{2} = \\theta \\,. \\] The estimator is unbiased. The MSE is thus \\[ MSE[\\hat{\\theta}] = V[X_1+X_2] = V[X_1] + V[X_2] = \\frac{\\theta^2}{12} + \\frac{\\theta^2}{12} = \\frac{\\theta^2}{6} \\,. \\] Compare the two MSE expressions, keeping in mind that the second one is meaningless if \\(n=1\\). For \\(n=2\\) the MSEs are equivalent, which makes sense since the estimators themselves are equivalent. If \\(n &gt; 2\\), then the MSE for \\(\\hat{\\theta} = 2\\bar{X}\\), is smaller, and it will continue getting smaller as \\(n\\) increases, unlike the MSE for \\(\\hat{\\theta} = X_1 + X_2\\). 1.14.2 Maximum Likelihood Estimate of Population Parameter Let’s assume that we are given \\(n\\) iid data sampled from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta}\\exp({-x/\\theta}) \\,, \\] with \\(x \\geq 0\\) and \\(\\theta &gt; 0\\). (To be clear: in real-life situations, we do not know the form of the pmf or pdf from which the data are sampled! We assume a family of distributions, then estimate the value of the population parameter of interest.) For this distribution, \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2\\). The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\theta}\\exp\\left(-\\frac{x_i}{\\theta}\\right) = \\frac{1}{\\theta^n} \\prod_{i=1}^n \\exp\\left(-\\frac{x_i}{\\theta}\\right) = \\frac{1}{\\theta^n} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i\\right) \\,, \\] and the log-likelihood is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\left[ \\frac{1}{\\theta^n} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i\\right) \\right] = -n \\log \\theta - \\frac{1}{\\theta} \\sum_{i=1}^n x_i \\,. \\] The next step in determining \\(\\hat{\\theta}_{MLE}\\) is to take the first derivative with respect to \\(\\theta\\), \\[ \\ell&#39;(\\theta \\vert \\mathbf{x}) = \\frac{d}{d\\theta} \\ell(\\theta \\vert \\mathbf{x}) = -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^n x_i \\,, \\] and set the result equal to zero: \\[ -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^n x_i = 0 = -n + \\frac{1}{\\theta} \\sum_{i=1}^n x_i \\,. \\] Solving for \\(\\theta\\), we get \\[ \\hat{\\theta}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\,. \\] When we switched from solving for the generic quantity \\(\\theta\\) to solving for an estimator, which is a function of random variables, we switched from using the lower-case generic variable \\(x\\) to using an upper-case-denoted random variable \\(X\\). (Note that we should check to see whether the second derivative is negative at the extremum, indicating that \\(\\hat{\\theta}_{MLE}\\) is located at a local maximum of the likelihood function rather than a minimum. So: \\[ \\ell&#39;&#39;(\\theta \\vert \\mathbf{x}) = \\frac{d}{d\\theta} \\ell&#39;(\\theta \\vert \\mathbf{x}) = \\frac{n}{\\theta^2} - \\frac{2}{\\theta^3} \\sum_{i=1}^n x_i = \\frac{n}{\\theta^2} \\left( 1 - \\frac{2n}{\\theta}\\bar{x} \\right)\\,. \\] Let’s plug in \\(\\theta = \\hat{\\theta}_{MLE} = \\bar{x}\\): \\[ \\ell&#39;&#39;(\\hat{\\theta}_{MLE} \\vert \\mathbf{x}) = \\frac{n}{\\bar{x}^2} \\left( 1 - 2n \\right) \\,. \\] We know that \\(n\\) is positive and \\(\\geq 1\\) and that \\(\\bar{x} &gt; 0\\), so indeed \\(\\ell&#39;&#39;(\\hat{\\theta}_{MLE} \\vert \\mathbf{x}) &lt; 0\\) and thus we have detected a maximum of the likelihood function.) Now, is this estimate biased? We know from results shown above that \\(E[\\bar{X}] = \\theta\\), thus \\(B[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}] - \\theta = \\theta - \\theta = 0\\) and thus the estimator is unbiased. We also know that \\(V[\\bar{X}] = \\sigma^2/n = \\theta^2/n\\). The question, to be answered in a future chapter, is whether we can possibly find an estimator with a lower variance via some other estimation approach…or if this indeed the best that we can do. Now, when we solve for, e.g., \\(\\hat{\\theta}_{MLE}\\), we can solve for other quantities as well…it is just algebra. Meaning, for instance, that if we want to estimate \\(\\hat{\\theta}_{MLE}^2\\) for whatever reason, we can just square both sides in the solution above: \\[ (\\hat{\\theta}_{MLE})^2 = (\\bar{X})^2 ~~\\Rightarrow~~ \\hat{\\theta}_{MLE}^2 = \\bar{X}^2 \\,. \\] (This is a manifestation of the so-called invariance property of the MLE.) Now, is this a biased estimator for \\(\\theta^2\\)? We can utilize the shortcut formula for computing variance to find that \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{\\sigma^2}{n} + \\mu^2 = \\frac{\\theta^2}{n} + \\theta^2 = \\theta^2 \\left( 1 + \\frac{1}{n} \\right) \\neq \\theta^2 \\,. \\] This estimator is biased…but the bias goes away as the sample size \\(n\\) increases. That means that we would call this estimator asymptotically unbiased, i.e., it is unbiased when the sample size is infinite. We note here that maximum likelihood estimates are always either unbiased or asymptotically unbiased. 1.15 Statistical Inference with Sampling Distributions A fundamental issue with point estimation is that, e.g., it does not provide a notion of how uncertain an estimate is. By this, we do not mean the standard error of the sampling distribution for the statistic we use when making the estimate (which is a quantity that we can derive), but rather how large (or small) is the range of plausible values of \\(\\theta\\) given the observed value of the statistic? Point estimation also does not allow us to answer the question of “is a particular hypothesized value of \\(\\theta\\) plausible given what we observe?” We can resolve the first issue by computing a confidence interval for \\(\\theta\\), and the second via hypothesis testing. We discuss each of these concepts in turn in the next two sections. Before doing so, however, we will show how the construction of confidence intervals and the performance of hypothesis tests both boil down to performing a root-finding exercise in which we work directly with the sampling distribution. Let \\(Y = g(X_1,\\ldots,X_n)\\) be a statistic, with the \\(X_i\\)’s being independent and identically distributed (iid) random variables. (To be clear, \\(Y\\) does not have to be a function of all the observed data, but rather at least some of them. Think of the sample median in contrast to the sample mean.) As we describe above, \\(Y\\) has a sampling distribution, whose probability density function (pdf) is \\(f_Y(y \\vert \\theta)\\) and whose cumulative distribution function (cdf) is \\(F_Y(y \\vert \\theta)\\). (For simplicity, and without loss of generality, we assume \\(Y\\) is a continuous random variable.) We can alter the shape and/or location of the sampling distribution by change the value(s) of \\(\\theta\\). To illustrate this, let’s make up a sampling distribution pdf: \\[ f_Y(y \\vert \\theta) = \\frac{1}{2} ~~~ y \\in [\\theta-1,\\theta+1] \\] In Figure 1.30, we show how the pdf moves with \\(\\theta\\); as \\(\\theta\\) changes from 2.8 to 5.2, the pdf shifts (smoothly) from the location indicated by the red lines to that indicated by the blue lines. Now, let’s assume that the green line in the figure, at value \\(y_{\\rm obs} = 3.4\\), is the statistic value that we actually observe. What can we conclude right away, on the basis of this figure? We can conclude that this observed value is plausible if \\(\\theta = 2.8\\), but implausible if \\(\\theta = 5.2\\). In other words, 2.8 is an “acceptable” value for \\(\\theta\\), but 5.2 is not. Figure 1.30: An illustration of how a sampling distribution pdf can move as the distribution parameter is changed. Here, \\(f_Y(y) = 1/2\\) for \\(y \\in [\\theta-1,\\theta+1]\\); the red lines represent the pdf for \\(\\theta = 2.8\\) and the blue lines represents the pdf for \\(\\theta=5.2\\). The green vertical line represents the observed value of the statistic \\(Y\\): \\(y_{\\rm obs} = 3.4\\). To tie this illustration back to confidence intervals and hypothesis testing, a confidence interval for \\(\\theta\\) is a range of “acceptable values” for \\(\\theta\\) given what we observe (which initially we can think of here as \\([\\hat{\\theta}_L,\\hat{\\theta}_U] = [2.4,4.4]\\), since these values generate pdfs that overlap \\(y_{\\rm obs}\\)), and a hypothesis test is the determination of whether a stated value of \\(\\theta\\) is “acceptable” given what we observe (here, a hypothesis that \\(\\theta = 3.1\\) would be acceptable, while a hypothesis that \\(\\theta = 10\\) definitely would not be). Let’s generalize this a bit: we cannot, for instance, construct a confidence interval simply by asking if the sampling distribution pdf for our statistic overlaps the observed value, because given its domain, it may always overlap the observed value! (Albeit perhaps with \\(f_Y(y)\\) having a very small value.) So, by convention, we adopt a value \\(\\alpha\\) and ask the following questions for a two-sided interval: for what value of \\(\\theta\\) is \\(\\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta) = \\alpha/2\\) (in other words, for what value of \\(\\theta\\) is \\(F_Y(y_{\\rm obs} \\vert \\theta) = \\alpha/2\\)); and for what value of \\(\\theta\\) is \\(\\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta) = \\alpha/2\\) (or is \\(F_Y(y_{\\rm obs} \\vert \\theta) = 1 - \\alpha/2\\))? Figure 1.31: Given the setting defined in Figure 1.30, we can ask (a) for what value of \\(\\theta\\) is the area under \\(f_Y(y)\\) to the left of \\(y_{\\rm obs}\\) equal to \\(\\alpha/2\\), and (b) for what other value of \\(\\theta\\) is the area under \\(f_Y(y)\\) to the right of \\(y_{\\rm obs}\\) equal to \\(\\alpha/2\\)? These two values comprise a two-sided confidence interval. Here, \\(\\alpha = 0.1\\). For \\(y_{\\rm obs} = 3.4\\), the two values of \\(\\theta\\) are 2.5 and 4.3. (See Figure 1.31.) And here we come back to the notion that this is a “root-finding exercise.” To answer the first bullet-pointed question above, we would find the root for the equation \\[ F_Y(y_{\\rm obs} \\vert \\theta) - \\frac{\\alpha}{2} = 0 \\,. \\] Let’s denote the root \\(\\hat{\\theta}\\). As we can see in the left panel of the figure, \\(\\hat{\\theta}\\) represents the upper bound on \\(\\theta\\), but in general that is not always going to be true; in the next section we’ll dig more into the details of how we determine whether or not a given \\(\\hat{\\theta}\\) is actually a lower bound or an upper bound on \\(\\theta\\). (See Figure 1.32 for an alternative illustration showing what is happening when we are “finding roots.”) To answer the second question, we would find the root for the equation \\[ F_Y(y_{\\rm obs} \\vert \\theta) - \\left(1-\\frac{\\alpha}{2}\\right) = 0 \\,. \\] Referring back to the right panel of the figure, we see that here, the root \\(\\hat{\\theta}\\) represents the lower bound on \\(\\theta\\). We will note here that in relatively rare circumstances, we can solve for the roots by hand, but as we will see in examples sprinkled throughout the rest of the book, we can always solve for the roots numerically using, e.g., R’s uniroot() function. Figure 1.32: In this alternative illustration to Figure 1.31, we show how the interval bounds are found by changing the value of \\(\\theta\\) until the cumulative distribution function \\(F_Y(y_{\\rm obs} \\vert \\theta) = \\alpha/2 = 0.05\\) (on the left, for \\(\\theta = 4.3\\), given that \\(y_{\\rm obs} = 3.4\\)) and until \\(F_Y(y_{\\rm obs} \\vert \\theta) = \\alpha/2 = 0.95\\) (on the right, for \\(\\theta = 2.5\\)). Now, what about hypothesis testing? As we will see below, in a hypothesis test, we adopt a null hypothesis (\\(\\theta = \\theta_o\\)) and an alternative hypothesis (e.g., \\(\\theta \\neq \\theta_o\\)), and we combine this information with \\(y_{\\rm obs}\\) to determine whether the null hypothesis is viable (in which case we “fail to reject the null”) or not (in which case we “reject the null”). For a two-tail test, we would determine the boundaries of the so-called rejection region by finding the roots \\(y_{\\rm RR,1}\\) and \\(y_{\\rm RR,2}\\) for each of the following equations \\[\\begin{align*} F_Y(y_{\\rm RR,1} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 ~~\\Rightarrow~~ y_{\\rm RR,1} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o) \\\\ F_Y(y_{\\rm RR,2} \\vert \\theta_o) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 ~~\\Rightarrow~~ y_{\\rm RR,2} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\,, \\end{align*}\\] where \\(F_Y^{-1}(\\cdot)\\) is the inverse cdf function associated with the sampling distribution. Here, if \\(y_{\\rm obs}\\) is smaller than the lesser of \\(y_{\\rm RR,1}\\) and \\(y_{\\rm RR,2}\\), or larger than the greater of the two, we would reject the null, i.e., we would deem \\(\\theta_o\\) implausible. Note how the equations above are the same ones we use when constructing confidence intervals; here we fix \\(\\theta = \\theta_o\\) (as opposed to solving for \\(\\theta\\)) and we solve for \\(y\\) (as opposed to fixing \\(y = y_{\\rm obs}\\)). As is the case for confidence intervals, sometimes we can find the roots by hand, but if not, we can often find them by using off-the-shelf inverse cdf codes like R’s qnorm() (if the sampling distribution is a normal distribution), etc. An important message to take away from this section is that confidences interval estimation and hypothesis testing both involve the same fundamental root-finding exercise: they only differ in terms of what values are fixed and what values we are solving for! 1.16 Confidence Intervals Interval estimation is a mechanism to determine a range of possible values for a distribution parameter \\(\\theta\\) that overlaps the true but unknown value with some stated probability. A two-sided interval estimate, or confidence interval, has the form \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the user-set confidence coefficient, which typically has the value 0.95 (or \\(\\alpha = 0.05\\)). One can also define one-sided intervals: \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\right) = 1 - \\alpha ~~\\mbox{and}~~ P\\left( \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,. \\] It is important for us to interpret an interval estimate correctly: it is a random interval, meaning that if we re-do the data-generating experiment, the interval will change; and we state that the probability that, e.g., the two-sided interval estimate \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) will overlap \\(\\theta\\) is \\(1 - \\alpha\\). In particular, one does not say “the probability that \\(\\theta\\) lies in the stated interval is \\(1-\\alpha\\).” \\(\\theta\\) is a population quantity, and thus we cannot make probabilistic statements about it. It has the (unknown) value it has. Stated another way, in reference to point 2: the probability of \\(1 - \\alpha\\) refers to the reliability of the estimation procedure and not to any one specific interval. For instance, if we compute an interval of [1,2] and then re-do the experiment and compute an interval of [2.5,3.5], and if we say there is a 95% chance that \\(\\theta\\) is in [1,2] and a 95% chance it is in [2.5,3.5], then there must be a 190% chance it is in either. A computed interval either overlaps the true value, or it does not; it is no longer a matter of probability. See Figure 1.33. Figure 1.33: Schematic illustration of ten confidence intervals constructed given ten separate independent datasets sampled from the same underlying population. We indicate the true parameter value \\(\\theta\\) with the vertical dashed line. Counting from the bottom, we observe that the fourth and seventh intervals do not overlap the true value; if we were to claim that the probability that \\(\\theta\\) lies within each interval is, e.g., 0.95, then the probability that \\(\\theta\\) lies in either the fourth or seventh interval is 1.9…which is clearly wrong. Thus the proper interpretation would be that 95 percent of evaluated intervals overlap the true value. As laid out in the last section above, to determine a confidence interval bound, one would determine the root of the following equation: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta) - q &amp;= 0 \\,. \\end{align*}\\] There is a very important nuance here, however, that we ignored in the last section, which is that it is not guaranteed that the value of the statistic \\(Y\\) that we adopt to construct the interval increases as \\(\\theta\\) increases. What that means here is that if, e.g., \\(q = \\alpha/2\\), then the root \\(\\hat{\\theta}\\) might represent an upper bound on \\(\\theta\\)…but it might represent a lower bound instead. It all comes down to whether the expected value \\(E[Y]\\) increases as \\(\\theta\\) increases, or decreases as \\(\\theta\\) increases, as we detail in the table below: Interval \\(E[Y]\\) Increases with \\(\\theta\\)? \\(q\\) for Lower Bound \\(q\\) for Upper Bound two-sided yes \\(1-\\alpha/2\\) \\(\\alpha/2\\) no \\(\\alpha/2\\) \\(1-\\alpha/2\\) one-sided lower yes \\(1-\\alpha\\) \\(-\\) no \\(\\alpha\\) \\(-\\) one-sided upper yes \\(-\\) \\(\\alpha\\) no \\(-\\) \\(1-\\alpha\\) This is the confidence interval reference table that we will refer back to throughout the rest of book. To be clear: this is not a table to memorize! Before we turn to examples, it is important to put the methodology we outline here for deriving interval bounds into historical context. In traditional calculus-based probability and statistical inference classes, the focus has been on introducing techniques for constructing confidence intervals analytically, but they are really only analytical up to a point: one always has to turn to statistical tables to derive final numerical answers. An example of such a technique is the pivotal method, which for illustrative purposes we discuss in an example in Chapter 2. However, the pivotal method is not generally applicable across all families of distributions, and thus other methods exist as well, methods that are not as commonly seen in introductory classes (see, e.g., Chapter 9 of Casella and Berger 2002). Our methodology is that described in section 9.2.3 of Casella and Berger, entitled “Pivoting the CDF,” where it is noted that “even if [the equations \\(F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) = 1 - \\alpha/2\\) and \\(F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) = \\alpha/2\\)] cannot be solved analytically, we really only need to solve them numerically since the proof that we have a \\(1-\\alpha\\) confidence interval [does] not require an analytic solution.” This is key: we can use this technique to solve for intervals bounds by hand, or by writing code when we cannot solve for the bounds by hand (as we will see beginning in Chapter 2). Our methodology is thus generally applicable and is thus the only one that a student ultimately needs to learn. 1.16.1 Confidence Interval Where \\(E[Y]\\) Increases With \\(\\theta\\) We have conducted an experiment in which we sample \\(n=1\\) datum from the following pdf: \\[ f_X(x) = \\theta x^{\\theta-1} ~~~ x \\in [0,1] \\,. \\] The value we observe is \\(x_{\\rm obs}\\). Below, we define a two-sided confidence interval on \\(\\theta\\) assuming \\(\\alpha = 0.05\\), as well as show how we would define either a lower or an upper bound on \\(\\theta\\). Before proceeding, we will make a notation change by saying our statistic \\(Y\\) equals \\(X\\) (because, with a single datum, what else could our statistic realistically be?), and thus the observed value is \\(y_{\\rm obs} = x_{\\rm obs}\\). (This makes our random variable notation consistent with that of the last section.) The sampling distribution pdf is \\(f_Y(y) = \\theta y^{\\theta-1}\\) and the cdf is \\[ F_Y(y) = \\int_0^y \\theta z^{\\theta-1} dz = \\left. z^\\theta \\right|_0^y = y^\\theta \\,. \\] Now, does \\(E[Y]\\) increase with \\(\\theta\\)? \\[\\begin{align*} E[Y] = \\int_0^1 y f_Y(y) dy &amp;= \\int_0^1 y \\theta y^{\\theta-1} dy \\\\ &amp;= \\theta \\int_0^1 y^{\\theta} dy \\\\ &amp;= \\frac{\\theta}{\\theta+1} \\left. y^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,. \\end{align*}\\] As \\(\\theta \\rightarrow \\infty\\), \\(E[Y]\\) increases towards 1. The confidence interval reference table thus tells us that for the lower bound, \\(q = 1-\\alpha/2\\), while for the upper bound, \\(q = \\alpha/2\\). For the lower bound, we find that \\[\\begin{align*} y_{\\rm obs}^{\\hat{\\theta}_L} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L \\log y_{\\rm obs} &amp;= \\log \\left( 1 - \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L &amp;= \\frac{\\log \\left( 1 - \\frac{\\alpha}{2} \\right)}{\\log y_{\\rm obs}} \\,, \\end{align*}\\] while for the upper bound, we simply switch \\(1-\\alpha/2\\) to \\(\\alpha/2\\): \\[ \\hat{\\theta}_U = \\frac{\\log \\left( \\frac{\\alpha}{2} \\right)}{\\log y_{\\rm obs}} \\,, \\] For instance, for \\(\\alpha = 0.05\\) and \\(y_{\\rm obs}\\) = 0.6, the confidence interval would be \\(\\hat{\\theta}_L = 0.05\\) to \\(\\hat{\\theta}_U = 7.22\\). Not surprisingly, if we have but a single datum, we cannot say that much about \\(\\theta\\)! We can also solve for a “95% upper bound,” wherein we use the same upper bound equation as above but substitute \\(\\alpha\\) for \\(\\alpha/2\\): \\[ \\hat{\\theta}_U = \\frac{\\log \\alpha}{\\log y_{\\rm obs}} \\,, \\] The one-sided interval would be \\((-\\infty,5.86]\\) (although because \\(\\theta\\) is limited to be \\(&gt; 0\\), we would actually write \\((0,5.86]\\)). The one-sided lower bound is then \\[ \\hat{\\theta}_L = \\frac{\\log (1 - \\alpha)}{\\log y_{\\rm obs}} \\,, \\] which in our example evaluates to \\([0.10,\\infty)\\). 1.16.2 Confidence Interval Where \\(E[Y]\\) Decreases With \\(\\theta\\) Let’s switch things up a bit from the previous example, and assume that instead of sampling a single datum from the pdf \\(\\theta x^{\\theta-1}\\), we instead sample one from the pdf \\[ f_X(x) = \\theta (1-x)^{\\theta-1} \\,, \\] where \\(x \\in [0,1]\\) and the cdf is \\[\\begin{align*} F_X(x) &amp;= \\int_0^x \\theta (1-y)^{\\theta-1} dy \\\\ &amp;= \\left. -(1-y)^\\theta \\right|_0^x = 1 - (1-x)^\\theta \\,. \\end{align*}\\] How does this change our confidence interval computations? We first compute the expected value (after changing the notation from \\(x\\) to \\(y\\) to be consistent about how we denote statistics). This computation is a bit more complicated to do, as it involves integration by parts, but it is still relatively straightforward: \\[\\begin{align*} E[Y] = \\int_0^1 y f_Y(y) dy &amp;= \\int_0^1 y \\theta (1-y)^{\\theta-1} dy \\\\ &amp;= \\left. -y(1-y)^\\theta \\right|_0^1 + \\int_0^1 (1-y)^\\theta dy \\\\ &amp;= 0 - \\frac{1}{\\theta+1} \\left. (1-y)^{\\theta+1} \\right|_0^1 = \\frac{1}{\\theta+1} \\,. \\end{align*}\\] As \\(\\theta\\) increases, \\(E[Y]\\) decreases. Thus, unlike in the last example, \\(q = \\alpha/2\\) will map to the interval lower bound, while \\(q = 1-\\alpha/2\\) will map to the upper bound. The calculations then proceed in an analogous manner to those in the last example: \\[\\begin{align*} 1-(1-y_{\\rm obs})^{\\hat{\\theta}_L} &amp;= \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ (1-y_{\\rm obs})^{\\hat{\\theta}_L} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L \\log (1-y_{\\rm obs}) &amp;= \\log \\left( 1 - \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L &amp;= \\frac{\\log \\left( 1 - \\frac{\\alpha}{2} \\right)}{\\log (1-y_{\\rm obs})} \\,, \\end{align*}\\] and \\[\\begin{align*} 1-(1-y_{\\rm obs})^{\\hat{\\theta}_U} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ (1-y_{\\rm obs})^{\\hat{\\theta}_U} &amp;= \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_U \\log (1-y_{\\rm obs}) &amp;= \\log \\left( \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_U &amp;= \\frac{\\log \\left( \\frac{\\alpha}{2} \\right)}{\\log (1-y_{\\rm obs})} \\,. \\end{align*}\\] For instance, if \\(\\alpha = 0.05\\) and \\(y_{\\rm obs} = 0.6\\), the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_U] = [0.028,4.03]\\). We can also construct one-sided intervals, as we do above. We leave the computation of these intervals to the reader, noting that the 95% upper bound is 3.27 and the 95% lower bound is 0.056. 1.17 Hypothesis Testing Hypothesis testing is, in short, an inference mechanism that takes a pre-conceived notion about \\(\\theta\\) into account. This is in contrast to both point estimation and interval estimation, where we derive \\(\\hat{\\theta}\\) or \\((\\hat{\\theta}_L,\\hat{\\theta}_U)\\) without regard to what we might think the true value of \\(\\theta\\) might be. Perhaps we have randomly sampled a set of students and recorded their weights. Point estimation provides an estimate of the average student weight; interval estimation provides an interval that has probability \\(1-\\alpha\\) of overlapping the true student weight; but hypothesis testing allows one to ask questions like “do my data support my hypothesis that the true average weight of students on campus is 140 pounds?” The steps of basic hypothesis testing are as follows. We formulate a null hypothesis, denoted \\(H_o\\) (“H naught”), in which we specify the value we are testing. Here, that hypothesis might be \\(H_o: \\mu = 140\\). We formulate an alternate or alternative hypothesis, denoted \\(H_a\\) (“H a”). This also takes our pre-conceived notions into account: do we wish to test whether the average student weight is higher, lower, or simply substantially different from \\(H_o\\)? (So here, the possibilities are \\(H_a: \\mu &lt; 140\\), \\(H_a: \\mu &gt; 140\\), or \\(H_a: \\mu \\neq 140\\).) We choose a test statistic (e.g., \\(\\bar{X}\\)) whose sampling distribution we can specify assuming \\(H_o\\) is true. We choose the level of the test \\(\\alpha\\). Combined with the sampling distribution for the test statistic, this allows us to determine a rejection region: if the test statistic falls into this region, we reject the null hypothesis…otherwise, we “fail to reject the null.” There is much to unpack here. We will start by stating that it is imperative that one specify \\(H_o\\), \\(H_a\\), and \\(\\alpha\\) before any data are collected (or at the very least before any data are examined). To look at the data first and then formulate hypotheses and set levels acts to bias the process: it is a manifestation of human nature that we might be tempted to define the test so as to maximize our chances observing the result we desire to achieve. In the end, testing is about assessing pre-conceived notions; actively working to achieve a particular result should never be our aim. Another thing to keep in mind is that hypothesis testing generates decisions, not proofs. When we formulate a test, we are defining a decision boundary: we decide whether we have sufficient evidence to reject the null. When we do not, we fail to reject the null, i.e., we do not say that we have proven that the null is correct, but rather we have concluded that we simply do not have enough data to convince ourselves that the null hypothesis is wrong. And the decisions we make can be wrong! This leads to some more important hypothesis-testing terminology: Type I error: this is the probability that we will decide to reject the null when it is actually true. We set this…this is \\(\\alpha\\) (the level of the test). A conventional choice for \\(\\alpha\\) is 0.05: if the null is true, we have a 1 in 20 chance of erroneously rejecting it. Type II error: this is the probability that we would fail to reject the null given an arbitrary value of \\(\\theta\\); it is denoted \\(\\beta\\) (or perhaps more clearly \\(\\beta(\\theta,\\alpha)\\), since the value is a function of both \\(\\theta\\) and \\(\\alpha\\)). Now that we have laid out the basic framework and terminology, we need to illustrate how hypothesis testing works in practice. But we have the same issue here that we have when constructing confidence intervals in the last section: we do not yet have the tools to derive the sampling distribution for the test statistic. (We introduce these later, beginning in Chapter 2.) So, for now, we illustrate hypothesis testing by assuming that we sample a single datum \\(X\\) from a pdf (which is, by definition, the sampling distribution for our test statistic \\(Y = X\\)). Let’s assume that our statistic \\(Y\\) is drawn from the following pdf: \\[ f_Y(y) = \\frac{1}{\\theta} \\exp\\left(-\\frac{y}{\\theta}\\right) \\,, \\] where \\(\\theta &gt; 0\\) and \\(x \\geq 0\\). (See Figure 1.34.) Let’s further assume that our null hypothesis is \\(\\theta_o = 2\\). To reiterate: if the null hypothesis holds, we expect to observe a statistic \\(y_{\\rm obs}\\) that is consistent with this distribution. The question now is: how close or far from zero does \\(y_{\\rm obs}\\) have to be for us to decide to reject the null? Figure 1.34: Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for \\(\\alpha = 0.1\\). Each curve represents the sampling distribution for the hypothesis test statistic \\(Y\\), given that the null hypothesis is true. If \\(y_{\\rm obs}\\) falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null. In the section before last, we discuss how we would answer this question for a two-tail hypothesis test: we determine the roots \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) that are solutions to the following equations: \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 ~~\\Rightarrow~~ y_{\\alpha/2} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o) \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 ~~\\Rightarrow~~ y_{1-\\alpha/2} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\,, \\end{align*}\\] where \\(F_Y^{-1}(\\cdot)\\) is the inverse cdf function associated with the sampling distribution. Both \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) are rejection region boundaries: if \\(y_{\\rm obs} &lt; y_{\\alpha/2}\\) or \\(y_{\\rm obs} &gt; y_{1-\\alpha/2}\\), we make the decision to reject the null. For the example we are working with here: \\[ F_Y(y) = \\int_0^y \\frac{1}{\\theta} \\exp\\left(-\\frac{z}{\\theta}\\right) dz = -\\exp\\left.\\left(-\\frac{z}{\\theta}\\right)\\right|_0^y = 1 - \\exp\\left(-\\frac{y}{\\theta}\\right) \\,, \\] and so \\[ 1 - \\exp\\left(-\\frac{y_{\\alpha/2}}{\\theta_o}\\right) = \\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ y_{\\alpha/2} = -\\theta_o \\log\\left(1-\\frac{\\alpha}{2}\\right) \\,, \\] and \\[ 1 - \\exp\\left(-\\frac{y_{1-\\alpha/2}}{\\theta_o}\\right) = 1-\\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ y_{1-\\alpha/2} = -\\theta_o \\log\\left(\\frac{\\alpha}{2}\\right) \\,. \\] For \\(\\theta_o = 2\\) and \\(\\alpha = 0.05\\), the rejection region bounds are \\(y_{\\alpha/2} = -2\\log(0.975) = 0.051\\) and \\(y_{1-\\alpha/2} = -2\\log(0.025) = 7.378\\). In other words, if the value we sample is \\(&lt; 0.051\\) or \\(&gt; 7.378\\), we decide to reject the null hypothesis that \\(\\theta = 2\\). If we wish to perform a lower- or upper-tail test instead, we need to tread a bit more carefully, in that we need determine how \\(E[Y]\\) varies with \\(\\theta\\): does it increase as \\(\\theta\\) increases, or does it decrease? This dictates whether \\(y_{\\alpha}\\), for instance, is the correct rejection region bound (as opposed to \\(y_{1-\\alpha}\\)). For our current example, \\(E[Y] = \\theta\\), which obviously increases with \\(\\theta\\)…and so \\(y_{\\alpha}\\) is appropriate for lower-tail tests and \\(y_{1-\\alpha}\\) is appropriate for upper-tail tests. Our rejection regions are \\[\\begin{align*} \\mbox{lower-tail:} &amp;~~~ y_{\\alpha} = -\\theta_o \\log(1-\\alpha) = -2 \\log(0.95) = 0.103 \\\\ \\mbox{upper-tail:} &amp;~~~ y_{1-\\alpha} = -\\theta_o \\log(\\alpha) = -2 \\log(0.05) = 5.991 \\,. \\end{align*}\\] We summarize the evaluation of rejection region boundaries in the table below. Note that as was the case with the confidence interval reference table, the contents of this first hypothesis test reference table are not to be memorized. Type \\(E[Y]\\) increases with \\(\\theta\\)? Rejection Region Boundary Reject If… two-tail yes \\(y_{\\rm RR,lo} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o)\\) \\(y_{\\rm obs} &lt; y_{\\rm RR,lo}\\) \\(y_{\\rm RR,hi} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\) \\(y_{\\rm obs} &gt; y_{\\rm RR,hi}\\) no \\(y_{\\rm RR,lo} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\) \\(y_{\\rm obs} &lt; y_{\\rm RR,lo}\\) \\(y_{\\rm RR,hi} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o)\\) \\(y_{\\rm obs} &gt; y_{\\rm RR,hi}\\) lower-tail yes \\(y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\theta_o)\\) \\(y_{\\rm obs} &lt; y_{\\rm RR}\\) no \\(y_{\\rm RR} = F_Y^{-1}(1-\\alpha \\vert \\theta_o)\\) \\(y_{\\rm obs} &gt; y_{\\rm RR}\\) upper-tail yes \\(y_{\\rm RR} = F_Y^{-1}(1-\\alpha \\vert \\theta_o)\\) \\(y_{\\rm obs} &gt; y_{\\rm RR}\\) no \\(y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\theta_o)\\) \\(y_{\\rm obs} &lt; y_{\\rm RR}\\) Note that “lower” and “upper” refer to being less than \\(\\theta_o\\) or greater than \\(\\theta_o\\), and not necessarily to the tail of the sampling distribution itself that contains the rejection region. (For instance, if \\(E[Y]\\) decreases with \\(\\theta\\), then the upper tail for \\(\\theta\\) would map to the lower tail for \\(Y\\) itself.) Also, we note the following. If we reject the null using a one-tail test, for a given dataset, it is not guaranteed that we would have rejected the null using a two-tail test. For instance, for our example, a value \\(y_{\\rm obs} = 0.075\\) would lead us to reject the null for a lower-tail test, but fail to reject the null for a two-tail test. If we reject the null using a two-tail test, it is also not guaranteed that we would have rejected the null using a one-tail test. For instance, a value of \\(y_{\\rm obs} = 8\\) would lead us to reject the null if we perform a two-tail test, but fail to reject the null if we perform a lower-tail test. 1.17.1 Hypothesis Test Where \\(E[Y]\\) Increases With \\(\\theta\\) We repeat the first example in the confidence interval section above, in which \\(f_X(x) = \\theta x^{\\theta-1}\\) for \\(x \\in [0,1]\\), and in which we sample a single datum (and thus assume that our test statistic is \\(Y = X\\), with sampling distribution given by \\(f_X(x)\\)). Let’s assume our null hypothesis is \\(H_o : \\theta = \\theta_o = 1\\), and that we are testing this against the alternative \\(H_a : \\theta &lt; \\theta_o\\). What is the rejection region for this test, assuming \\(\\alpha = 0.05\\)? We start by reminding ourselves that \\(E[Y] = \\theta/(\\theta+1)\\) and that it increases with \\(\\theta\\). Given this information, we examine the table above and see that the rejection region boundary is \\[ y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\theta_o) \\,. \\] Stated another way, the boundary is the solution to the equation \\[ F_Y(y \\vert \\theta_o) - \\alpha = 0 \\,. \\] It is important to point out here, before continuing, that the numerical evaluation of rejection region boundaries only depends on \\(\\theta_o\\), and specifically not on the observed statistic value. (Where the regions are, and whether there are one or two, is a function of the alternative hypothesis, but not the numerical evaluation of the boundaries themselves.) The cdf for \\(Y\\) is \\(F_Y(y) = y^\\theta\\). Hence \\[\\begin{align*} F_Y(y_{\\alpha} \\vert \\theta_o) - \\alpha &amp;= 0 \\\\ \\Rightarrow ~~~ y_{\\alpha}^{\\theta_o} &amp;= \\alpha \\\\ \\Rightarrow ~~~ y_{\\alpha} &amp;= \\alpha^{1/\\theta_o} = \\alpha \\,. \\end{align*}\\] Thus if we observe \\(y_{\\rm obs} = x &lt; \\alpha = 0.05\\), we reject the null hypothesis. 1.17.2 Hypothesis Test Where \\(E[Y]\\) Decreases With \\(\\theta\\) Here we repeat the second example in the confidence interval section above, in which \\(f_X(x) = \\theta (1-x)^{\\theta - 1}\\) for \\(x \\in [0,1]\\), and in which we again sample a single datum. Here, we assume the null hypothesis \\(H_o : \\theta = \\theta_o = 2\\), and will we test this against the alternative \\(H_a : \\theta &lt; \\theta_o\\). We know that \\(E[Y] = 1/(\\theta+1)\\), which decreases with \\(\\theta\\). Thus we define the rejection region boundary as \\[ y_{\\rm RR} = F_Y^{-1}(1-\\alpha \\vert \\theta_o) \\,. \\] We know that \\(F_Y(y) = 1 - (1-y)^\\theta\\). So \\[\\begin{align*} F_Y(y_{1-\\alpha} \\vert \\theta_o) - (1-\\alpha) &amp;= 0 \\\\ \\Rightarrow ~~~ 1 - (1-y_{1-\\alpha})^{\\theta_o} &amp;= 1-\\alpha \\\\ \\Rightarrow ~~~ (1-y_{1-\\alpha})^{\\theta_o} &amp;= \\alpha \\\\ \\Rightarrow ~~~ y_{1-\\alpha} &amp;= 1 - \\alpha^{1/\\theta_o} = 1 - \\alpha^{1/2} \\,. \\end{align*}\\] Thus if we observe \\(y_{\\rm obs} = x &gt; 1 - \\alpha^{1/2} = 1 - 0.05^{1/2} = 0.776\\), we reject the null hypothesis. 1.18 Working With R: Simulating Statistical Inference We wrap up this chapter by discussing how one might simulate aspects of statistical inference. For instance, we might want to know, e.g., the empirical distribution of a maximum likelihood estimator \\(\\hat{\\theta}_{MLE}\\), from which we can estimate the standard error or the mean-squared error; whether a proposed confidence interval has its advertised coverage; and whether a proposed hypothesis test has its advertised Type I error, etc. All of these examples utilize the same basic algorithm, which we render in pseudocode as set random number seed initialize vector(s) for storing result(s) begin for-loop sample dataset compute statistic compute and store result end for-loop optional: process result vector display result This builds upon the last “Working With R” section, which describes dataset sampling. Note that we don’t always need to follow this pseudocoded algorithm to the letter, as R’s vectorization facility plus its apply() function can help us both eliminate the for loop. We illustrate these shortcuts in the examples below. 1.18.1 Estimating the Sampling Distribution for the Sample Median The median of a data sample is defined as the middle value of the sorted data: if we have \\(n=5\\) data, for instance, the median is \\(x_{(3)}\\). (Recall that the parentheses indicating that we are examining sorted data, with \\(x_{(1)}\\) and \\(x_{(n)}\\) being the smallest and largest observed values.) Here, we show how to simulate the distribution of a sample median. In Chapter 3, we show how we can attempt to derive distributions related to sorted data mathematically. Let’s assume that we draw data according to the following distribution: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] To simulate the distribution of the sample median, for a given sample size \\(n\\), we repeatedly sample datasets (here, we use our rejection sampling code defined earlier in this chapter) and we record the median value for each. In the code chunk below, we show the estimated mean and standard error for our distribution, while in Figure 1.35 we show the empirical distribution of the median values. set.seed(101) # Let&#39;s put the rejection sampling code inside a function. sample_data &lt;- function(n,x.lo,x.hi,f.x.hi) { X &lt;- rep(NA,n) ii &lt;- 0 while ( ii &lt; n ) { u &lt;- runif(1,min=x.lo,max=x.hi) v &lt;- runif(1,min=0,max=f.x.hi) if ( v &lt; u*sin(u)/pi ) { ii &lt;- ii+1 X[ii] &lt;- u } } return(X) } num.sim &lt;- 10000 # number of medians to record n &lt;- 11 x.lo &lt;- 0 x.hi &lt;- pi f.x.hi &lt;- 0.58 X.median &lt;- rep(NA,num.sim) # initialize vector to store result for ( ii in 1:num.sim ) { X &lt;- sample_data(n,x.lo,x.hi,f.x.hi) X.median[ii] &lt;- median(X) } # What do we want to do with the result? Here... # - we visualize the empirical distribution of medians with a histogram # - we process the median vector to estimate the mean and standard error ggplot(data=data.frame(X.median=X.median),aes(x=X.median,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(1,2.7,by=0.1)) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.35: The empirical distribution of the sample median for the pdf \\(f_X(x) = (x \\sin x)/\\pi\\), assuming a sample size of \\(n = 11\\). cat(&quot;The empirical mean is &quot;,round(mean(X.median),3),&quot;\\n&quot;) ## The empirical mean is 1.907 cat(&quot;The empirical standard error is &quot;,round(sd(X.median),3),&quot;\\n&quot;) ## The empirical standard error is 0.255 1.18.2 The Empirical Distribution of Maximum Likelihood Estimates To drive home the point that maximum likelihood estimates are random variables, we simulate the process of estimating \\(\\theta\\) for the pdf \\(f_X(x) = \\theta x^{\\theta-1}\\), where \\(x \\in [0,1]\\). Let’s write down the (log-)likelihood: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n \\theta x_i^{\\theta-1} \\\\ &amp;= \\theta^n \\left( \\prod_{i=1}^n x_i \\right)^{\\theta - 1} \\\\ \\Rightarrow ~ \\ell(\\theta \\vert \\mathbf{x}) &amp;= n \\log \\theta + (\\theta-1)\\log \\left( \\prod_{i=1}^n x_i \\right) \\\\ &amp;= n \\log \\theta + (\\theta-1) \\sum_{i=1}^n \\log x_i \\,, \\end{align*}\\] and solve for the MLE: \\[ \\frac{d}{d\\theta} \\ell(\\theta \\vert \\mathbf{x}) = \\frac{n}{\\theta} + \\sum_{i=1}^n \\log x_i ~ \\Rightarrow ~ \\hat{\\theta}_{MLE} = -\\frac{n}{\\sum_{i=1}^n \\log x_i} \\,. \\] Given this expression, we can now determine the empirical distribution. See Figure 1.36. We observe immediately that the distribution is right-skewed. (As we will see in Chapter 2, as \\(n \\rightarrow \\infty\\), the empirical distribution will tend more and more to a bell-curve-like shape.) set.seed(101) theta &lt;- 2.25 n &lt;- 40 # Here, we show an alternative means by which to sample data. # (This assumes that there is a data-sampling code that we can easily call.) sample_data &lt;- function(n,theta) { q &lt;- runif(n,min=0,max=1) X &lt;- q^(1/theta) } num.sim &lt;- 1000 # first: create one long vector of data, of length num.sim * n X.all &lt;- sample_data(n*num.sim,theta) # second: &quot;fold&quot; the vector into a matrix with num.sim rows and n columns # every row of X is thus a separate independent dataset X &lt;- matrix(X.all,nrow=num.sim) # third: use R&#39;s apply() function to compute MLE for each row/dataset # the argument 1 means &quot;apply the following function to each row&quot; # x in the third argument is the data row; the function returns the mle # mle is thus a vector of maximum-likelihood estimates mle &lt;- apply(X,1,function(x){-n/sum(log(x))}) # What do we want to do with the result? Here... # - we visualize the empirical distribution of medians with a histogram # - we process the median vector to estimate the mean and standard error ggplot(data=data.frame(mle=mle),aes(x=mle,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(1.2,4.2,by=0.2)) + geom_vline(xintercept=2.25,col=&quot;red&quot;,lwd=1) + labs(x=expression(theta)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.36: The empirical distribution of maximum likelihood estimates for \\(\\theta\\) for the distribution \\(f_X(x) = \\theta x^{\\theta-1}\\) (\\(x \\in [0,1]\\)) with \\(\\theta = 2.25\\) (red line) and \\(n = 40\\). cat(&quot;The empirical mean is &quot;,round(mean(mle),3),&quot;\\n&quot;) ## The empirical mean is 2.317 cat(&quot;The empirical standard deviation is &quot;,round(sd(mle),3),&quot;\\n&quot;) ## The empirical standard deviation is 0.377 1.18.3 Empirically Verifying the Confidence Coefficient Value In the example above, where \\[ f_X(x) = \\theta x^{\\theta-1} \\,, \\] with \\(\\theta &gt; 0\\) and \\(x \\in [0,1]\\), we find the two-sided confidence interval \\[ \\left[ -\\frac{0.0253}{\\log y_{\\rm obs}} , -\\frac{3.689}{\\log y_{\\rm obs}} \\right] \\,, \\] where we assume a confidence coefficient of \\(\\alpha = 0.05\\). A question that naturally arises is: does this confidence interval actually overlap, or cover, the true value \\(\\theta\\) 95% of the time? We can check this with a relatively simple simulation. Let’s assume that the true value of \\(\\theta\\) is 1. Let’s also specify here that \\(f_X(x)\\) is part of the beta family of distributions, which we officially introduce in Chapter 3. (This allows us to utilize the R random variable sampling function rbeta() rather than having to create our own inverse-transform data sampler.) set.seed(101) num.sim &lt;- 10000 theta &lt;- 1 y.obs &lt;- rbeta(num.sim,theta,1) # R can do the sampling for us here hat.theta.L &lt;- -0.0253/log(y.obs) hat.theta.U &lt;- -3.689/log(y.obs) (coverage &lt;- sum(theta&gt;=hat.theta.L &amp; theta&lt;=hat.theta.U)/num.sim) ## [1] 0.9523 Before discussing the result, let’s talk about what is happening in the last line of code above. First of all, by surrounding this line with parentheses, we are telling R to not only assign a value to the variable coverage, but to print the value of coverage as well. As far as the argument passed to sum(): theta&gt;=hat.theta.L is a logical comparison, an implicit function call that returns TRUE if \\(\\theta \\geq \\hat{\\theta}_L\\) and FALSE otherwise. The ampersand &amp; is the logical AND operator that combines the results of the comparison on the left and the one on the right: TRUE &amp; TRUE returns TRUE, otherwise FALSE is returned. (This stands in contrast to the logical OR operator, |, which only returns FALSE for FALSE | FALSE.) So the long argument passed to sum() turns into a logical vector of TRUEs (if the true value is inside the bounds) and FALSEs (if the true value is outside the bounds). What happens when we pass a logical vector to sum()? It treats TRUE as 1 and FALSE as 0…in other words, sum() returns the number of TRUE values. We find that 9,523 of the 10,000 intervals overlap \\(\\theta = 1\\). Because we do not perform an infinite number of simulations, the coverage that we compute is a random variable: we do not expect the value to match 9500 exactly. Thus what we observe is very strong evidence that our interval construction algorithm has the coverage that we defined it to have. (Once we introduce the binomial distribution in Chapter 3, we can turn this “strong evidence” into something more firmly quantitative!) 1.18.4 Empirically Verifying the Type I Error \\(\\alpha\\) In the second example in the last section above, we determine if we sample a statistic \\(Y\\) from the distribution \\(f_Y(y) = \\theta (1-y)^{\\theta-1}\\), with \\(y \\in [0,1]\\), then if \\(y_{\\rm obs} \\geq 0.776\\), we reject the null hypothesis that \\(\\theta = 1\\). This threshold is based on our adoption of the Type I error value \\(\\alpha = 0.05\\): if the null is correct and we repeatedly randomly sample data from the sampling distribution \\(f_Y(y) = 2y\\), we should observe values of \\(y\\) larger than 0.776 only five percent of the time. Is this what we actually observe? As a reminder, we have identified \\(f_Y(y)\\) as being part of the beta family of distributions, which we officially introduce in Chapter 3; thus we can utilize rbeta() like we do in the last example above. set.seed(101) theta &lt;- 2 num.sim &lt;- 10000 y.obs &lt;- rbeta(num.sim,1,theta) (typeIerror &lt;- sum(y.obs&gt;(1-sqrt(0.05)))/num.sim) ## [1] 0.0508 We find that we make Type I errors 508 times, or that the empirical Type I error rate is 508/10,000 = 0.0508. Because we do not perform an infinite number of simulations, the coverage that we compute is a random variable: we do not expect the value to match 500 exactly. Thus what we observe is very strong evidence that our Type I error rate is what we expect it to be; in Chapter 3, we learn how to turn this evidence into something more quantitative!) 1.19 Exercises We are given that for the events \\(A\\) and \\(B\\), \\(P(A \\vert B) = P(B \\vert A)\\). We are also given that \\(P(\\bar{A} \\cap \\bar{B}) = 0.2\\). Express \\(P(A \\cap B)\\) as a function of \\(P(A)\\). We are given that \\(A \\subset B\\) and that \\(P(A) &gt; 0\\) and \\(0 &lt; P(B) &lt; 1\\). Can the events \\(A\\) and \\(B\\) be independent events? Re-express the conditional probability \\(P(A \\cap B \\vert A \\cup B)\\) in terms of two unconditional probabilities that reference only the events \\(\\bar{A}\\) and \\(\\bar{B}\\). We are given that \\(P(B) = 1/2\\), \\(P(B \\vert A) = 1/4\\), and \\(P(A \\vert \\bar{B}) = 1/3\\). (a) What is \\(P(A \\cap B)\\)? (b) What is \\(P(B \\vert \\bar{A})\\)? Re-express the probability \\(P(\\bar{A} \\vert \\bar{B})\\) as a function of \\(A\\) and \\(B\\) only. We draw two balls from an urn containing two white and three black balls, without replacement. What is the probability that the second drawn ball is black? A student answers a multiple-choice question with five possible answers. Assume the probability that the student knows the answer is 0.6 (and thus that the probability that the student will guess is 0.4). Also assume that the probability of getting the question right when guessing is 0.2. If the student gets the question right, what is the probability that he knew the answer? A study of the residents of a region showed that 15 percent were smokers. The probability of death due to lung cancer, given that a person smoked, was ten times the probability of death due to lung cancer, given that the person did not smoke. If the probability of death due to lung cancer in the region is 0.01, what is the probability that a person was a smoker, given that he or she died of lung cancer? We play a particular game in which we roll a three-sided die. (Assume one exists!) We repeatedly roll the die until we fail to roll a 1. Our score is the total sum of all rolls, including the last roll. We win the game if our score is greater than 3. What is the probability that we win a single instance of this game? (It may help to recall that if we have a sequence of events {\\(A,BA,BBA,...\\)}, with associated unchanging probabilities \\(P(A)\\) and \\(P(B)\\), then the sum of the event probabilities is \\(P(A)/[1-P(B)]\\).) We are given a coin with a memory. Let \\(H_i\\) and \\(T_i\\) denote the events of observing heads and tails on the \\(i^{\\rm th}\\) flip, respectively. Assume \\(P(H_1) = P(T_1) = 0.5\\), and that \\(P(H_{i+1} \\vert T_i) = 0.6\\) and \\(P(T_{i+1} \\vert H_i) = 0.3\\). Compute \\(P(H_1 \\cup T_2)\\). If a water specimen contains nitrates, a solution dropped into it will cause it to turn red 90 percent of the time. When used on water specimens without nitrates, the solution causes the water to turn red 15 percent of the time. Nitrates exist in 20 percent of the specimens. (a) What is the probability that a randomly selected specimen will turn red when tested? (b) What is the probability that a randomly selected specimen contains nitrates, if it turns red when tested? Of travelers arriving at a small airport, 60 percent fly on major airlines, 30 percent on private planes, and 10 percent on other aircraft. Of those on the major airlines, 50 percent travel for business, while 60 percent of the private plane takers are business travelers and 90 percent of those on other aircraft are business travelers. We select a traveler at random. (a) What is the probability that he or she is a business traveler? (b) What is the probability that he or she arrived on a private plane, given that he or she is a business traveler? We have an urn with four balls, two of which are red and two of which are black, and we sample without replacement from the urn. Let \\(X\\) be the number of draws that we make until the first red ball is drawn. (a) What is the expected value of \\(X\\)? (b) What is the variance of \\(X\\)? We possess three bowls that are labeled 1, 2, and 3. Bowl 1 contains one white and two black balls. Bowl 2 contains two white and one black balls. Bowl 3 contains three white balls. We select a bowl at random, and then sample two balls from that bowl, without replacement. (a) What is the probability of observing two white balls? (b) Given that we observe two white balls, what is the probability that they were sampled from bowl number 3? Two methods are available for learning a certain skill, method \\(A\\) and method \\(\\bar{A}\\). Method \\(A\\) is less expensive, and thus it is used 80 percent of the time (with method \\(\\bar{A}\\) being used the rest of the time). Let \\(F\\) be the event of failing to learn the skill: if method \\(A\\) is used, the failure rate is 20 percent, while for \\(\\bar{A}\\) the rate is 10 percent. (a) What is the probability of failure, \\(P(F)\\)? (b) What is the probability that a person who failed was taught using method \\(A\\)? Below we display a (partially known) probability mass function. We know that \\(E[X] = 1.6\\). What is \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma)\\)? \\(x\\) \\(p_X(x)\\) 0 ? 1 0.5 2 ? 3 0.3 We are given the following pdf: \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{cc} c/x^2 &amp; 1 \\leq x \\leq 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] What is \\(P(X &gt; 1.25 \\vert X &lt; 1.5)\\)? We are given the following pdf: \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{cc} c x^2 (1-x) &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] What is \\(c\\)? (b) What is \\(P(X &gt; 1/2)\\)? (c) What is \\(E[X]\\)? We flip a fair coin two times. Let \\(X_i\\) denote the result of flip \\(i\\): \\(X_i = 1\\) for heads, and \\(X_i = 0\\) for tails. Now, let \\(Y = \\vert X_2 - X_1 \\vert\\). (a) What is the standard deviation of \\(Y\\)? (b) What is \\(P(\\mu - \\sigma &lt; Y &lt; \\mu + \\sigma)\\)? We independently sample two data, \\(X_1\\) and \\(X_2\\), from the distribution \\[\\begin{align*} f_X(x) = 3x^2 ~~~ x \\in [0,1] \\,. \\end{align*}\\] (a) What is the mean of this distribution? (b) What is the variance of this distribution? (c) What is the mean of \\(2X_1-2X_2\\)? (d) What is the variance of \\(2X_1-2X_2\\)? Let the cumulative distribution function \\(F_X(x)\\) be given by \\[\\begin{align*} F_X(x) = \\left\\{ \\begin{array}{cc} 0 &amp; x &lt; 0 \\\\ x^3 &amp; 0 \\leq x \\leq 1 \\\\ 1 &amp; x &gt; 1 \\end{array} \\right. \\,. \\end{align*}\\] (a) Write down the probability density function. (b) Compute \\(V[X]\\). (c) What is the probability that an observed random variable \\(X\\) takes on a value less than 1/2, given that it is greater than 1/4? Below we display a cumulative density function for a discrete random variable. What is \\(V[X]\\)? \\(x &lt;\\) 0 \\(0 \\leq x &lt; 1\\) \\(1 \\leq x &lt; 3\\) \\(x \\geq 3\\) \\(F_X(x)\\) 0 0.5 0.75 1 We are given the following cumulative density function: \\[\\begin{align*} F_X(x) = \\left\\{ \\begin{array}{cc} cx^2 &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) What is the value of \\(c\\)? (b) What is \\(E[X]\\)? (c) What is \\(\\sigma\\)? We are given the following probability density function: \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{cc} a &amp; 0 \\leq x &lt; 1 \\\\ bx &amp; 1 \\leq x &lt; 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\end{align*}\\] where \\(a\\) and \\(b\\) are constants. (a) What is the largest value that \\(b\\) can have such that \\(f_X(x)\\) is a valid pdf? (b) Let \\(a\\) = 1/2 and \\(b\\) = 1/3. What is \\(F_X(x)\\)? See the probability mass function table below. (a) If \\(E[X] = 2\\), what is the value of \\(y\\)? (b) If \\(y = 3\\), what is \\(\\sigma\\) for this distribution? (c) Assume \\(y = 3\\). What is \\(P(\\mu-\\sigma \\leq X \\leq \\mu+\\sigma)\\)? \\(x\\) 0 1 \\(y\\) \\(p_X(x)\\) 0.4 0.4 0.2 We are given the following hybrid probability density/mass function: \\[\\begin{align*} g_X(x) = \\left\\{ \\begin{array}{ll} 0.2 &amp; x = -1 \\\\ c &amp; x \\in [0,1] \\\\ 0.2 &amp; x = 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) What is the value for \\(c\\)? (b) What is \\(E[X]\\)? (c) What is the cumulative distribution function \\(F_X(x)\\)? We are given the probability density function \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{ll} e^{-x} &amp; x \\in [0,\\infty) \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) What is the cdf for \\(x \\geq 0\\)? (b) What is \\(P(X &gt; 1)\\)? (c) What is \\(P(X &lt; 1/2 \\cup X &gt; 2)\\)? (d) What is the probability that \\(X &lt; 2\\), given that \\(X &gt; 1\\)? We are given the probability density function \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{ll} -x &amp; x \\in [-1,0) \\\\ x &amp; x \\in [0,1] \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) What is \\(E[X]\\)? (b) What is \\(V[X]\\)? (c) What is an expression for \\(F_X(x)\\) that is valid over the range \\(x \\in [0,1]\\)? We are given the following cumulative distribution function: \\[\\begin{align*} F_X(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x \\leq 0 \\\\ cx^2 &amp; x \\in (0,1] \\\\ 1 &amp; x &gt; 1 \\end{array} \\right. \\,. \\end{align*}\\] (a) What is the value of \\(c\\) that makes \\(f_X(x)\\) a valid pdf? (b) What is \\(E[X]\\)? We are given the following probability density function: \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{cc} c &amp; 0 \\leq x &lt; 1 \\\\ e^{-x} &amp; 1 \\leq x &lt; \\infty \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\end{align*}\\] where \\(c\\) is a constant. (a) What is the value of \\(c\\) that makes \\(f_X(x)\\) a valid pdf? (b) What is the cumulative distribution function? Below we display a probability mass function. (a) What is \\(c\\)? (b) What is the value of the cumulative distribution function at \\(x=0.5\\)? (c) What is the inverse cdf for \\(q = 0.9\\)? (d) What is the variance \\(V[X]\\)? \\(x\\) -1 0 1 \\(p_X(x)\\) 1/6 \\(c\\) 1/6 We are given the following cumulative density function: \\[\\begin{align*} F_X(x) = x^3 \\,, \\end{align*}\\] for \\(x \\in [0,1]\\). (a) What is the probability density function, \\(f_X(x)\\), for \\(x \\in [0,1]\\)? (b) What is \\(P(1/4 \\leq X \\leq 3/4)\\)? (c) What is \\(P(X \\leq 1/4 \\vert X \\leq 1/2)\\)? (d) If \\(q = F_X(x)\\), what is the inverse cdf, i.e., what is \\(F_X^{-1}(q)\\)? We draw three iid data \\(\\{X_1,X_2,X_3\\}\\) according to a probability distribution that has expected value \\(\\mu = 1\\) and variance \\(\\sigma^2 = 1\\). Let \\(Y = X_1 + 2X_2 - 3X_3 - 4\\). (a) What is \\(E[Y]\\)? (b) What is the variance of \\(Y\\)? (c) What is \\(E[Y^2]\\)? We are working with a probability density whose domain is \\(x \\in [0,2]\\), and we are given that \\[\\begin{align*} F_X(x) = cx^3 \\end{align*}\\] within that domain. (a) What is the value of \\(c\\)? (b) What is \\(f_X(x)\\)? (c) What is \\(P(X &lt; 1/2 | X &lt; 1)\\)? We sample a single datum \\(X\\) according to the probability density function \\(\\theta x^{\\theta-1}\\), with \\(x \\in [0,1]\\). Let \\(\\theta\\) itself be a random variable sampled according to a probability mass function with values 1/2 for \\(\\theta = 1\\) and 1/2 for \\(\\theta = 2\\). What is \\(p_X(x)\\) or \\(f_X(x)\\)? (Whichever is appropriate.) We are given the following probability density function: \\[\\begin{align*} f_X(x) = 6 x (1-x) \\,, \\end{align*}\\] for \\(x \\in [0,1]\\). (a) What is \\(E[X]\\)? (b) Now assume that we have drawn \\(n\\) iid data according to this distribution and that we compute the sample mean \\(\\bar{X} = (1/n)\\sum_{i=1}^n X_i\\). What is \\(E[\\bar{X}]\\)? We are given the following probability density function: \\[\\begin{align*} f_X(x) = \\frac{1}{2} ~~~ x \\in [-1,1] \\,, \\end{align*}\\] for which \\(E[X] = \\mu = 0\\) and \\(V[X] = \\sigma^2 = 1/3\\). We draw two iid data from this distribution, and we propose two estimators for \\(\\mu\\): \\(\\hat{\\mu}_1 = X_1-X_2\\), and \\((X_1+X_2)/2\\). (a) What is the bias for each estimator? (b) What is the variance for each estimator? (c) What is the mean-squared error for each estimator. (d) Which estimator is better? In an experiment, we sample \\(n\\) iid data that we assume were drawn according to the following probability density function: \\[\\begin{align*} f_X(x) = a (1+x)^{-(a+1)} \\,, \\end{align*}\\] for \\(x \\geq 0\\) and \\(a &gt; 0\\). (a) What is the log-likelihood function \\(\\ell(a \\vert \\mathbf{x})\\)? (b) What is the maximum likelihood estimate \\(\\hat{a}_{MLE}\\)? (c) The mean of the distribution is \\(\\mu = 1/(a-1)\\). What is \\(\\hat{\\mu}_{MLE}\\)? We are given the following probability density function: \\[\\begin{align*} f_X(x) = \\theta (1-x)^{\\theta - 1} \\,, \\end{align*}\\] where \\(x \\in [0,1]\\) and \\(\\theta &gt; 0\\). Assume that we have sampled \\(n\\) iid data according to this pdf. (a) What is the log-likelihood \\(\\ell(\\theta \\vert \\mathbf{x})\\)? (b) What is the maximum-likelihood estimate for \\(\\theta\\)? (c) For the distribution above, the population mean is \\(\\mu = E[X] = 1/(1+\\theta)\\). What is the MLE for \\(\\mu\\)? We are given \\(n\\) iid data that were sampled according to the following probability density function: \\[\\begin{align*} f_X(x) = \\alpha x^{\\alpha-1} \\,, \\end{align*}\\] with \\(\\alpha &gt; 0\\) and \\(0 \\leq x \\leq 1\\). What is the maximum likelihood estimator for \\(\\alpha\\)? We are given \\(n\\) iid data that are sampled according to the following probability mass function: \\[\\begin{align*} p_X(x) = (1-p)^{x-1}p \\,, \\end{align*}\\] with \\(0 &lt; p &lt; 1\\) and \\(x = \\{1,2,3,\\ldots\\}\\). For this distribution, \\(E[X] = 1/p\\) and \\(V[X] = (1-p)/p^2\\). (a) What is the maximum likelihood estimator for \\(1/p\\)? (b) Write down \\(V[\\widehat{1/p}_{MLE}]\\), i.e., the variance of the MLE for \\(1/p\\). We sample \\(n\\) iid data according to the following probability density function: \\[\\begin{align*} f_X(x) = 1 ~~~ x \\in \\left[ \\theta - \\frac12 , \\theta + \\frac12 \\right] \\end{align*}\\] For this distribution, \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2/12\\). A friend, who didn’t really think things through, suggested \\(\\hat{\\theta} = (X_1 - X_2)/2\\) as an estimator for \\(\\theta\\). It is up to us to characterize this estimator. (a) What is the bias of \\(\\hat{\\theta}\\)? (b) What is the variance of \\(\\hat{\\theta}\\)? (c) What is the mean-squared error for \\(\\hat{\\theta}\\)? (d) One of us suggests changing the minus sign in the estimator to a plus sign: that at least makes the estimator unbiased. What is the mean-squared error for this new estimator? We sample \\(n\\) iid data according to an exponential distribution: \\[\\begin{align*} f_X(x) = \\frac{1}{\\theta} e^{-x/\\theta} \\,, \\end{align*}\\] where \\(x \\geq 0\\) and \\(\\theta &gt; 0\\). For this distribution, \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2\\). We propose the estimator \\(\\hat{\\theta} = X_1 - X_2/n\\). (a) What is the bias for this estimator? (b) What is the variance for this estimator? We are given a sample of \\(n\\) iid data drawn according to the probability density function \\[\\begin{align*} f_X(x) = c e^{-\\theta x} \\theta^{x-1} \\,, \\end{align*}\\] where \\(\\theta \\in [0,1]\\), \\(c &gt; 0\\), and \\(x\\) is a positive integer. (a) What is the log-likelihood function? (b) What is the maximum likelihood estimate for \\(\\theta\\)? (c) The mean of this distribution is \\(\\mu = 1/(1-\\theta)\\). What is the MLE for \\(\\mu\\)? We sample a single datum \\(X = x_{\\rm obs}\\) according to the following probability density function: \\[\\begin{align*} f_X(x) = a(1+x)^{-a-1} \\,, \\end{align*}\\] where \\(x \\geq 0\\) and where we will assume \\(a &gt; 1\\). Let the statistic \\(Y = X\\); the expected value for \\(Y\\) is \\(E[Y] = 1/(a-1)\\). (a) What is \\(F_Y(y)\\)? (b) Given \\(\\alpha = 0.05\\), derive a lower bound on the parameter \\(a\\). A logistic distribution with scale parameter 1 has the probability density function \\[\\begin{align*} f_X(x) = \\frac{\\exp[-(x-\\mu)]}{(1+\\exp[-(x-\\mu)])^2} \\,, \\end{align*}\\] for \\(x\\) and \\(\\mu\\) both \\(\\in (-\\infty,\\infty)\\). The cumulative distribution function is \\[\\begin{align*} F_X(x) = \\frac{1}{1+\\exp[-(x-\\mu)]} \\,, \\end{align*}\\] and the expected value of \\(X\\) is \\(E[X] = \\mu\\). We draw a single datum \\(X\\) from this distribution. Let the statistic \\(Y = X\\). (a) What is the rejection region boundary for a lower-tail test given the null hypothesis \\(H_o : \\mu = \\mu_o = 0\\)? Assume \\(\\alpha\\) = 0.05. (b) For the test in (a), if \\(y_{\\rm obs} &gt; y_{\\rm RR}\\), do we reject the null hypothesis, or fail to reject the null hypothesis? (c) If instead we were to perform a two-tail test given the same value of \\(\\alpha\\), would the new lower rejection region boundary be further from \\(\\mu_o\\) than the boundary you found in part (a), or closer to \\(\\mu_o\\)? "],["the-normal-and-related-distributions.html", "2 The Normal (and Related) Distributions 2.1 Motivation 2.2 Probability Density Function 2.3 Cumulative Distribution Function 2.4 Moment-Generating Functions 2.5 Linear Functions of Normal Random Variables 2.6 Standardizing a Normal Random Variable with Known Variance 2.7 General Transformations of a Single Random Variable 2.8 Squaring Standard Normal Random Variables 2.9 Sample Variance of Normal Random Variables 2.10 Standardizing a Normal Random Variable with Unknown Variance 2.11 Point Estimation 2.12 The Central Limit Theorem 2.13 Confidence Intervals 2.14 Hypothesis Testing: Testing for Normality 2.15 Hypothesis Testing: Population Mean 2.16 Hypothesis Testing: Population Variance 2.17 Simple Linear Regression 2.18 One-Way Analysis of Variance 2.19 The Exponential Family of Distributions", " 2 The Normal (and Related) Distributions 2.1 Motivation In this chapter, we illustrate probability and statistical inference concepts utilizing the normal distribution. The normal, also known in the physical sciences as the Gaussian distribution and, colloquially, as the “bell curve,” is the most often utilized probability distribution in data analyses, for a number of reasons. We often observe that the data that we sample in many experiments are at least approximately normally distributed. And while other, more general families of distributions (such as the gamma distribution) might explain data just as well as the normal, the normal has the advantage of having intuitively easy-to-understand parameters: \\(\\mu\\) for the mean, and \\(\\sigma^2\\) for the variance (meaning that \\(\\sigma\\) directly indicates the “width” of the region on the real-number line over which \\(f_X(x)\\) is effectively non-zero). The normal distribution is the limiting distibution for many other distributions (i.e., there are many families of distributions that, with the right choice(s) for parameter value(s), can mimic the shape of the normal. The normal distribution figures prominently in the Central Limit Theorem, which states that the sample mean of a sufficiently large sample of data, from any distribution with finite variance, is approximately normally distributed. 2.2 Probability Density Function Recall: a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) \\(f_X(x) \\geq 0\\) and (b) \\(\\int_x f_X(x) dx = 1\\), where the integral is over all values of \\(x\\) in the distribution’s domain. Let \\(X\\) be a continuous random variable sampled from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\): \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\). The pdf for \\(X\\) is \\[ f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) ~~~~~~ x \\in (-\\infty,\\infty) \\,. \\] A first thing we notice about this pdf is that it is symmetric around \\(\\mu\\). A second thing that we notice is that the integral under this curve approaches 1 over the range \\(\\mu - 3\\sigma \\leq x \\leq \\mu + 3\\sigma\\). (The value of the integral of \\(f_X(x)\\) over this range is 0.9973. This gives rise to one aspect of the so-called empirical rule: if data are approximately normally distributed, we expect all of the data to lie with three standard deviations of the sample mean, with rare exceptions.) See Figure 2.1. Figure 2.1: A normal probability density function with mean 0 and standard deviation 1. Recall: the expected value of a continuously distributed random variable is \\[ E[X] = \\int_x x f_X(x) dx\\,, \\] where the integral is over all values of \\(x\\) within the domain of the pdf \\(f_X(x)\\). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(f_X(x)\\). The expected value of \\(X\\) is \\[ E[X] = \\int_{-\\infty}^\\infty x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx \\] We implement a variable substitution to evaluate this integral. Recall that the three steps of variable substitution are… to write down a viable substitution \\(u = g(x)\\); to then derive \\(du = h(u,x) dx\\); and finally to use \\(u = g(x)\\) to transform the bounds of the integral. For this particular integral: \\[ (1) ~~ u = \\frac{x-\\mu}{\\sigma} ~~ \\implies ~~ (2) ~~ du = \\frac{1}{\\sigma}dx \\] and \\[ (3) ~~ x = -\\infty ~\\implies~ u = -\\infty ~~~ \\mbox{and} ~~~ x = \\infty ~\\implies~ u = \\infty \\,, \\] Thus \\[\\begin{align*} E[X] &amp;= \\int_{-\\infty}^\\infty (\\sigma u + \\mu) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{u^2}{2}\\right) \\sigma du \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{\\sigma u}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du + \\int_{-\\infty}^\\infty \\frac{\\mu}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du \\end{align*}\\] The first integrand is the product of an odd function (\\(u\\)) and an even function (\\(\\exp(-u^2/2)\\)), and thus the first integral evaluates to zero. The second integral is \\[ E[X] = \\mu \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du = \\mu \\,, \\] since the integrand is a normal pdf (with parameters \\(\\mu = 0\\) and \\(\\sigma^2\\) = 1) and the bounds of the integral match that of the domain of the normal pdf, making the value of the integral 1. 2.2.1 Variance of a Normal Probability Density Function Recall: the variance of a continuously distributed random variable is \\[ V[X] = \\int_x (x-\\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\\,, \\] where the integral is over all values of \\(x\\) within the domain of the pdf \\(f_X(x)\\). The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of \\(x\\) for which \\(f_X(x)\\) is effectively non-zero. As seen above, we want to compute \\(E[X^2] - (E[X])^2\\). We have already computed \\(E[X]\\): it is equal to \\(\\mu\\). Here, we compute \\(E[X^2]\\). The variable substitution setup is entirely the same, except that now \\[\\begin{align*} E[X^2] &amp;= \\int_{-\\infty}^\\infty (\\sigma t + \\mu)^2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{t^2}{2}\\right) \\sigma dt \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{\\sigma^2 t^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\int_{-\\infty}^\\infty \\frac{2 \\mu \\sigma t}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\int_{-\\infty}^\\infty \\frac{\\mu^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt \\,. \\end{align*}\\] The second integral is that of an odd function and thus evaluates to zero, and the third integral is, given results above, \\(\\mu^2\\). This leaves \\[ E[X^2] = \\int_{-\\infty}^\\infty \\frac{\\sigma^2 t^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\mu^2 \\,. \\] We note that if we were to apply the shortcut formula, the \\(\\mu^2\\) immediately above would cancel with \\((E[X])^2 = \\mu^2\\), so now we can say that \\[ V[X] = \\frac{\\sigma^2}{\\sqrt{2 \\pi}} \\int_{-\\infty}^\\infty t^2 \\exp\\left(-\\frac{t^2}{2}\\right) dt \\,. \\] This requires integration by parts. Setting aside the constants outside the integral, we have that \\[ \\begin{array}{ll} u = -t &amp; dv = -t \\exp(-t^2/2) dt \\\\ du = -dt &amp; v = \\exp(-t^2/2) \\end{array} \\,, \\] and so \\[\\begin{align*} \\int_{-\\infty}^\\infty t^2 \\exp\\left(-\\frac{t^2}{2}\\right) dt &amp;= \\left. uv \\right|_{-\\infty}^{\\infty} - \\int_{-\\infty}^\\infty v du \\\\ &amp;= - \\left. t \\exp(-t^2/2) \\right|_{-\\infty}^{\\infty} + \\int_{-\\infty}^\\infty \\exp(-t^2/2) dt \\\\ &amp;= 0 + \\sqrt{2\\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2) dt \\\\ &amp;= \\sqrt{2\\pi} \\,. \\end{align*}\\] The first term evaluates to zero between for each bound, as \\(e^{-t^2/2}\\) goes to zero much faster than \\(\\vert t \\vert\\) goes to infinity. As for the second term: we recognize that the integrand is a normal pdf with mean zero and variance one…so the integral by definition evaluates to 1. In the end, after restoring the constants we set aside above, we have that \\[ V[X] = \\frac{\\sigma^2}{\\sqrt{2 \\pi}} \\sqrt{2\\pi} = \\sigma^2 \\,. \\] 2.2.2 Skewness of the Normal Probability Density Function The skewness of a pmf or pdf is a metric that indicates its level of asymmetry. Fisher’s moment coefficient of skewness is \\[ E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right] \\,. \\] which, expanded out, becomes \\[ \\frac{E[X^3] - 3 \\mu \\sigma^2 - \\mu^3}{\\sigma^3} \\,. \\] What is the skewness of the normal distribution? At first, this appears to require a long and involved series of integrations so as to solve \\(E[X^3]\\). But let’s try to be more clever. We know, from above, that the quantity \\((\\sigma t + \\mu)^3\\) will appear in the integral for \\(E[X^3]\\). Let’s expand this out: \\[ \\sigma^3 t^3 + 3 \\sigma^2 \\mu t^2 + 3 \\sigma \\mu^2 t + \\mu^3 \\,. \\] Each of these terms will appear separately in integrals of \\(\\exp(-t^2/2)/\\sqrt{2\\pi}\\) over the domain \\(-\\infty\\) to \\(\\infty\\). From above, what do we already know? First, we know that if \\(t\\) is raised to an odd power, the integral will be zero. This eliminates \\(\\sigma^3 t^3\\) and \\(3 \\sigma \\mu^2 t\\). Second, we know that for the \\(t^0\\) term, the result will be \\(\\mu^3\\), and we know that for the \\(t^2\\) term, the result will be \\(3 \\sigma^2 \\mu\\). Thus \\[ E[X^3] = 3\\sigma^2\\mu + \\mu^3 \\] and the skewness is \\[ \\frac{3 \\mu \\sigma^2 + \\mu^3 - 3 \\mu \\sigma^2 - \\mu^3}{\\sigma^3} = 0 \\,. \\] The skewness is zero, meaning that a normal pdf is symmetric around its mean \\(\\mu\\). 2.2.3 Computing Probabilities Before diving into this example, we will be clear that this is not the optimal way to compute probabilities associated with normal random variables, as we should utilize R’s pnorm() function, as shown in the next section. However, showing how to utilize integrate() is useful review. (We also show how to pass distribution parameters to integrate(), which we did not do in the last chapter.) If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? To find this probability, we integrate over the normal pdf with mean \\(\\mu = 10\\) and variance \\(\\sigma^2 = 4\\). Visually, we are determining the area of the red-shaded region in Figure 2.2. Figure 2.2: The probability \\(P(8 \\leq X \\leq 13.5)\\) is the area of the red-shaded region, i.e., the integral of the normal probability density function from 8 to 13.5, assuming \\(\\mu = 10\\) and \\(\\sigma^2 = 4\\). integrand &lt;- function(x,mu,sigma2) { return(exp(-(x-mu)^2/2/sigma2)/sqrt(2*pi*sigma2)) } integrate(integrand,8,13.5,mu=10,sigma2=4)$value ## [1] 0.8012856 The result is 0.801. Note how the integrand() function requires extra information above and beyond the value of x. This information is passed in using the extra arguments mu and sigma2, the values of which are set in the call to integrate(). If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? Recall that \\(P(a \\leq X \\leq b \\vert X \\leq c) = P(a \\leq X \\leq b)/P(X \\leq c)\\), assuming that \\(a &lt; b &lt; c\\). So this probability is, visually, the ratio of the area of the brown-shaded region in Figure 2.3 to the area of the red-shaded region underlying it. We can reuse the integrand() function from above, calling it twice: integrate(integrand,8,13.5,mu=13,sigma2=5)$value / integrate(integrand,-Inf,14,mu=13,sigma2=5)$value ## [1] 0.8560226 The result is 0.856. Note how we use -Inf in the second call to integrate(): Inf, like pi, is a reserved word in the R programming language. Figure 2.3: The conditional probability \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\) is the ratio of the area of the brown-shaded region to the area of the red-shaded region underlying the brown-shaded region. 2.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as \\(F_X(x) = \\int_{y \\leq x} f_Y(y) dy\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). The cdf for the normal distribution is the “accumulated probability” between \\(-\\infty\\) and the functional input \\(x\\): \\[ F_X(x) = \\int_{-\\infty}^x f_Y(y) dy = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) dy \\,. \\] (See Figure 2.4.) Recall that because \\(x\\) is the upper bound of the integral, we have to replace \\(x\\) with some other variable in the integrand itself. (Here we choose \\(y\\). The choice is arbitrary.) Figure 2.4: The cdf is the area of the red-shaded polygon. To evaluate this integral, we again implement a variable substitution strategy: \\[ (1) ~~ t = \\frac{(y-\\mu)}{\\sqrt{2}\\sigma} ~~\\implies~~ (2) ~~ dt = \\frac{1}{\\sqrt{2}\\sigma}dy \\] and \\[ (3) ~~ y = -\\infty ~\\implies~ t = -\\infty ~~~ \\mbox{and} ~~~ y = x ~\\implies~ t = \\frac{(x-\\mu)}{\\sqrt{2}\\sigma} \\,, \\] so \\[\\begin{align*} F_X(x) &amp;= \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) dy \\\\ &amp;= \\int_{-\\infty}^{\\frac{x-\\mu}{\\sqrt{2}\\sigma}} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-t^2) \\sqrt{2}\\sigma dt = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\frac{x-\\mu}{\\sqrt{2}\\sigma}} \\exp(-t^2) dt \\,. \\end{align*}\\] The error function is defined as \\[ \\mbox{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp(-t^2)dt \\,, \\] which is close to, but not quite, our expression for \\(F_X(x)\\). (The integrand is the same, but the bounds of integration differ.) To match the bounds: \\[\\begin{align*} \\frac{2}{\\sqrt{\\pi}} \\int_{-\\infty}^z \\exp(-t^2)dt &amp;= \\frac{2}{\\sqrt{\\pi}} \\left( \\int_{-\\infty}^0 \\exp(-t^2)dt + \\int_0^z \\exp(-t^2)dt \\right) \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\left( -\\int_0^{-\\infty} \\exp(-t^2)dt + \\int_0^z \\exp(-t^2)dt \\right) \\\\ &amp;= -\\mbox{erf}(-\\infty) + \\mbox{erf}(z) = \\mbox{erf}(\\infty) + \\mbox{erf}(z) = 1 + \\mbox{erf}(z) \\,. \\end{align*}\\] Here we make use of two properties of the error function: \\(\\mbox{erf}(-z) = -\\mbox{erf}(z)\\), and \\(\\mbox{erf}(\\infty)=1\\). Thus \\[ \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^z \\exp(-t^2)dt = \\frac{1}{2}[1 + \\mbox{erf}(z)] \\,. \\] By matching this expression with that given for \\(F(x)\\) above, we see that \\[ F_X(x) = \\frac{1}{2}\\left[1 + \\mbox{erf}\\left(\\frac{x-\\mu}{\\sqrt{2}\\sigma}\\right)\\right] \\,. \\] In Figure 2.5, we plot \\(F_X(x)\\). We note that while the error function is available to use directly in some R packages, it is provided only indirectly in base-R, via the pnorm() function. (In general, one computes cdf values for probability distributions in R using functions prefixed with p: pnorm(), pbinom(), punif(), etc.) Examining this figure, we see that this cdf abides by the properties listed in the previous chapter: \\(F(-\\infty) = 0\\) and \\(F(\\infty) = 1\\), and it is (strictly) monotonically increasing. Figure 2.5: The cdf for a normal distribution with mean 0 and standard deviation 1. To compute the probability \\(P(a &lt; X &lt; b)\\), we make use of the cdf. (As a reminder: if we have the cdf at our disposal and need to compute a probability…we should use it!) \\[\\begin{align*} P(a &lt; X &lt; b) &amp;= P(X &lt; b) - P(X &lt; a) \\\\ &amp;= F(b) - F(a) = \\frac{1}{2}\\left[ \\mbox{erf}\\left(\\frac{b-\\mu}{\\sqrt{2}\\sigma}\\right) - \\mbox{erf}\\left(\\frac{a-\\mu}{\\sqrt{2}\\sigma}\\right)\\right] \\,, \\end{align*}\\] which is more simply rendered in R as pnorm(b,mean=mu,sd=sigma) - pnorm(a,mean=mu,sd=sigma) Let’s look again at Figure 2.4. What if, instead of asking the question “what is the shaded area under the curve,” which is answered by integrating the normal pdf from \\(-\\infty\\) to a specified coordinate \\(x\\), we instead ask the question “what value of \\(x\\) is associated with a given area under the curve”? This is the inverse problem, one that we can solve so long as the relationship between \\(x\\) and \\(F(x)\\) is bijective (i.e., one-to-one): \\(x = F_X^{-1}[F_X(x)]\\) for all \\(x\\). Recall: an inverse cdf function \\(F_X^{-1}(q)\\) takes as input a distribution quantile \\(q \\in [0,1]\\) and returns the value of \\(x\\) such that \\(q = F_X(x)\\). Let \\(q = F_X(x)\\). Then, for the case of the normal cdf, we can write that \\[ x = \\sqrt{2} \\sigma~ \\mbox{erf}^{-1}\\left(2q-1\\right) + \\mu \\,, \\] where \\(\\mbox{erf}^{-1}(\\cdot)\\) is the inverse error function. Like the error function itself, the inverse error function is available for use in some R packages, but it is most commonly accessed, indirectly via the base-R function qnorm(). (In general, one computes inverse cdf values for probability distributions in R using functions prefixed with q: qnorm(), qpois(), qbeta(), etc.) 2.3.1 Computing Probabilities We utilize the two examples provided in the last section to show how one would compute probabilities associated with the normal pdf by hand. But we note that a computer is still needed to derive the final numbers! If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? We have that \\[ P(8 \\leq X \\leq 13.5) = P(X \\leq 13.5) - P(X \\leq 8) = F_X(13.5 \\vert 10,4) - F_X(8 \\vert 10,4) \\,. \\] Well, this is where analytic computation ends, as we cannot evaluate the cdfs using pen and paper. Instead, we utilize the function pnorm(): pnorm(13.5,mean=10,sd=sqrt(4)) - pnorm(8,mean=10,sd=sqrt(4)) ## [1] 0.8012856 We get the same result as we got in the last section: 0.801. If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? Knowing that we cannot solve this by hand, we go directly to R: (pnorm(13.5,mean=13,sd=sqrt(5)) - pnorm(8,mean=13,sd=sqrt(5))) / pnorm(14,mean=13,sd=sqrt(5)) ## [1] 0.8560226 The answer, as before, is 0.856. But let’s go ahead and add some complexity here. How would we answer the following question? If \\(\\mu = 20\\) and \\(\\sigma^2 = 3\\), what is the value of \\(a\\) such that \\(P(20 - a \\leq X \\leq 20 + a) = 0.48\\)? (The first thing to thing about here is: does the value of \\(\\mu\\) actually matter? The answer here is no. Think about why that may be.) We have that \\[\\begin{align*} P(20-a \\leq X \\leq 20+a) = 0.48 &amp;= P(X \\leq 20+a) - P(X \\leq 20-a)\\\\ &amp;= F_X(20+a \\vert 20,3) - F_X(20-a \\vert 20,3) \\,. \\end{align*}\\] Hmm…we’re stuck. We cannot invert this equation so as to isolate \\(a\\)…or can we? Remember that a normal pdf is symmetric around the mean. Hence \\[ P(X \\leq 20+a) = 1 - P(X &gt; 20+a) = 1 - P(X \\leq 20-a) \\,. \\] The area under the normal pdf from \\(20+a\\) to \\(\\infty\\) is the same as the area from \\(-\\infty\\) to \\(20-a\\). So… \\[\\begin{align*} P(20-a \\leq X \\leq 20+a) &amp;= 1 - F_X(20-a \\vert 20,3) - F_X(20-a \\vert 20,3)\\\\ &amp;= 1 - 2F_X(20-a \\vert 20,3) \\,, \\end{align*}\\] or \\[ F_X(20-a \\vert 20,3) = \\frac{1}{2}[1 - P(20-a \\leq X \\leq 20+a)] = \\frac{1}{2} 0.52 = 0.26 \\,. \\] Now, we invert the cdf and rearrange terms to get: \\[ a = 20 - F_X^{-1}(0.26 \\vert 20,3) \\,, \\] and once again, we transition to R: a &lt;- 20 - qnorm(0.26,mean=20,sd=sqrt(3)) The result is \\(a = 1.114\\), i.e., \\(P(18.886 \\leq X \\leq 21.114) = 0.48\\). The reader might quibble with our assertion that \\(\\mu\\) doesn’t matter here, because, after all, the value of \\(\\mu\\) appears in the final code. However, if we changed both values of 20 to some other, arbitrary value, we would derive the same value for \\(a\\). 2.3.2 Visualizing a Cumulative Distribution Function Let’s say we want to make a figure like that in Figure 2.4, where we are given that our normal distribution has mean \\(\\mu = 10\\) and variance \\(\\sigma^2 = 4\\), and we want to overlay the shaded region associated with \\(F_X(9)\\). How do we do this? The first step is to determine the population standard deviation. Here, that would be \\(\\sigma = \\sqrt{4} = 2\\). The second step is to determine an appropriate range of values of \\(x\\) over which to plot the cdf. Here we will assume that \\(\\mu - 4\\sigma\\) to \\(\\mu + 4\\sigma\\) is sufficient; this corresponds to \\(x = 2\\) to \\(x = 18\\). The third step is to code. The result of our coding is shown in Figure 2.6. x &lt;- seq(2,18,by=0.05) # vector with x = {2,2.05,2.1,...,18} f.x &lt;- dnorm(x,mean=10,sd=2) # compute f(x) for all x df &lt;- data.frame(x=x,f.x=f.x) # define a dataframe with above data df.shaded &lt;- subset(df,x&lt;=9) # get subset of data with x values &lt;= 9 ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_area(data=df.shaded,fill=&quot;red&quot;,col=&quot;blue&quot;,outline.type=&quot;full&quot;) + geom_hline(yintercept=0,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.6: The cumulative distribution function is the area of the red-shaded region. For the polygon, the first \\((x,y)\\) pair is \\((9,0)\\), the second is \\((2,0)\\) (where \\(x = 2\\) is the lower limit of the plot…there is no need to take the polygon further “to the left”), and the next set are \\((x,f_X(x))\\) for all \\(x\\) values \\(\\leq 9\\). The last point is the first point: this closes the polygon. 2.4 Moment-Generating Functions In the previous chapter, we learned that if we linearly transform a random variable, i.e., if we define a new random variable \\(Y = \\sum_{i=1}^n a_i X_i + b\\), where \\(\\{a_1,\\ldots,a_n\\}\\) and \\(b\\) are constants, then \\[ E[Y] = E\\left[\\sum_{i=1}^n a_i X_i + b\\right] = b + \\sum_{i=1}^n a_i E[X_i] \\,. \\] Furthermore, if we assume that the \\(X_i\\)’s are independent random variables, then the variance of \\(Y\\) is \\[ V[Y] = V\\left[\\sum_{i=1}^n a_i X_i + b\\right] = \\sum_{i=1}^n a_i^2 V[X_i] \\,. \\] (Because the \\(X_i\\)’s are independent, we do not need to take into account any covariance, or linear dependence, between the \\(X_i\\)’s, simplifying the equation for \\(V[Y]\\). We discuss how covariance is taken into account in Chapter 6.) So we know where \\(Y\\) is centered and we know how “wide” it is. However, we don’t yet know the shape of the distribution for \\(Y\\), i.e., we don’t yet know \\(f_Y(y)\\). To show how we might derive the distribution, we introduce a new concept, that of the moment-generating function, or mgf. In the previous chapter, we introduced the concept of distribution moments, i.e., the expected values \\(E[X^k]\\) and \\(E[(X-\\mu)^k]\\). The moments of a given probability distribution are unique, and can often (but not always) be neatly encapsulated in a single mathematical expression: the moment-generating function (or mgf). To derive an mgf for a given distribution, we invoke the Law of the Unconscious Statistician: \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\int_x e^{tX} f_X(x) \\\\ &amp;= \\int_x \\left[1 + tx + \\frac{t^2}{2!}x^2 + \\cdots \\right] f_X(x) \\\\ &amp;= \\int_x \\left[ f_X(x) + t x f_X(x) + \\frac{t^2}{2!} x^2 f_X(x) + \\cdots \\right] \\\\ &amp;= \\int_x f_X(x) + t \\int_x x f_X(x) + \\frac{t^2}{2!} \\int_x x^2 f_X(x) + \\cdots \\\\ &amp;= 1 + t E[X] + \\frac{t^2}{2!} E[X^2] + \\cdots \\,. \\end{align*}\\] An mgf only exists for a particular distribution if there is a constant \\(b\\) such that \\(m_X(t)\\) is finite for \\(\\vert t \\vert &lt; b\\). (This is a detail that we will not concern ourselves with here, i.e., we will assume that the expressions we derive for \\(E[e^{tX}]\\) are valid expressions.) An example of a distribution for which the mgf does not exist is the Cauchy distribution. Moment-generating functions are called such because, as one might guess, they generate moments (via differentiation): \\[ \\left.\\frac{d^k m_X(t)}{dt^k}\\right|_{t=0} = \\frac{d^k}{dt^k} \\left. \\left[1 + t E[X] + \\frac{t^2}{2!} E[X^2] + \\cdots \\right]\\right|_{t=0} = \\left. \\left[E[X^k] + tE[X^{k+1}] + \\cdots\\right] \\right|_{t=0} = E[X^k] \\,. \\] The \\(k^{\\rm th}\\) derivative of an mgf, with \\(t\\) set to zero, yields the \\(k^{\\rm th}\\) moment \\(E[X^k]\\). But, the reader may ask, why are mgfs important here, in the context of normal random variables? We already know all the moments of this distribution that we care to know: they are written down. The answer is that a remarkably useful property of mgfs is the following: if \\(Y = b + \\sum_{i=1}^n a_i X_i\\), where the \\(X_i\\)’s are independent random variables sampled from a distribution whose mgf exists, then \\[ m_Y(t) = e^{bt} m_{X_1}(a_1 t) \\cdot m_{X_2}(a_2 t) \\cdots m_{X_n}(a_n t) = e^{bt} \\prod_{i=1}^n m_{X_i}(a_i t) \\,, \\] where \\(m_{X_i}(\\cdot)\\) is the moment-generating function for the random variable \\(X_i\\). An mgf is typically written as a function of \\(t\\); the notation \\(m_{X_i}(a_it)\\) simply means that when we evaluate the above equation, wherever there is a \\(t\\), we plug in \\(a_it\\). Here’s the key point: if we recognize the form of \\(m_Y(t)\\) as matching that of the mgf for a given distribution, then we know the distribution for \\(Y\\). Below, we show how the method of moment-generating functions allows us to derive distributions for linearly transformed normal random variables. 2.4.1 Moment-Generating Function for a Probability Mass Function Let’s assume that we sample data from the following pmf: \\(x\\) \\(p_X(x)\\) 0 0.2 1 0.3 2 0.5 What is the mgf for this distribution? What is its expected value? To derive the mgf, we compute \\(E[e^{tX}]\\): \\[ m_X(t) = E[e^{tX}] = \\sum_x e^{tx} p_X(x) = 1 \\cdot 0.2 + e^t \\cdot 0.3 + e^{2t} \\cdot 0.5 = 0.2 + 0.3 e^t + 0.5 e^{2t} \\,. \\] This cannot be simplified, and thus this is the final answer. As far as the expected value is concerned: we could simply compute \\(E[X] = \\sum_x x p_X(x)\\), but since we have the mgf now, we can use it too: \\[ E[X] = \\left.\\frac{d}{dt}m_X(t)\\right|_{t=0} = \\left. (0.3 e^t + e^{2t})\\right|_{t=0} = 0.3 + 1 = 1.3 \\,. \\] 2.4.2 Moment-Generating Function for a Probability Density Function Let’s assume that we now sample data from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta} e^{-x/\\theta} \\,, \\] where \\(x \\geq 0\\) and \\(\\theta &gt; 0\\). What is the mgf of this distribution? \\[\\begin{align*} E[e^{tX}] &amp;= \\int_0^\\infty e^{tx} \\frac{1}{\\theta} e^{-x/\\theta} dx = \\frac{1}{\\theta} \\int_0^\\infty e^{-x\\left(\\frac{1}{\\theta}-t\\right)} dx \\\\ &amp;= \\frac{1}{\\theta} \\int_0^\\infty e^{-x/\\theta&#39;} dx = \\frac{\\theta&#39;}{\\theta} = \\frac{1}{\\theta} \\frac{1}{(1/\\theta - t)} = \\frac{1}{(1-\\theta t)} = (1-\\theta t)^{-1} \\,. \\end{align*}\\] The expected value of this distribution can be computed via the integral \\(\\int_x x f_X(x) dx\\), but again, as we now have the mgf, \\[ E[X] = \\left.\\frac{d}{dt}m_X(t)\\right|_{t=0} = \\left. -(1-\\theta t)^{-2} (-\\theta)\\right|_{t=0} = \\theta \\,. \\] 2.5 Linear Functions of Normal Random Variables Let’s assume we are given \\(n\\) normal random variables \\(\\{X_1,\\ldots,X_n\\}\\), which are independent (but not necessarily identically distributed), and we wish to determine the distribution of the sum \\(Y = b + \\sum_{i=1}^n a_i X_i\\). We recall that the expected value operator \\(E[X]\\) and the variance operator \\(V[X]\\) are linear operators, thus we can note immediately that… the expected value is \\[ E[Y] = E\\left[\\sum_{i=1}^n a_i X_i\\right] = \\sum_{i=1}^n E[a_i X_i] = \\sum_{i=1}^n a_i E[X_i] = \\sum_{i=1}^n a_i \\mu_i \\,; \\] and the variance is \\[ V[Y] = V\\left[\\sum_{i=1}^n a_i X_i\\right] = \\sum_{i=1}^n V[a_i X_i] = \\sum_{i=1}^n a_i^2 V[X_i] = \\sum_{i=1}^n a_i^2 \\sigma_i^2 \\,. \\] As far as deriving the distribution: the mgf for the normal is \\[ m_X(t) = \\exp\\left(\\mu t + \\sigma^2\\frac{t^2}{2} \\right) \\,, \\] and thus if we have \\(n\\) independent normal random variables, we find that \\[\\begin{align*} m_Y(t) &amp;= \\exp\\left(\\mu_1a_1t+a_1^2\\sigma_1^2\\frac{t^2}{2}\\right) \\cdot \\cdots \\cdot \\exp\\left(\\mu_na_nt+a_n^2\\sigma_n^2\\frac{t^2}{2}\\right) \\\\ &amp;= \\exp\\left[ (a_1\\mu_1+\\cdots+a_n\\mu_n)t + \\left(a_1^2\\sigma_1^2 + \\cdots + a_n^2\\sigma_n^2\\right)\\frac{t^2}{2} \\right] \\\\ &amp;= \\exp\\left[ \\left(\\sum_{i=1}^n a_i\\mu_i\\right)t + \\left(\\sum_{i=1}^n a_i^2\\sigma_i^2\\right)\\frac{t^2}{2} \\right] \\,. \\end{align*}\\] When we examine the result, we see immediately that it retains the functional form of a normal mgf, and thus we conclude that \\(Y\\) itself is normally distributed, with mean \\(\\sum_{i=1}^n a_i\\mu_i\\) and variance \\(\\sum_{i=1}^n a_i^2\\sigma_i^2\\), i.e., \\[ Y \\sim \\mathcal{N}\\left(\\sum_{i=1}^n a_i\\mu_i,\\sum_{i=1}^n a_i^2\\sigma_i^2\\right) \\,. \\] 2.5.1 The Distribution of the Sample Mean of iid Normal Random Variables We have previously seen that when have a sample of \\(n\\) iid random variables, \\(E[\\bar{X}] = \\mu\\) and \\(V[\\bar{X}] = \\sigma^2/n\\). Now we want to determine the distribution of \\(\\bar{X}\\), not just its mean and variance. In general, if \\(\\bar{X} = (1/n)\\sum_{i=1}^n X_i\\), then \\[\\begin{align*} m_{\\bar{X}}(t) &amp;= m_{X_1}(a_1 t) \\cdot m_{X_2}(a_2 t) \\cdots m_{X_n}(a_n t) \\\\ &amp;= m_{X_1}\\left(\\frac{t}{n}\\right) \\cdots m_{X_n}\\left(\\frac{t}{n}\\right) = \\prod_{i=1}^n m_{X_i}\\left(\\frac{t}{n}\\right) \\,, \\end{align*}\\] and thus \\[\\begin{align*} m_{\\bar{X}}(t) &amp;= \\exp\\left[ \\left(\\sum_{i=1}^n \\frac{\\mu}{n}\\right)t + \\left(\\sum_{i=1}^n \\frac{\\sigma^2}{n^2}\\right)\\frac{t^2}{2} \\right] \\\\ &amp;= \\exp\\left( n \\frac{\\mu}{n} t + n \\frac{\\sigma^2}{n^2} \\frac{t^2}{2} \\right) = \\exp\\left(\\mu t + \\frac{\\sigma^2}{n} \\frac{t^2}{2} \\right) \\,. \\end{align*}\\] We see that \\(\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n)\\), i.e., that the sample mean observed in any given experiment is sampled from a normal distribution centered on the true mean, with a width that goes to zero as \\(n \\rightarrow \\infty\\). 2.5.2 Using Variable Substitution to Determine Distribution of Y = aX + b We set \\(y = ax+b\\) and note that \\(dy = a dx\\) and that if \\(x = \\pm \\infty\\) then \\(y\\) also equals \\(\\pm \\infty\\). Thus \\[\\begin{align*} \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma^2} \\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] dx &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma^2} \\exp\\left[-\\frac{([y-b]/a-\\mu)^2}{2\\sigma^2}\\right] \\frac{dy}{a} \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi a^2\\sigma^2} \\exp\\left[-\\frac{(y-b-a\\mu)^2}{2a^2\\sigma^2}\\right] dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi a^2\\sigma^2} \\exp\\left[-\\frac{(y-[a\\mu+b])^2}{2a^2\\sigma^2}\\right] dy \\end{align*}\\] The key point here is that the integrand has the functional form of a normal pdf and the integral bounds match the domain of a normal pdf…hence \\(Y\\) is normally distributed, with mean \\(a\\mu+b\\) and variance \\(a^2\\sigma^2\\). (This key point holds in general…if we have a pmf or pdf whose functional form and domain match that of a known, “named” family of distributions, then the pmf or pdf belongs to that family.) 2.6 Standardizing a Normal Random Variable with Known Variance To standardize any random variable, we subtract the expected value and divide by the standard deviation, i.e., we set \\[ Z = \\frac{X - E[X]}{\\sqrt{V[X]}} \\,. \\] If \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), what are the mean and variance of \\(Z\\), and can we derive \\(f_Z(z)\\)? Expected Value. If we write \\(Z = aX+b\\), where \\(a\\) is \\(1/\\sigma\\) and \\(b = -\\mu/\\sigma\\), then \\[ E[Z] = E[aX+b] = \\frac{1}{\\sigma}E[X] - b = \\frac{\\mu}{\\sigma} - \\frac{\\mu}{\\sigma} = 0 \\,. \\] Variance. The variance is \\[ V[Z] = V[aX+b] = a^2V[X] = \\frac{1}{\\sigma^2} \\sigma^2 = 1 \\,. \\] OK…so far, so good: the distribution is centered at 0 and has variance 1. To derive the distribution, we use the methods of mgfs. The mgf for a normal random variable is \\[ m_X(t) = \\exp\\left(\\mu t + \\frac{\\sigma^2 t^2}{2}\\right) \\,. \\] Harkening back to the last section, the mgf for \\(Z = aX+b\\) is \\[\\begin{align*} m_Z(t) &amp;= e^{bt} m_X(at) \\\\ &amp;= \\exp\\left(-\\frac{\\mu t}{\\sigma}\\right) \\exp\\left(a \\mu t + \\frac{a^2 \\sigma^2 t^2} {2}\\right) \\\\ &amp;= \\exp\\left(-\\frac{\\mu t}{\\sigma}\\right) \\exp\\left(\\frac{\\mu t}{\\sigma} + \\frac{\\sigma^2 t^2}{ 2 \\sigma^2}\\right) \\\\ &amp;= \\exp\\left(0 t + \\frac{1^2 t^2}{2}\\right) \\,. \\end{align*}\\] This mgf retains the functional form of a normal mgf, but with \\(\\mu = 0\\) and \\(\\sigma = 1\\). Thus \\(Z \\sim \\mathcal{N}(0,1)\\), and thus the pdf for \\(Z\\) is \\[ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) ~~~~ z \\in (-\\infty,\\infty) \\,, \\] while the cdf for \\(Z\\) is \\[ F_Z(z) = \\Phi(z) = \\frac{1}{2}\\left[1 + \\mbox{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right] \\,. \\] \\(Z\\) is a so-called standard normal random variable. By historical convention, the cdf of the standard normal distribution is denoted \\(\\Phi(z)\\) (“phi” of z, pronounced “fye”) rather than \\(F_Z(z)\\). Statisticians often standardize normally distributed random variables and perform probability calculations using the standard normal. This is unnecessary in the age of computers, but in the pre-computer era standardization greatly simplified calculations since all one needed was a single table of values of \\(\\Phi(z)\\) to compute probabilities. That said, standardization has its uses. For instance, mentally computing an approximate value for \\(P(6 &lt; X &lt; 12)\\) when \\(X \\sim \\mathcal{N}(9,9)\\) can be quite a bit more taxing than if we were to write down the equivalent expression \\(P(-1 &lt; Z &lt; 1)\\) and then evaluate that. (A skilled practitioner would know right away that the latter expression evaluates to \\(\\approx\\) 0.68.) Common \\(Z\\)-Range/Probability Conversions \\((z_L,z_U)\\) \\(\\pm\\) 1 \\(\\pm\\) 2 \\(\\pm\\) 3 \\(\\pm\\) 1.645 \\(\\pm\\) 1.960 \\(\\pm\\) 2.576 prob. 0.6837 0.9544 0.9973 0.90 0.95 0.99 2.6.1 Computing Probabilities Here we will reexamine the three examples that we worked through above in the section in which we introduce the normal cumulative distribution function; here, we will utilize standardization. Note: the final results will all be the same! If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? We standardize \\(X\\): \\(Z = (X-\\mu)/\\sigma = (X-10)/2\\). Hence the bounds of integration are \\((8-10)/2 = -1\\) and \\((13.5-10)/2 = 1.75\\), and the probability we seek is \\[ P(-1 \\leq Z \\leq 1.75) = \\Phi(1.75) - \\Phi(-1) \\,. \\] To compute the final number, we utilize pnorm() with its default values of mean=0 and sd=1: pnorm(1.75) - pnorm(-1) ## [1] 0.8012856 The probability is 0.801. If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? The integral bounds are \\((8-13)/\\sqrt{5} = -\\sqrt{5}\\) and \\((13.5-13)/\\sqrt{5} = \\sqrt{5}/10\\) in the numerator, and \\(-\\infty\\) and \\((14-13/\\sqrt{5} = \\sqrt{5}/5\\) in the denominator. In R: (pnorm(sqrt(5)/10) - pnorm(-sqrt(5))) / pnorm(sqrt(5)/5) ## [1] 0.8560226 The probability is 0.856. If \\(\\mu = 20\\) and \\(\\sigma^2 = 3\\), what is the value of \\(a\\) such that \\(P(20-a \\leq X \\leq 20+a) = 0.48\\)? Here, \\[ P(20-a \\leq X \\leq 20+a) = P\\left( \\frac{20-a-20}{\\sqrt{3}} \\leq \\frac{X - 20}{\\sqrt{3}} \\leq \\frac{20+a-20}{\\sqrt{3}}\\right) = P\\left(-\\frac{a}{\\sqrt{3}} \\leq Z \\leq \\frac{a}{\\sqrt{3}} \\right) \\,, \\] and we utilize the symmetry of the standard normal around zero to write \\[ P\\left(-\\frac{a}{\\sqrt{3}} \\leq Z \\leq \\frac{a}{\\sqrt{3}} \\right) = 1 - 2P\\left(Z \\leq -\\frac{a}{\\sqrt{3}}\\right) = 1 - 2\\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) \\,. \\] Thus \\[ 1 - 2\\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) = 0.48 ~\\Rightarrow~ \\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) = 0.26 ~\\Rightarrow~ -\\frac{a}{\\sqrt{3}} = \\Phi^{-1}(0.26) = -0.64 \\,, \\] and thus \\(a = 1.114\\). (Note that we numerically evaluate \\(\\Phi^{-1}(0.26)\\) using qnorm(0.26)). 2.7 General Transformations of a Single Random Variable Thus far, when discussing transformations of a random variable, we have limited ourselves to linear transformations of independent r.v.’s, i.e., \\(Y = b + \\sum_{i=1}^n a_i X_i\\). What if instead we want to make a more general transformation of a single random variable, e.g., \\(Y = X^2 + 3\\) or \\(Y = \\sin X\\)? We cannot use the method of moment-generating functions here…we need a new method. Let’s assume we have a random variable \\(X\\), and we transform it according to a function \\(g(\\cdot)\\): \\(U = g(X)\\). Then: we identify the inverse function \\(X = g^{-1}(U)\\); we derive the cdf of \\(U\\): \\(F_U(u) = P(U \\leq u) = P(g(X) \\leq u) = P(X \\leq g^{-1}(u))\\); and last we derive the pdf of \\(U\\): \\(f_U(u) = dF_U(u)/du\\). Recall: a continuous distribution’s pdf is the derivative of its cdf. 2.7.1 Distribution of a Transformed Random Variable We are given the following pdf: \\[ f_X(x) = 3x^2 \\,, \\] where \\(x \\in [0,1]\\). What is the distribution of \\(U = X/3\\)? We follow the three steps outlined above. First, we note that \\(U = X/3\\) and thus \\(X = 3U\\). Next, we find \\[ F_U(u) = P(U \\leq u) = P\\left(\\frac{X}{3} \\leq u\\right) = P(X \\leq 3u) = \\int_0^{3u} 3x^2 dx = \\left. x^3 \\right|_0^{3u} = 27u^3 \\,, \\] for \\(u \\in [0,1/3]\\). (The bounds are determined by plugging \\(x=0\\) and \\(x=1\\) into \\(u = x/3\\).) Now we can derive the pdf for \\(U\\): \\[ f_U(u) = \\frac{d}{du} 27u^3 = 54u^2 \\,, \\] for \\(u \\in [0,1/3]\\). What is the distribution of \\(U = -X\\)? We note that \\(U = -X\\) and thus \\(X = -U\\). Next, we find \\[ F_U(u) = P(U \\leq u) = P(-X \\leq u) = P(X \\geq -u) = \\int_{-u}^{1} 3x^2 dx = \\left. x^3 \\right|_{-u}^{1} = 1 - (-u)^3 = 1 + u^3 \\,, \\] for \\(u \\in [-1,0]\\). (Note that the direction of the inequality changed in the probability statement because of the sign change from \\(-X\\) to \\(X\\), and the bounds are reversed to be in ascending order.) Now we derive the pdf: \\[ f_U(u) = \\frac{d}{du} (1+u^3) = 3u^2 \\,, \\] for \\(u \\in [-1,0]\\). What is the distribution of \\(U = 2e^X\\)? We note that \\(U = 2e^X\\) and thus \\(X = \\log(U/2)\\). Next, we find \\[\\begin{align*} F_U(u) = P(U \\leq u) &amp;= P\\left(2e^X \\leq u\\right) = P\\left(X \\leq \\log\\frac{u}{2}\\right)\\\\ &amp;= \\int_0^{\\log(u/2)} 3x^2 dx = \\left. x^3 \\right|_0^{\\log(u/2)} = \\left(\\log\\frac{u}{2}\\right)^3 \\,, \\end{align*}\\] for \\(u \\in [2,2e]\\). The pdf for \\(U\\) is thus \\[ f_U(u) = \\frac{d}{du} \\left(\\log\\frac{u}{2}\\right)^3 = 3\\left(\\log\\frac{u}{2}\\right)^2 \\frac{1}{u} \\,, \\] for \\(u \\in [2,2e]\\). Hint: if we want to see if our derived distribution is correct, we can code the pdf in R and integrate over the inferred domain. integrand &lt;- function(u) { return(3*(log(u/2))^2/u) } integrate(integrand,2,2*exp(1))$value ## [1] 1 Our answer is 1, as it should be for a properly defined pdf. 2.8 Squaring Standard Normal Random Variables The square of a standard normal random variable is an important quantity that arises, e.g., in statistical model assessment (through the “sum of squared errors” when the “error” is normally distributed) and in hypothesis tests like the chi-square goodness-of-fit test. But for this quantity to be useful in statistical inference, we need to know its distribution. In step (1) of the algorithm laid out in the last section, we identify the inverse function: if \\(U = Z^2\\), with \\(Z \\in (-\\infty,\\infty)\\), then \\(Z = \\pm \\sqrt{U}\\). Then, following step (2), we state that \\[ F_U(u) = P(U \\leq u) = P(Z^2 \\leq u) = P(-\\sqrt{u} \\leq Z \\leq \\sqrt{u}) = \\Phi(\\sqrt{u}) - \\Phi(-\\sqrt{u}) \\,, \\] where, as we recall, \\(\\Phi(\\cdot)\\) is the notation for the standard normal cdf. Because of symmetry, we can simplify this expression: \\[ F_U(u) = \\Phi(\\sqrt{u}) - [1 - \\Phi(\\sqrt{u})] = 2\\Phi(\\sqrt{u}) - 1 \\,. \\] To carry out step (3), we utilize the chain rule of differentiation to determine that the pdf is \\[\\begin{align*} f_U(u) = \\frac{d}{du} F_U(u) = \\frac{d}{du} [2\\Phi(\\sqrt{u}) - 1] &amp;= 2 \\frac{d\\Phi}{du}(\\sqrt{u}) \\cdot \\frac{d}{du}\\sqrt{u} \\\\ &amp;= 2 f_Z(\\sqrt{u}) \\cdot \\frac{1}{2\\sqrt{u}} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u}{2}\\right) \\cdot \\frac{1}{\\sqrt{u}} \\\\ &amp;= \\frac{u^{-1/2}}{\\sqrt{2\\pi}} \\exp(-\\frac{u}{2}) \\,, \\end{align*}\\] with \\(u \\in [0,\\infty)\\). This is the pdf for a chi-square distribution with one “degree of freedom”: \\[ U \\sim \\chi_1^2 \\,, \\] with the subscript “1” indicating the number of degrees of freedom. In general, the number of degrees of freedom can be an arbitrary positive integer, and it is conventionally denoted with the Greek letter \\(\\nu\\) (nu, pronounced “noo”). Let’s now look at a sample of \\(n\\) independent standard-normal random variables \\(\\{Z_1,\\ldots,Z_n\\}\\). What is the distribution of \\(W = \\sum_{i=1}^n Z_i^2\\)? This is a sum of independent random variables, so we can use mgfs to try to answer this question. The mgf for a chi-square random variable with one degree of freedom is \\[ m_{Z^2}(t) = (1-2t)^{-\\frac{1}{2}} \\,, \\] and thus the mgf for the sum of independent chi-square-distributed random variables will be \\[ m_W(t) = \\prod_{i=1}^n m_{Z_i^2}(t) = \\prod_{i=1}^n (1-2t)^{-\\frac{1}{2}} = (1-2t)^{-\\frac{n}{2}} \\,. \\] We identify \\(m_W(t)\\) as the mgf for a chi-square distribution with \\(\\nu = n\\) degrees of freedom. Thus: if we sum chi-square-distributed random variables, the summed quantity is itself chi-square distributed, with \\(\\nu\\) being the sum of the numbers of degrees of freedom for the original random variables. (This result can be generalized: if \\(X_1 \\sim \\chi_a^2\\) and \\(X_2 \\sim \\chi_b^2\\), then \\(X_1 + X_2 = W \\sim \\chi_{a+b}^2\\).) For completeness, we write down the pdf for \\(\\chi_{\\nu}^2\\): \\[ f_X(x) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2} \\Gamma(\\nu/2)} \\exp\\left(-\\frac{x}{2}\\right) \\,, \\] where \\(x \\geq 0\\) and \\(\\nu\\) is a positive integer; \\(E[X] = \\nu\\) and \\(V[X] = 2\\nu\\). (As we will see in Chapter 4, the chi-square family of distributions is a “sub-family” of the more general gamma family of distributions.) The chi-square distribution is highly skew for small values of \\(\\nu\\), while chi-square random variables converge in distribution to normal random variables as \\(\\nu \\rightarrow \\infty\\). See Figure 2.7. Figure 2.7: Chi-square distributions for \\(\\nu = 3\\) (left) and \\(\\nu = 30\\) (right) degrees of freedom. Chi-square random variables converge in distribution to normal random variables as \\(\\nu \\rightarrow \\infty\\). 2.8.1 The Expected Value of a Chi-Square Random Variable The expected value of a chi-square distribution for one degree of freedom is \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty x f_X(x) dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{1/2}}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x}{2}\\right) dx \\,. \\end{align*}\\] To find the value of this integral, we are going to utilize the gamma function \\[ \\Gamma(t) = \\int_0^\\infty u^{t-1} \\exp(-u) du \\,, \\] which we first introduced in Chapter 1. (Note that \\(t &gt; 0\\).) Recall that if \\(t\\) is an integer, then \\(\\Gamma(t) = (t-1)! = (t-1) \\times (t-2) \\times \\cdots \\times 1\\), with the exclamation point representing the factorial function. If \\(t\\) is a half-integer and small, one can look up the value of \\(\\Gamma(t)\\) online. The integral we are trying to compute doesn’t quite match the form of the gamma function integral…but as one should recognize by now, we can attempt a variable substitution to change \\(e^{-x/2}\\) to \\(e^{-u}\\): \\[ u = x/2 ~,~ du = dx/2 ~,~ x = 0 \\implies u = 0 ~,~ x = \\infty \\implies u = \\infty \\] We thus rewrite our expected value as \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\frac{\\sqrt{2}u^{1/2}}{\\sqrt{2\\pi}} \\exp(-u) (2 du) = \\frac{2}{\\sqrt{\\pi}} \\int_0^\\infty u^{1/2} \\exp(-u) du \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{3}{2}\\right) = \\frac{2}{\\sqrt{\\pi}} \\frac{\\sqrt{\\pi}}{2} = 1 \\,. \\end{align*}\\] As we saw above, the sum of \\(n\\) chi-square random variables, each distributed with 1 degree of freedom, is itself chi-square-distributed for \\(n\\) degrees of freedom. Hence, in general, if \\(W \\sim \\chi_{n}^2\\), then \\(E[W] = n\\). 2.8.2 Computing Probabilities Let’s assume at first that we have a single random variable \\(Z \\sim \\mathcal{N}(0,1)\\). What is \\(P(Z^2 &gt; 1)\\)? We know that \\(Z^2\\) is sampled from a chi-square distribution for 1 degree of freedom. So \\[ P(Z^2 &gt; 1) = 1 - P(Z^2 \\leq 1) = 1 - F_{W(1)}(1) \\,. \\] This is not easily computed by hand, so we utilize R: 1 - pchisq(1,1) ## [1] 0.3173105 The probability is 0.3173. With the benefit of hindsight, we can see why this was going to be the value all along: we know that \\(P(-1 \\leq Z \\leq 1) = 0.6827\\), and thus \\(P(\\vert Z \\vert &gt; 1) = 1-0.6827 = 0.3173\\). If we square both sides in this last probability statement, we see that \\(P(\\vert Z \\vert &gt; 1) = P(Z^2 &gt; 1)\\). What is the value \\(a\\) such that \\(P(W &gt; a) = 0.9\\), where \\(W = Z_1^2 + \\cdots + Z_4^2\\) and \\(\\{Z_1,\\ldots,Z_4\\}\\) are iid standard normal random variables? First, we recognize that \\(W \\sim \\chi_4^2\\), i.e., \\(W\\) is chi-square distributed for 4 degrees of freedom. Second, we re-express \\(P(W &gt; a)\\) in terms of the cdf of the chi-square distribution, \\[ P(W &gt; a) = 1 - P(W \\leq a) = 1 - F_{W(4)}(a) = 0.9 \\,, \\] and we rearrange terms: \\[ F_{W(4)}(a) = 0.1 \\,. \\] To isolate \\(a\\), we use the inverse CDF function: \\[ F_{W(4)}^{-1} [F_{W(4)}(a)] = a = F_{W(4)}^{-1}(0.1) \\,. \\] To compute \\(a\\), we use the R function qchisq(): (a = qchisq(0.1,4)) ## [1] 1.063623 We find that \\(a = 1.064\\). 2.9 Sample Variance of Normal Random Variables Recall the definition of the sample variance: \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The reason why we divide by \\(n-1\\) instead of \\(n\\) is that it makes \\(S^2\\) an unbiased estimator of \\(\\sigma^2\\). We will illustrate this point below when we cover point estimation. Above, we showed how the mean for a sample of independent and identically distributed normal random variables is itself normal, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). What, on the other hand, is the distribution of \\(S^2\\)? Let’s suppose that we have \\(n\\) iid normal random variables. We can write down that \\[ W = \\sum_{i=1}^n \\left( \\frac{X_i-\\mu}{\\sigma} \\right)^2 \\,, \\] and we know from results above that \\(W \\sim \\chi_n^2\\). We can work with the expression to the right of the equals sign now to determine the distribution not of \\(S^2\\) itself, but of \\((n-1)S^2/\\sigma^2\\): \\[\\begin{align*} \\sum_{i=1}^n \\left( \\frac{X_i-\\mu}{\\sigma} \\right)^2 &amp;= \\sum_{i=1}^n \\left( \\frac{(X_i-\\bar{X})+(\\bar{X}-\\mu)}{\\sigma} \\right)^2 \\\\ &amp;= \\sum_{i=1}^n \\left( \\frac{X_i-\\bar{X}}{\\sigma}\\right)^2 + \\sum_{i=1}^n \\left( \\frac{\\bar{X}-\\mu}{\\sigma}\\right)^2 + \\mbox{cross~term~equaling~zero} \\\\ &amp;= \\frac{(n-1)S^2}{\\sigma^2} + n\\left(\\frac{\\bar{X}-\\mu}{\\sigma}\\right)^2 = \\frac{(n-1)S^2}{\\sigma^2} + \\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\right)^2 \\,. \\end{align*}\\] The expression farthest to the right, \\[ Z = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\,, \\] is the standardization of \\(\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n)\\), and thus it is standard-normal distributed, and thus \\(Z^2 \\sim \\chi_1^2\\). By utilizing the general result about the sum of chi-square-distributed random variables given at the end of the last section, we can immediately see that \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^2 \\,. \\] We note that if we want to determine the distribution of \\(S^2\\) itself, all we would need to do is perform a general transformation, following the steps outlined earlier. 2.9.1 Computing Probabilities Let’s suppose we sample \\(16\\) iid normal random variables, and we know that \\(\\sigma^2 = 10\\). What is the probability \\(P(S^2 &gt; 12)\\)? (We’ll stop for a moment to answer a question that might occur to the reader. “Why are we doing a problem where we assume \\(\\sigma^2\\) is known? In real life, it almost certainly won’t be.” This is an entirely fair question. This example is contrived, but it builds towards a particular situation where we can assume a value for \\(\\sigma^2\\): hypothesis testing. The calculation we will do below is analogous to calculations we will do later when testing hypotheses about normal population variances.) The first question that we should always ask ourselves is whether we know the distribution of the quantity in the probability statement, in this case \\(S^2\\). The answer is no. (As noted above, we can derive it, but we have not explicitly done so.) However, we do know the distribution of \\((n-1)S^2/\\sigma^2\\). Hence: \\[\\begin{align*} P(S^2 &gt; 12) &amp;= P\\left(\\frac{(n-1)S^2}{\\sigma^2} &gt; \\frac{(n-1) \\cdot 12}{\\sigma^2}\\right) \\\\ &amp;= P\\left(W &gt; \\frac{15 \\cdot 12}{10}\\right) = P(W &gt; 18) = 1 - P(W \\leq 18) = 1 - F_{W(15)}(18) \\,, \\end{align*}\\] where \\(W \\sim \\chi_{n-1}^2\\) and \\(n-1 = 15\\). To compute the probability, we use pchisq(): 1 - pchisq(18,15) ## [1] 0.2626656 The probability is 0.263: there is only a 26.3% chance that we will sample a value of \\(S^2\\) greater than 12 when the true value is 10. To estimate the probability via simulation, we can repeatedly generate data samples of size \\(n = 16\\) from a normal distribution with some arbitrary mean (the value doesn’t matter) and variance \\(\\sigma^2 = 10\\); compute and record \\(S^2\\); and determine the proportion of our simulated \\(S^2\\) values that are greater than 12. Note that we record \\(S^2\\) and not \\((n-1)S^2/\\sigma^2\\); as we are performing a simulation, we need not know the distribution of \\(S^2\\) (because…we are simulating it!). All we need to know is the distribution from which the individual data are sampled. Also note that because we are not simulating an infinite number of data, the proportion that we observe will itself be a random variable with some mean, some variance, and some sampling distribution. The key here is “some variance”: to generate a more accurate accounting of the proportion of \\(S^2\\) values greater than 12, we want to sample as much data as we can (a) without having to wait too long for the result, and (b) without causing memory allocation issues by recording too many values of \\(S^2\\), if we choose to record them all. set.seed(101) # set so that the same data are generated every time m &lt;- 100000 # the number of data samples n &lt;- 16 # the size of each data sample sigma2 &lt;- 10 # the true variance s2 &lt;- rep(NA,m) # allocated space for S^2 for ( ii in 1:m ) { x &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) s2[ii] &lt;- var(x) } round(sum(s2&gt;12)/m,3) ## [1] 0.262 Another way to code this to circumvent memory allocation is set.seed(101) # set so that the same data are generated every time m &lt;- 100000 # the number of data samples n &lt;- 16 # the size of each data sample sigma2 &lt;- 10 # the true variance s2true &lt;- 0 # a counter of the number of S^2 values &gt; 12 for ( ii in 1:m ) { x &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) if ( var(x) &gt; 12) s2true = s2true+1 } round(s2true/m,3) ## [1] 0.262 The tradeoff is that while this code uses less memory, it takes (slightly) longer to run. 2.9.2 Expected Value of the Sample Variance and Standard Deviation It is extremely straightforward to determine the expected value for the sample variance: \\[ E\\left[ \\frac{(n-1)S^2}{\\sigma^2} \\right] = n-1 ~~~ \\Rightarrow ~~~ E[S^2] = \\frac{(n-1)\\sigma^2}{(n-1)} = \\sigma^2 \\,. \\] Here, we use the fact that \\(W = (n-1)S^2/\\sigma^2\\) is a chi-square-distributed random variable, and that \\(E[W]\\) equals the number of degrees of freedom, which here is \\(n-1\\). What is more difficult to determine is \\(E[(S^2)^a]\\), where \\(a\\) is some constant. For instance, \\(E[S^4]\\) is not \\(\\sigma^4\\) (making the variance of \\(S^2\\) somewhat more difficult to compute than we might initiall expect)…and \\(E[S]\\) is not \\(\\sigma\\). Let’s determine \\(E[S]\\) here. The way we do this is by performing a random variable transformation (\\(S = \\sqrt{S^2}\\)) to determine the pdf \\(f_S(s)\\), and then computing \\(E[S] = \\int s f_S(s) ds\\). We start by writing \\[\\begin{align*} P(S \\leq s) &amp;= P(\\sqrt{S^2} \\leq s) = P(S^2 \\leq s^2) = P\\left( \\frac{(n-1)S^2}{\\sigma^2} \\leq \\frac{(n-1)s^2}{\\sigma^2} \\right) \\\\ &amp;= P\\left( W \\leq \\frac{(n-1)s^2}{\\sigma^2} \\right) = F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) \\,. \\end{align*}\\] We can then use the chain rule of differentiation to write that \\[ f_S(s) = \\frac{d}{ds}F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) = f_{W(n-1)}F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) \\frac{2(n-1)s}{\\sigma^2} \\,. \\] We then substitute in the pdf: \\[ f_S(s) = \\frac{2(n-1)s}{\\sigma^2} \\frac{[(n-1)s^2/\\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) \\,. \\] The expected value of \\(S\\) is then \\[ E[S] = \\int_0^\\infty s f_S(s) ds = \\int_0^\\infty \\frac{2(n-1)s^2}{\\sigma^2} \\frac{[(n-1)s^2/\\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) ds \\,. \\] OK…how should we pursue this? Let’s try a variable substitution approach, with \\[ x = \\frac{(n-1)s^2}{2\\sigma^2} ~~~ \\Rightarrow ~~~ dx = \\frac{(n-1)s}{\\sigma^2}ds = \\frac{n-1}{\\sigma^2} \\left(\\frac{2 \\sigma^2 x}{n-1}\\right)^{1/2} ds = \\left(\\frac{ 2 (n-1) x }{\\sigma^2} \\right)^{1/2} ds \\,. \\] We note that if \\(s = 0\\), \\(x = 0\\), and if \\(s = \\infty\\), \\(x = \\infty\\), so the integral bounds are unchanged. So now we have that \\[\\begin{align*} E[S] &amp;= \\int_0^\\infty 4x \\frac{(2x)^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp(-x) \\frac{\\sigma}{\\sqrt{2(n-1)x}} dx \\\\ &amp;= \\sqrt{2} \\int_0^\\infty \\frac{x^{(n-2)/2}}{\\Gamma((n-1)/2)} \\exp(-x) \\frac{\\sigma}{\\sqrt{n-1}} dx \\\\ &amp;= \\sqrt{2} \\frac{\\sigma}{\\sqrt{n-1}} \\frac{1}{\\Gamma((n-1)/2)} \\int_0^\\infty x^{n/2-1} \\exp(-x) dx \\\\ &amp;= \\sigma \\sqrt{\\frac{2}{n-1}} \\frac{\\Gamma(n/2)}{\\Gamma((n-1)/2)} \\,. \\end{align*}\\] The integral in the second-to-last line is that which defines the gamma function \\(\\Gamma(\\cdot)\\). So we see that while \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\), \\(S\\) is a biased estimator of \\(\\sigma\\). We note, without getting into details, that as \\(n \\rightarrow \\infty\\), \\(E[S] \\rightarrow \\sigma\\), so \\(S\\) is asymptotically unbiased. 2.10 Standardizing a Normal Random Variable with Unknown Variance It is generally the case in real-life that when we assume our data are sampled from a normal distribution, both the mean and the variance are unknown. This means that instead of this \\[ Z = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1) \\,, \\] we have \\[ \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\sim \\mbox{?} \\,. \\] This last expression contains two random variables, one in the numerator and one in the denominator, and each is independent of the other. (We state this without proof…but we will appeal to intuition. For many distributions, the expected value and variance are both functions of one or more parameters \\(\\theta\\) [e.g., the exponential, binomial, and Poisson distributions, etc., etc.]. However, for a normal distribution, the expected value only depends on \\(\\mu\\) and the variance only depends on \\(\\sigma^2\\).) How would we determine the distribution of the ratio above? There is no unique way by which to approach the derivation of the distribution: we simply illustrate one way of doing so. First, we rewrite the ratio above as \\[ T = \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} / \\frac{S}{\\sigma} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} / \\sqrt{\\frac{S^2}{\\sigma^2}} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} / \\left( \\sqrt{\\frac{\\nu S^2}{\\sigma^2}} \\frac{1}{\\sqrt{\\nu}} \\right) = Z / \\left(\\sqrt{\\frac{W_{\\nu}}{\\nu}}\\right) \\,, \\] where \\(Z \\sim \\mathcal{N}(0,1)\\) and \\(W_{\\nu}\\) is sampled from a chi-square distribution with \\(\\nu = n-1\\) degrees of freedom. Our next step is to determine the distribution of the random variable \\(U = \\sqrt{W_{\\nu}/\\nu}\\). We identify that \\(V = \\sqrt{W_{\\nu}}\\) is sampled from a chi distribution (note: no square!), whose pdf may be derived via a general transformation (since \\(V = \\sqrt{\\nu}S/\\sigma\\) and since we have written down \\(f_S(s)\\) above), but can also simply be looked up: \\[ f_V(v) = \\frac{v^{\\nu-1} \\exp(-v^2/2)}{2^{\\nu/2-1}\\Gamma(\\nu/2)} \\,, \\] for \\(v \\geq 0\\) and \\(\\nu &gt; 0\\). However, we do still have to apply a general transformation to determine the pdf for \\(U = V/\\sqrt{\\nu}\\). Doing so, we find that \\[ f_U(u) = f_V(\\sqrt{\\nu}u) \\sqrt{\\nu} = \\frac{\\sqrt{\\nu} (\\sqrt{\\nu}u)^{\\nu-1} \\exp(-(\\sqrt{\\nu}u)^2/2)}{2^{\\nu/2-1}\\Gamma(\\nu/2)} \\,. \\] (So, yes, we could have derived \\(f_U(u)\\) directly from \\(f_S(s)\\) via a general transformation…but here we at least indicate to the reader that there is such a thing as the chi distribution.) At this point, we know the distributions for both the numerator and the denominator. Now we write down a general result dubbed the ratio distribution. For \\(T = Z/U\\), where \\(Z\\) and \\(U\\) are independent variables, that distribution is \\[ f_T(t) = \\int_{-\\infty}^\\infty \\vert u \\vert f_Z(tu) f_{U}(u) du \\rightarrow \\int_0^\\infty u f_Z(tu) f_{U}(u) du \\,, \\] where we make use of the fact that \\(u &gt; 0\\). Thus \\[\\begin{align*} f_T(t) &amp;= \\int_0^\\infty u \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{t^2u^2}{2}\\right) \\frac{\\sqrt{\\nu} (\\sqrt{\\nu}u)^{\\nu-1} \\exp(-(\\sqrt{\\nu}u)^2/2)}{2^{\\nu/2-1}\\Gamma(\\nu/2)} du \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{2^{\\nu/2-1}\\Gamma(\\nu/2)} \\nu^{\\nu/2} \\int_0^\\infty u^\\nu \\exp\\left(-\\frac{(t^2+\\nu)u^2}{2}\\right) du \\,. \\end{align*}\\] Given the form of the integral and the fact that it is evaluated from zero to infinity, we will use a variable substitution approach and eventually turn the integral into a gamma function: \\[ x = \\frac{(t^2+\\nu)u^2}{2} ~~~ \\Rightarrow ~~~ dx = (t^2+\\nu)~u~du \\,. \\] When \\(u = 0\\), \\(x = 0\\), and when \\(u = \\infty\\), \\(x = \\infty\\), so the integral bounds are unchanged. Hence we can rewrite the integral above as \\[\\begin{align*} f_T(t) &amp;= \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{2^{\\nu/2-1}\\Gamma(\\nu/2)} \\nu^{\\nu/2} \\int_0^\\infty \\left(\\sqrt{\\frac{2x}{(t^2+\\nu)}}\\right)^\\nu \\exp\\left(-x\\right) \\frac{dx}{\\sqrt{2 x (t^2+\\nu)}} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{2^{\\nu/2-1}\\Gamma(\\nu/2)} \\nu^{\\nu/2} 2^{\\nu/2} 2^{-1/2} \\frac{1}{(t^2+\\nu)^{(\\nu+1)/2}} \\int_0^\\infty \\frac{x^{\\nu/2}}{x^{1/2}} \\exp\\left(-x\\right) dx \\\\ &amp;= \\frac{1}{\\sqrt{\\pi}} \\frac{1}{\\Gamma(\\nu/2)} \\nu^{\\nu/2} \\frac{1}{(t^2+\\nu)^{(\\nu+1)/2}} \\int_0^\\infty x^{\\nu/2-1/2} \\exp\\left(-x\\right) dx \\\\ &amp;= \\frac{1}{\\sqrt{\\pi}} \\frac{1}{\\Gamma(\\nu/2)} \\nu^{\\nu/2} \\frac{1}{(t^2+\\nu)^{(\\nu+1)/2}} \\Gamma((\\nu+1)/2) \\\\ &amp;= \\frac{1}{\\sqrt{\\pi}} \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)} \\nu^{\\nu/2} \\frac{1}{\\nu^{(\\nu+1)/2}(t^2/\\nu+1)^{(\\nu+1)/2}} \\\\ &amp;= \\frac{1}{\\sqrt{\\nu \\pi}} \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2)} \\left(1+\\frac{t^2}{\\nu}\\right)^{-(\\nu+1)/2} \\,. \\end{align*}\\] \\(T\\) is sampled from a Student’s t distribution for \\(\\nu\\) degrees of freedom. Assuming that \\(\\nu\\) is integer-valued, the expected value of \\(T\\) is \\(E[T] = 0\\) for \\(\\nu \\geq 2\\) (and is undefined otherwise), while the variance is \\(\\nu/(\\nu-2)\\) for \\(\\nu \\geq 3\\), is infinite for \\(\\nu=2\\), and is otherwise undefined. Appealing to intuition, we can form a (symmetric) \\(t\\) distribution by taking a standard normal \\(\\mathcal{N}(0,1)\\) and “pushing down” in the center, the act of which transfers probability density equally to both the lower and upper tails. In Figure 2.8, we see that the smaller the number of degrees of freedom, the more density is transferred to the tails, and that as \\(\\nu\\) increases, the more and more \\(f_{T(\\nu)}(t)\\) becomes indistinguishable from a standard normal. (The more technical way of stating this is that the random variable \\(T\\) converges in distribution to a standard normal random variable as \\(\\nu \\rightarrow \\infty\\).) Figure 2.8: The natural logarithm of the pdf for the standard normal (black) and for \\(t\\) distributions with 3 (red dashed), 6 (green dotted), and 12 (blue dot-dashed) degrees of freedom. We observe that as \\(n\\) decreases, there is more probability density in the lower and upper tails of the distributions. 2.10.1 Computing Probabilities In a study, the diameters of 8 widgets are measured. It is known from previous work that the widget diamaters are normally distributed, with sample standard deviation \\(S = 1\\) unit. What is the probability that the the sample mean \\(\\bar{X}\\) observed in the current study will be within one unit of the population mean \\(\\mu\\)? The probability we seek is \\[ P( \\vert \\bar{X} - \\mu \\vert &lt; 1 ) = P(\\mu - 1 \\leq \\bar{X} \\leq \\mu + 1) \\,. \\] The key here is that we don’t know \\(\\sigma\\), so the probability can only be determined if we manipulate this expression such that the random variable inside it is \\(t\\)-distributed…and \\(\\bar{X}\\) itself is not. So the first step is to standardize: \\[ P\\left( \\frac{\\mu - 1 - \\mu}{S/\\sqrt{n}} \\leq \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\leq \\frac{\\mu + 1 - \\mu}{S/\\sqrt{n}}\\right) = P\\left( -\\frac{\\sqrt{n}}{S} \\leq T \\leq \\frac{\\sqrt{n}}{S}\\right) \\,. \\] We know that \\(\\sqrt{n}(\\bar{X} - \\mu)/S\\) is \\(t\\)-distributed (with \\(\\nu = n-1\\) degrees of freedom), so long as the individual data are normal iid random variables. (If the individual data are not normally distributed, this question might have to be solved via simulations…if we cannot determine the distribution of \\(\\bar{X}\\) using, e.g., moment-generating functions.) The probability is the difference between two cdf values: \\[ P\\left( -\\frac{\\sqrt{n}}{S} \\leq T \\leq \\frac{\\sqrt{n}}{S}\\right) = F_{T(7)}(\\sqrt{8}) - F_{T(7)}(-\\sqrt{8}) \\,, \\] where \\(n-1\\), the number of degrees of freedom, is 7. This is the end of the problem if we are using pen and paper. If we have R at our disposal, we would code the following: pt(sqrt(8),7) - pt(-sqrt(8),7) ## [1] 0.9745364 The probability that \\(\\bar{X}\\) will be within one unit of \\(\\mu\\) is 0.975. 2.11 Point Estimation In the previous chapter, we introduced the concept of point estimation, using statistics to make inferences about a population parameter \\(\\theta\\). Recall that a point estimator \\(\\hat{\\theta}\\), being a statistic, is a random variable and has a sampling distribution. One can define point estimators arbitrarily, but in the end, we can choose the best among a set of estimators by assessing properties of their sampling distributions: Bias: \\(B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta]\\) Variance: \\(V[\\hat{\\theta}]\\) Recall: the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If \\(E[\\hat{\\theta}-\\theta] = 0\\), then the estimator \\(\\hat{\\theta}\\) is said to be unbiased. Assuming that our observed data are independent and identically distributed (or iid), then computing each of these quantities is straightforward. Choosing a best estimator based on these two quantities might not lead to a clear answer, but we can overcome that obstacle by combining them into one metric, the mean-squared error (or MSE): MSE: \\(MSE[\\hat{\\theta}] = B[\\hat{\\theta}]^2 + V[\\hat{\\theta}]\\) In the previous chapter, we also discussed how guessing the form of an estimator is sub-optimal, and how there are various approaches to deriving good estimators. The first one that we highlight is maximum likelihood estimation (or MLE). We will apply the MLE here to derive an estimator for the normal population mean. Then we will introduce methodology by which to assess the estimator in an absolute sense, and extend these results to write down the asymptotic sampling distribution for the MLE, i.e., the sampling distribution as the sample size \\(n \\rightarrow \\infty\\). Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). The setting, again, is that we have randomly sampled \\(n\\) data from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). This means that the likelihood function is \\[\\begin{align*} \\mathcal{L}(\\mu,\\sigma \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x \\vert \\mu,\\sigma) \\\\ &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right) \\\\ &amp;= \\frac{1}{(2 \\pi)^{n/2}} \\frac{1}{\\sigma^n} \\exp\\left({-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2}\\right) \\,, \\end{align*}\\] and the log-likelihood is \\[ \\ell(\\mu,\\sigma \\vert \\mathbf{x}) = -\\frac{n}{2} \\log (2 \\pi) - n \\log \\sigma - \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\,. \\] Because there is more than one parameter, we take the partial derivative of \\(\\ell\\) with respect to \\(\\mu\\): \\[\\begin{align*} \\ell&#39;(\\mu,\\sigma \\vert \\mathbf{x}) &amp;= \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2} \\log (2 \\pi) - n \\log \\sigma - \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right) \\\\ &amp;= -\\frac{1}{\\sigma^2} \\sum_{i=1}^n 2(x_i - \\mu)(-1) \\\\ &amp;= \\frac{2}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) \\,. \\end{align*}\\] After setting this quantity to zero and dividing out the term \\(2/\\sigma^2\\), we find that \\[ \\sum_{i=1}^n x_i = \\sum_{i=1}^n \\mu ~~\\Rightarrow~~ \\sum_{i=1}^n x_i = n \\mu ~~\\Rightarrow~~ \\frac{1}{n} \\sum_{i=1}^n x_i = n \\mu ~~\\Rightarrow~~ \\mu = \\bar{x} \\,, \\] and thus \\(\\hat{\\mu}_{MLE} = \\bar{X}\\). (Recall that as we go from a purely mathematical derivation to a final definition of an estimator, we need to take into account that the estimator is a random variable and thus we need to change from lower-case to upper-case when writing out the variable. Also recall that for this MLE to be valid, the second derivative of the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\) at \\(x = \\bar{x}\\) has to be negative. We leave checking this as an exercise to the reader.) Because the estimator is \\(\\bar{X}\\), we know immediately its expected value and variance given previous results: \\(E[\\hat{\\mu}_{MLE}] = E[\\bar{X}] = \\mu\\) (the estimator is unbiased); and \\(V[\\hat{\\mu}_{MLE}] = V[\\bar{X}] = \\sigma^2/n\\) (and thus \\(MSE[\\hat{\\mu}_{MLE}] = \\sigma^2/n\\)). While \\(\\hat{\\mu}_{MLE}\\) is an unbiased estimator, recall that MLEs can be biased…but if they are, they are always asymptotically unbiased, meaning that the bias goes away as \\(n \\rightarrow \\infty\\). Here, we will introduce one more estimator concept, that of consistency: do both the bias and the variance of the estimator go to zero as the sample size \\(n\\) increases? If so, our estimator is said to be consistent. If an estimator is not consistent, it will never converge to the true value \\(\\theta\\), regardless of sample size. Here, we need not worry about the bias (\\(\\hat{\\mu}_{MLE} = \\bar{X}\\) in unbiased for all \\(n\\)), and we note that \\[ \\lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} = 0 \\,. \\] Thus \\(\\hat{\\mu}_{MLE}\\) is a consistent estimator. The question that we are going to pose now refers to a point made obliquely in the previous chapter: how do we assess \\(V[\\hat{\\mu}_{MLE}]\\) in absolute terms? Can we come up with an estimator that is more accurate for any given value of \\(n\\)? The answer lies in the Cramer-Rao inequality, which specifies the lower bound on the variance of an estimator (so long as the domain of \\(p_X(x \\vert \\theta)\\) or \\(f_X(x \\vert \\theta)\\) does not depend on \\(\\theta\\) itself…which is true for the normal). In this work, we focus on determining lower bounds for unbiased estimators, but bounds can be found for biased estimators as well, as shown here. If we sample \\(n\\) iid data, then the Cramer-Rao lower bound (CRLB) on the variance of an unbiased estimator \\(\\hat{\\theta}\\) is \\[ V[\\hat{\\theta}] = \\frac{1}{I_n(\\theta)} = \\frac{1}{nI(\\theta)} \\,, \\] where \\(I(\\theta)\\) represents the Fisher information for \\(\\theta\\) contained in a single random variable \\(X\\). Assuming we sample our data from a pdf, the Fisher information is \\[ I(\\theta) = E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f_X(x \\vert \\theta) \\right)^2\\right] ~~~\\mbox{or}~~~ I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log f_X(x \\vert \\theta) \\right] \\,. \\] (The equation to the right is valid only under certain regularity conditions, but those conditions are usually met in the context of this book and thus this is the equation that we will use in general, as it makes for more straightforward computation.) Let’s step back for a second and think about what the Fisher information represents. It is the average value of the square of the first derivative of a pdf. If a pdf’s slope is relatively small (because the pdf is wide, like a normal with a large \\(\\sigma\\) parameter), then the average value of the slope, squared, is relatively small, i.e., the information that \\(X\\) provides about \\(\\mu\\) or \\(\\sigma\\) is relatively small. This makes intuitive sense: a wide pdf means that, for instance, when we sample data and try to subsequently estimate \\(\\theta\\), we will be more uncertain about its actual value. On the other hand, if the pdf is highly concentrated, then the average slope of the pdf is larger and…\\(I(\\theta)\\) is relatively large; a sampled datum would contain more information by which to constrain \\(\\theta\\). (See Figure 2.9, in which the pdf to the left has less Fisher information content about \\(\\mu\\) than the pdf to the right.) Figure 2.9: An illustration of Fisher information. For the pdf to the left, the slope changes slowly, and thus the rate of change of the slope is smaller than for the pdf to the right. Thus the pdf to the left contains less information about the population mean, i.e., its Fisher information value \\(I(\\mu)\\) is smaller. For the normal, \\[\\begin{align*} \\log f_X(X \\vert \\mu) &amp;= -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} \\\\ \\frac{\\partial}{\\partial \\mu} \\log f_X(X \\vert \\mu) &amp;= 0 - \\frac{2(x-\\mu)(-1)}{2\\sigma^2} = \\frac{(x-\\mu)}{\\sigma^2} \\\\ \\frac{\\partial^2}{\\partial \\mu^2} \\log f_X(X \\vert \\mu) &amp;= -\\frac{1}{\\sigma^2} \\,, \\end{align*}\\] so \\[ I(\\mu) = -E\\left[-\\frac{1}{\\sigma^2}\\right] = \\frac{1}{\\sigma^2} \\,, \\] and \\[ V[\\hat{\\mu}] = \\frac{1}{n (1/\\sigma^2)} = \\frac{\\sigma^2}{n} \\,. \\] We see that the variance of \\(\\hat{\\mu} = \\bar{X}\\) achieves the CRLB, meaning that indeed we cannot propose a better unbiased estimator for the normal population mean. We conclude this section by using the Fisher information to state an important result about maximum likelihood estimates. Recall that an estimator is a statistic, and thus it is a random variable with a sampling distribution. What do we know about this distribution? For arbitrary values of \\(n\\), nothing, per se; we would have to run simulations to derive an empirical sampling distribution. However, as \\(n \\rightarrow \\infty\\), the MLE converges in distribution to a normal random variable: \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) ~\\mbox{or}~ (\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y&#39; \\sim \\mathcal{N}\\left(0,\\frac{1}{nI(\\theta)}\\right) \\,. \\] (As usual, some regularity conditions apply that do not concern us at the current time.) We already knew about the mean zero part: we had stated that the MLE is asymptotically unbiased. What’s new is the variance, and the fact that the variance achieves the CRLB, and the identification of the sampling distribution as being normal. (It turns out that the normality of the distribution is related to the Central Limit Theorem, the subject of the next section, and thus this is ultimately not a surprising result.) A sketch of how one would prove the asymptotic normality of the MLE is provided as optional material in Chapter 7. 2.11.1 Maximum Likelihood Estimation for Normal Variance As above, we will suppose that we are given a data sample \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)\\), but here, our desire is to estimate the population variance \\(\\sigma^2\\) instead of the population mean. Borrowing the form of the log-likelihood \\(\\ell(\\mu,\\sigma^2 \\vert \\mathbf{x})\\) from above, we find that \\[\\begin{align*} \\frac{\\partial \\ell}{\\partial \\sigma^2} &amp;= -\\frac{n}{2}\\frac{1}{\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (x_i-\\mu)^2 = 0 \\\\ ~\\implies~ \\hat{\\sigma^2}_{MLE} &amp;= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2 = \\hat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2 \\,, \\end{align*}\\] where the last equality follows from the fact that \\(\\hat{\\mu}_{MLE} = \\bar{X}\\). We can see immediately that \\[ \\hat{\\sigma^2}_{MLE} = \\frac{n-1}{n}S^2 \\,. \\] Thus \\[ E\\left[\\hat{\\sigma^2}_{MLE}\\right] = E\\left[\\frac{n-1}{n}S^2\\right] = \\frac{n-1}{n}\\sigma^2 \\,. \\] At this point, one might say, “wait a minute…how do we know \\(E[S^2] = \\sigma^2\\)? We know this because \\[ E[S^2] = \\frac{\\sigma^2}{n-1} E\\left[\\frac{(n-1)S^2}{\\sigma^2}\\right] = \\frac{\\sigma^2}{n-1} E[W] = \\frac{\\sigma^2}{n-1} (n-1) = \\sigma^2 \\,, \\] where \\(W\\) is a chi-square-distributed random variable for \\(n-1\\) degrees of freedom; \\(E[W] = n-1\\). Now, to get back to the narrative at hand: the MLE for \\(\\sigma^2\\) is a biased estimator, but it is asymptotically unbiased: \\[ \\lim_{n \\rightarrow \\infty} \\left( E\\left[\\hat{\\sigma^2}_{MLE}\\right] - \\sigma^2 \\right) = \\lim_{n \\rightarrow \\infty} \\left( \\frac{n-1}{n}\\sigma^2 - \\sigma^2 \\right) = 0 \\,. \\] 2.11.2 Asymptotic Normality of the MLE for the Normal Population Variance We will derive the Fisher information and use it to to write down the Cramer-Rao lower bound for the variances of estimates of the normal population variance. We have that \\[\\begin{align*} \\log f_X(X \\vert \\sigma^2) &amp;= -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} \\\\ \\frac{\\partial}{\\partial \\sigma^2} \\log f_X(X \\vert \\sigma^2) &amp;= -\\frac{1}{2}\\frac{1}{2 \\pi \\sigma^2} 2 \\pi + \\frac{(x-\\mu)^2}{2(\\sigma^2)^2} = -\\frac{1}{2 \\sigma^2} + \\frac{(x-\\mu)^2}{2(\\sigma^2)^2} \\\\ \\frac{\\partial^2}{\\partial (\\sigma^2)^2} \\log f_X(X \\vert \\sigma^2) &amp;= \\frac{1}{2 (\\sigma^2)^2} - \\frac{2(x-\\mu)^2}{2(\\sigma^2)^3} = \\frac{1}{2(\\sigma^2)^2} - \\frac{(x-\\mu)^2}{(\\sigma^2)^3} \\,, \\end{align*}\\] The expected value is \\[ E\\left[ \\frac{1}{2(\\sigma^2)^2} - \\frac{(x-\\mu)^2}{(\\sigma^2)^3} \\right] = \\frac{1}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} E[(x-\\mu)^2] = \\frac{1}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} \\sigma^2 = -\\frac{1}{2\\sigma^4} \\,, \\] and thus \\(I(\\sigma^2) = -E[-1/(2\\sigma^4)] = 1/(2\\sigma^4)\\), \\(I_n(\\sigma^2) = n/(2\\sigma^4)\\), and \\[ \\hat{\\sigma^2} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(\\sigma^2,\\frac{2\\sigma^4}{n}\\right) \\,. \\] 2.11.3 Simulating the Sampling Distribution of the MLE for the Normal Population Variance The result that we derive immediately above is an asymptotic result…but we never actually have an infinite sample size. If our sample size is low, we should expect that the distribution of \\(\\hat{\\sigma^2}\\) will deviate substantially from the asymptotic expectation, but we cannot know by how much unless we run simulations. Below, we show how we might run a simulation if we assume that we have \\(n = 15\\) data drawn from a \\(\\mathcal{N}(0,4)\\) distribution. See Figure 2.10. set.seed(101) n &lt;- 15 sigma2 &lt;- 4 k &lt;- 1000 X &lt;- matrix(rnorm(n*k,sd=sqrt(sigma2)),nrow=k) sigma2.hat &lt;- (n-1)*apply(X,1,var)/n cat(&quot;The sample mean for sigma2.hat = &quot;,mean(sigma2.hat),&quot;\\n&quot;) ## The sample mean for sigma2.hat = 3.679903 cat(&quot;The sample standard deviation for sigma2.hat = &quot;,sd(sigma2.hat),&quot;\\n&quot;) ## The sample standard deviation for sigma2.hat = 1.362853 empirical.dist &lt;- data.frame(sigma2.hat=sigma2.hat) x &lt;- seq(0.1,3*sigma2,by=0.1) y &lt;- dnorm(x,mean=sigma2,sd=sqrt(2)*sigma2/sqrt(n)) asymptotic.pdf &lt;- data.frame(x=x,y=y) ggplot(data=empirical.dist,aes(x=sigma2.hat,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0,10,by=1)) + geom_line(data=asymptotic.pdf,aes(x=x,y=y),col=&quot;red&quot;,lwd=1) + labs(x=&quot;Estimate of sigma^2&quot;) + coord_cartesian(xlim=c(0,10)) + base_theme Figure 2.10: The empirical pdf for \\(\\hat{\\sigma^2}_{MLE}\\) given \\(n = 15\\) and true variance \\(\\sigma^2=4\\). We overlay the asymptotic pdf in red. We see that if \\(n\\) is small, the distribution of the MLE for \\(\\hat{\\sigma^2}\\) is definitely not normal, but right skew with a mean smaller than the true mean (4) and standard deviation slightly smaller than the true standard deviation (1.46). 2.12 The Central Limit Theorem Thus far in this chapter, we have assumed that we have sampled individual data from normal distributions. While we have been able to illustrate many concepts related to probability and statistical inference in this setting, the reader may feel that this is unduly limiting: what if the data we sample are not normally distributed? While we do examine the world beyond normality in future chapters, there is one major concept we can discuss now: the idea that statistics computed using non-normal data can themselves have sampling distributions that are at least approximately normal. This is big: it means that, e.g., we can utilize the same machinery for deriving normal-based confidence intervals and for conducting normal-based hypothesis tests, machinery that we outline below, to generate inferences for these (approximately) normally distributed statistics. The central limit theorem, or CLT, is one of the most important probability theorems, if not the most important. It states that if we have \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) with mean \\(E[X_i] = \\mu\\) and finite variance \\(V[X_i] = \\sigma^2 &lt; \\infty\\), and if \\(n\\) is sufficiently large, then \\(\\bar{X}\\) is approximately normally distributed: \\[ \\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq z \\right) = \\Phi(z) \\,. \\] In other words, \\[ {\\bar X} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right) ~~\\mbox{or, alternatively,}~~ X_+ = \\sum_{i=1}^n X_i \\stackrel{d}{\\rightarrow} Y_+ \\sim \\mathcal{N}\\left(n\\mu,n\\sigma^2\\right) \\,. \\] Note that above we added the caveat “if \\(n\\) is sufficiently large.” How large is sufficiently large? The historical rule of thumb is that if \\(n \\gtrsim 30\\), then we may utilize the CLT. However, the true answer is that it depends on the distribution from which the data are sampled, and thus that it never hurts to perform simulations to see if, e.g., fewer (or more!) data are needed in a particular setting. A proof of the CLT that utilizes moment-generating functions is given in Chapter 7. We note an apparent limitation of the CLT: here, we say that \\[ \\frac{\\sqrt{n}(\\bar{X}-\\mu)}{\\sigma} \\stackrel{d}{\\rightarrow} Z \\sim \\mathcal{N}(0,1) \\,, \\] but in reality we rarely, if ever, know \\(\\sigma\\). If we use the sample standard deviation \\(S\\) instead, how does that effect our use of the CLT? The answer is some, but not much…effectively, it does not change the sample size rule of thumb, but it does mean that we will need a few more samples to achieve the same level of accuracy that we would achieve if we know \\(\\sigma\\). Refer to the proof in Chapter 7. We see that initially, we standardize the random variables (i.e., transform \\(X\\) to \\(Z = (X-\\mu)/\\sigma\\)) and declare that \\(E[Z] = 0\\) and \\(V[Z] = 1\\). The latter equality no longer holds if we use \\(Z&#39; = (X-\\mu)/S\\) instead! But \\(V[Z&#39;]\\) does converge to 1 as \\(n \\rightarrow \\infty\\): \\(S^2 \\rightarrow \\sigma^2\\) by the Law of Large Numbers, and \\(S \\rightarrow \\sigma\\) by the continuous mapping theorem. So even if we do not know \\(\\sigma\\), the CLT will “kick in” eventually. Another question the reader may have is whether or not it is the case that, since \\(\\sqrt{n}(\\bar{X}-\\mu)/\\sigma\\) converges in distribution to a standard normal random variable, it would also be the case that \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) will converge in distribution to a \\(t\\)-distributed random variable. The short answer: no. \\(t\\)-distributed data are such because when data are drawn from a normal distribution, \\(\\bar{X}\\) is normally distributed and \\((n-1)S^2/\\sigma^2\\) is chi-square distributed, and the ratio of \\(\\bar{X}-\\mu\\) and \\(S/\\sqrt{n}\\) is a \\(t\\)-distributed random variable. If the individual data are drawn from an arbitrary distribution, then \\((n-1)S^2/\\sigma^2\\) will deviate, perhaps markedly, from being chi-square distributed, thus we cannot say anything about the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\)…other than it eventually converges in distribution to a standard normal random variable. An important final note: if we have data drawn from a known, non-normal distribution and we need to derive the distribution of \\(\\bar{X}\\) in order to, e.g., construct confidence intervals or to perform hypothesis tests, we should not just default to utilizing the CLT! We might be able to compute the exact distribution for \\(\\bar{X}\\), for all sample sizes \\(n\\), via, e.g., the method of moment-generating functions. 2.12.1 Computing Probabilities Let’s assume that we have a sample of \\(n = 64\\) iid data drawn from unknown distribution with mean \\(\\mu = 10\\) and finite variance \\(\\sigma^2 = 9\\). What is the probability that the observed sample mean will be larger than 11? This is a good example of a canonical CLT exercise: \\(\\mu\\) and \\(\\sigma\\) are given to us, but the distribution is left unstated…and \\(n \\geq 30\\). This is a very unrealistic: when will we know population parameters exactly? But such an exercise has its place: it allows us to practice the computation of probabilities. \\[\\begin{align*} P\\left( \\bar{X} &gt; 11 \\right) &amp;= 1 - P\\left( \\bar{X} \\leq 11 \\right) \\\\ &amp;= 1 - P\\left( \\frac{\\sqrt{n}(\\bar{X}-\\mu)}{\\sigma} \\leq \\frac{\\sqrt{n}(11-\\mu)}{\\sigma}\\right) \\\\ &amp;\\approx 1 - P\\left( Z \\leq \\frac{8(11-10)}{3} \\right) = 1 - \\Phi\\left(\\frac{8}{3}\\right) \\,. \\end{align*}\\] As we know by now, we cannot go any further by hand, so we call upon R: 1 - pnorm(8/3) ## [1] 0.003830381 The probability that \\(\\bar{X}\\) is greater than 11 is small: 0.0038. To reiterate a point made above: if we were given \\(\\mu\\) but not \\(\\sigma^2\\), we would plug in \\(S\\) instead and expect that the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) would be not as close to a standard normal as the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/\\sigma\\)…it takes \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) “longer” to converge in distribution to a standard normal as \\(n\\) increases. 2.13 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}\\) by solving the following equation: \\[ F_Y(y_{\\rm obs} \\vert \\theta) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\), \\(y_{\\rm obs}\\) is the observed value of the statistic, and \\(q\\) is an appropriate quantile value that is determined using the confidence interval reference table introduced in section 16 of Chapter 1. When we construct confidence intervals, we are allowed to utilize any arbitrary statistic, so long as we know the sampling distribution of that statistic (which links the statistic value to the underlying parameter of interest). However, in the context of the normal distribution, there are conventional choices: \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is known: we use the sample mean \\(\\bar{X}\\) \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is unknown: we still use \\(\\bar{X}\\) \\(\\theta = \\sigma^2\\): we use the sample variance \\(S^2\\) These are conventional choices, but we will state here that there are well-established reasons for using these statistics based on the idea of interval length: if we examine the use of two separate statistics for constructing a confidence interval for \\(\\theta\\), we generally want to use the one that generates shorter confidence intervals (i.e., the one that indicates the least amount of uncertainty about \\(\\theta\\)). And \\(\\bar{X}\\) and \\(S^2\\) are generally the best statistics to use with normally distributed data. (We keep saying “generally.” In an academic setting, where there are no outlier data and the data-generating process is clean, \\(\\bar{X}\\) and \\(S^2\\) will be the best choices of statistics. In a real-world setting, where data are messy, there may be times where utilizing, e.g., the sample median is warranted. Luckily for us, here we exist in the world of pristine data…the real world can wait.) Let’s assume that we have collected a sample of \\(n\\) iid data, drawn from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and with sample mean and variance \\(\\bar{X}\\) and \\(S^2\\), and let’s examine each of the three situations outlined above. \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is known. We adopt \\(Y = \\bar{X}\\), with the observed statistic being \\(y_{\\rm obs} = \\bar{x}\\). By using the method of moment-generating functions, we have found that \\[ Y \\sim \\mathcal{N}(\\mu,\\sigma^2/n) \\] and thus that \\[ F_Y(y) = \\frac{1}{2}\\left[ 1 + {\\rm erf}\\left(\\frac{\\sqrt{n}(y - \\mu)}{\\sqrt{2}\\sigma}\\right) \\right] \\,. \\] We can see immediately that we are not solving interval bounds by hand! For instance, \\[ \\hat{\\mu}_U = y_{\\rm obs} - \\frac{\\sigma}{\\sqrt{n}}{\\rm erf}^{-1}(2q-1) \\,, \\] where \\(q = \\alpha/2\\) and where erf\\(^{-1}(\\cdot)\\) is the inverse error function, which is not analytically tractable. Thus we fall back upon numerical methods, utilizing cdf-evaluation functions. In an example below, we provide R code, utilizing pnorm(), for computing confidence interval bounds. \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is unknown. We adopt \\(Y = \\bar{X}\\), with the observed statistic being \\(y_{\\rm obs} = \\bar{x}\\). Earlier in this chapter, we show that \\[ T = \\frac{Y-\\mu}{S/\\sqrt{n}} \\sim t(n-1) \\,, \\] where \\(t(n-1)\\) is the \\(t\\)-distribution for \\(n-1\\) degrees of freedom. We immediately sense a problem here: we need to know \\(F_Y(y)\\) for our root-finding algorithm to work; \\(F_{T(n-1)}(t)\\) will not help us! (\\(T\\) subsumes the parameter of interest \\(\\mu\\), leaving us unable to solve for \\(\\mu\\) when applying the root-finding algorithm.) However, we can determine \\(F_Y(y)\\) through a random-variable transformation. Let \\(Y = aT+b\\), where \\(a = s_{\\rm obs}/\\sqrt{n}\\) and \\(b = \\mu\\). Then \\[ F_Y(y) = P(Y \\leq y) = P\\left(T \\leq \\frac{y-b}{a}\\right) = F_{T,n-1}\\left(\\frac{y-b}{a}\\right) \\,. \\] Thus, in place of, e.g., \\(F_Y(y_{\\rm obs} \\vert \\mu_{\\alpha/2})\\), we plug into the root-finding equation the quantity \\(F_{T,n-1}((y_{\\rm obs}-\\mu_{\\alpha/2})/(s_{\\rm obs}/\\sqrt{n}))\\); the solution in this case would be \\[ \\hat{\\mu}_U = y_{\\rm obs} - \\frac{s_{\\rm obs}}{\\sqrt{n}}F_{T,n-1}^{-1}\\left(\\frac{\\alpha}{2}\\right) \\,. \\] As we cannot work with \\(F_{T(n-1)}^{-1}\\left(\\frac{\\alpha}{2}\\right)\\) by band, we will again fall back on numerical methods, and we will show in an example below how to compute intervals using R code. \\(\\theta = \\sigma^2\\). We adopt \\(Y = S^2\\), with the observed statistic being \\(y_{\\rm obs} = s_{\\rm obs}^2\\). Earlier in this chapter, we show that \\[ W = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^2 \\,, \\] where \\(\\chi_{n-1}^2\\) is the chi-square distribution for \\(n-1\\) degrees of freedom. Here, we face the same problem that we faced immediately above: we want \\(F_Y(y)\\), but are given \\(F_{W(n-1)}(w)\\). We mitigate this issue in the same manner as above, by employing a random-variable transformation. \\[ F_Y(y) = P(Y \\leq y) = P\\left(W \\leq \\frac{(n-1)y_{\\rm obs}}{\\sigma^2}\\right) = F_{W,n-1}\\left(\\frac{(n-1)y_{\\rm obs}}{\\sigma^2}\\right) \\] As was the case above when we deal with the \\(t\\) distribution, we end up having inverse cdfs that we cannot evaluate by hand, such as in the expression \\[ \\widehat{\\sigma^2}_U = \\frac{(n-1)y_{\\rm obs}}{F_{W,n-1}^{-1}(\\alpha/2)} \\,. \\] As has been the case thus far, the evaluation of bounds requires coding, and we show an example of such an evaluation below. As this point, the reader may say, “all this looks nothing like the confidence interval stuff I learned in introductory statistics.” And the reader would be correct. The way in which the construction of confidence intervals is described above and in Chapter 1 is different from the way in which it is described in introductory statistics classes and as well as in traditional calculus-based probability and statistical inference classes. In short: we propose a modern approach that is appropriate in the age of computers that yields the same results as traditional approaches. In introductory statistics classes, equations are generally all that are provided for confidence intervals, such as the one for putting bounds on the normal mean \\(\\mu\\) when the variance is known: \\[ \\bar{x}_{\\rm obs} \\pm z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\,, \\] where \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\). (We note that historically this equation has also been applied in situations where we have sampled iid normal data but the variance is unknown, if \\(n \\gtrsim 30\\). This context generally falls under the textbook heading of “large-sample confidence intervals.” This is just simply wrong! The only reason this is done is because textbook writers have not wanted to include \\(t\\) tables in their books that go beyond \\(n = 30\\).) We would argue that such equations obscure the reality what is actually happening when we construct confidence intervals: “moving” the sampling distribution for a statistic \\(Y\\) back and forth, by changing the value of a parameter \\(\\theta\\), until the value of \\(\\theta\\) becomes either too low or too high for the observed value \\(y_{\\rm obs}\\) to continue to be plausible. 2.13.1 Confidence Interval for the Normal Mean With Variance Known Here we construct a two-sided confidence interval for the normal mean \\(\\mu\\) when the variance \\(\\sigma^2\\) is known. We assume that we have collected 25 iid data and that \\(\\sigma^2 = 1\\). set.seed(101) alpha &lt;- 0.05 n &lt;- 25 sigma2 &lt;- 1 X &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) f &lt;- function(mu,sigma2,n,y.obs,q) { pnorm(y.obs,mean=mu,sd=sqrt(sigma2/n))-q } uniroot(f,interval=c(-100,100),sigma2=sigma2,n=n,y.obs=mean(X),1-alpha/2)$root ## [1] -0.4875524 uniroot(f,interval=c(-100,100),sigma2=sigma2,n=n,y.obs=mean(X),alpha/2)$root ## [1] 0.2964302 In this code above, we search between \\(\\mu = -10,000\\) and \\(\\mu = 10,000\\) to see where the function \\(F_Y(y_{\\rm obs} \\vert \\theta_q) - q\\) crosses zero, where \\(q\\) is \\(1 - \\alpha/2\\) (lower bound) or \\(\\alpha/2\\) (upper bound). (Making the range of possible \\(\\mu\\) values large has no noticeable impact on computation time and helps ensure that the root will not lie outside the specified interval.) We test the code by sampling \\(n = 25\\) data from a standard normal distribution \\(\\mathcal{N}(0,1)\\), and passing the statistic \\(y_{\\rm obs} = \\bar{x}_{\\rm obs}\\) to uniroot(). We find that the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_U] = [-0.488,0.296]\\), which overlaps the true value. (See Figure 2.11.) Figure 2.11: Sampling distributions for \\(Y = \\bar{X} = \\sum_{i=1}^n X_i\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,1)\\), and where (left) \\(\\mu=-0.488\\) and (right) \\(\\mu=0.296\\). We observe \\(y_{\\rm obs} = -0.096\\) and we want to construct a 95% confidence interval. \\(\\mu=-0.488\\) is the smallest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.975) = -0.096\\), while \\(\\mu=0.296\\) is the largest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.025) = -0.096\\). We note that to verify the “coverage” of intervals, i.e., the proportion of the intervals that overlap the true value, we can simply wrap the code provided above with a for loop, and save the lower and upper bounds for each generated dataset. set.seed(101) num.sim &lt;- 10000 alpha &lt;- 0.05 n &lt;- 25 sigma2 &lt;- 1 lower &lt;- rep(NA,num.sim) upper &lt;- rep(NA,num.sim) f &lt;- function(mu,sigma2,n,y.obs,q) { pnorm(y.obs,mean=mu,sd=sqrt(sigma2/n))-q } for ( ii in 1:num.sim ) { X &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) lower[ii] &lt;- uniroot(f,interval=c(-100,100),sigma2=sigma2,n=n,y.obs=mean(X),1-alpha/2)$root upper[ii] &lt;- uniroot(f,interval=c(-100,100),sigma2=sigma2,n=n,y.obs=mean(X),alpha/2)$root } We can then see how often the true value is within the bounds: truth &lt;- 0 in.bound &lt;- (lower &lt;= truth) &amp; (upper &gt;= truth) cat(&quot;The empirical coverage is &quot;,sum(in.bound)/num.sim,&quot;\\n&quot;) ## The empirical coverage is 0.9518 Here, if lower &lt;= truth, then TRUE is returned, as is the case if upper &gt;= truth. &amp; is the logical and operator, which returns TRUE if the input is TRUE &amp; TRUE and returns FALSE otherwise. R treats TRUE as equivalent to 1 (and FALSE as equivalent to 0), so sum(in.bound) returns the number of final TRUE values, i.e., the number of evaluated intervals that overlap the true value. Our estimated coverage is 0.9518. We do not expect a value of exactly 0.95 given that we only run a finite number of simulations and thus the coverage will exhibit random variation; however, we can say that this value is “close” to 0.95 and thus almost certainly compatible with 0.95. Once we discuss the binomial distribution in Chapter 3, we will more easily be able to quantify the notion of being “close” to the expected value. 2.13.2 Confidence Interval for the Normal Mean With Variance Unknown We extend the previous example by assuming the variance is unknown. set.seed(101) alpha &lt;- 0.05 n &lt;- 25 X &lt;- rnorm(n,mean=0,sd=1) f &lt;- function(mu,s2,n,y.obs,q) { pt((y.obs-mu)/sqrt(s2/n),n-1)-q } uniroot(f,interval=c(-100,100),s2=var(X),n=n,y.obs=mean(X),1-alpha/2)$root ## [1] -0.4483761 uniroot(f,interval=c(-100,100),s2=var(X),n=n,y.obs=mean(X),alpha/2)$root ## [1] 0.2572646 The code above is largely equivalent to that in the previous example, with the only changes being swapping in the call to pt() (and altering the argument in that call) in place of the call to pnorm(), and replacing the known value of \\(\\sigma^2\\) (sigma2 in the previous example) with \\(s_{\\rm obs}^2\\) (s2). We find that the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_U] = [-0.448,0.257]\\), which is similar to the interval derived in the last example and which still overlaps the true value. (See Figure 2.12.) Figure 2.12: Sampling distributions for \\(Y = \\bar{X}\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and where (left) \\(\\mu=-0.448\\) and (right) \\(\\mu=0.257\\). We observe \\(y_{\\rm obs} = -0.096\\) and \\(s_{\\rm obs}^2 = 0.855\\) and we want to construct a 95% confidence interval. \\(\\mu=-0.448\\) is the smallest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.975) = -0.096\\), while \\(\\mu=0.257\\) is the largest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.025) = -0.096\\). 2.13.3 Confidence Interval for the Normal Variance Below, we adapt our confidence-interval code to the problem of estimating an interval for the variance \\(\\sigma^2\\). set.seed(101) alpha &lt;- 0.05 n &lt;- 25 X &lt;- rnorm(n,mean=0,sd=1) f &lt;- function(sigma2,n,y.obs,q) { pchisq(y.obs*(n-1)/sigma2,n-1)-q } uniroot(f,interval=c(1.e-6,10000),n=n,y.obs=var(X),1-alpha/2)$root ## [1] 0.4454088 uniroot(f,interval=c(1.e-6,10000),n=n,y.obs=var(X),alpha/2)$root ## [1] 1.413897 The changes made to the code above include swapping in pchisq(), removing the variable mean, removing the variable s2 (as y.obs itself is now \\(s_{\\rm obs}^2\\)), and adjusting the lower bound on the interval (since \\(\\sigma^2 &gt; 0\\)). We find that the interval is \\([\\widehat{\\sigma}_L^2,\\widehat{\\sigma}_U^2] = [0.445,1.414]\\), which overlaps the true value. (See Figure 2.13.) Figure 2.13: Sampling distributions for \\(Y = S^2\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and where (left) \\(\\sigma^2=0.445\\) and (right) \\(\\sigma^2=1.414\\). We observe \\(y_{\\rm obs} = s_{\\rm obs}^2 = 0.731\\) and we want to construct a 95% confidence interval. \\(\\sigma^2=0.445\\) is the smallest value of \\(\\sigma^2\\) such that \\(F_Y^{-1}(0.975) = 0.731\\), while \\(\\mu=1.414\\) is the largest value of \\(\\sigma^2\\) such that \\(F_Y^{-1}(0.025) = 0.731\\). 2.13.4 Confidence Interval: Using the CLT Let’s assume that we have a iid sample of \\(n\\) data drawn from the distribution \\[ f_X(x) = \\theta x^{\\theta-1} ~~~~ x \\in [0,1] \\,. \\] This distribution is decidedly not normal. Can we derive a confidence interval for the mean of this distribution, where the mean is \\[ E[X] = \\int_0^1 \\theta x x^{\\theta-1} dx = \\left. \\frac{\\theta}{\\theta+1} x^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,? \\] If we wanted to try to do this “exactly,” then one workflow would be (a) to attempt to determine the sampling distribution of \\(Y = \\bar{X}\\) (utilizing, e.g., moment-generating functions), and then (b) code up a variant of the codes given in the examples above. However, regarding (a): the mgf of the distribution is \\[ m_X(t) = E[e^{tX}] = \\theta \\int_0^1 x^{\\theta-1} e^{tx} dx \\,. \\] This looks suspiciously like a gamma-function integral, except the bounds here are 0 and 1, not 0 and \\(\\infty\\)…what we actually have (or, technically, what we would have after an appropriate variable substitution) is an incomplete gamma function integral. We cannot easily work with this…and so we turn to the Central Limit Theorem. If \\(n \\gtrsim 30\\), we may write that \\[ \\bar{X} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}(\\mu,s_{\\rm obs}^2/n) \\,. \\] (Recall that the CLT assumes that we know \\(\\sigma^2\\), but if we do not and use \\(S^2\\) instead, the result will still hold as \\(n \\rightarrow \\infty\\).) Thus we would use the confidence interval approach that we describe in the first example above, while swapping in \\(s_{\\rm obs}^2\\) for \\(\\sigma^2\\). set.seed(101) alpha &lt;- 0.05 n &lt;- 25 theta &lt;- 2 # so the mean is 2/3 X &lt;- rbeta(n,theta,1) # theta x^(theta-1) is a beta distribution f &lt;- function(mu,y.obs,s2,n,q) { pnorm(y.obs,mean=mu,sd=sqrt(s2/n))-q } uniroot(f,interval=c(-100,100),y.obs=mean(X),s2=var(X),n=n,1-alpha/2)$root ## [1] 0.5888778 uniroot(f,interval=c(-100,100),y.obs=mean(X),s2=var(X),n=n,alpha/2)$root ## [1] 0.7532408 The evaluated confidence interval is \\([0.589,0.753]\\) (which overlaps the true value \\(\\mu = 2/3\\)). Note, however, that because an approximate sampling distribution is being used, the coverage can (and will) deviate from expectation (e.g., 0.95). Let’s see how much the coverage deviates in this one case via simulation: set.seed(101) alpha &lt;- 0.05 num.sim &lt;- 10000 n &lt;- 25 theta &lt;- 2 lower &lt;- rep(NA,num.sim) upper &lt;- rep(NA,num.sim) f &lt;- function(mu,y.obs,s2,n,q) { pnorm(y.obs,mean=mu,sd=sqrt(s2/n))-q } for ( ii in 1:num.sim ) { X &lt;- rbeta(n,theta,1) lower[ii] &lt;- uniroot(f,interval=c(-100,100),y.obs=mean(X),s2=var(X),n=n,1-alpha/2)$root upper[ii] &lt;- uniroot(f,interval=c(-100,100),y.obs=mean(X),s2=var(X),n=n,alpha/2)$root } truth &lt;- 2/3 in.bound &lt;- (lower &lt;= truth) &amp; (upper &gt;= truth) cat(&quot;The estimated coverage is &quot;,sum(in.bound)/num.sim,&quot;\\n&quot;) ## The estimated coverage is 0.936 The estimated coverage is 0.936; it is not quite 0.95. Thus it appears that are evaluated confidence intervals do not overlap the true value as often as we would expect. Note that we would expect that as \\(n \\rightarrow 0\\), the approximation will become progressively worse, and that as \\(n \\rightarrow \\infty\\), the coverage should asymptotically approach \\(1 - \\alpha\\). 2.13.5 Historical Digression: the Pivotal Method The pivotal method is an often-used analytical method for constructing confidence intervals wherein one “reformats” the problem to solve in such a way that one can ultimately utilize statistical tables to determine interval bounds. In the age of computers, the pivotal method has become unnecessary; the numerical root-finding algorithm presented in this chapter constructs the same intervals (and does so even in situations where the pivotal method cannot be used). The steps of the pivotal method are as follows: Determine a pivotal quantity \\(Y\\), a statistic that is a function of both the observed data and the parameter \\(\\theta\\), and that has a sampling distribution that does not depend on \\(\\theta\\). (Note that, as hinted at above, there is no guarantee that a pivotal quantity exists in any given situation. But there may also be situations where we can define multiple pivotal quantities; if so, it does not matter which we adopt, as they all will help generate the same interval.) Determine constants \\(a\\) and \\(b\\) such that \\(P(a \\leq Y \\leq b) = 1-\\alpha\\). (Here, we are assuming that we are constructing a two-sided interval.) Rearrange the terms within the probability statement such that \\(\\theta\\) is alone in the middle: the quantities on either end are the interval bounds. Let’s demonstrate how this plays out in the situation where we sample \\(n\\) iid data from a normal distribution with known variance, and wish to construct an interval for \\(\\mu\\). As we know, in this situation, \\[ \\bar{X} \\sim \\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\,. \\] \\(\\bar{X}\\) is not a pivotal quantity: its sampling distribution depends on \\(\\mu\\). However… \\[ Y = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1) \\] is a pivotal quantity, as it depends on both \\(\\bar{X}\\) and \\(\\mu\\) and has a sampling distribution that does not depend on \\(\\mu\\). Thus step 1 is complete. As for step 2: \\[\\begin{align*} P(a \\leq Y \\leq b) = P(Y \\leq b) - P(Y \\leq a) &amp;= 1-\\alpha \\\\ \\Phi(b) - \\Phi(a) &amp;= 1-\\alpha \\,. \\end{align*}\\] OK…we appear stuck here, except that we can fall back upon the fact that the standard normal distribution is symmetric around zero, and thus we can say that \\(a = -b\\). That, and the fact that symmetry allows us to write that \\(\\Phi(-b) = 1 - \\Phi(b)\\), allows us to continue: \\[\\begin{align*} \\Phi(b) - \\Phi(a) &amp;= 1-\\alpha \\\\ \\Phi(b) - (1 - \\Phi(b)) &amp;= 1-\\alpha \\\\ 2\\Phi(b) - 1 &amp;= 1-\\alpha \\\\ \\Phi(b) &amp;= \\frac{2-\\alpha}{2} = 1 - \\frac{\\alpha}{2} \\\\ b &amp;= \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) = z_{1-\\alpha/2} \\\\ \\end{align*}\\] Historically, at this point, one would make use of a \\(z\\) table to determine that, e.g., \\(b = z_{0.975} = 1.96\\). So now we move on to step 3: \\[\\begin{align*} P\\left(-z_{1-\\alpha/2} \\leq Y \\leq z_{1-\\alpha/2}\\right) = P\\left(-z_{1-\\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\alpha/2}\\right) &amp;= 1-\\alpha \\\\ \\Rightarrow P\\left(\\bar{X} - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right) &amp;= 1-\\alpha \\,. \\end{align*}\\] Thus our confidence interval is \\[ \\bar{x}_{\\rm obs} \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\,. \\] 2.14 Hypothesis Testing: Testing for Normality Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-tail test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bound given by \\(y_{RR}\\), which is a solution to the equation \\[ F_Y(y_{RR} \\vert \\theta_o) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\) and \\(q\\) is an appropriate quantile value that is determined using the hypothesis test reference table introduced in section 17 of Chapter 1. Note that the hypothesis test framework only allows us to make a decision about a null hypothesis; nothing is proven. In a conventional data analysis workflow, we might receive a sample of \\(n\\) iid data without any knowledge about the distribution from which the individual data are drawn, and we might want to test a hypothesis about, e.g., the population mean. There are potentially many hypothesis tests from which to choose, but the ones that are generally the most powerful (i.e., the ones that do the best at discriminating between a null hypothesis and a given alternative hypothesis) are the ones that assume a functional form for the distribution of the individual data. Below, we provide details about tests of population means and variances that are built upon the assumption that the individual data are iid draws from a normal distribution…but before we talk about them, we will introduce hypothesis tests that allow us to decide whether our data are plausibly normally distributed in the first place. To be clear about the analysis workflow… If we perform a hypothesis test for which the null hypothesis is that the individual data are normally distributed, and we fail to reject this null hypothesis, we can feel free to use the hypothesis tests in the next two sections, regardless of the sample size \\(n\\). If we reject the null hypothesis but the sample size is sufficiently large (\\(n \\gtrsim 30\\)), and we wish to test hypotheses about the population mean, we can fall back on the central limit theorem and construct a variant of the tests presented in the next section. If we reject the null hypothesis that the data are normally distributed, and the sample size is small or if we wish to test hypotheses about population variances (or both), we should tread carefully if we decide to use the hypothesis testing frameworks presented in the next two sections, as the results we get might be very inaccurate(!). However, regarding the third point above, we have to keep in mind that as \\(n\\) gets larger and larger, tests of normality become more and more prone to rejecting the null even when the deviation of the true distribution from a normal is small, meaning that the hypothesis tests detailed in the next two sections might actually yield “useful” results! Again, we would need to tread carefully. Alternative approaches include implementing so-called nonparametric tests (which we do not cover in this book), or trying to determine another distribution with which the data are consistent and building a test utilizing its properties, etc. In the end: if in doubt, simulate. If we can make an assumption about the distribution from which our \\(n\\) iid data are sampled, we can always carry out hypothesis tests numerically. The reader should always keep this in mind: in the age of computers, one rarely if ever needs to “force” an assumption of normality into the solution of a hypothesis testing problem. Let’s assume that we have collected \\(n\\) iid data from some unknown (and unassumed) distribution \\(P\\). Could \\(P\\) be the normal distribution? There are several varieties of tests whose null hypotheses are “the data are sampled from the [insert name here] distribution.” The most well-known and often-used is the Kolmogorov-Smirnov test, or KS test. (We note that it is not necessarily the most powerful test among those that assess the consistency of data with named distributions, but it is easy to implement and simple to understand. The interested reader should investigate alternatives like, e.g., the Anderson-Darling test and Cramer-von Mises test, and for the specific case of testing for normality, the Shapiro-Wilk test, which we discuss below in an example.) The empirical cumulative distribution function (or ecdf) for a dataset is defined as \\[ F_{X,n}(x) = \\frac{1}{n} \\left(\\mbox{number of observed data} \\, \\leq x\\right) \\,. \\] We see immediately that \\(F_{X,n}(x)\\) behaves like a cdf, in the sense that \\(F_{X,n}(-\\infty) = 0\\) and \\(F_{X,n}(\\infty) = 1\\), etc. It is a monotonically increasing step function, with steps of size \\(1/n\\) taken at each datum’s coordinate \\(x_i\\). If we assume a particular distribution as the null distribution, with cdf \\(F_X(x)\\), then the KS test statistic is \\[ D_n = \\mbox{sup} \\vert F_{X,n}(x) - F_X(x) \\vert \\,. \\] We have seen the abbrevation “inf” before, signaling that we are taking the infimum, or smallest value, of a set; here, “sup” stands for supremum, the largest value of a set. So for the KS test, the test statistic is simply the largest distance observed between the empirical and null cdfs; if the null is wrong, we expect this distance to be large. Under the null, \\(K = \\sqrt{n}D_n\\) is assumed to be, at least approximately, sampled from the Kolmogorov distribution, whose cdf is \\[ P(K \\leq k) = \\frac{\\sqrt{2\\pi}}{k} \\sum_{x=1}^\\infty \\exp\\left(-\\frac{(2x-1)^2\\pi^2}{8k^2}\\right) \\,. \\] Having sampled \\(k_{obs} = \\sqrt{n}D_n\\), we can use this cdf to establish whether our not the test statistic falls into the rejection region. If it does, we reject the null hypothesis that the individual data are normally distributed. 2.14.1 The Kolmogorov-Smirnov Test Let’s suppose we are given a dataset that has the empirical distribution given in Figure 2.14. Figure 2.14: Histogram of a dataset with sample size \\(n = 30\\). Is it plausible that these data are normally distributed? The sample statistics for these data are \\(\\bar{X} = 100.6\\) and \\(S = 29.8\\). In Figure 2.15, we plot the empirical cdf for these data, overlaying the cdf for a \\(\\mathcal{N}(100.6,29.8^2)\\) distribution. Figure 2.15: Empirical cumulative distribution function of our dataset with the cdf for a \\(\\mathcal{N}(100.6,29.8^2)\\) distribution overlaid as the red dashed line. The largest difference between the empirical cdf and the overlaid normal cdf in Figure 2.15 occurs at \\(x \\approx 93\\). Is this difference large enough for us to decide the data are not normally distributed with the inferred parameters \\(\\mu = 100.6\\) and \\(\\sigma = 29.8\\)? ks.test(x,&quot;pnorm&quot;,100.6,29.8) ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.13858, p-value = 0.5649 ## alternative hypothesis: two-sided By default, ks.test() performs a two-sided test; type ?ks.test in the R console to see how to specify alternatives. According to this test, the \\(p\\)-value is 0.56. We formally introduce \\(p\\)-values below, in the next section; it suffices to say here that if \\(p &lt; \\alpha\\), where \\(\\alpha\\) is the user-specified Type I error (typically 0.05), we reject the null, so here we fail to reject the null hypothesis that the data are normally distributed. In truth, the simulated data are sampled from a gamma distribution (see Chapter 4); with a larger sample size, the test becomes more better able to differentiate between normally distributed and gamma-distributed data, so eventually we would reject the null hypothesis. 2.14.2 The Shapiro-Wilk Test The Shapiro-Wilk test has the null hypothesis that our \\(n\\) iid data are sampled from a normal distribution. The test statistic \\(W\\), which utilizes order statistics (a concept we introduce in Chapter 3), is complicated and will not be reproduced here. It suffices for now for us to make three points. The Shapiro-Wilk test is better able to reject the null hypothesis that our data are normally distributed than the KS test. (This makes sense because the SW test is built to be more “specific”\\(-\\)it is just used to test for normality). In other words, given the same data, the SW test will almost always output a smaller \\(p\\)-value than the KS test. The sampling distribution for \\(W\\) is non-analytic and thus the rejection region is estimated numerically, and thus the Shapiro-Wilk test cannot be used in R with samples of size \\(&gt;\\) 5000. If our sample size is larger, we should randomly sub-sample the data before performing the SW test. The Shapiro-Wilk test will often reject the null hypothesis of normality even when the data appear by eye to be very nearly normally distributed. This goes back to one of the points made above: we still might be able to glean “useful” results when performing normal-based hypothesis tests using technically non-normal data. Tread carefully, and simulate if necessary. Below, we demonstrate the use of the Shapiro-Wilk test with the gamma-distributed data of the first example above. shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.9508, p-value = 0.1776 We see that while the KS test returned a \\(p\\)-value of 0.56, the Shapiro-Wilk test returns the value 0.18. This value is still larger than \\(\\alpha = 0.05\\), so we still fail to reject the null. 2.15 Hypothesis Testing: Population Mean Let’s begin by assuming we are given \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) sampled from a normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\). Because \\(Y = \\bar{X}\\) is the MLE for the normal mean, we identify it as a plausible statistic for testing hypotheses about \\(\\mu\\). (We will justify this choice in in Chapters 3 and 4 when we discuss the Neyman-Pearson Lemma and the Likelihood Ratio Test.) Refer back to the hypothesis test reference table in section 17 of Chapter 1. We observe that \\(E[Y] = E[\\bar{X}] = \\mu\\), so \\(E[Y]\\) increases with \\(\\mu\\)…and thus the rejection regions for a two-tail test are \\[ y_{\\rm obs} &lt; y_{\\rm RR,lo} ~~~ \\mbox{and} ~~~ y_{\\rm obs} &gt; y_{\\rm RR,hi} \\,, \\] where \\(y_{\\rm RR,lo}\\) and \\(y_{\\rm RR,hi}\\) are solutions to the equations \\[ F_Y(y \\vert \\mu_o,\\sigma^2) - q = 0 \\,, \\] with \\(q = \\alpha/2\\) and \\(q = 1-\\alpha/2\\), respectively. Can we solve for rejection region boundaries analytically? The answer is no: the pdf for our test statistic is \\(\\mathcal{N}(\\mu,\\sigma^2/n)\\), and its cdf contains the error function; this is not a cdf we can work with by hand. But finding a numerical solution is straightforward: for instance, \\[ y_{\\rm RR,lo} = F_Y^{-1}\\left(\\frac{\\alpha}{2} \\vert \\mu_o,\\sigma^2\\right) \\] can be numerically evaluated using the R function qnorm(). What happens if the variance is unknown? In this case, we would borrow (and slightly amend) a result from the confidence interval section above, namely that \\[ F_Y(y) = F_{T(n-1)}\\left( \\frac{y-\\mu_o}{s_{\\rm obs}/\\sqrt{n}} \\right) \\,, \\] which means that for instance, \\[ y_{\\rm RR,lo} = \\mu_o + \\frac{s_{\\rm obs}}{\\sqrt{n}} F_{T(n-1)}^{-1}\\left(\\frac{\\alpha}{2}\\right) \\,, \\] which can be numerically evaluated using the R function qt(). (See Figure 2.16.) Figure 2.16: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with \\(\\mu_o = 0\\) and \\(s_{\\rm obs}^2 = 1\\)), assuming \\(\\alpha = 0.05\\) and \\(n-1 = 5\\) degrees of freedom. There are two new hypothesis-test-related concepts that we introduce here. The first is the concept of the p-value. This is the probability of observing a hypothesis test statistic value \\(y_{\\rm obs}\\) or a more “extreme” value (i.e., a value even further from the null), given that the null hypothesis is true. We reject the null when \\(p &lt; \\alpha\\), the user-specified Type I error. See below for more information about \\(p\\)-values. Here, it suffices to say that when we can compute a \\(p\\)-value, we should: it is more informative than simply saying that the test statistic does, or does not, lie in the rejection region. The second new concept that we introduce here is hypothesis test power. In words, it is the probability of rejecting the null hypothesis given an arbitrary parameter value \\(\\theta\\): \\[ \\mbox{power}(\\theta) = P(\\mbox{reject}~H_o \\vert \\theta) \\,. \\] The power is a function of \\(\\theta\\), and its value is \\(1-\\beta(\\theta,\\alpha)\\), where \\(\\beta(\\theta,\\alpha)\\) is the Type II error. We note that if we set \\(\\theta = \\theta_o\\), the null hypothesis value, then the test power is by definition \\(\\alpha\\) (the probability of rejecting the null if the null is correct). Below we provide a second hypothesis test reference table, one that contains details on the computations that one needs to carry out to compute \\(p\\)-values and test power. Like the first table, this is not a table to memorize! Type \\(E[Y]\\) increases with \\(\\theta\\)? \\(p\\)-Value Test Power two-tail yes 2 \\(\\times\\) min[\\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\), \\(F_Y(y_{\\rm RR,lo} \\vert \\theta)\\) + \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\)] \\(1 - F_Y(y_{\\rm RR,hi} \\vert \\theta)\\) no same as above same as above lower-tail yes \\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\) \\(F_Y(y_{\\rm RR} \\vert \\theta)\\) no \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\) \\(1-F_Y(y_{\\rm RR} \\vert \\theta)\\) upper-tail yes \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\) \\(1-F_Y(y_{\\rm RR} \\vert \\theta)\\) no \\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\) \\(F_Y(y_{\\rm RR} \\vert \\theta)\\) 2.15.1 Testing a Hypothesis About the Normal Mean with Variance Known Let’s assume that we have sampled \\(n = 25\\) iid data from a normal distribution whose variance is \\(\\sigma^2 = 1\\). We wish to test the null hypothesis \\(H_o : \\mu = \\mu_o = 2\\) versus the alternative \\(H_a : \\mu \\neq \\mu_o\\). What are the rejection regions for this test? What is the \\(p\\)-value if we observe \\(\\bar{x}_{\\rm obs} = 1.404\\)? Last, what is the power of this test for \\(\\mu = 1.8\\)? We assume the level of the test is \\(\\alpha = 0.05\\). To answer each of these questions, we will refer to the hypothesis test reference tables given in Chapter 1 and above. First we determine the rejection region boundaries: \\[ y_{\\rm RR,lo} = F_Y^{-1}\\left(\\frac{\\alpha}{2} \\vert \\mu_o,\\sigma^2\\right) ~~~ \\mbox{and} ~~~ y_{\\rm RR,hi} = F_Y^{-1}\\left(1-\\frac{\\alpha}{2} \\vert \\mu_o,\\sigma^2\\right) \\,, \\] where \\(Y = \\bar{X}\\). The R function calls are n &lt;- 25 alpha &lt;- 0.05 mu.o &lt;- 2 sigma2 &lt;- 1 (y.rr.lo &lt;- qnorm(alpha/2,mean=mu.o,sd=sqrt(sigma2/n))) ## [1] 1.608007 (y.rr.hi &lt;- qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n))) ## [1] 2.391993 We reject the null hypothesis if \\(y_{\\rm obs}\\) is less than 1.608 or greater than 2.392. Note that these boundaries each lie the same distance from \\(\\mu_o = 2\\), due to the symmetry of the normal distribution from which \\(\\bar{X}\\) is sampled. Also note that the placement of these boundaries does not depend on the observed statistic value \\(y_{\\rm obs} = 1.404\\)). See Figure 2.17, where the rejection regions are displayed in red. Next, we compute the \\(p\\)-value, noting that we already know that is has to be less than 0.05, since the observed value of 1.404 lies in the rejection region. Using the table above, we infer the relevant code: n &lt;- 25 y.obs &lt;- 1.404 mu.o &lt;- 2 sigma2 &lt;- 1 2*min(c(pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)), 1-pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)))) ## [1] 0.002882484 The \\(p\\)-value is 0.0029. This value is to be interpreted as the probability that we would observe a value as far or farther from 2 as 1.404 (and 2.596, by symmetry) is 0.29 percent, if the null is correct. That is sufficiently small that we conclude that the null hypothesis is incorrect. Last, we look at the test power assuming \\(\\mu = 1.8\\). Again, we use the reference table above and infer the relevant code: n &lt;- 25 alpha &lt;- 0.05 mu.o &lt;- 2 mu &lt;- 1.8 sigma2 &lt;- 1 pnorm(y.rr.lo,mean=mu,sd=sqrt(sigma2/n)) + (1-pnorm(y.rr.hi,mean=mu,sd=sqrt(sigma2/n))) ## [1] 0.170075 The test power is 0.170: if \\(\\mu\\) is equal to 1.8, then 17.0 percent of the time we would sample a value of our hypothesis test statistic that lies in the rejection region. The farther \\(\\mu\\) is from \\(\\mu_o\\), the greater the power gets. Figure 2.17: The sampling distribution \\(f_Y(y)\\) (blue curve), rejection regions (red polygons), and observed statistic value \\(y_{m obs}\\) (vertical green line) for \\(Y = \\bar{X}\\) given \\(n = 25\\) iid data that are assumed to be sampled from a \\(\\mathcal{N}(2,1)\\) distribution under the null hypothesis. We perform a two-tailed test, with \\(\\alpha = 0.05\\), and the variance \\(\\sigma^2 = 1\\) is assumed known. The value we observe is in the rejection region, thus we reject the null hypothesis and conclude that \\(\\mu \\neq 2\\). 2.15.2 Testing a Hypothesis About the Normal Mean with Variance Unknown Let’s assume that we have sampled \\(n = 15\\) iid data from a normal distribution whose variance is unknown. We wish to test the null hypothesis \\(H_o : \\mu = \\mu_o = 4\\) versus the alternative \\(H_a : \\mu &gt; \\mu_o\\). What is the rejection region for this test? What is the \\(p\\)-value if we observe \\(\\bar{x}_{\\rm obs} = 4.5\\)? Last, what is the power of this test for \\(\\mu = 4.5\\)? We assume the level of the test is \\(\\alpha = 0.05\\) and that the sample variance is \\(s_{\\rm obs}^2 = 3\\). To answer each of these questions, we will refer to the hypothesis test reference tables given in Chapter 1 and above. First we determine the rejection region boundary: \\[ y_{\\rm RR} = F_Y^{-1}\\left(1-\\alpha \\vert \\mu_o,s_{\\rm obs}^2\\right) \\,, \\] where \\(Y = (\\bar{X}-\\mu_o)/\\sqrt{s_{\\rm obs}^2/n}\\) is a \\(t\\)-distributed random variable for \\(n-1\\) degrees of freedom. The R function call is alpha &lt;- 0.05 n &lt;- 15 (y.rr &lt;- qt(1-alpha,n-1)) ## [1] 1.76131 We reject the null hypothesis if \\(y_{\\rm obs}\\) is greater then 1.761. See Figure 2.18, in which the rejection region is displayed in red. Next, we compute the \\(p\\)-value. Using the table above, we infer the relevant code: n &lt;- 15 mu.o &lt;- 4 s2 &lt;- 3 y.obs &lt;- (4.5-mu.o)/sqrt(s2/n) 1-pt(y.obs,n-1) ## [1] 0.1411855 The \\(p\\)-value is 0.141, which is \\(&gt; \\alpha\\). The probability that we would observe a value as far or farther from 4 as 4.5 is 14.1 percent, if the null is correct. We fail to reject the null hypothesis; we cannot conclude that we have sufficient data to reject the idea that \\(\\mu = 4\\). Last, we look at the test power assuming \\(\\mu = 4.5\\). Again, we use the reference table above…but there is an issue: a \\(t\\)-distribution pdf is not a function of \\(\\mu\\)! Thus we have to add some extra computational steps to complete the power calculation. We “map” \\(y_{\\rm RR}\\) to \\(\\bar{x}_{\\rm RR}\\): \\[ y_{\\rm RR} = \\frac{\\bar{x}_{\\rm RR} - \\mu_o}{s_{\\rm obs}/\\sqrt{n}} ~~~ \\Rightarrow ~~~ \\bar{x}_{\\rm RR} = \\mu_o + y_{\\rm RR}\\frac{s_{\\rm obs}}{\\sqrt{n}} \\,. \\] We determine \\(y_{\\rm RR}&#39;\\), the rejection-region boundary given the alternative value \\(\\mu\\): \\[ y_{\\rm RR}&#39; = \\frac{\\bar{x}_{\\rm RR} - \\mu}{s_{\\rm obs}/\\sqrt{n}} \\,. \\] We plug this value of the rejection region boundary into the power calculation. Here’s the relevant code: mu &lt;- 4.5 x.bar.rr &lt;- mu.o + y.rr*sqrt(s2/n) y.rr.prime &lt;- (x.bar.rr - mu)/sqrt(s2/n) 1-pt(y.rr.prime,n-1) ## [1] 0.2652206 The test power is 0.265: if \\(\\mu\\) is equal to 4.5, then 26.5 percent of the time we would sample a value of our hypothesis test statistic that lies in the rejection region. The farther \\(\\mu\\) is from \\(\\mu_o\\) (in the positive direction), the higher this percentage gets. See Figure 2.19. Figure 2.18: The sampling distribution \\(f_Y(y)\\) (blue curve), rejection region (red polygon), and observed statistic value \\(y_{m obs}\\) (vertical green line) for \\(Y = \\bar{X}\\) given \\(n = 15\\) iid data that are assumed to be sampled from a normal distribution with mean \\(\\mu_o = 4\\) and variance unknown under the null hypothesis. We perform an upper-tailed test, with \\(\\alpha = 0.05\\). The value we observe is not in the rejection region, thus we fail to reject the null hypothesis and conclude that \\(\\mu = 4\\) is a plausible value. Figure 2.19: The power curve for the upper-tail test \\(H_o : \\mu = \\mu_o = 4\\) versus \\(H_a : \\mu &gt; \\mu_o\\). The red dot at \\(\\mu = 4\\) indicates the test power under the null: the power is \\(\\alpha\\), as it should be given the definition of test power. We see that \\(n = 15\\) is a sufficient amount of data to clearly disambiguate between \\(\\mu_o = 4\\) and, e.g., \\(\\mu \\gtrsim 5\\). We note that if the alternative hypothesis is \\(\\mu &lt; \\mu_o\\), the curve would be reversed\\(-\\)it would rise towards the left\\(-\\)and if the alternative hypothesis is \\(\\mu \\neq \\mu_o\\), the power curve would have a U shape, with the minimum power being \\(\\alpha\\) at \\(\\mu = \\mu_o\\). 2.15.3 Hypothesis Testing: Using the CLT Let’s go back to the example in the confidence interval section above, in which we assumed that we sampled \\(n\\) iid data from the decidedly not normal distribution \\[ f_X(x) = \\theta x^{\\theta-1} ~~ x \\in [0,1] \\,, \\] with \\(\\theta &gt; 0\\), and for which \\(E[X] = \\mu = \\theta/(\\theta+1)\\). We found that because we cannot express the sampling distribution of \\(Y = \\bar{X}\\) analytically, our only path to determining a confidence interval was by making use of the Central Limit Theorem. Confidence interval construction and the performance of hypothesis tests are “two sides of the same coin”: in both we are solving for roots of equations, and the only difference is in terms of what is fixed (the observed statistic for confidence intervals and the null hypothesis value \\(\\theta_o\\) for hypothesis tests) and what we solve for (the parameter value \\(\\theta\\) for confidence intervals and the rejection-region boundaries for hypothesis tests). Thus what “works” for confidence intervals will work for hypothesis tests, meaning that we can utilize the CLT to test the null hypothesis \\(H_o : \\theta = \\theta_o\\). (So long as \\(n \\gtrsim 30\\)!) To find the rejection region(s), \\(p\\)-value, and power for tests of the mean when the CLT is involved, we utilize the same codes as we utilize for the “variance known” case (i.e., the ones based on pnorm() and qnorm()), except that instead of plugging in \\(\\sigma^2\\) (i.e., sigma2), we would plug in the sample variance \\(s_{\\rm obs}^2\\) (i.e., s2). For instance: n &lt;- 25 alpha &lt;- 0.05 theta.o &lt;- 2 mu.o &lt;- theta.o/(theta.o+1) s2 &lt;- 0.0440 (y.rr.lo &lt;- qnorm( alpha/2,mean=mu.o,sd=sqrt(s2/n))) ## [1] 0.5844416 (y.rr.hi &lt;- qnorm(1-alpha/2,mean=mu.o,sd=sqrt(s2/n))) ## [1] 0.7488918 The rejection region boundaries are thus 0.5844 and 0.7489. We note that, as is the case for confidence interval coverage, our result is only approximate, i.e., the Type I error rate is not exactly \\(\\alpha\\), because the distribution of \\(\\bar{X}\\) is not exactly normal. However, as we do in the confidence interval case, we can perform simulations to assess just how far off our actual Type I error rate \\(\\alpha\\) is for any given analysis setting. set.seed(101) num.sim &lt;- 10000 # repeat the data-generating process 10,000 times n &lt;- 25 alpha &lt;- 0.05 theta.o &lt;- 2 # the mean is 3/(3+1) = 3/4 mu.o &lt;- theta.o/(theta.o+1) typeIerr &lt;- 0 for ( ii in 1:num.sim ) { X &lt;- rbeta(n,theta.o,1) y.obs &lt;- mean(X) s2 &lt;- var(X) y.rr.lo &lt;- qnorm( alpha/2,mean=mu.o,sd=sqrt(s2/n)) y.rr.hi &lt;- qnorm(1-alpha/2,mean=mu.o,sd=sqrt(s2/n)) if ( y.obs &lt;= y.rr.lo || y.obs &gt;= y.rr.hi ) typeIerr = typeIerr+1 } cat(&quot;The observed proportion of Type I errors =&quot;,typeIerr/num.sim,&quot;\\n&quot;) ## The observed proportion of Type I errors = 0.064 We observe an empirical Type I error rate of 0.064. This is sufficiently far from the expected value of 0.05 that we conclude that the distribution of the sample mean is only approximately normal. 2.15.4 Testing With Two Data Samples: the t Test Let’s assume that we are given two independent samples of normal iid data: \\[\\begin{align*} \\{U_1,\\ldots,U_{n_U}\\} &amp;\\sim \\mathcal{N}(\\mu_U,\\sigma_U^2) \\\\ \\{V_1,\\ldots,V_{n_V}\\} &amp;\\sim \\mathcal{N}(\\mu_V,\\sigma_V^2) \\,. \\end{align*}\\] The sample means are \\(\\bar{U}\\) and \\(\\bar{V}\\), respectively, and we know that \\[ \\bar{U} \\sim \\mathcal{N}\\left(\\mu_U,\\frac{\\sigma_U^2}{n_U}\\right) ~~\\mbox{and}~~ \\bar{V} \\sim \\mathcal{N}\\left(\\mu_V,\\frac{\\sigma_V^2}{n_V}\\right) \\,. \\] In the two-sample t test, the null hypothesis \\(H_o\\) is that \\(\\mu_U = \\mu_V\\). Thus a natural test statistic is \\(\\bar{U}-\\bar{V}\\), which, as the reader may confirm using the method of moment-generating functions, is normally distributed: \\[ \\bar{U} - \\bar{V} \\sim \\mathcal{N}\\left(\\mu_U-\\mu_V,\\frac{\\sigma_U^2}{n_U}+\\frac{\\sigma_V^2}{n_V}\\right) \\,. \\] However, when neither variance is known, we cannot model the observed data with the normal distribution; instead, in Welch’s t test, we assume the following: \\[ T = \\frac{\\bar{U}-\\bar{V}}{S_\\Delta} = \\frac{\\bar{U}-\\bar{V}}{\\sqrt{\\frac{S_U^2}{n_U}+\\frac{S_V^2}{n_V}}} \\sim t({\\nu}) \\,, \\] i.e., \\(T\\) is sampled from a \\(t\\) distribution with \\(\\nu\\) degrees of freedom, where \\(\\nu\\) is estimated using the Welch-Satterthwaite equation: \\[ \\nu \\approx \\frac{S_\\Delta^4}{\\frac{S_U^4}{n_U^2(n_U-1)} + \\frac{S_V^4}{n_V^2(n_V-1)}} \\,. \\] The rule-of-thumb for this approximation to hold is that both \\(n_U\\) and \\(n_V\\) are larger than 5. Note that in some applications, one might see \\(\\nu\\) rounded off to an integer, but this is not necessary: one can work with the \\(t\\) distribution’s probability density function just fine even if \\(\\nu\\) in not an integer! (Any rounding off is done to allow one to use \\(t\\) tables to estimate \\(p\\)-values.) Can we “reformat” this test so as to be consistent with how we perform one-sample tests? The answer is yes. Let the test statistic be \\(Y = \\bar{U}-\\bar{V} = S_{\\Delta} T\\), and let the null hypothesis be \\(H_o : \\mu_U - \\mu_V = 0\\). Then \\[ F_Y(y) = P(Y \\leq y) = P(S_{\\Delta} T \\leq y) = P\\left(T \\leq \\frac{y}{S_{\\Delta}}\\right) = F_{T(\\nu)}\\left(\\frac{y}{S_{\\Delta}}\\right) \\,, \\] and thus the rejection-region boundaries are given by \\[ y_q = S_{\\Delta} F_{T(\\nu)}^{-1}(q) \\,, \\] where \\(q = \\alpha/2\\) or \\(1-\\alpha/2\\). In R code, these boundaries are given by y.rr.lo &lt;- S.Delta * qt(alpha/2,nu) y.rr.hi &lt;- S.Delta * qt(1-alpha/2,nu) Additionally, the \\(p\\)-values and power are given by p &lt;- 2*min(c(pt(y.obs/S.Delta,nu),1-pt(y.obs/S.Delta,nu))) power &lt;- pt((y.rr.lo-mu.Delta)/S.Delta,nu) + (1-pt((y.rr.hi-mu.Delta)/S.Delta,nu)) where mu.Delta is the specified alternative value for \\(\\mu_U-\\mu_V\\). We leave it as an exercise to the reader to generalize the expressions and code above for cases in which the null hypothesis value differs from zero. We note that the above code yields equivalent results to the R function t.test(). As a last point: the reader may ask “what do I do if I have three or more samples and I want to test whether all of them have the same mean, with the alternative being that at least one of the means is different?” In such a situation, one would employ a one-way analysis of variance test, or an ANOVA test; we introduce this test in the final section of this chapter. 2.15.5 More About p-Values As described above, the \\(p\\)-value is the probability of observing a hypothesis test statistic value \\(y_{\\rm obs}\\) or a more extreme value, i.e., one further from the null value. In mathematical terms, for a continuous sampling distribution, a \\(p\\)-value is \\[\\begin{align*} p = 2 \\times \\mbox{min}\\left[ \\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta_o) dy , \\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta_o) dy \\right] ~~~&amp; \\mbox{(two-tail)} \\\\ p = \\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta_o) dy ~~~&amp; \\mbox{(lower-tail)} \\\\ p = \\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta_o) dy ~~~&amp; \\mbox{(upper-tail)} \\,. \\end{align*}\\] If the null hypothesis is correct, then \\(p\\) is sampled uniformly between 0 and 1, i.e., 0.83 is just as likely to be observed as 0.14 and as 0.23, etc. This makes sense: the probability that \\(p &lt; \\alpha\\) is exactly \\(\\alpha\\), the Type I error rate. If we set \\(\\alpha\\) to 0.05, then when the null is true, \\(p\\) will be less than \\(\\alpha\\) five percent of the time, i.e., we will wrongly make the decision to reject a true null on average one time in every 20 hypothesis tests we perform. (See Figure 2.20.) Figure 2.20: The empirical distribution of \\(p\\)-values under the null hypothesis, for a lower-tail test with \\(n = 20\\), \\(\\mu_o = 3\\), and \\(\\sigma^2 = 1\\). We can see that the \\(p\\)-values are uniformly distributed between 0 and 1 (with some random variation due to the fact that our simulation contains a finite sample of data). When the null is true, the probability of sampling a \\(p\\)-value less than \\(\\alpha\\) is thus exactly \\(\\alpha\\). Because we are performing a simulation, the number of observations falling into each histogram bin is a random variable, and thus we observe some amount of “noise” in Figure 2.20 (i.e., some amount of variation around the dashed red line)…but nevertheless we can visually infer that the sampling distribution for \\(p\\) is uniform (i.e., flat). (For completeness, the number of counts in each bin are collectively sampled from a multinomial distribution, where the probability of a count being recorded in any given bin being the reciprocal of the number of bins. We discuss the multinomial distribution in Chapter 3.) Now, what if the null hypothesis is not correct? What will happen to the distribution of \\(p\\)-values? The answer depends on the circumstance: if we are performing an lower-tail test (with \\(E[Y]\\) increasing with \\(\\theta\\)) and the true mean is larger than the hypothesized mean, then the \\(p\\)-values will skew towards 1: we are less likely to reject the null when its value is closer to the tail of interest than the true value is. If the true mean is smaller than the hypothesized mean, the \\(p\\)-values will skew towards 0. See Figure 2.21 for a demonstration of the latter point. Figure 2.21: The empirical distribution of \\(p\\)-values when the stated alternative hypothesis is consistent with the truth. We can see that the \\(p\\)-values skew towards 0, meaning that when the alternative hypothesis is true, the probability of sampling a \\(p\\)-value less than \\(\\alpha\\) can be \\(\\gg \\alpha\\). ## The proportion of p-values &lt; 0.05 is 0.29707 In Figure 2.21, 29.71% of the counts are observed as having value less than \\(\\alpha = 0.05\\)…so the test power is 0.2971. As the true mean gets smaller and smaller than \\(\\mu_o = 3\\), the number of counts in the first bins increase…which is equivalent to saying that the test power increases. We will make three important statements about \\(p\\)-values here, ones that are good for all readers to remember. A p-value is not the probability that the null hypothesis is correct! A hypothesis is not a random variable, and there is thus no probability associated with it. A \\(p\\)-value is the probability of observing a more extreme hypothesis test statistic than what we actually observe, conditioned on the null hypothesis being correct. We cannot select the Type I error rate \\(\\alpha\\) after computing the p-value. We specify the Type I error rate \\(\\alpha\\) before data are analyzed; specifying it afterwards opens up the analysis procedure to bias: “my \\(p\\)-value is 0.09…so I’ll set \\(\\alpha = 0.1\\) so that I can reject the null.” We cannot perform multiple hypothesis tests without correcting our result. Suppose that we perform 100 independent hypothesis tests, each with \\(\\alpha = 0.05\\). Even if the null is true for every test, we will end up rejecting the null approximately five times. (Not necessarily exactly five times: the number of rejections we observe is a random variable with mean of five.) That we will reject an increasing number of null hypotheses with an increasing number of tests is inevitable: this is a feature of hypothesis testing, not a bug. When performing multiple tests, we need to apply a correction factor to each observed \\(p\\)-value, such as a Bonferroni correction (e.g., if \\(m\\) is the number of conducted tests, reset \\(\\alpha\\) to \\(\\alpha/m\\)) or the less conservative false-discovery rate (FDR) correction. Otherwise, we are engaging in p-hacking and data dredging. We discuss correction factors in more detail in Chapter 5. 2.15.6 Test Power: Sample-Size Computation We can phrase a typical experimental design exercise as follows. “We will collect data that we assume are iid from a normal distribution with specified mean \\(\\mu\\) and specified variance \\(\\sigma^2\\).” (Alternatively, it could be a specification of \\(s_{\\rm obs}^2\\), motivating the use of \\(t\\) distribution.) “How many data do we need to collect to achieve a test power of 0.8, assuming \\(\\alpha = 0.05\\) and assuming that the null hypothesis is \\(H_o : \\mu = \\mu_o\\), but the true value is \\(\\mu &gt; \\mu_o\\)?” We will begin answering this question using math, and then we will transition to code. We refer back to the hypothesis test reference tables and note that for an upper-tail test where \\(E[Y]\\) increases with \\(\\theta\\), \\[ \\mbox{power}(\\theta) = 1 - F_Y(y_{\\rm RR} \\vert \\mu_o,\\sigma^2/n) \\,. \\] We substitute in our target power value and solve for \\(n\\): \\[\\begin{align*} 1 - F_Y(y_{\\rm RR} \\vert \\mu_o,\\sigma^2/n) &amp;= 0.8 \\\\ \\Rightarrow ~~~ F_Y(y_{\\rm RR} \\vert \\mu_o,\\sigma^2/n) &amp;= 0.2 \\\\ \\Rightarrow ~~~ &amp;\\mbox{...?} \\,. \\end{align*}\\] The issue is two-fold, as it turns out: first, \\(n\\) appears to the right of the condition symbol, which means that we will not be solving this by hand but rather numerically via root-finding. But there’s another less-readily apparent issue as well: \\(y_{\\rm RR}\\) itself depends on \\(n\\)! Can we still pursue root-finding? Yes…we simply will have a more complicated expression. We transition here to code. We refer back to the rejection-region table and note that for an upper-tail normal mean test, \\(y_{\\rm RR}\\) is qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) # the null enters here The analogous code for test power is 1 - pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n)) # the alternative enters here Combining the two expressions, we get 1 - pnorm(qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)),mean=mu,sd=sqrt(sigma2/n)) To solve the problem at hand, we would subtract 0.8 from this expression, set the result equal to zero, and utilize uniroot() to solve for \\(n\\): # Assume alpha &lt;- 0.05 ; mu.o &lt;- 8 ; mu &lt;- 9 ; sigma2 &lt;- 6 f &lt;- function(n) { pnorm(qnorm(0.975,mean=8,sd=sqrt(6/n)),mean=9,sd=sqrt(6/n)) - 0.2 } ceiling(uniroot(f,interval=c(1,1000))$root) ## [1] 48 Note how we use the function ceiling() here, which rounds up the result to the next higher integer. We do this because the sample size should be an integer, and because we want the power to be at least 0.8; if we round our result down (via the floor() function), the power will be less than 0.8. In sample size-determination exercises, always round the final result up! 2.16 Hypothesis Testing: Population Variance When performing hypothesis tests for the population mean \\(\\mu\\), above, we adopt \\(\\bar{X}\\) as our hypothesis test statistic because it is the MLE for \\(\\mu\\); we do not theoretically justify the choice (by showing, e.g., that its use leads us to defining the most powerful test out of a set of possible alternatives). Here, we follow a similar logic: \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), so we will adopt \\(Y = S^2\\) as our test statistic when testing hypotheses about \\(\\sigma^2\\). Assume that we are given \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) sampled from a normal distribution with (unknown) mean \\(\\mu\\) and variance \\(\\sigma^2\\). Refer back to the table in the hypothesis testing section of Chapter 1. We observe that \\(E[Y] = E[S^2] = \\sigma^2\\), so \\(E[Y]\\) increases with \\(\\sigma^2\\). Thus the rejection region boundaries for, e.g., a two-tail test are found by solving \\[\\begin{align*} F_Y(y_{\\rm RR,lo} \\vert \\sigma_o^2) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm RR,hi} \\vert \\sigma_o^2) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] As is the case for the population mean, we have to borrow (and amend) a result from the confidence interval section to proceed here, namely that \\[ F_Y(y) = F_{W(n-1)}\\left(\\frac{(n-1)y}{\\sigma_o^2}\\right) \\,, \\] where \\(F_{W(n-1)}\\) is the cdf for a chi-square distribution for \\(n-1\\) degrees of freedom. So, for instance, \\[ y_{\\rm RR,hi} = \\frac{\\sigma_o^2}{n-1}F_{W(n-1)}^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\,, \\] or, in code, y.rr.hi &lt;- sigma2.o*qchisq(1-alpha/2,n-1)/(n-1) (See Figure 2.22.) Figure 2.22: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming \\(\\alpha = 0.1\\) and \\(n-1 = 3\\) degrees of freedom. 2.16.1 Testing a Hypothesis About the Normal Population Variance Let’s assume that we have sampled \\(n = 14\\) iid data from a normal distribution of unknown mean and variance. We wish to test the null hypothesis \\(H_o : \\sigma^2 = \\sigma_o^2 = 4\\) versus the alternative \\(H_a : \\sigma^2 &gt; \\sigma_o^2\\). What are the rejection regions for this test? What is the \\(p\\)-value if we observe \\(s_{\\rm obs}^2 = 5.5\\)? Last, what is the power of this test for \\(\\sigma^2 = 8\\)? We assume the level of the test is \\(\\alpha = 0.05\\). To answer each of these questions, we utilize the hypothesis test reference tables. For the rejection region, the R function call is alpha &lt;- 0.05 n &lt;- 14 sigma2.o &lt;- 4 (y.rr &lt;- (sigma2.o/(n-1))*qchisq(1-alpha,n-1)) ## [1] 6.880625 The rejection region boundary is 6.881: we reject the null hypothesis if \\(y_{\\rm obs} = s_{\\rm obs}^2 &gt; 6.881\\). Next, we compute the \\(p\\)-value, noting that it has to be greater than 0.05, since the observed value of 5.5 does not lie in the rejection region. The relevant code is y.obs &lt;- 5.5 1-pchisq((n-1)*y.obs/sigma2.o,n-1) ## [1] 0.1623236 The \\(p\\)-value is 0.162, which is indeed \\(&gt; \\alpha\\). There is thus a 16.2 percent chance that we would sample a variance value of 5.5 or greater if the null is correct. This is too high a percentage for us to confidently reject the idea that \\(\\sigma^2 = 4\\). Last, we look at the test power assuming \\(\\sigma^2 = 8\\): sigma2 &lt;- 8 1-pchisq((n-1)*y.rr/sigma2,n-1) ## [1] 0.5956563 The test power is 0.596: if \\(\\sigma^2\\) is equal to 8, then 59.6 percent of the time we would sample a value of \\(y_{\\rm obs}\\) that lies in the rejection region. This is not a particularly high value; if being able to differentiate between \\(\\sigma_o^2 = 4\\) and \\(\\sigma^2 = 8\\) is an important research goal, we would want to collect more data! In the previous section, we show in an example how to determine the sample size that is needed to achieve a particular power. Here we will repeat that exercise, but with a twist: we will visualize the power of the test as a function of \\(n\\). Because we are not solving for \\(n\\) for a particular power value, there is no need to utilize uniroot() here; we simply need to loop over calls to qchisq() and pchisq(). (But note that we don’t actually “loop,” as R’s vectorization capabilities allow us to simply pass vectors of sample-size values into the function calls, with vectors of results being returned.) See Figure 2.23. alpha &lt;- 0.05 sigma2.o &lt;- 4 sigma2 &lt;- 8 n &lt;- 2:100 y.rr &lt;- (sigma2.o/(n-1))*qchisq(1-alpha,n-1) power &lt;- 1-pchisq((n-1)*y.rr/sigma2,n-1) df &lt;- data.frame(n,power) ggplot(data=df,aes(x=n,y=power)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_line(data=data.frame(x=c(0,14,14,14),y=c(0.596,0.596,0,0.596)), aes(x=x,y=y),col=&quot;red&quot;,lty=2,lwd=0.85) + geom_point(x=14,y=0.596,size=4,col=&quot;red&quot;) + geom_hline(yintercept=0,lty=2,col=&quot;blue&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;blue&quot;) + labs(x=&quot;n&quot;,y=&quot;Test Power&quot;) + base_theme Figure 2.23: The test power as a function of sample size \\(n\\) assuming \\(\\sigma_o^2 = 4\\), \\(\\sigma^2 = 8\\), and \\(\\alpha = 0.05\\). The red dot indicates the test power for \\(n = 14\\), which as shown in the text is 0.596. 2.16.2 Testing With Two Data Samples: the F Test As was the case above for the two-sample \\(t\\) test, described above, we assume we are given two independent samples of iid data: \\[\\begin{align*} \\{U_1,\\ldots,U_{n_U}\\} &amp;\\sim \\mathcal{N}(\\mu_U,\\sigma_U^2) \\\\ \\{V_1,\\ldots,V_{n_V}\\} &amp;\\sim \\mathcal{N}(\\mu_V,\\sigma_V^2) \\,, \\end{align*}\\] with sample variances \\(S_U^2\\) and \\(S_V^2\\), respectively. Given what we know about the distribution of sample variance values, we can write that \\[ W_U = \\frac{(n_U-1)S_U^2}{\\sigma_U^2} \\sim \\chi_{n_U-1}^2 ~~\\mbox{and}~~ W_V = \\frac{(n_V-1)S_V^2}{\\sigma_V^2} \\sim \\chi_{n_V-1}^2 \\,. \\] The ratio of \\(W_U/(n_U-1)\\) to \\(W_V/(n_V-1)\\) is \\[ F = \\left. \\left(\\frac{\\frac{(n_U-1)S_U^2}{\\sigma_U^2}}{n_U-1}\\right) \\right/ \\left(\\frac{\\frac{(n_V-1)S_U^2}{\\sigma_V^2}}{n_V-1}\\right) = \\frac{S_U^2/\\sigma_U^2}{S_V^2/\\sigma_V^2} \\sim F_{n_U-1,n_V-1} \\,, \\] where \\(F_{n_U-1,n_V-1}\\) is the F distribution for \\(n_U-1\\) numerator, and \\(n_V-1\\) denominator, degrees of freedom. Examining the equation above, we see that \\(F\\) is an appropriate statistic for testing hypotheses about the relationship between the variances of both samples. Let the test statistic be \\[ Y = \\frac{s_U^2}{s_V^2} \\,. \\] (Note that it doesn’t matter which sample is identified as \\(U\\) and which is identified as \\(V\\) so long as care is taken to not accidentally “reverse” the samples when coding the test.) As was the case with the two-sample \\(t\\) test, we observe \\(Y\\) but only know the sampling distribution for \\(F\\), so we need to enact a variable transformation: \\[ F_Y(y) = P(Y \\leq y) = P\\left(\\frac{\\sigma_U^2}{\\sigma_V^2}F \\leq y \\right) = P\\left( F \\leq \\frac{\\sigma_V^2}{\\sigma_U^2} y \\right) = F_{F(n_U-1,n_V-1)}\\left(\\frac{\\sigma_V^2}{\\sigma_U^2} y\\right) \\,. \\] Thus the rejection-region boundaries for a two-tail test are given by y.rr.lo &lt;- (sigmaU2.o/sigmaV2.o) * qf(alpha/2,n.U-1,n.V-1) y.rr.hi &lt;- (sigmaU2.o/sigmaV2.o) * qf(1-alpha/2,n.U-1,n.V-1) with the \\(p\\)-values and power given by p &lt;- 2*min(c(pf(sigmaV2.o*y.obs/sigmaU2.o,n.U-1,n.V-1), 1-pf(sigmaV2.o*y.obs/sigmaU2.o,n.U-1,n.V-1))) power &lt;- pf(sigmaV2*y.rr.lo/sigmaU2,n.U-1,n.V-1) + (1-pf(sigmaV2*y.rr.hi/sigmaU2,n.U-1,n.V-1)) where \\(\\sigma_U^2/\\sigma_V^2\\) is the specified alternative variance ratio. We leave it as an exercise to the reader to derive appropriate expressions for one-tail tests. We note that the above code yields equivalent results to the R function var.test(). 2.17 Simple Linear Regression Thus far in this chapter, we have assumed that we have been given a sample of iid data \\(\\{X_1,\\ldots,X_n\\} \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), or, sometimes, two samples that are independent of one another. Here we will move beyond this assumption to the case where our data consists of tuples \\(\\{(x_1,Y_1),\\ldots,(x_n,Y_n)\\}\\) and where we want to determine (or learn) the association between the variables \\(x_i\\) and \\(Y_i\\). (Assuming there is one! See Figure 2.24. Also, note the use of the term “association”: a relationship might exist, but it is not necessarily the case that \\(x\\) “causes” \\(Y\\); to determine causation, one would utilize methods of causal inference, which are beyond the scope of this book.) In other words, we want to regress \\(\\mathbf{Y}\\) upon \\(\\mathbf{x}\\). \\(\\mathbf{x} = \\{x_1,\\ldots,x_n\\}\\): the predictor variable (or independent variable or covariate or feature) \\(\\mathbf{Y} = \\{Y_1,\\ldots,Y_n\\}\\): the response variable (or the dependent variable or target) Figure 2.24: Illustration of the setting for simple linear regression. The data \\(Y \\vert x\\) (blue points) are randomly distributed around the true regression line \\(y \\vert x = 4 + x/2\\) (red dashed line), with the error term \\(\\epsilon_i \\sim \\mathcal{N}(y \\vert 0,1)\\). If the relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\) is deterministic, then there exists a function \\(f\\) such that \\(y_i = f(x_i)\\) for all \\(i\\). As statisticians, we are more interested in learning non-deterministic associations (so-called stochastic, probabilistic, or statistical relationships) between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\). One standard model for the data-generating process is \\[ Y_i \\vert x_i = f(x_i) + \\epsilon_i \\,, \\] where \\(\\{\\epsilon_1,\\ldots,\\epsilon_n\\}\\) are random variables with \\(E[\\epsilon_i] = 0\\) and uncorrelated variances \\(V[\\epsilon_i] = \\sigma^2\\) (i.e., the variances are the same for all \\(i\\)…they are homoscedastic). (Note that we are not saying anything about the distribution of \\(\\epsilon_i\\) yet; we are just stating assumptions about its expected value and variance.) Now we can say why the predictor variable is represented in lower case while the response variable is in upper case: we consider the predictor variable values to be fixed, while the response variable values are random variables because the \\(\\epsilon_i\\)’s are random variables. We now suppose that there is a linear relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\) such that \\[\\begin{align*} Y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\ E[Y_i] &amp;= E[\\beta_0 + \\beta_1 x_i + \\epsilon_i] = \\beta_0 + \\beta_1 x_i + E[\\epsilon_i] = \\beta_0 + \\beta_1 x_i \\\\ V[Y_i] &amp;= V[\\beta_0 + \\beta_1 x_i + \\epsilon_i] = V[\\epsilon_i] = \\sigma^2 \\,. \\end{align*}\\] This is a simple linear regression model, where the word “simple” follows from the fact that there is only one predictor variable. (Note that this is a linear model because it is linear in the parameters; e.g., \\(\\beta_0 + \\beta_1x_i^2\\) is also a linear model.) \\(\\beta_0\\) is dubbed the intercept and \\(\\beta_1\\) is the slope. In analyses, we are generally interested in estimating \\(\\beta_1\\), as it tells us the average change in \\(Y\\) given a one-unit change in \\(x\\). Once a simple linear regression model is learned, we can make predictions \\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] and we can compute residuals \\[ e_i = Y_i - \\hat{Y}_i \\] The goal in the estimation process is to make the residuals as small as possible. Since negative values of \\(e_i\\) are as “bad” as positive values, we estimate \\(\\beta_0\\) and \\(\\beta_1\\) by minimizing the sum of squared errors or SSE: \\(\\sum_{i=1}^n e_i^2\\). (Note that this term is often used interchangeably with the term residual sum of squares, or RSS.) This is the so-called ordinary least squares estimator, or OLS. Our use of the OLS estimator for \\(\\beta_0\\) and \\(\\beta_1\\) (as opposed to, e.g., \\(\\sum_{i=1}^n \\vert e_i \\vert\\)) is motivated by the Gauss-Markov theorem, which states that the unbiased OLS estimator is the best estimator if the \\(\\epsilon_i\\)’s are uncorrelated, have equal variances, and expected values of zero (hence motivating the assumptions stated above). Note that we have still not said anything about the distribution of the \\(\\epsilon_i\\)’s: the Gauss-Markov theorem does not mandate that they have a particular distribution! (As an aside: what happens if, e.g., we have data for which the variances are unequal, or heteroscedastic? We lose the ability to make statements like “the OLS estimator is the best linear unbiased estimator.” We can always learn the simple linear model we define above…a computer cannot stop us from doing so. It just may not be the best model choice…for instance, weighted OLS regression, where the weighting given each datum is related to how uncertain its value is, may provide better estimates.) To estimate \\(\\beta_0\\) and \\(\\beta_1\\), we take partial derivatives of \\[ \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), set the results to zero, and solve for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Here, we will quote results, and provide the estimate of \\(\\sigma^2\\), while leaving the derivations as exercises to the reader: Parameter Estimate Expected Value Variance \\(\\beta_0\\) \\(\\bar{Y}-\\hat{\\beta}_1\\bar{x}\\) \\(\\beta_0\\) \\(\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}\\right)\\) \\(\\beta_1\\) \\(\\frac{S_{xY}}{S_{xx}}\\) \\(\\beta_1\\) \\(\\frac{\\sigma^2}{S_{xx}}\\) \\(\\sigma^2\\) \\(\\frac{1}{n-2}\\) \\(\\sum_{i=1}^n\\) \\(e_i^2\\) \\(=\\frac{SSE}{n-2}\\) \\(\\sigma^2\\) \\(\\frac{2\\sigma^4}{n-2}\\) The quantities \\(S_{xx}\\) and \\(S_{xY}\\) (and \\(S_{YY}\\)) are shorthand for the following: \\[\\begin{align*} S_{xx} &amp;= \\sum_{i=1}^n (x_i-\\bar{x})^2 = \\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2 \\\\ S_{xY} &amp;= \\sum_{i=1}^n (x_i-\\bar{x})(Y_i-\\bar{Y}) = \\left(\\sum_{i=1}^n x_iY_i\\right) - n\\bar{x}\\bar{Y} \\\\ S_{YY} &amp;= \\sum_{i=1}^n (Y_i-\\bar{Y})^2 = \\left(\\sum_{i=1}^n Y_i^2\\right) - n\\bar{Y}^2 \\,. \\end{align*}\\] In Figure 2.25, we display the estimated regression line for the data shown above in Figure 2.24. Figure 2.25: Same as the previous figure, with the estimated regression line \\(\\hat{y} = 4.52 + 0.46 x\\) overlaid (solid red line). Ultimately, we are interested in determining if there is a statistically significant relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\), a question that we can answer using a hypothesis test: \\[\\begin{align*} H_o&amp;: \\beta_1 = 0 \\implies Y_i = \\beta_0 + \\epsilon_i \\\\ H_a&amp;: \\beta_1 \\neq 0 \\implies Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{align*}\\] In order to carry out hypothesis testing, however, we have to now make a distributional assumption regarding the \\(\\epsilon_i\\)’s. The standard assumption is that \\[ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2) \\,, \\] which means that \\[ Y_i \\vert x_i \\sim \\mathcal{N}(\\beta_0+\\beta_1x_i,\\sigma^2) \\] Adding this assumption to the model does not affect estimation; if, for instance, we were to derive the MLE estimators for \\(\\beta_0\\) and \\(\\beta_1\\), we would find that they are identical to the OLS estimators given above. Does our distributional assumption for the \\(\\epsilon_i\\)’s allow us to specify the distribution of, e.g., \\(\\hat{\\beta}_1\\)? The answer is yes. We begin by showing that \\(\\hat{\\beta}_1\\) can be written as a linear function of the response variable values: \\[\\begin{align*} \\hat{\\beta}_1 &amp;= \\frac{\\left(\\sum_{i=1}^n Y_i x_i\\right) - n \\bar{x}\\bar{Y}}{\\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2} \\\\ &amp;= \\frac{1}{k} \\left[ \\left(\\sum_{i=1}^n Y_i x_i\\right) - n \\bar{x}\\bar{Y} \\right] \\\\ &amp;= \\frac{1}{k} \\left[ \\left(\\sum_{i=1}^n Y_i x_i\\right) - \\sum_{i=1}^n \\bar{x} Y_i \\right] \\\\ &amp;= \\frac{1}{k} \\sum_{i=1}^n (x_i - \\bar{x}) Y_i = \\sum_{i=1}^n \\left( \\frac{x_i-\\bar{x}}{k}\\right) Y_i = \\sum_{i=1}^n a_i Y_i \\,. \\end{align*}\\] Because the \\(Y_i\\)’s are normally distributed, we know (via the method of moment-generating functions) that \\(\\hat{\\beta}_1\\) is also normally distributed, with mean \\[ E[\\hat{\\beta}_1] = \\sum_{i=1}^n a_i E[Y_i] = \\cdots = \\beta_1 \\] and variance \\[ V[\\hat{\\beta}_1] = \\sum_{i=1}^n a_i^2 V[Y_i] = \\sum_{i=1}^n a_i^2 \\sigma^2 = \\cdots = \\frac{\\sigma^2}{\\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2} \\,. \\] We can finally perform the hypothesis test. If \\(\\sigma^2\\) is unknown, the standardized slope is assumed to be \\(t\\)-distributed for \\(n-2\\) degrees of freedom: \\[ \\frac{\\hat{\\beta}_1 - \\beta_1}{se({\\hat{\\beta}_1})} = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{V[{\\hat{\\beta}_1}]}} \\sim t_{n-2} \\,. \\] To reiterate: The assumption that \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) allows us to test hypotheses about, e.g., \\(\\beta_1\\). If this assumption is violated, then OLS regression is still the best linear unbiased estimator. If, in addition, the assumption that \\(\\epsilon_i\\)’s are homoscedastic and uncorrelated is violated, the OLS regression will no longer be the best linear unbiased estimator, and we would need to seek alternatives, such as weighted OLS regression. Last…if, in addition, the assumption of \\(E[\\epsilon_i] = 0\\) is violated, then using OLS will be a biased estimator. At this point, we would generally conclude that there is a better, nonlinear representation of the data-generating process that we should use. 2.17.1 Correlation: the Strength of Linear Association We can measure the strength of a linear association via the metric of correlation. We define the correlation between a pair of random variables \\(X\\) and \\(Y\\) as \\[ \\rho_{XY} = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{V[X]V[Y]}}\\,, \\] with \\(\\vert \\rho_{XY} \\vert \\leq 1\\). (We will discuss correlation and the related concept of covariance more in depth in Chapter 6. For now, just treat the equation above as a definition. The main thing to keep in mind is that if \\(X\\) and \\(Y\\) are independent random variables, \\(\\rho_{XY} = 0\\), while if \\(X = Y\\) or \\(X = -Y\\), \\(\\rho_{XY} = 1\\) and \\(-1\\) respectively.) If instead of a pair of random variables we have a set of tuples, we can still define a correlation, which we can estimate using the Pearson correlation coefficient: \\[ R = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(Y_i-\\bar{Y})}{\\left[\\sum_{i=1}^n (x_i-\\bar{x})^2\\right]\\left[\\sum_{i=1}^n (Y_i-\\bar{Y})^2\\right]} = \\frac{S_{xY}}{\\sqrt{S_{xx}S_{YY}}} = \\hat{\\beta}_1 \\sqrt{\\frac{S_{xx}}{S_{YY}}} \\,. \\] The last equality follows from the fact that \\(\\hat{\\beta}_1 = S_{xY}/S_{xx}\\). It follows from this expression that \\(R\\) and \\(\\hat{\\beta}_1\\) have the same sign. Related to the correlation is the coefficient of determination, or \\(R^2\\), a quantity ranging from 0 to 1 that is interpreted as the proportion of the variation in the response variable values explained by the predictor variable values. The closer \\(R^2\\) is to 1, the more strictly linear is the association between \\(x\\) and \\(Y\\). For completeness, we mention “adjusted \\(R^2\\),” as this is an oft-quoted output from R’s linear model function, lm(). (See below for example usage of lm().) Adjusted \\(R^2\\) attempts to correct for the tendency of \\(R^2\\) to over-optimistically indicate the quality of fit of a linear model. There are several formulae for computing adjusted \\(R^2\\); R uses Wherry’s formula: \\[ \\mbox{Adj.}~R^2 = 1 - (1-R^2)\\frac{n-1}{n-p-1} \\,, \\] where \\(p\\) is the number of predictor variables (with \\(p = 1\\) for simple linear regression). 2.17.2 The Expected Value of the Sum of Squared Errors (SSE) Here we show that \\(SSE/(n-2)\\) is an unbiased estimator of \\(\\sigma^2\\). This is a non-trivial algebraic exercise, and thus we will leave the calculation of \\(V[\\hat{\\sigma^2}] = V[SSE/(n-2)] = 2\\sigma^4/(n-2)\\) as an exercise to the (masochistic) reader. We start by writing that \\[\\begin{align*} SSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 &amp;= \\sum_{i=1}^n [Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\\\ &amp;= \\sum_{i=1}^n [Y_i - (\\bar{Y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i)]^2 \\\\ &amp;= \\sum_{i=1}^n [(Y_i - \\bar{Y}) - \\hat{\\beta}_1 (x_i-\\bar{x})]^2 \\\\ &amp;= \\sum_{i=1}^n [(Y_i - \\bar{Y})^2 - 2 \\hat{\\beta}_1 (x_i-\\bar{x})(Y_i - \\bar{Y}) + \\hat{\\beta}_1^2(x_i-\\bar{x})^2] \\\\ &amp;= S_{YY} - 2 \\hat{\\beta}_1 S_{xY} + \\hat{\\beta}_1^2 S_{xx} \\\\ &amp;= S_{YY} - 2 \\hat{\\beta}_1 (S_{xx} \\hat{\\beta}_1) + \\hat{\\beta}_1^2 S_{xx} \\\\ &amp;= S_{YY} - \\hat{\\beta}_1^2 S_{xx} \\,. \\end{align*}\\] To find the expected value of this difference, we look first at \\(S_{YY}\\) and then at \\(\\hat{\\beta}_1^2 S_{xx}\\). \\[\\begin{align*} E[S_{YY}] &amp;= E\\left[ \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\left(\\beta_0 + \\beta_1x_i + \\epsilon_i - \\beta_0 - \\beta_1 \\bar{x} - \\bar{\\epsilon} \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\left(\\beta_1(x_i -\\bar{x}) + (\\epsilon_i - \\bar{\\epsilon}) \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\beta_1^2(x_i -\\bar{x})^2 + \\sum_{i=1}^n 2 \\beta_1 (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon}) + \\sum_{i=1}^n (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\\\ &amp;= E\\left[ \\beta_1^2 S_{xx} + 2 \\beta_1 \\sum_{i=1}^n (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon}) + \\sum_{i=1}^n (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\\\ &amp;= \\beta_1^2 S_{xx} + 2 \\beta_1 \\sum_{i=1}^n (x_i - \\bar{x}) E\\left[ \\epsilon_i - \\bar{\\epsilon} \\right] + \\sum_{i=1}^n E\\left[ (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\,. \\end{align*}\\] Since \\(E[\\epsilon_i] = E[\\bar{\\epsilon}] = 0\\), the middle term vanishes. As for the right-most term: \\[\\begin{align*} \\sum_{i=1}^n E\\left[ (\\epsilon_i - \\bar{\\epsilon})^2 \\right] &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 - 2\\epsilon_i \\bar{\\epsilon} + \\bar{\\epsilon}^2 \\right] \\\\ &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 \\right] - 2 \\sum_{i=1}^n E\\left[\\epsilon_i \\bar{\\epsilon}\\right] + \\sum_{i=1}^n E\\left[\\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 \\right] - 2 E\\left[ \\bar{\\epsilon} \\sum_{i=1}^n \\epsilon_i \\right] + E\\left[ \\sum_{i=1}^n \\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n V[\\epsilon_i] + \\sum_{i=1}^n \\left(E[\\epsilon_i]\\right)^2 - 2 n E\\left[ \\bar{\\epsilon}^2 \\right] + n E\\left[ \\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n \\sigma^2 - n E\\left[ \\bar{\\epsilon}^2 \\right] \\\\ &amp;= n \\sigma^2 - n \\frac{\\sigma^2}{n} = (n-1)\\sigma^2 \\,. \\end{align*}\\] So at this point, we have that \\[ E[S_{YY}] = S_{xx} \\beta_1^2 + (n-1)\\sigma^2 \\,. \\] Now let’s look at \\(E[\\hat{\\beta}_1^2 S_{xx}]\\): \\[\\begin{align*} E\\left[\\hat{\\beta}_1^2 S_{xx}\\right] &amp;= S_{xx} E\\left[\\hat{\\beta}_1^2\\right] \\\\ &amp;= S_{xx} \\left( V\\left[ \\hat{\\beta}_1 \\right] + \\left( E\\left[ \\hat{\\beta}_1 \\right] \\right)^2 \\right) \\\\ &amp;= S_{xx} \\left( \\frac{\\sigma^2}{S_{xx}} + \\beta_1^2 \\right) \\\\ &amp;= \\sigma^2 + S_{xx} \\beta_1^2 \\,. \\end{align*}\\] So \\[ E[SSE] = S_{xx} \\beta_1^2 + (n-1)\\sigma^2 - \\sigma^2 - S_{xx} \\beta_1^2 = (n-2)\\sigma^2 \\,. \\] Thus \\(SSE/(n-2)\\) is an unbiased estimator of \\(\\sigma^2\\). 2.17.3 Linear Regression in R Below, we show the results of learning a linear regression model using R. The data are the same as used to produce Figure 2.25, with the parameter estimates output by lm() mapping directly to the solid red line in that figure. # lm(): &quot;linear model&quot; # y~x : this is a &quot;model formula&quot; that in words translates as # &quot;regress the response data y upon the predictor data x&quot; # or &quot;estimate beta_0 and beta_1 for the model E[Y] = beta_0 + beta_1 x&quot; lm.out = lm(y~x) # show a summary of the model summary(lm.out) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 The model residuals \\(Y_i - \\hat{Y}_i\\) are summarized via five numbers (minimum, maximum, and median, along with the 25\\(^{\\rm th}\\) and 75\\(^{\\rm th}\\) percentile values). If, e.g., the median is substantially different from zero, it is possible that the assumption that \\(Y \\vert x\\) is normally distributed is violated…and it is possible that the assumption \\(E[\\epsilon_i] = 0\\) is violated as well. Under Coefficients, there are four numerical columns. The first shows \\(\\hat{\\beta}_0\\) ((Intercept)) and \\(\\hat{\\beta}_1\\) (x), the second shows the estimated standard errors for these statistics, the third is simply the ratio of the values in the first and second columns, and the fourth is the \\(p\\)-value. The third and fourth columns are distribution-specific: the hypothesis test being carried out has a null of zero, and the t value is assumed to be \\(t\\)-distributed for \\(n-2\\) degrees of freedom. The \\(p\\)-values in the fourth column are \\(\\ll \\alpha = 0.05\\), which lead us to decide that both the intercept and the slope are truly non-zero. (In our simulation, they are: \\(\\beta_0 = 4\\) and \\(\\beta_1 = 0.5\\).) The Residual standard error shows the value of \\(\\hat{\\sigma} = \\sqrt{SSE/(n-2)}\\), while the “38 degrees of freedom” indicates that \\(n = 40\\). We note that in this simulation, \\(\\sigma^2 = 1\\). The meanings behind the R-squared values are explained above in the correlation example. In words, approximately 60% of the variation exhibited by the data is accounted for with the linear model. Last, the line with the phrase F-statistic shows the result of a hypothesis test in which (in general) the null is that all the \\(\\beta_i\\)’s (excluding \\(\\beta_0\\)) are zero and the alternative is that at least one of the \\(\\beta_i\\)’s is non-zero. Here, since there is only one non-intercept coefficient, the \\(p\\)-value is equivalent to the one printed under Coefficients. We will discuss where the F-statistic and DF numbers come from below in the next section. 2.18 One-Way Analysis of Variance In the simple linear regression setting, our data consists of a continuously distributed predictor variable \\(\\mathbf{x}\\) and response variable \\(\\mathbf{Y}\\). What if, instead, the predictor variable values come in groups? For instance, we might wish to determine the time a person needs to accomplish a task after eating either chocolate or vanilla ice cream. To do this, we might map the eating of chocolate to a particular value of \\(x\\), say \\(x = 0\\), and the eating of vanilla to another value of \\(x\\), say \\(x = 1\\). Then, if we continue to adopt a simple linear regression framework, we have that \\[\\begin{align*} Y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\ E[Y_i \\vert x_i = 0] &amp;= \\beta_0 \\\\ E[Y_i \\vert x_i = 1] &amp;= \\beta_0 + \\beta_1 \\,. \\end{align*}\\] To see if there is a difference in task-accomplishment time between groups, we would test the hypothesis \\(H_o: \\beta_1 = 0\\) versus \\(H_a: \\beta_1 \\neq 0\\). As the value of the slope is meaningless, beyond whether or not it is zero (because the mapping from group characteristics to values of \\(x\\) is arbitrary), we would not run a conventional linear regression analysis; rather, if we assume that \\(\\bar{Y} \\vert x_i\\) is normally distributed, we would run a two-sample \\(t\\) test like the Welch’s \\(t\\) test described earlier in this chapter to determine whether \\(\\bar{Y} \\vert x_i=0\\) is drawn from a distribution with the same population mean as \\(\\bar{Y} \\vert x_i=1\\). What if there are more than two groups (or treatments)? For instance, what if we redo the experiment but add strawberry ice cream? When the number of groups is greater than two, we move from the two-sample \\(t\\) test to one-way analysis of variance (or one-way ANOVA; the “one-way” indicates that there is a single predictor variable, which in our example is ice cream flavor). See Figure 2.26. Figure 2.26: Illustration of the setting for a one-way analysis of variance. To the left, the spread of the data within each group is large compared to the differences in means between each group, so an ANOVA is less likely to reject the null hypothesis that \\(\\mu_0 = \\mu_1 = \\mu_2\\). To the right, the spread of data within groups is small relative to the differences in means between groups, so an ANOVA is more likely to reject the null hypothesis. Let the index \\(i\\) denote a particular group (chocolate ice-cream eaters, etc.) and let the index \\(j\\) denote different data within the same group. Let \\(n_i\\) denote the sample size within each group, and let \\(k\\) denote the total number of groups. The total sum of squares is defined as the sum of squared differences between \\(Y_i\\) and \\(\\bar{Y}\\), which in this setting can be expressed via a double summation, one over data within a group, and another over groups: \\[ \\mbox{Total SS} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y})^2 = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij}-\\bar{Y}_{i\\bullet})^2 + \\sum_{i=1}^k n_i(\\bar{Y}_{i\\bullet}-\\bar{Y})^2 = SSE + SST \\,, \\] where \\(\\bar{Y}_{i\\bullet}\\) is the average value for group \\(i\\), \\(SSE\\) has its usual meaning as the sum of squared errors (although here \\(\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) is replaced with the estimated group mean \\(\\bar{Y}_{i\\bullet}\\)), and \\(SST\\) is the sum of squared average treatment effects. If the \\(SSE\\) value is large relative to the \\(SST\\) value, then we are in a situation where the data are spread widely within each group relative to the spread in values of the group means. In such a situation, it would be difficult to detect a statistically significant difference between the group means. (See the left panel of Figure 2.26.) On the other hand, if the \\(SST\\) value is large relative to the \\(SSE\\) value, then the group means have a wide spread compared to the data in each group. In such a situation, it would be much easier to detect a statistically significant difference between the group means. (See the right panel of Figure 2.26.) This reasoning motivates using the ratio \\[ \\frac{SST}{SSE} \\] as the hypothesis test statistic. However, we do not know its distribution…or at least, not yet. We do know is that under the null, \\[\\begin{align*} W_T = \\frac{SST}{\\sigma^2} &amp;\\sim \\chi_{k-1}^2 \\\\ W_E = \\frac{SSE}{\\sigma^2} &amp;\\sim \\chi_{n-k}^2 \\,. \\end{align*}\\] And we know that \\[ F = \\frac{W_1/\\nu_1}{W_2/\\nu_2} \\sim F_{\\nu_1,\\nu_2} \\,, \\] i.e., the ratio of chi-square distributed random variables, each divided by their respective number of degrees of freedom, is \\(F\\)-distributed for \\(\\nu_1\\) numerator and \\(\\nu_2\\) denominator degrees of freedom. Thus \\[ F = \\frac{SST/(k-1)}{SSE/(n-k)} = \\frac{MST}{MSE} \\sim F_{k-1,n-k} \\,. \\] In the ANOVA hypothesis test, the null hypothesis \\(H_o\\) is that \\(E[Y_1] = E[Y_2] = \\cdots = E[Y_k]\\), and the alternative hypothesis is that at least one mean is different from the others. If we test at level \\(\\alpha\\), we reject the null hypothesis if \\(F &gt; F_{1-\\alpha,k-1,n-k}\\). In R, the boundary of the rejection region would be given by qf(1-alpha,k-1,n-k) while the \\(p\\)-value would be given by 1-pf(F.obs,k-1,n-k). 2.18.1 One-Way ANOVA: the Statistical Model In the notes above, we build up the test statistic for the one-way ANOVA, but we do not discuss the underlying statistical model. We can write down the model as \\[ Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij} \\,, \\] where \\(i\\) denotes the treatment group and \\(j\\) denotes an observed datum within group \\(i\\). \\(\\mu\\) is the overall mean response, while \\(\\tau_i\\) is the deterministic effect of treatment in group \\(i\\). (\\(\\tau\\) is the Greek letter tau, which is pronounced “tao” [rhyming with “ow,” an expression of pain]. Recall that by “deterministic” we mean that \\(\\tau_i\\) is not random.) The error terms \\(\\epsilon_{ij}\\) are independent, normally distributed, and homoscedastic with variance \\(\\sigma^2\\). Recall that \\(\\bar{Y}_{i\\bullet}\\) is the sample mean within group \\(i\\). We know it is distributed normally (by assumption) but what are its expected value and variance? The expected value is \\[\\begin{align*} E[\\bar{Y}_{i\\bullet}] &amp;= E\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} Y_{ij}\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i} E[Y_{ij}] \\\\ &amp;= \\frac{1}{n_i}\\sum_{j=1}^{n_i} E[\\mu + \\tau_i + \\epsilon_{ij}] = \\mu + \\tau_i + E[\\epsilon_{ij}] = \\mu+\\tau_i \\,, \\end{align*}\\] while the variance is \\[ V[\\bar{Y}_{i\\bullet}] = V\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} Y_{ij}\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i^2} V[Y_{ij}] = \\frac{1}{n_i^2}\\sum_{j=1}^{n_i} V[\\mu + \\tau_i + \\epsilon_{ij}] = \\frac{n_i V[\\epsilon_{ij}]}{n_i^2} = \\frac{\\sigma^2}{n_i} \\,. \\] Thus \\(\\bar{Y}_{i\\bullet} \\sim \\mathcal{N}(\\mu+\\tau_i,\\sigma^2/n_i)\\). 2.18.2 Linear Regression in R: Redux The last example of the last section showed the output from lm(), R’s linear model function: summary(lm.out) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 In that example, we indicated that the numbers on the line beginning F-statistic would make more sense after we covered one-way analysis of variance. There are two coefficients in this model, which is analogous to two “treatment groups.” Hence \\(k = 2\\), \\(SST/\\sigma^2 \\sim \\chi_{k-1=1}^2\\), and \\(SSE/\\sigma^2 \\sim \\chi_{n-k=38}^2\\). This explains on 1 and 38 DF. The \\(F\\) statistic is \\(MST/MSE = 61.76\\), which under the null is sampled from an \\(F\\) distribution with 1 numerator and 38 denominator degrees of freedom. This is an extreme value under the null, as indicated by the \\(p\\)-value 1.749 \\(\\times 10^{-9}\\)…so we conclude that the true value of the slope \\(\\beta_1\\) is not zero. But there’s more. To access more information about the \\(F\\) test that is carried out, we can call the anova() function. anova(lm.out) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 59.208 59.208 61.756 1.749e-09 *** ## Residuals 38 36.432 0.959 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This function call outputs information we could not see above: the \\(SST\\) (the first row of Sum Sq), \\(SSE\\) (second row), \\(MST\\) (the first row of Mean Sq), and \\(MSE\\) (second row). The F value is the ratio \\(MST/MSE\\); here, that is 59.208/0.959 = 61.756. 2.19 The Exponential Family of Distributions We conclude this chapter by placing the normal distribution into a larger context: the exponential family of distributions, which encompasses the normal as well as, e.g., the binomial, beta, Poisson, and gamma distributions, the ones that serve as the foci for chapters 3 and 4. Knowing about the exponential family and its properties will help us better understand concepts of statistical inference that we introduce in those chapters, such as the concept of sufficiency. Let’s assume that we are examining a distribution \\(P\\) that has one freely varying parameter \\(\\theta\\). \\(P\\) is a member of the exponential family of distributions if we can express, e.g., its probability density function as \\[\\begin{align*} f_X(x \\vert \\theta) = h(x) \\exp\\left[\\eta(\\theta) T(x) - A(\\theta)\\right] \\,. \\end{align*}\\] (This same result holds for a mass function \\(p_X(x \\vert \\theta)\\) as well. Note that \\(\\theta\\) cannot be a domain-specifying parameter, so a distribution whose domain is, e.g., \\(x \\in [0,\\theta]\\) cannot belong to the exponential family.) Let’s see how this works with a normal distribution of known variance: \\[\\begin{align*} f_X(x \\vert \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right] &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\exp\\left(\\frac{x\\mu}{\\sigma^2}\\right) \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2}\\right) \\,. \\end{align*}\\] From this, we can directly read off the functions that together comprise the exponential family form: \\[\\begin{align*} h(x) &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\\\ \\eta(\\mu) &amp;= \\frac{\\mu}{\\sigma^2} \\\\ T(x) &amp;= x \\\\ A(\\mu) &amp;= \\frac{\\mu^2}{2\\sigma^2} \\,. \\end{align*}\\] We note that we can also define \\(\\eta(\\mu) = \\mu\\) and \\(T(x) = x/\\sigma^2\\). Ultimately, the choice of where to place any constants (either within the \\(\\eta(\\cdot)\\) function or within \\(T(\\cdot)\\) function) makes no difference when we are performing inferential tasks. 2.19.1 Exponential Family with a Vector of Parameters The more general functional form of the exponential family that we use when a distribution has \\(p \\geq 2\\) parameters is \\[\\begin{align*} f_X(x \\vert \\boldsymbol{\\theta}) = h(x) \\exp\\left[\\left(\\sum_{i=1}^p \\eta_i(\\boldsymbol{\\theta}) T_i(x)\\right) - A(\\boldsymbol{\\theta})\\right] \\,. \\end{align*}\\] For the normal distribution with unknown mean and variance, we can examine the expression \\[\\begin{align*} f_X(x \\vert \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\exp\\left(\\frac{x\\mu}{\\sigma^2}\\right) \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2}\\right) \\,, \\end{align*}\\] and, after noting that \\[\\begin{align*} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} = \\exp\\left(\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)\\right) = \\exp\\left(-\\frac12 \\log\\left(2 \\pi \\sigma^2\\right) \\right) \\,, \\end{align*}\\] we can read off the constituent functions: \\[\\begin{align*} h(x) &amp;= 1 \\\\ \\eta_1(\\mu,\\sigma^2) &amp;= -\\frac{1}{2\\sigma^2} \\\\ T_1(x) &amp;= x^2 \\\\ \\eta_2(\\mu,\\sigma^2) &amp;= \\frac{\\mu}{\\sigma^2} \\\\ T_2(x) &amp;= x \\\\ A(\\mu,\\sigma^2) &amp;= \\frac{\\mu^2}{2\\sigma^2} + \\frac12 \\log\\left(2 \\pi \\sigma^2\\right) \\,. \\end{align*}\\] "],["the-binomial-and-related-distributions.html", "3 The Binomial (and Related) Distributions 3.1 Motivation 3.2 Probability Mass Function 3.3 Cumulative Distribution Function 3.4 Linear Functions of Random Variables 3.5 Order Statistics 3.6 Point Estimation 3.7 Confidence Intervals 3.8 Hypothesis Testing 3.9 Logistic Regression 3.10 Naive Bayes Regression 3.11 The Beta Distribution 3.12 The Multinomial Distribution 3.13 Chi-Square-Based Hypothesis Testing 3.14 Exercises", " 3 The Binomial (and Related) Distributions 3.1 Motivation Let’s assume we are holding a coin. It may be a fair coin (meaning that the probabilities of observing heads or tails in a given flip of the coin are each 0.5)…or perhaps it is not. We decide that we are going to flip the coin some fixed number of times \\(k\\), and we will record the outcome of each flip: \\[ \\mathbf{Y} = \\{Y_1,Y_2,\\ldots,Y_k\\} \\,, \\] where, e.g., \\(Y_1 = 1\\) if we observe heads or \\(0\\) if we observe tails. This is an example of a Bernoulli process, where “process” denotes a sequence of observations, and “Bernoulli” indicates that there are two possible discrete outcomes for each observation. A binomial experiment is one that generates Bernoulli process data through the running of \\(k\\) trials (e.g., \\(k\\) separate coin flips). The properties of such an experiment are that: The number of trials \\(k\\) is fixed in advance. Each trial has two possible outcomes, generically denoted as \\(S\\) (success) or \\(F\\) (failure). The probability of success remains \\(p\\) throughout the experiment. The outcome of any one trial is independent of the outcomes of the others. The random variable of interest for a binomial experiment is the number of observed successes. A closely related alternative to a binomial experiment is a negative binomial experiment, where the number of successes \\(s\\) is fixed in advance, instead of the number of trials \\(k\\), and the random variable of interest is the number of failures that we observe before achieving \\(s\\) successes. A simple example would be flipping a coin until \\(s\\) heads are observed and recording the overall number of tails that we observe. As a side note to the third point above, about the probability of success remaining \\(p\\) throughout the experiment: binomial and negative binomial experiments rely on sampling with replacement…if we observe a head for a given coin flip, we can observe heads again in the future. In the real world, however, the reader will observe instances where, e.g., a binomial distribution is used to model experiments featuring sampling without replacement: we have \\(K = 100\\) widgets, of which ten are defective; we check one to see if it is defective (with probability \\(p = 0.1\\)) and set it aside, then check another (with probability either 10/99 or 9/99, depending on the outcome of the first trial), etc. The convention for using the binomial distribution to model data in such a situation is that it is fine to do so if the number of trials \\(k \\lesssim K/10\\). However, in the age of computers, there is no reason to apply the binomial distribution when we can apply the hypergeometric distribution instead. And, as a side note to the fourth point above, about the outcome of each trial being independent of the outcomes of the others: in a general process, each datum can be dependent on the data observed previously. How each datum is dependent on previous data defines the type of process that is observed: a Markov process, a Gaussian process, etc. A Bernoulli process is termed a memoryless process because it is comprised of independent (and identically distributed) data. 3.2 Probability Mass Function Let’s focus first on the outcome of a binomial experiment, with the random variable \\(X\\) being the number of observed successes in \\(k\\) trials. What is the probability of observing \\(X=x\\) successes, if the probability of observing a success in any one trial is \\(p\\)? \\[\\begin{align*} \\mbox{$x$ successes}&amp;: p^x \\\\ \\mbox{$k-x$ failures}&amp;: (1-p)^{k-x} \\,. \\end{align*}\\] So \\(P(X=x) = p^x (1-p)^{k-x}\\)…but, no, this isn’t right. Let’s start again and think this through. Assume \\(k = 2\\). The sample space of possible experimental outcomes is \\[ \\Omega = \\{ SS, SF, FS, FF \\} \\,. \\] If \\(p\\) = 0.5, then we can see that the probability of observing one success in two trials is 0.5…but our proposed probability mass function tells us that \\(P(X=1) = (0.5)^1 (1-0.5)^1 = 0.25\\). What are we missing? We are missing that there are two ways of observing a single success…and we need to count both. Because we ultimately do not care about the order in which successes and failures are observed, we utilize counting via combination: \\[ \\binom{k}{x} = \\frac{k!}{x! (k-x)!} \\,, \\] where the exclamation point represents the factorial function \\(x! = x(x-1)(x-2)\\cdots 1\\). So now we can correctly write down the binomial probability mass function: \\[ P(X=x) = p_X(x) = \\binom{k}{x} p^x (1-p)^{k-x} ~~~ x \\in \\{0,\\ldots,k\\} \\,. \\] We denote the distribution of the random variable \\(X\\) as \\(X \\sim\\) Binomial(\\(k\\),\\(p\\)). Note that when \\(k = 1\\), we have a Bernoulli distribution. Recall: a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) \\(0 \\leq p_X(x) \\leq 1\\) and (b) \\(\\sum_x p_X(x) = 1\\), where the sum is over all values of \\(x\\) in the distribution’s domain. (The reader should note that the number of trials is commonly denoted as \\(n\\), not as \\(k\\). However, since \\(n\\) is conventionally used to denote the sample size in an experiment, to avoid confusion we use \\(k\\) to denote the number of trials in this text.) In Figure 3.1, we display three binomial pmfs, one each for probabilities of success 0.1 (red, to the left), 0.5 (green, to the center), and 0.8 (blue, to the right). This figure indicates an important aspect of the binomial pmf, namely that it can attain a shape akin to that of a normal distribution, if \\(p\\) is such that any truncation observed at the values \\(x=0\\) and \\(x=k\\) is minimal. In fact, a binomial random variable converges in distribution to a normal random variable in certain limiting situations, as we show in an example below. Figure 3.1: Binomial probability mass functions for number of trials \\(k = 10\\) and success probabilities \\(p = 0.1\\) (red squares), 0.5 (green triangles), and 0.8 (blue circles). Recall: the expected value of a discretely distributed random variable is \\[ E[X] = \\sum_x x p_X(x) \\,, \\] where the sum is over all values of \\(x\\) within the domain of the pmf p_X(x). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(p_X(x)\\). For the binomial distribution, the expected value is \\[ E[X] = \\sum_{x=0}^k x \\binom{k}{x} p^x (1-p)^{k-x} \\,. \\] At first, this does not appear to be easy to evaluate. One trick in our arsenal is to pull constants out of the summation such that whatever is left as the summand is a pmf (and thus sums to 1). Let’s try this here: \\[\\begin{align*} E[X] &amp;= \\sum_{x=0}^k x \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=1}^k x \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=1}^k \\frac{k!}{(x-1)!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= kp \\sum_{x=1}^k \\frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\,. \\end{align*}\\] The summation appears almost like that of a binomial random variable. Let’s set \\(y = x-1\\). Then \\[\\begin{align*} E[X] &amp;= kp \\sum_{x=1}^k \\frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\\\ &amp;= kp \\sum_{y=0}^{k-1} \\frac{(k-1)!}{y!(k-(y+1))!} p^y (1-p)^{k-(y+1)} \\\\ &amp;= kp \\sum_{y=0}^{k-1} \\frac{(k-1)!}{y!((k-1)-y)!} p^y (1-p)^{(k-1)-y} \\,. \\end{align*}\\] The summand is now the pmf for the random variable \\(Y \\sim\\) Binomial(\\(k-1\\),\\(p\\)), summed over all values of \\(y\\) in the domain of the distribution. Thus the summation evaluates to 1: \\(E[X] = kp\\). In an example below, we use a similar strategy to determine the variance \\(V[X] = kp(1-p)\\). A negative binomial experiment is governed by the negative binomial distribution, whose pmf is \\[ p_X(x) = \\binom{x+s-1}{x} p^s (1-p)^x ~~~ x \\in \\{0,1,\\ldots,\\infty\\} \\] The form of this pmf follows from the fact that the underlying Bernoulli process would consist of \\(x+s\\) data, with the last datum being the observed success that ends the experiment. The first \\(x+s-1\\) data would feature \\(s-1\\) successes and \\(x\\) failures, with the order of success and failure not mattering…so we can view these data as being binomially distributed (albeit with \\(x\\) representing failures…hence the “negative” in negative binomial!): \\[ p_X(x) = \\underbrace{\\binom{x+s-1}{x} p^{s-1} (1-p)^x}_{\\mbox{first $x+s-1$ trials}} \\cdot \\underbrace{p}_{\\mbox{last trial}} \\,. \\] Note that when \\(s = 1\\), the resulting distribution is called the geometric distribution. Figure 3.2: Negative binomial probability mass functions for the number of successes \\(s = 2\\) and success probabilities \\(p = 0.7\\) (red squares), 0.5 (green triangles), and 0.2 (blue circles). In an example below, we show how one would derive the expected value of a negative binomial random variable, \\(E[X] = s(1-p)/p\\). (We can enact a similar calculation to show that the variance is \\(V[X] = s(1-p)/p^2\\).) 3.2.1 Variance of a Binomial Random Variable Recall: the variance of a discretely distributed random variable is \\[ V[X] = \\sum_x (x-\\mu)^2 p_X(x) = E[X^2] - (E[X])^2\\,, \\] where the sum is over all values of \\(x\\) in the domain of the pmf \\(p_X(x)\\). The variance represents the square of the “width” of a probability mass function, where by “width” we mean the range of values of \\(x\\) for which \\(p_X(x)\\) is effectively non-zero. The variance of a random variable is given by the shortcut formula that we have been using since Chapter 1: \\(V[X] = E[X^2] - (E[X])^2\\). So we would expect that we would need to compute \\(E[X^2]\\) here, since we already know that \\(E[X] = kp\\). But for reasons that will become apparent below, it is actually far easier for us to compute \\(E[X(X-1)]\\), and to work with that to eventually derive the variance: \\[\\begin{align*} E[X(X-1)] &amp;= \\sum_{x=0}^k x(x-1) \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=0}^k x(x-1) \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=2}^k \\frac{k!}{(x-2)!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= k(k-1) p^2 \\sum_{x=2}^k \\frac{(k-2)!}{(x-2)!(k-x)!} p^{x-2} (1-p)^{k-x} \\,. \\end{align*}\\] The advantage to using \\(x(x-1)\\) was that it matches the first two terms of \\(x! = x(x-1)\\cdots(1)\\), allowing easy cancellation. If we set \\(y = x-2\\), we find that the summand above will, in a similar manner as in the calculation of \\(E[X]\\), become the pmf for the random variable \\(Y \\sim\\) Binomial(\\(k-2,p\\))…and thus the summation will evaluate to 1. So \\(E[X(X-1)] = E[X^2] - E[X] = k(k-1)p^2\\), and \\(E[X^2] = k^2p^2-kp^2 + kp = V[X] + (E[X])^2\\), and \\(V[X] = k^2p^2-kp^2+kp-k^2p^2 = kp-kp^2 = kp(1-p)\\). Done. 3.2.2 The Expected Value of a Negative Binomial Random Variable The calculation for the expected value \\(E[X]\\) for a negative binomial random variable is similar to that for a binomial random variable: \\[\\begin{align*} E[X] &amp;= \\sum_{x=0}^{\\infty} x \\binom{x+s-1}{x} p^s (1-p)^x \\\\ &amp;= \\sum_{x=0}^{\\infty} x \\frac{(x+s-1)!}{(s-1)!x!} p^s (1-p)^x \\\\ &amp;= \\sum_{x=1}^{\\infty} \\frac{(x+s-1)!}{(s-1)!(x-1)!} p^s (1-p)^x \\,. \\end{align*}\\] Let \\(y = x-1\\). Then \\[\\begin{align*} E[X] &amp;= \\sum_{y=0}^{\\infty} \\frac{(y+s)!}{(s-1)!y!} p^s (1-p)^{y+1} \\\\ &amp;= \\sum_{y=0}^{\\infty} s(1-p) \\frac{(y+s)!}{s!y!} p^s (1-p)^y \\\\ &amp;= \\sum_{y=0}^{\\infty} \\frac{s(1-p)}{p} \\frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\\\ &amp;= \\frac{s(1-p)}{p} \\sum_{y=0}^{\\infty} \\frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\\\ &amp;= \\frac{s(1-p)}{p} \\,. \\end{align*}\\] The summand is that of a negative binomial distribution for \\(s+1\\) successes, hence the summation is 1, and thus \\(E[X] = s(1-p)/p\\). We can use a similar calculation in which we evaluate \\(E[X(X-1)]\\) in order to derive the variance of a negative binomial random variable: \\(V[X] = s(1-p)/p^2\\). 3.2.3 Binomial Distribution: Normal Approximation In certain limiting situations, a binomial random variable converges in distribution to a normal random variable. In other words, if \\[ P\\left(\\frac{X-\\mu}{\\sigma} &lt; a \\right) = P\\left(\\frac{X-kp}{\\sqrt{kp(1-p)}} &lt; a \\right) \\approx P(Z &lt; a) = \\Phi(a) \\,, \\] then we can state that \\(X \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}(kp,kp(1-p))\\), or that \\(X\\) converges in distribution to a normal random variable \\(Y\\). Now, what do we mean by “certain limiting situations”? For instance, if \\(p\\) is close to zero or one, then the binomial distribution is truncated at 0 or at \\(k\\), and the shape of the pmf does not appear to be like that of a normal pdf. One convention is that the normal approximation is adequate if \\[ k &gt; 9\\left(\\frac{\\mbox{max}(p,1-p)}{\\mbox{min}(p,1-p)}\\right) \\,. \\] The reader might question why we would mention this approximation at all: if we have binomially distributed data and a computer, then we need not ever utilize such an approximation to, e.g., compute probabilities. This point is correct (and is the reason why, for instance, we do not mention the so-called continuity correction here; our goal is not to compute probabilities). The reason we mention this is that this approximation underlies a commonly used hypothesis test framework, the Wald interval, that we will mention later in the chapter. 3.2.4 Computing Probabilities Let \\(X\\) be a random variable sampled from a binomial distribution with number of trials \\(k = 6\\) and success probability \\(p = 0.4\\), and let \\(Y\\) be a random variable sampled from a negative binomial distribution with number of successes \\(s = 3\\) and success probability \\(p = 0.6\\). What is \\(P(2 \\leq X &lt; 4)\\)? To find this probability, we sum over the binomial probability mass function for values \\(x = 2\\) and \\(x = 3\\). (The form of the inequalities matter for discrete distributions!) We can perform this computation analytically: \\[\\begin{align*} P(2 \\leq X &lt; 4) &amp;= \\binom{6}{2} (0.4)^2 (1-0.4)^4 + \\binom{6}{3} (0.4)^3 (1-0.4)^3 \\\\ &amp;= \\frac{6!}{2!4!} \\cdot 0.16 \\cdot 0.1296 + \\frac{6!}{3!3!} \\cdot 0.064 \\cdot 0.216 = 15 \\cdot 0.0207 + 20 \\cdot 0.0138 = 0.5875 \\,. \\end{align*}\\] There is a 58.75% chance that the next time we sample data according to this distribution, we will observe a value of 2 or 3. It is ultimately simpler to use R to perform this calculation. While we can compute the probability as the difference of two cumulative distribution function values, for discrete distributions it is often more straightforward to sum over the relevant probability masses directly, since then we need not worry about whether the input to the cdf is correct given the form of the inequality: sum(dbinom(2:3,size=6,prob=0.4)) ## [1] 0.58752 What is \\(P(Y &gt; 1)\\)? If we were to determine this probability by hand, the first thing we would do is specify that we will compute \\(1 - P(Y \\leq 3)\\), as this has a finite (and small!) number of terms: \\[\\begin{align*} P(Y &gt; 1) &amp;= 1 - \\binom{0+3-1}{0} (0.6)^3 (1-0.6)^0 - \\binom{1+3-1}{1} (0.6)^3 (1-0.6)^1 \\\\ &amp;= 1 - 1 \\cdot 0.216 \\cdot 1 - 3 \\cdot 0.216 \\cdot 0.4 = 0.5248 \\,. \\end{align*}\\] There is a 52.48% chance that we would observe more than one failure the next time we sample data according to this distribution. Like before, it is ultimately simpler to use R: 1 - sum(dnbinom(0:1,size=3,prob=0.6)) ## [1] 0.5248 3.2.5 The Binomial Distribution as Part of the Exponential Family Recall that the exponential family of distributions, introduced in the last chapter, comprises distributions whose probability mass or density functions can be written in the form \\[\\begin{align*} h(x) \\exp\\left( \\eta(\\theta)T(x) - A(\\theta) \\right) \\,. \\end{align*}\\] Is the binomial distribution a member of the larger exponential family of distributions? It would initially appear that the answer is no (as there are no exponential functions in its pmf), but if we note that \\[\\begin{align*} p^x = \\exp\\left(\\log p^x\\right) = \\exp\\left(x \\log p\\right) \\end{align*}\\] and that \\[\\begin{align*} (1-p)^{k-x} = \\exp\\left(\\log (1-p)^{k-x}\\right) = \\exp\\left((k-x) \\log (1-p)\\right) \\,, \\end{align*}\\] we can see that \\[\\begin{align*} p_X(x \\vert p) = \\binom{k}{x} \\exp\\left( x [ \\log(p) - \\log(1-p) ] + k \\log (1-p) \\right) \\end{align*}\\] and thus that \\[\\begin{align*} h(x) &amp;= \\binom{k}{x} \\\\ \\eta(p) &amp;= \\log(p) - \\log(1-p) = \\log \\frac{p}{1-p} \\\\ T(x) &amp;= x \\\\ A(p) &amp;= -k \\log(1-p) \\,. \\end{align*}\\] The binomial distribution is indeed a member of the exponential family. We will leave it as an exercise to the reader to show that the negative binomial distribution lies within the exponential family as well. 3.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as \\(F_X(x) = \\sum_{y\\leq x} p_Y(y)\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). For the binomial distribution, the cdf is \\[ F_X(x) = \\sum_{y=0}^{\\lfloor x \\rfloor} p_Y(y) = \\sum_{y=0}^{\\lfloor x \\rfloor} \\binom{k}{y} p^y (1-p)^{k-y} \\,, \\] where \\(\\lfloor x \\rfloor\\) denotes the floor function, which returns the largest integer that is less than or equal to \\(x\\) (e.g., if \\(x\\) = 6.75, \\(\\lfloor x \\rfloor\\) = 6). (In closed form, we can represent this cdf with a regularized incomplete beta function, which is not analytically easy to work with.) Also, because a pmf is defined at discrete values of \\(x\\), its associated cdf is a step function, as illustrated in the left panel of Figure 3.3. As we can see in this figure, the cdf steps up at each value of \\(x\\) in the domain of \\(p_X(x)\\), and unlike the case for continuous distributions, the form of the inequalities in a probabilistic statement matter: \\(P(X &lt; x)\\) and \\(P(X \\leq x)\\) will not be the same, if \\(x\\) is an integer with value \\(\\{0,1,2,\\ldots,k\\}\\). Recall: an inverse cdf function \\(x = F_X^{-1}(q)\\) takes as input a distribution quantile \\(q \\in [0,1]\\) and returns the value of \\(x\\). A discrete distribution has no unique inverse cdf; it is convention to utilize the generalized inverse cdf, \\(x = \\mbox{inf}\\{x : F_X(x) \\geq q\\}\\), where “inf” indicates that the function is to return the smallest value of \\(x\\) such that \\(F_X(x) \\geq q\\). In the right panel of Figure 3.3, we display the inverse cdf for the same distribution used to generate the figure in the left panel (\\(k=4\\) and \\(p=0.5\\)). Like the cdf, the inverse cdf for a discrete distribution is a step function. Below, in an example, we show how we adapt the inverse transform sampler algorithm of Chapter 1 to accommodate the step-function nature of an inverse cdf. Figure 3.3: Illustration of the cumulative distribution function \\(F_X(x)\\) (left) and inverse cumulative distribution function \\(F_X^{-1}(q)\\) (right) for a binomial distribution with number of trials \\(k = 4\\) and probability of success \\(p=0.5\\). 3.3.1 Computing Probabilities Because computing the binomial pmf for a range of values of \\(x\\) can be laborious, we typically utilize R functions when computing probabilities. If \\(X \\sim\\) Binomial(10,0.6), which is \\(P(4 \\leq X &lt; 6)\\)? We first note that due to the form of the inequality, we do not include \\(X=6\\) in the computation. Thus \\(P(4 \\leq X &lt; 6) = p_X(4) + p_X(5)\\), which equals \\[ \\binom{10}{4} (0.6)^4 (1-0.6)^6 + \\binom{10}{5} (0.6)^5 (1-0.6)^5 \\,. \\] Even computing this is unnecessarily laborious; instead, we call on R: sum(dbinom(4:5,size=10,prob=0.6)) ## [1] 0.3121349 (This utilizes R’s vectorization feature: we need not explicitly define a for-loop to evaluate dbinom() for \\(x=4\\) and then at \\(x=5\\).) We can also utilize cdf functions here: \\(P(4 \\leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \\leq 5) - P(X \\leq 3) = F_X(5) - F_X(3)\\), which in R is computed via pbinom(5,size=10,prob=0.6) - pbinom(3,size=10,prob=0.6) ## [1] 0.3121349 As we can see, the direct summation approach is the more straightforward one. \\(X \\sim\\) Binomial(10,0.6), what is the value of \\(a\\) such that \\(P(X \\leq a) = 0.9\\)? First, we set up the inverse cdf formula: \\[ P(X \\leq a) = F_X(a) = 0.9 ~~ \\Rightarrow ~~ a = F_X^{-1}(0.9) \\] Note that we didn’t do anything differently here than we would have done in a continuous distribution setting…and we can proceed directly to R because it utilizes the generalized inverse cdf algorithm. qbinom(0.9,size=10,prob=0.6) ## [1] 8 We can see immediately how the cdf for a discrete distribution is not a one-to-one function, as if we plug \\(x = 8\\) into the cdf, we will not recover the initial value \\(q = 0.9\\): pbinom(8,size=10,prob=0.6) ## [1] 0.9536426 3.3.2 Sampling Data From an Arbitrary Probability Mass Function While we would always utilize R shortcut functions like rbinom() when they exist, there may be instances when we need to code our own functions for sampling data from discrete distributions. The code below shows such a function for an arbitrary probability mass function. set.seed(101) x &lt;- c(1,2,4,8) # domain of x p.x &lt;- c(0.2,0.35,0.15,0.3) # p_X(x) F.x &lt;- cumsum(p.x) # cumulative sum -&gt; produces F_X(x) n &lt;- 100 q &lt;- runif(n) # we still ultimately need runif! i &lt;- findInterval(q,F.x)+1 # the output are bin numbers [0,3], and not [1,4] # hence we add 1 # 1 means q is between 0 and F.x[1], etc. x.sample &lt;- x[i] Figure 3.4: Histogram of \\(n = 100\\) iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of \\(x\\). 3.4 Linear Functions of Random Variables Let’s assume we are given \\(n\\) iid binomial random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Binomial(\\(k\\),\\(p\\)). Can we determine the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Yes, we can…via the method of moment-generating functions. Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(m_X(t) = E[e^{tX}]\\). Also, if \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) as the mgf for a known family of distributions, then we can immediately identify the distribution for \\(Y\\) and the parameters of that distribution. The mgf for the binomial distribution is \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^k e^{tx} \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=0}^k \\binom{k}{x} (pe^t)^x (1-p)^{k-x} \\,. \\end{align*}\\] We utilize the binomial theorem \\[ (x+y)^k = \\sum_{i=0}^k \\binom{k}{i} x^i y^{k-i} \\] to re-express \\(m_X(t)\\): \\[ m_X(t) = [pe^t + (1-p)]^k \\,. \\] Note that one may see this written as \\((pe^t+q)^k\\), where \\(q = 1-p\\). The mgf for \\(Y = \\sum_{i=1}^n X_i\\) is thus \\[ m_Y(t) = \\prod_{i=1}^n m_{X_i}(t) = [m_X(t)]^n = [pe^t + (1-p)]^{nk} \\,. \\] We can see that this has the form of a binomial mgf: \\(Y \\sim\\) Binomial(\\(nk\\),\\(p\\)), with expected value \\(E[Y] = nkp\\) and variance \\(V[Y] = nkp(1-p)\\). This makes sense, as the act of summing binomial data is equivalent to concatenating \\(n\\) separate Bernoulli processes into one longer Bernoulli process…whose data can subsequently be modeled using a binomial distribution. While we can identify the distribution of the sum by name, we cannot say the same about the sample mean. We know that the expected value is \\(E[\\bar{X}] = \\mu = kp\\) and that the variance is \\(V[\\bar{X}] = \\sigma^2/n = kp(1-p)/n\\), but when we attempt to use the mgf method with \\(a_i = 1/n\\) instead of \\(a_i = 1\\), we find that \\[ m_{\\bar{X}}(t) = [pe^{t/n} + (1-p)]^{nk} \\,. \\] Changing \\(t\\) to \\(t/n\\) has the effect of creating an mgf that does not have the form of any known mgf. However, we do know the distribution: it has a pmf that is identical in form to that of the binomial distribution, but has the domain \\(\\{0,1/n,2/n,...,k\\}\\). (We can derive this result mathematically by making the transformation \\(\\sum_{i=1}^n X_i \\rightarrow (\\sum_{i=1}^n X_i)/n\\), as we see below in an example.) We could define the pmf ourselves using our own R function, but there is no real need to: as we will see, if we wish to construct a confidence interval for \\(p\\), we can just use the sum \\(\\sum_{i=1}^n X_i\\) as our statistic. (We could also, in theory, utilize the Central Limit Theorem if \\(n \\gtrsim 30\\), but there is absolutely no reason to do that to make inferences about \\(p\\): we know the distribution of the sum of the data exactly, and thus there is no need to fall back upon approximations.) 3.4.1 The MGF for a Geometric Random Variable Recall that a geometric distribution is equivalent to a negative binomial distribution with number of successes \\(s = 1\\); its probability mass function is \\[ p_X(x) = p (1-p)^x \\,, \\] with \\(x = \\{0,1,\\ldots\\}\\) and \\(p \\in [0,1]\\). The moment-generating function for a geometric random variable is thus \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^{\\infty} e^{tx} p (1-p)^x = p \\sum_{x=0}^\\infty e^{tx} (1-p)^x \\\\ &amp;= p \\sum_{x=0}^\\infty [e^t(1-p)]^x = \\frac{p}{1-e^t(1-p)} \\,. \\end{align*}\\] The last equality utilizes the formula for the sum of an infinite geometric series: \\(\\sum_{i=0}^\\infty x^i = (1-x)^{-1}\\), when \\(\\vert x \\vert &lt; 1\\). (If \\(t &lt; 0\\) and \\(p &gt; 0\\), then the condition that \\(\\vert e^t(1-p) \\vert &lt; 1\\) holds.) The sum of \\(s\\) geometric random variables has the moment-generating function \\[ m_Y(t) = \\prod_{i=1}^s m_{X_i}(t) = \\left[\\frac{p}{1-e^t(1-p)}\\right]^s \\,. \\] This is the mgf for a negative binomial distribution for \\(s\\) successes. In the same way that the sum of \\(k\\) Bernoulli random variables is a binomially distributed random variable, the sum of \\(s\\) geometric random variables is a negative binomially distributed random variable. 3.4.2 The PMF for the Sample Mean Let’s assume we are given \\(n\\) iid binomial random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Binomial(\\(k\\),\\(p\\)). As we observe above, the distribution of the sum \\(Y = \\sum_{i=1}^k X_i\\) is binomial with mean \\(nkp\\) and variance \\(nkp(1-p)\\). The sample mean is \\(\\bar{X} = Y/n\\), and so \\[ F_{\\bar{X}}(\\bar{x}) = P(\\bar{X} \\leq \\bar{x}) = P(Y \\leq n\\bar{x}) = \\sum_{y=0}^{n\\bar{x}} p_Y(y) \\,. \\] (We note that \\(n\\bar{x}\\) is integer-valued by definition; we do not need to round down here.) Because we are dealing with a pmf, we cannot simply take the derivative of \\(F_{\\bar{X}}(\\bar{x})\\) to find \\(f_{\\bar{X}}(\\bar{x})\\)…but what we can do is assess the jump in the cumulative distribution function at each step, because that is the pmf. In other words, we can compute \\[ f_{\\bar{X}}(\\bar{x}) = P(Y \\leq n\\bar{x}) - P(Y \\leq n\\bar{x}-1) \\] and store this as a numerically expressed pmf for \\(\\bar{X}\\). See Figure 3.5. But it turns out we can say more about this pmf, by looking at the problem in a different way. We know that \\(Y \\sim\\) Binomial\\((nkp,nkp(1-p))\\) and thus that \\(Y \\in [0,1,\\ldots,nk]\\). When we compute the quotient \\(\\bar{X} = Y/n\\), all we are doing is redefining the domain of the pmf from being \\([0,1,\\ldots,nk]\\) to being \\([0,1/n,2/n,\\ldots,k]\\). We do not actually change the probability masses! So we can write \\[ p_{\\bar{X}}(\\bar{x}) = \\binom{nk}{n\\bar{x}} p^{n\\bar{x}} (1-p)^{nk-n\\bar{x}} ~~ \\bar{x} \\in [0,1/n,2/n,\\ldots,k] \\,. \\] This pmf has the functional form of a binomial pmf…but not the domain of a binomial pmf. For that reason, we cannot say that \\(\\bar{X}\\) is binomially distributed. The pmf has a functional form, it has a domain, but it has no known “name” and thus no associated R functions that we can utilize when performing statistical inference. (This is why we will utilize the sum of the data instead: R functions for its distribution exist!) Figure 3.5: Probability mass function for the sample mean of \\(n = 10\\) iid binomial random variables, for \\(k = 10\\) and \\(p = 0.6\\). 3.5 Order Statistics Let’s suppose that we have sampled \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) from some arbitrary distribution. Previously, we have summarized such data with the sample mean and the sample variance. However, there are other summary statistics, some of which are only calculable if we sort the data into ascending order: \\(\\{X_{(1)},\\ldots,X_{(n)}\\}\\). These are dubbed order statistics and the \\(j^{th}\\) order statistic is the sample’s \\(j^{th}\\) smallest value (i.e., the smallest-valued datum in the sample is \\(X_{(1)}\\) and the largest-valued datum is \\(X_{(n)}\\)). Examples of statistics based on ordering include \\[\\begin{align*} \\mbox{Range:}&amp; ~~X_{(n)} - X_{(1)} \\\\ \\mbox{Median:}&amp; ~~X_{(n+1)/2} ~ \\mbox{if $n$ is odd} \\\\ &amp; ~~(X_{n/2}+X_{(n+1)/2})/2 ~ \\mbox{if $n$ is even} \\,. \\end{align*}\\] The most important point to keep in mind is that the probability mass and density functions for order statistics differ from the pmfs and pdfs for their constituent iid data. For instance, if we sample \\(n\\) data from a \\(\\mathcal{N}(0,1)\\) distribution, we would not expect the minimum value to be distributed the same way; if anything, the mean should take on larger and larger negative values, and the variance on those values should decrease, as \\(n\\) increases. So: why are we discussing order statistics here, in the middle of a discussion of the binomial distribution? It is because we can derive, e.g., the pdf for an order statistic of a continuous distribution using the binomial pmf. (Note that order statistics exist for discretely valued data, but the probability mass functions for them are not easily derived and thus we will only consider order statistics for continuously valued data here.) Figure 3.6: If we have, e.g., a probability density function \\(f_X(x)\\) whose domain is \\([a,b]\\), and we view success as sampling a datum less than a given value \\(x\\), then when we sample \\(n\\) data, the number that have values \\(\\leq x\\) is a binomial random variable with \\(k=n\\) and \\(p = F_X(x)\\). See Figure 3.6. Without loss of generality, we can assume that \\(f_X(x) &gt; 0\\) for \\(x \\in [a,b]\\) and that we sample \\(n\\) data from this distribution. The number of data \\(X\\) that have value less than some arbitrarily chosen \\(x\\) is a binomial random variable: \\[ Y \\sim \\mbox{Binomial}(n,p=F_X(x)) \\] What is the probability that the \\(j^{th}\\) ordered datum has a value \\(\\leq x\\)? That’s equivalent to asking for the probability that \\(Y \\geq j\\), i.e., did we see at least \\(j\\) successes in \\(n\\) trials? \\[ F_{(j)}(x) = P(X_{(j)} \\leq x) = P(Y \\geq j) = \\sum_{i=j}^n \\binom{n}{i} [F_X(x)]^i [1 - F_X(x)]^{n-i} \\,. \\] \\(F_{(j)}(x)\\) is the cdf for the \\(j^{th}\\) ordered datum. Recall: a continuous distribution’s pdf is the derivative of its cdf. Leaving aside algebraic details, we can write down the pdf for \\(X_{(j)}\\): \\[ f_{(j)}(x) = \\frac{d}{dx}F_{(j)}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j} \\,, \\] and write down simplified expressions for the pdfs for the minimum and maximum data values: \\[ f_{(1)}(x) = n f_X(x) [1 - F_X(x)]^{n-1} ~~\\mbox{and}~~ f_{(n)}(x) = n f_X(x) [F_X(x)]^{n-1} \\,. \\] 3.5.1 Distribution of the Minimum Value Sampled from an Exponential Distribution The probability density function for an exponential random variable is \\[ f_X(x) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right) \\,, \\] for \\(x \\geq 0\\) and \\(\\theta &gt; 0\\), and the expected value of \\(X\\) is \\(E[X] = \\theta\\). What is the pdf for the smallest value among \\(n\\) iid data sampled from an exponential distribution? What is the expected value for the smallest value? First, if we do not immediately recall the cumulative distribution function \\(F_X(x)\\), we can easily derive it: \\[ F_X(x) = \\int_0^x \\frac{1}{\\theta} e^{-y/\\theta} dy = 1 - e^{-x/\\theta} \\,. \\] We plug \\(F_X(x)\\) into the expression of the pdf of the minimum datum given above: \\[\\begin{align*} f_{(1)}(x) &amp;= n \\frac{1}{\\theta} e^{-x/\\theta} \\left[ 1 - (1-e^{-x/\\theta}) \\right]^{n-1} \\\\ &amp;= n \\frac{1}{\\theta} e^{-x/\\theta} e^{-(n-1)x/\\theta} \\\\ &amp;= \\frac{n}{\\theta} e^{-nx/\\theta} \\,. \\end{align*}\\] \\(X_{(1)}\\) is thus an exponentially distributed random variable with parameter \\(\\theta/n\\) and expected value \\(\\theta/n\\). We can derive this result as follows. \\[ E[X_{(1)}] = \\int_0^\\infty x \\frac{n}{\\theta} e^{-nx/\\theta} dx \\,. \\] We recognize this as almost having the form of a gamma-function integral: \\[ \\Gamma(u) = \\int_0^\\infty x^{u-1} e^{-x} dx \\,. \\] We affect a variable transformation \\(y = nx/\\theta\\); for this transformation, \\(dy = (n/\\theta)dx\\), and if \\(x = 0\\) or \\(\\infty\\), \\(y = 0\\) or \\(\\infty\\) (meaning the integral bounds are unchanged). Our new integral is \\[ E[X] = \\int_0^\\infty \\frac{\\theta y}{n} \\frac{n}{\\theta} e^{-y} \\frac{\\theta}{n} dy = \\frac{\\theta}{n} \\int_0^\\infty y e^{-y} dy = \\frac{\\theta}{n} \\Gamma(2) = \\frac{\\theta}{n} 1! = \\frac{\\theta}{n} = \\frac{E[X]}{n} \\,. \\] 3.5.2 Distribution of the Median Value Sampled from a Uniform(0,1) Distribution The probability density function for a Uniform(0,1) distribution is \\[ f_X(x) = 1 \\] for \\(x \\in [0,1]\\). The cdf for this distribution is thus \\[ F_X(x) = \\int_0^x 1 dy = x \\,. \\] Let’s assume that we sample \\(n\\) iid data from this distribution, where \\(n\\) is an odd number. The index of the median value is thus \\((n+1)/2\\), and if we plug into the general expression for the pdf of the \\(m^{th}\\) ordered datum, we find that \\[\\begin{align*} f_{(n+1)/2} &amp;= \\frac{n!}{\\left(\\frac{n+1}{2}-1\\right)!\\left(n - \\frac{n+1}{2}\\right)!} \\cdot 1 \\cdot x^{\\left(\\frac{n+1}{2}\\right)-1} \\cdot (1-x)^{n - \\left(\\frac{n+1}{2}\\right)} \\\\ &amp;= \\frac{n!}{2\\left(\\frac{n-1}{2}\\right)!} x^{\\left(\\frac{n-1}{2}\\right)} (1-x)^{\\left(\\frac{n-1}{2}\\right)} \\,. \\end{align*}\\] As we will see later, this is a beta distribution with parameters \\(\\alpha = \\beta = (n+1)/2\\). The median value has expected value 1/2 and a variance that shrinks with \\(n\\). 3.6 Point Estimation In the first two chapters, we introduce a number of concepts related to point estimation, the act of using statistics to make inferences about a population parameter \\(\\theta\\). We… assess point estimators using the metrics of bias, variance, mean-squared error, and consistency; utilize the Fisher information metric to determine the lower bound on the variance for unbiased estimators (the Cramer-Rao Lower Bound, or CRLB); and define estimators via the maximum likelihood algorithm, which generates estimators that are at least asymptotically unbiased and at least asymptotically reach the CRLB, and which converge in distribution to normal random variables. We will review these concepts in the context of estimating population quantities for binomial distributions below, in the body of the text and in examples. For now… Recall: the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If \\(E[\\hat{\\theta}-\\theta] = 0\\), then the estimator \\(\\hat{\\theta}\\) is said to be unbiased. Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). Here we will introduce another means by which to define an estimator. The minimum variance unbiased estimator (or MVUE) is the one that has the smallest variance among all unbiased estimators of \\(\\theta\\). The reader’s first thought might be “well, why didn’t we use this estimator in the first place…after all, the MLE is not guaranteed to yield an unbiased estimator, so why have we put off discussing the MVUE?” The primary reasons are that MVUEs are sometimes not definable (i.e., we can reach insurmountable roadblocks when trying to derive them), and unlike MLEs, they do not exhibit the invariance property. (For instance, if \\(\\hat{\\theta}_{MLE} = \\bar{X}\\), then \\(\\hat{\\theta^2}_{MLE} = \\bar{X}^2\\), but if \\(\\hat{\\theta}_{MVUE} = \\bar{X}\\), it is not necessarily the case that \\(\\hat{\\theta^2}_{MVUE} = \\bar{X}^2\\).) However, we should always at least try to define the MVUE, because if we can, it will be at least equal the performance of, if not do better than, the MLE, in terms of bias and/or variance. There are two steps to carry out when deriving the MVUE: determining a sufficient statistic for \\(\\theta\\); and correcting any bias that is observed when we utilize that sufficient statistic as our initial estimator. A sufficient statistic for a parameter \\(\\theta\\) captures all information about \\(\\theta\\) contained in the sample. The sufficiency principle holds that if, e.g., \\(Y\\) is a sufficient statistic for \\(\\theta\\), and we collect two datasets \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) such that \\(Y(\\mathbf{U}) = Y(\\mathbf{V})\\), then the inferences we make about \\(\\theta\\) given that we observe \\(\\mathbf{U}\\) will be exactly the same as those we would make if we were to observe \\(\\mathbf{V}\\) instead. Using any statistic beyond a sufficient statistic will not lead to improved inferences about \\(\\theta\\). This means that, for instance, combining \\(\\bar{X}\\) (a sufficient statistic for the normal mean \\(\\mu\\) when the variance is known) with, say, the sample median will not reduce the length of confidence intervals for \\(\\mu\\) or change the power of hypothesis tests about \\(\\mu\\), relative to what we would derive using \\(\\bar{X}\\) alone. Note that utilizing the sufficiency principle is but one way by which statisticians can attempt data reduction prior to inference; two others, which are beyond the scope of this book, are the likelihood and equivalence principles. For a deeper treatment of sufficiency and of data reduction than we provide here, the interested reader should consult Chapter 6 of Casella and Berger (2002). A statistic \\(Y(\\mathbf{X})\\) is a sufficient statistic for \\(\\theta\\) if the ratio \\[\\begin{align*} \\frac{f_X(\\mathbf{x} \\vert \\theta)}{f_Y(y \\vert \\theta)} \\mathrel{{iid}\\rightarrow} \\frac{\\prod_{i=1}^n f_X(x_i \\vert \\theta)}{f_Y(y \\vert \\theta)} \\,, \\end{align*}\\] where \\(f_Y(y \\vert \\theta)\\) is the probability density function of the sampling distribution for \\(Y\\), is constant as a function of \\(\\theta\\). We note that by this definition, \\(\\mathbf{X}\\) itself comprises a sufficient statistic for \\(\\theta\\), as does the full set of order statistics \\(\\{X_{(1)},\\ldots,X_{(n)}\\}\\). The relevant questions are: can we define a sufficient statistic that actually reduces the data? and if so, how can we identify it? Let’s answer the second question first. The simplest way by which to identify a sufficient statistic is to write down the likelihood function and then to factorize it into two separate functions, one of which depends only on the observed data and the other of which depends on both the observed data and the parameter of interest: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = g(\\mathbf{x},\\theta) \\cdot h(\\mathbf{x}) \\,. \\] This is the so-called factorization criterion. In the expression \\(g(\\mathbf{x},\\theta)\\), the data will appear within, e.g., a summation (e.g., \\(\\sum_{i=1}^n x_i\\)) or a product (e.g., \\(\\prod_{i=1}^n x_i\\)), and we would identify that summation or product as a sufficient statistic for \\(\\theta\\). For instance, for the binomial distribution, the factorized likelihood (given \\(n\\) iid data) is \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n \\binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \\underbrace{\\left[ \\prod_{i=1}^n \\binom{k}{x_i} \\right]}_{h(\\mathbf{x})} \\underbrace{p^{\\sum_{i=1}^n x_i} (1-p)^{nk-\\sum_{i=1}^n x_i}}_{g(\\sum_{i=1}^n x_i,p)} \\,. \\] By inspecting the function \\(g(\\mathbf{x},p)\\), we immediately determine that a sufficient statistic for \\(p\\) is \\(Y = \\sum_{i=1}^n X_i\\). We say “a” sufficient statistic because sufficient statistics are not unique: any one-to-one function of a sufficient statistic is also a sufficient statistic. For instance, if \\(Y = \\sum_{i=1}^n X_i\\) is a sufficient statistic for \\(\\theta\\), so is \\(Y = \\bar{X}\\), etc. Now we return to the first question above: can we define a sufficient statistic that actually reduces the data? The answer is “not always.” In fact, across all possible distributions, it is relatively rare that we can do this. The Pitman-Koopman-Darmois theorem holds that, among families of distributions that do not have domain-specifying parameters, only the exponential family of distributions have sufficient statistics whose dimension does not increase as the sample size increases. (This theorem motivates, in large part, our introduction of the exponential family in Chapter 2.) Recall that the exponential family of distributions comprises a set of distributions whose probability mass or density functions can be written as \\[\\begin{align*} h(x) \\exp\\left( \\eta(\\theta) T(x) - A(\\theta) \\right) \\,. \\end{align*}\\] (Here we assume that the distribution has just one free parameter \\(\\theta\\).) In this expression, \\(T(x)\\) is the sufficient statistic; as seen in a previous example, for the binomial distribution \\(T(X) = X\\). If we collect \\(n\\) iid data, then the sufficient statistic is \\(T(\\mathbf{X}) = \\sum_{i=1}^n X_i\\), and as we can see it is still a single number\\(-\\)i.e., it has a dimension of one\\(-\\)regardless of the value of \\(n\\). Now, restricting ourselves to exponential family distributions in inferential situations would appear to limit the practical utility of the sufficiency principle…except it is the case that many (if not most) of the distributions that we utilize in inference are indeed exponential-family distributions, including all the ones that we focus on in this book. (We note here for completeness that once we identify a sufficient statistic, we are technically required to demonstrate that it is both minimally sufficient and complete, as it needs to be both so that we can use it, e.g., to determine the minimum variance unbiased estimator. It suffices to say here that the sufficient statistics that we identify for exponential family distributions via likelihood factorization are minimally sufficient and complete. See Chapter 7 for more details on minimal sufficiency and completeness.) If \\(Y\\) is a sufficient statistic for \\(\\theta\\), and there is a function \\(h(Y)\\) that is an unbiased estimator for \\(\\theta\\) and that depends on the data only through \\(Y\\), then \\(h(Y)\\) is the MVUE for \\(\\theta\\). (Recall from above that one-to-one functions of sufficient statistics are themselves sufficient statistics, hence \\(h(Y)\\) is sufficient for \\(\\theta\\).) Here, given \\(Y = \\sum_{i=1}^n X_i\\), we need to find a function \\(h(\\cdot)\\) such that \\(E[h(Y)] = p\\). Earlier in this chapter, we determined that the distribution for the sum of iid binomial random variables is Binomial(\\(nk\\),\\(p\\)), and thus we know that this distribution has expected value \\(nkp\\). Thus \\[ E\\left[Y\\right] = nkp ~\\implies~ E\\left[\\frac{Y}{nk}\\right] = p ~\\implies~ h(Y) = \\frac{Y}{nk} = \\frac{\\bar{X}}{k} ~\\mbox{is the MVUE for}~p \\,. \\] The variance of \\(\\hat{p}\\) is \\[ V[\\hat{p}] = V\\left[\\frac{\\bar{X}}{k}\\right] = \\frac{1}{k^2}V[\\bar{X}] = \\frac{1}{k^2} \\frac{V[X]}{n} = \\frac{1}{k^2}\\frac{kp(1-p)}{n} = \\frac{p(1-p)}{nk} \\,. \\] We know that this variance abides by the restriction \\[ V[\\hat{p}] \\geq \\frac{1}{nI(p)} = -\\frac{1}{nE\\left[\\frac{d^2}{dp^2} \\log p_X(X \\vert p) \\right]} \\,. \\] But is it equivalent to the lower bound for unbiased estimators, the CRLB? (Note that in particular situations, the MVUE may have a variance larger than the CRLB; when this is the case, unbiased estimators that achieve the CRLB simply do not exist.) For the binomial distribution, \\[\\begin{align*} p_{X}(x) &amp;= \\binom{k}{x} p^{x} (1-p)^{k-x} \\\\ \\log p_{X}(x) &amp;= \\log \\binom{k}{x} + x \\log p + (k-x) \\log (1-p) \\\\ \\frac{d}{dp} \\log p_{X}(x) &amp;= 0 + \\frac{x}{p} - \\frac{k-x}{(1-p)} \\\\ \\frac{d^2}{dp^2} \\log p_{X}(x) &amp;= -\\frac{x}{p^2} - \\frac{k-x}{(1-p)^2} \\\\ E\\left[\\frac{d^2}{dp^2} \\log p_{X}(X)\\right] &amp;= -\\frac{1}{p^2}E[X] - \\frac{1}{(1-p)^2}E[k-X] \\\\ &amp;= -\\frac{kp}{p^2}-\\frac{k-kp}{(1-p)^2} \\\\ &amp;= -\\frac{k}{p}-\\frac{k}{1-p} = -\\frac{k}{p(1-p)} \\,. \\end{align*}\\] The lower bound on the variance is thus \\(p(1-p)/(nk)\\), and so the MVUE does achieve the CRLB. We cannot define a better unbiased estimator for \\(p\\) than \\(\\bar{X}/k\\). 3.6.1 The MLE for the Binomial Success Probability Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Above, we determined that the likelihood function for \\(n\\) iid binomial random variables \\(\\{X_1,\\ldots,X_n\\}\\) is \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\left[\\prod_{i=1}^n \\binom{k}{x_i} \\right] p^{\\sum_{i=1}^n x_i} (1-p)^{nk-\\sum_{i=1}^n x_i} \\,. \\] Recall that the value \\(\\hat{p}_{MLE}\\) that maximizes \\(\\mathcal{L}(p \\vert x)\\) also maximizes \\(\\ell(p \\vert x) = \\log \\mathcal{L}(p \\vert x)\\), which is considerably easier to work with: \\[\\begin{align*} \\ell(p \\vert \\mathbf{x}) &amp;= \\left(\\sum_{i=1}^n x_i\\right) \\log p + \\left(nk - \\sum_{i=1}^n x_i\\right) \\log (1-p) \\\\ \\frac{d}{dp} \\ell(p \\vert \\mathbf{x}) &amp;= \\frac{1}{p} \\sum_{i=1}^n x_i - \\frac{1}{1-p} \\left(nk - \\sum_{i=1}^n x_i\\right) = 0 \\,. \\end{align*}\\] (Here, we drop the binomial coefficient, which does not depend on \\(p\\) and thus differentiates to zero.) After rearranging terms, we find that \\[ p = \\frac{1}{nk}\\sum_{i=1}^n x_i ~\\implies~ \\hat{p}_{MLE} = \\frac{\\bar{X}}{k} \\,. \\] The MLE matches the MVUE, thus we know that the MLE is unbiased and we know that it achieves the CRLB. A useful property of MLEs is the invariance property, whereby the MLE for a function of \\(\\theta\\) is given by applying the same function to the MLE itself. Thus the MLE for the population mean \\(E[X] = \\mu = kp\\) is \\(\\hat{\\mu}_{MLE} = \\bar{X}\\); and the MLE for the population variance \\(V[X] = \\sigma^2 = kp(1-p)\\) is \\(\\widehat{\\sigma^2}_{MLE} = \\bar{X}(1-\\bar{X}/k)\\). Last, note that asymptotically, \\(\\hat{p}_{MLE}\\) converges in distribution to a normal random variable: \\[ \\hat{p}_{MLE} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(p,\\frac{1}{nI(p)} = \\frac{p(1-p)}{nk}\\right) \\,. \\] 3.6.2 Sufficient Statistics for the Normal Distribution If we have \\(n\\) iid data drawn from a normal distribution with unknown mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\), then the factorized likelihood is \\[ \\mathcal{L}(\\mu,\\sigma^2 \\vert \\mathbf{x}) = \\underbrace{(2 \\pi \\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right)\\exp\\left(\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^n x_i\\right)\\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)}_{g(\\sum x_i^2, \\sum x_i,\\mu,\\sigma)} \\cdot \\underbrace{1}_{h(\\mathbf{x})} \\,. \\] Here, we identify \\(\\sum x_i^2\\) and \\(\\sum x_i\\) as joint sufficient statistics: we need two pieces of information to jointly estimate \\(\\mu\\) and \\(\\sigma^2\\). (To be clear: it is not necessarily the case that one of the parameters matches up to one of the statistics…rather, the two statistics are jointly sufficient for estimation.) We thus cannot proceed further to define an MVUE for \\(\\mu\\) or for \\(\\sigma^2\\), without knowing the joint bivariate probability density function for \\(Y_1 = \\sum_{i=1}^n X_i^2\\) and \\(Y_2 = \\sum_{i=1}^n X_i\\). (Note that we can proceed if we happen to know either \\(\\mu\\) or \\(\\sigma^2\\); if one of these values is fixed, then there will only be one sufficient statistic and we can determine the MVUE for the other, freely varying parameter.) 3.6.3 The Sufficiency Principle: Examples of When We Cannot Reduce Data As stated above, it is often impossible to reduce data to, e.g., a single-number summary. The following examples, utilizing distributions that are not exponential-family distributions, illustrate this. A Laplace distribution with scale parameter 1 has the probability density function \\[\\begin{align*} f_X(x \\vert \\theta) &amp;= \\frac12 e^{-\\vert x - \\theta \\vert} \\,, \\end{align*}\\] where \\(x \\in (-\\infty,\\infty)\\) and \\(\\theta \\in (-\\infty,\\infty)\\). If we observe \\(n\\) iid data, then likelihood factorization yields \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\frac{1}{2^n} e^{-\\sum_{i=1}^n \\vert x_i - \\theta \\vert} \\,. \\end{align*}\\] \\(Y = \\sum_{i=1}^n \\vert X_i - \\theta \\vert\\) is not a sufficient statistic as it contains the (unknown) parameter: a sufficient statistic for \\(\\theta\\) cannot contain \\(\\theta\\) itself! We cannot isolate a function of \\(\\mathbf{X}\\) alone, and thus no data reduction is possible. A Cauchy distribution with scale parameter 1 has the probability density function \\[\\begin{align*} f_X(x \\vert \\theta) = \\frac{1}{\\pi (x-\\theta)^2} \\,, \\end{align*}\\] where \\(x \\in (-\\infty,\\infty)\\) and \\(\\theta \\in (-\\infty,\\infty)\\). If we observe \\(n\\) iid data, then likelihood factorization yields \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\frac{1}{\\pi^n} \\frac{1}{\\prod_{i=1}^n (x_i-\\theta)^2} \\,. \\end{align*}\\] Again, we cannot isolate a function of the \\(X_i\\)’s alone: no data reduction is possible. 3.6.4 The MVUE for the Exponential Mean The exponential distribution is \\[ f_X(x) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right) \\,, \\] where \\(x \\geq 0\\) and \\(\\theta &gt; 0\\), and where \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2\\). Let’s assume that we have \\(n\\) iid data drawn from this distribution. Can we define the MVUE for \\(\\theta\\)? For \\(\\theta^2\\)? The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\frac{1}{\\theta^n}\\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i \\right) = h(\\mathbf{x}) \\cdot g(\\theta,\\mathbf{x}) \\,. \\] Here, there are no terms that are functions of only the data, so \\(h(\\mathbf{x}) = 1\\) and a sufficient statistic is \\(Y = \\sum_{i=1}^n X_i\\). We compute the expected value of \\(Y\\): \\[ E[Y] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n \\theta = n\\theta \\,. \\] The expected value of \\(Y\\) is not \\(\\theta\\), so \\(Y\\) is not unbiased…but we can see immediately that \\(E[Y/n] = \\theta\\), so that \\(Y/n\\) is unbiased. Thus the MVUE for \\(\\theta\\) is thus \\(\\hat{\\theta}_{MVUE} = Y/n = \\bar{X}\\). Note that the MVUE does not possess the invariance property…it is not necessarily the case that \\(\\hat{\\theta^2}_{MVUE} = \\bar{X}^2\\). Let’s propose a function of \\(Y\\) and see if we can use that to define \\(\\hat{\\theta^2}_{MVUE}\\): \\(h(Y) = Y^2/n^2 = \\bar{X}^2\\). (To be clear, we are simply proposing a function and seeing if it helps us define what we are looking for. It might not. If not, we can try again with another function of \\(Y\\).) Utilizing what we know about the sample mean, we can write down that \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{V[X]}{n} + (E[X])^2 = \\frac{\\theta^2}{n}+\\theta^2 = \\theta^2\\left(\\frac{1}{n} + 1\\right) \\,. \\] So \\(\\bar{X}^2\\) itself is not an unbiased estimator of \\(\\theta^2\\)…but we can see that \\(\\bar{X}^2/(1/n+1)\\) is. Hence \\[ \\hat{\\theta^2}_{MVUE} = \\frac{\\bar{X}^2}{\\left(\\frac{1}{n}+1\\right)} \\,. \\] 3.6.5 The MVUE for the Geometric Distribution Recall that the geometric distribution is a negative binomial distribution with \\(s = 1\\): \\[\\begin{align*} p_X(x) = \\binom{x + s - 1}{x} p^s (1-p)^x ~ \\rightarrow ~ p(1-p)^x \\,, \\end{align*}\\] where \\(p \\in (0,1]\\) and where \\(E[X] = (1-p)/p\\) and \\(V[X] = (1-p)/p^2\\). Let’s assume that we sample \\(n\\) iid data from this distribution. Can we define the MVUE for \\(p\\)? For \\(1/p\\)? The likelihood function is \\[\\begin{align*} \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n p_X(x_i \\vert p) = p^n (1-p)^{\\sum_{i=1}^n x_i} = 1 \\cdot g(p,\\mathbf{x}) \\,. \\end{align*}\\] We identify a sufficient statistic as \\(Y = \\sum_{i=1}^n X_i\\); the expected value of \\(Y\\) is \\[\\begin{align*} E[Y] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n \\frac{1-p}{p} = n\\left(\\frac{1}{p}-1\\right) \\,. \\end{align*}\\] We cannot “debias” this expression to find the MVUE for \\(p\\): \\[\\begin{align*} E\\left[\\frac{Y}{n}+1\\right] = \\frac{1}{p} \\,. \\end{align*}\\] Specifically, \\[\\begin{align*} E\\left[\\frac{1}{Y/n+1}\\right] \\neq 1/E\\left[\\frac{Y}{n}+1\\right] = p \\,. \\end{align*}\\] However, we did determine the MVUE for \\(1/p\\): \\(Y/n+1 = \\bar{X}+1\\). But as there is no invariance property for the MVUE, we cannot use this expression to write down an MVUE for \\(p\\). 3.7 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}\\) by solving the following equation: \\[ F_Y(y_{\\rm obs} \\vert \\theta) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\), \\(y_{\\rm obs}\\) is the observed value of the statistic, and \\(q\\) is an appropriate quantile value that is determined using the confidence interval reference table introduced in section 16 of Chapter 1. A concept that we have not explicitly discussed up until now is the “duality” between confidence intervals and hypothesis tests: as they are mathematically related, one can, in theory, “invert” hypothesis tests to derive confidence intervals and vice-versa. We illustrate this duality in Figure 3.7, in which we assume that the distribution from which we are to sample data is a normal distribution with mean \\(\\mu\\) and known variance \\(\\sigma^2\\). The parallel green lines in the figure represent lower and upper rejection region boundaries as a function of (the null hypothesis value) \\(\\mu\\). Let’s say that we pick a specific value of \\(\\mu\\), say \\(\\mu = \\mu_o = 1\\), and draw a vertical line at that coordinate. The part of that line that lies between the parallel green lines (indicated in solid red) indicates the range of observed statistic values \\(y_{\\rm obs}\\) for which we would fail to reject the null, and the parts above and below the parallel lines (shown in dashed red) would indicate \\(y_{\\rm obs}\\) values for which we would reject the null. (These are the acceptance and rejection regions, respectively; “acceptance region” is a term that we have not used up until now, but it simply denotes the range of statistic values for which we would fail to reject a null hypothesis, when it is indeed correct.) Confidence intervals, on the other hand, are ranges of values of \\(\\mu\\) for which a given value of \\(y_{\\rm obs}\\) lies in the acceptance region. (For instance, the dashed horizontal blue line in the figure shows the range of values associated with a two-sided confidence interval for \\(\\mu\\) given \\(y_{\\rm obs} = 0.5\\).) We can see that for a given value of \\(\\mu\\), if we were to sample a value of \\(y_{\\rm obs}\\) between the two green lines (with probability \\(1-\\alpha\\)), then the associated confidence interval will overlap \\(\\mu\\), and if we sample a value of \\(y_{\\rm obs}\\) outside the green lines (with probability \\(\\alpha\\)), the confidence interval will not overlap \\(\\mu\\). The confidence interval coverage is thus exactly \\(100(1-\\alpha)\\) percent. Figure 3.7: An illustration of the relationship between hypothesis testing and confidence interval estimation for a continuous sampling distribution (here, a normal distribution with known variance). The two parallel green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value \\(\\mu\\). Assume \\(\\mu = 1\\): the dashed red lines indicate the values of \\(y_{\\rm obs}\\) in the rejection region, while the solid red line indicates the values of \\(y_{\\rm obs}\\) in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of \\(y_{\\rm obs}\\). Given \\(\\mu\\), the probability of sampling a statistic in the acceptance region is exactly \\(1-\\alpha\\), and thus the coverage of confidence intervals will also be exactly \\(1-\\alpha\\). When we work with discrete sampling distributions, the overall picture is similar, but we sometimes need to make what we will dub “discreteness corrections” so that the codes we use generate the correct results. Figure 3.8 is an analogue to Figure 3.7 in which we illustrate the relationship between confidence intervals and hypothesis tests for a situation in which we conduct \\(k = 10\\) binomial trials. Because the statistic is discretely valued, the acceptance-region boundaries are step functions, but this is not the only change introduced with discrete distributions: In the continuous case, the probability of sampling a datum that lies within the acceptance region for a given value of \\(\\theta\\) is exactly \\(1 - \\alpha\\). In the discrete case, that probability is \\(\\geq 1 - \\alpha\\). (And it will rarely, if ever, be the case that the probability masses within the acceptance region will sum to exactly \\(1 - \\alpha\\)…although that sum will trend towards \\(1 - \\alpha\\) as the sample size and/or the number of trials increases. This motivates our use of the word “level,” as in “we conduct a level-\\(\\alpha\\) test,” rather than “size.” For a size-\\(\\alpha\\) test, the probability of sampling a statistic value in the acceptance region is, by definition, exactly \\(1-\\alpha\\), while for a level-\\(\\alpha\\) test, it is \\(\\geq 1 - \\alpha\\).) Because the probability of sampling a datum in the acceptance region is equal to the confidence interval coverage, the coverage will also be \\(\\geq 1 - \\alpha\\). Figure 3.8: An illustration of the relationship between hypothesis testing and confidence interval estimation for a discrete sampling distribution (here, a binomial distribution with \\(k=10\\)). The two green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value \\(p\\). Assume \\(p = 0.45\\): the red dots indicates the values of \\(y_{\\rm obs}\\) in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of \\(y_{\\rm obs}\\). Given \\(p\\), the probability of sampling a statistic in the acceptance region is generally greater than \\(1-\\alpha\\), and thus the coverage of confidence intervals, which has the same value, will also be generally greater than \\(1-\\alpha\\). The blue dashed line in Figure 3.8 is an example of a confidence interval (specifically for \\(y_{\\rm obs} = 4\\)). In order to construct an interval like this, we would use a uniroot()-style code as we have before, but with the following changes. If we are determining a lower bound \\(\\hat{\\theta}_L\\) in a situation where \\(E[Y]\\) increases with \\(\\theta\\), we need to replace \\(y_{\\rm obs}\\) in the input to uniroot() with the next smaller value in the distribution’s domain (e.g., \\(y_{\\rm obs} - 1\\) for a binomially distributed statistic). If we are determining an upper bound \\(\\hat{\\theta}_U\\) in a situation where \\(E[Y]\\) decreases with \\(\\theta\\), we need to replace \\(y_{\\rm obs}\\) in the input to uniroot() with the next smaller value in the distribution’s domain (e.g., \\(y_{\\rm obs} - 1\\) for a negative binomially distributed statistic). We note that for the specific case of the binomial distribution, the confidence intervals that we construct are dubbed exact, or Clopper-Pearson, intervals. Because binomial distributions have historically been difficult to work with analytically, a number of algorithms have been developed through the years for constructing approximate confidence intervals for binomial probabilities. (See, e.g., this Wikipedia page for examples.) It is our opinion that there is no reason to utilize any of these algorithms when one can compute exact intervals, particularly since the coverages of exact intervals are easily derived for any given value \\(p\\). However, for completeness, we illustrate how one would construct the most-commonly used approximating interval, the Wald interval, in an example below. 3.7.1 Confidence Interval for the Binomial Success Probability Assume that we sample \\(n\\) iid data from a binomial distribution with number of trials \\(k\\) and probability \\(p\\). Then, as shown above, \\(Y = \\sum_{i=1}^n X_i \\sim\\) Binom(\\(nk,p\\)); our observed test statistic is \\(y_{\\rm obs} = \\sum_{i=1}^n x_i\\). For this statistic, \\(E[Y] = nkp\\) increases with \\(p\\), so \\(q = 1-\\alpha/2\\) maps to the lower bound, while \\(q = \\alpha/2\\) maps to the upper bound. set.seed(101) alpha &lt;- 0.05 n &lt;- 12 k &lt;- 5 p &lt;- 0.4 X &lt;- rbinom(n,size=k,prob=p) f &lt;- function(p,y.obs,n,k,q) { pbinom(y.obs,size=n*k,prob=p)-q } uniroot(f,interval=c(0,1),y.obs=sum(X)-1,n,k,1-alpha/2)$root # note correction ## [1] 0.2459379 uniroot(f,interval=c(0,1),y.obs=sum(X),n,k,alpha/2)$root ## [1] 0.5010387 We find that the interval is \\([\\hat{p}_L,\\hat{p}_U] = [0.246,0.501]\\), which overlaps the true value of 0.4. (See Figure 3.9.) Note that, unlike in Chapter 2, the interval over which we search for the root is [0,1], which is the range of possible values for \\(p\\). We can compute the coverage of this interval following the prescription given above: y.rr.lo &lt;- qbinom(alpha/2,n*k,p) y.rr.hi &lt;- qbinom(1-alpha/2,n*k,p) sum(dbinom(y.rr.lo:y.rr.hi,n*k,p)) ## [1] 0.9646363 Due to the discreteness of the binomial distribution, the true coverage of our two-sided intervals, in this particular circumstance, is 96.5%, not 95%. Figure 3.9: Probability mass functions for binomial distributions for which \\(n \\cdot k= 12 \\cdot 5 = 60\\) and (left) \\(p=0.246\\) and (right) \\(p=0.501\\). We observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 22\\) successes and we want to construct a 95% confidence interval. \\(p=0.246\\) is the smallest value of \\(p\\) such that \\(F_Y^{-1}(0.975) = 22\\), while \\(p=0.501\\) is the largest value of \\(p\\) such that \\(F_Y^{-1}(0.025) = 22\\). 3.7.2 Confidence Interval for the Negative Binomial Success Probability Let’s assume that we have performed \\(n = 10\\) separate negative binomial trials, each with a target number of successes \\(s\\), and recorded the number of failures \\(X_1,\\ldots,X_{n}\\) for each. Further, assume that the probability of success is \\(p\\). Below we will show how to compute the confidence interval for \\(p\\), but before we start, we recall that \\(Y = \\sum_{i=1}^n X_i\\) is a negative binomially distributed random variable for \\(ns\\) successes and probability of success \\(p\\). Here, \\(E[Y] = s(1-p)/p\\)…as \\(p\\) increases, \\(E[Y]\\) decreases. Thus when we adapt the confidence interval code we use for binomially distributed data, we need to switch the mapping of \\(q = 1-\\alpha/2\\) and \\(q = \\alpha/2\\) to the upper and lower bounds, respectively. set.seed(101) alpha &lt;- 0.05 n &lt;- 12 s &lt;- 5 p &lt;- 0.4 X &lt;- rnbinom(n,size=s,prob=p) f &lt;- function(p,y.obs,n,s,q) { pnbinom(y.obs,size=n*s,prob=p)-q } uniroot(f,interval=c(0.0001,1),y.obs=sum(X),n,s,alpha/2)$root ## [1] 0.3255305 uniroot(f,interval=c(0.0001,1),y.obs=sum(X)-1,n,s,1-alpha/2)$root # note correction ## [1] 0.4853274 The confidence interval is \\([\\hat{p}_L,\\hat{p}_U] = [0.326,0.485]\\), which overlaps the true value 0.4. We note that in the code, we change the lower bound on the interval from 0 (in the binomial case) to 0.0001 (something suitably small but non-zero): a probability of success of 0 maps to an infinite number of failures, which R cannot tolerate! We can compute the coverage of this interval following the prescription given above: y.rr.lo &lt;- qnbinom(1-alpha/2,n*s,p) y.rr.hi &lt;- qnbinom(alpha/2,n*s,p) sum(dnbinom(y.rr.lo:y.rr.hi,n*s,p)) ## [1] 0.9510741 Due to the discreteness of the binomial distribution, the true coverage of our two-sided intervals is 95.1%, not 95%. Figure 3.10: Probability mass functions for negative binomial distributions for which \\(n \\cdot s = 12 \\cdot 5 = 60\\) and (left) \\(p=0.326\\) and (right) \\(p=0.485\\). We observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 88\\) failures and we want to construct a 95% confidence interval. \\(p=0.326\\) is the smallest value of \\(p\\) such that \\(F_Y^{-1}(0.025) = 88\\), while \\(p=0.485\\) is the largest value of \\(p\\) such that \\(F_Y^{-1}(0.975) = 88\\). 3.7.3 Wald Interval for the Binomial Success Probability Let’s assume that we have sampled \\(n\\) iid binomial variables with number of trials \\(k\\) and probability of success \\(p\\). When \\(k\\) is sufficiently large and \\(p\\) is sufficiently far from 0 or 1, we can assume that \\(\\bar{X}\\) has a distribution whose shape is approximately that of a normal distribution, with mean \\(E[\\bar{X}] = kp\\) and with variance and standard error \\[ V[\\bar{X}] = \\frac{kp(1-p)}{n} ~~~ \\mbox{and} ~~~ se(\\bar{X}) = \\sqrt{V[\\bar{X}]} = \\sqrt{\\frac{kp(1-p)}{n}} \\,. \\] Furthermore, we can assume that \\(\\hat{p} = \\bar{X}/k\\) is approximately normally distributed, with mean \\(p\\), variance \\(V[\\hat{p}] = V[\\bar{X}]/k^2 = p(1-p)/nk\\), and standard error \\(\\sqrt{p(1-p)/nk}\\). Given this information, it is simple to express, e.g., an approximate two-sided \\(100(1-\\alpha)\\)% confidence interval for \\(p\\): \\[ \\hat{p} \\pm z_{1-\\alpha/2} se(\\hat{p}) ~~ \\Rightarrow ~~ \\left[ \\hat{p} - z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{nk}} \\, , \\, \\hat{p} + z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{nk}} \\right] \\,, \\] where \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\). However, we don’t actually know the true value of \\(p\\)…so we plug in \\(p = \\hat{p}\\). This is the so-called Wald interval that is typically provided to students in introductory statistics courses, although it is typically provided assuming \\(n = 1\\) and assuming that \\(\\alpha = 0.05\\): \\[ \\left[ \\hat{p} - 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{k}} \\, , \\, \\hat{p} + 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{k}} \\right] \\,, \\] where in this case \\(\\hat{p} = X/k\\). What is the Wald interval for the situation provided in the first example above? set.seed(101) alpha &lt;- 0.05 n &lt;- 12 k &lt;- 5 p &lt;- 0.4 X &lt;- rbinom(n,size=k,prob=p) z &lt;- qnorm(1-alpha/2) p.hat &lt;- sum(X)/(n*k) p.hat - z*sqrt(p.hat*(1-p.hat)/(n*k)) ## [1] 0.2447328 p.hat + z*sqrt(p.hat*(1-p.hat)/(n*k)) ## [1] 0.4886005 We can compare these values to those generated in the first example: \\([0.236,0.501]\\). Here, the interval is smaller relative to the Clopper-Pearson interval, and thus the coverage for \\(p = 0.4\\) (which we would have to estimate via simulation) will be smaller as well. 3.8 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-tail test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bound given by \\(y_{RR}\\), which is a solution to the equation \\[ F_Y(y_{RR} \\vert \\theta_o) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\) and \\(q\\) is an appropriate quantile value that is determined using the hypothesis test reference table introduced in section 17 of Chapter 1. Note that the hypothesis test framework only allows us to make a decision about a null hypothesis; nothing is proven. In the previous chapter, we utilized \\(\\bar{X}\\) when testing hypotheses about the normal mean \\(\\mu\\). This is a principled choice for a test statistic\\(-\\)after all, \\(\\bar{X}\\) is the MLE for \\(\\mu-\\)but we do not yet know whether or not we can choose a better one. Can we differentiate hypotheses more easily if we use a test statistic other than \\(\\bar{X}\\)? To help answer this question, we now introduce a method for defining the most powerful test of a simple null hypothesis versus a simple alternative hypothesis: \\[ H_o : \\theta = \\theta_o ~~\\mbox{and}~~ H_a : \\theta = \\theta_a \\,. \\] Note that the word “simple” has a precise meaning here: it means that when we set \\(\\theta\\) to a particular value, we are completely fixing the shape and location of the pmf or pdf from which data are sampled. If, for instance, we are dealing with a normal distribution with unknown variance \\(\\sigma^2\\), the hypothesis \\(\\mu = \\mu_o\\) would not be simple, since the width of the pdf can still vary: the shape is not completely fixed. (The hypothesis \\(\\mu = \\mu_o\\) with variance unknown is dubbed a composite hypothesis. We will examine how to work with composite hypotheses in the next chapter.) For a given test level \\(\\alpha\\), the Neyman-Pearson lemma states that the test that maximizes the power has a rejection region of the form \\[ \\lambda_{NP} = \\frac{\\mathcal{L}(\\theta_o \\vert \\mathbf{x})}{\\mathcal{L}(\\theta_a \\vert \\mathbf{x})} &lt; c(\\alpha) \\,, \\] where \\(c\\) is a constant whose value depends on \\(\\alpha\\) that we have to determine. While this formulation initially appears straightforward, it is in fact not necessarily clear how to derive \\(c(\\alpha)\\). However: when we work with distributions that belong to the exponential family, we don’t need to! The NP lemma utilizes the likelihood function, so in this situation it is implicitly telling us that the best statistic for differentiating between two simple hypotheses is a sufficient statistic. We simply determine a sufficient statistic and and use its sampling distribution to define a hypothesis test via the procedure we have laid out in previous chapters. Full stop. (What if the distribution we are working with is not a member of the exponential family? We would need to determine the sampling distribution of \\(\\lambda_{NP}\\) itself, something most easily done via the use of simulations. We illustrate this in an example below.) For instance, when we draw \\(n\\) iid data from a binomial distribution, the sufficient statistic \\(\\sum_{i=1}^n X_i\\) has a known and easily utilized sampling distribution\\(-\\)namely, Binom(\\(nk,p\\))\\(-\\)while \\(\\bar{X} = (\\sum_{i=1}^n X_i)/n\\) does not. So we would utilize \\(Y = \\sum_{i=1}^n X_i\\). (Note that it ultimately doesn’t matter which function of a sufficient statistic we use: if we can carry out the math, we will end up with tests that have the same power and result in the same \\(p\\)-values. It would be very problematic if this wasn’t the case: we’d have to “fish around” to determine which function of a sufficient statistic would give us the best test, which clearly would not be a good situation to find ourselves in.) Let’s assume that we use \\(Y = \\sum_{i=1}^n X_i\\) to define, e.g., a lower-tail test \\(H_o : p = p_o\\) versus \\(H_a : p = p_a &lt; p_o\\). Since \\(E[Y] = nkp\\) increases with \\(p\\), we can go to the hypothesis test reference tables and write (in code) that u.rr &lt;- qbinom(1-alpha,size=k,prob=p.o) We see that our rejection region boundary depends on the value of \\(p_o\\), but not on the value of \\(p_a\\). This means that the test we define above is the most-powerful test regardless of the value \\(p_a &lt; p_o\\). We have thus constructed a uniformly most powerful (or UMP) test for disambiguating the simple hypotheses \\(H_o : p = p_o\\) and \\(H_a : p = p_a &lt; p_o\\). It is typically the case that when we use the NP lemma to define a most powerful test for \\(\\theta_o\\) versus \\(\\theta_a\\), we end up defining a UMP test as well. (That is because the families of distributions that we work with typically exhibit so-called monotone likelihood ratios [or MLRs]. For more information on MLRs and UMPs, the interested reader should look at the Karlin-Rubin theorem.) We note that we cannot use the NP lemma to construct most powerful two-tail hypothesis tests. When we construct a two-tail test, it is convention to define one rejection region boundary assuming \\(q = \\alpha/2\\) and the other assuming \\(q = 1 - \\alpha/2\\). But that is just convention; we could put \\(\\alpha/10\\) on “one side” and \\(1-9\\alpha/10\\) on the other, etc., and in general we cannot guarantee that any one way of splitting \\(\\alpha\\) will yield a more powerful test in a given situation than any other possible split. We conclude our present discussion of hypothesis tests with an overview of how working with a discrete sampling distribution affects the computation of \\(p\\)-values and test power. In the last section, we introduced the idea of a “discreteness correction”; for confidence interval calculations, this involved changing the observed statistic value to the next lower value in the domain of the sampling distribution when estimating a lower interval bound (with \\(E[Y]\\) increasing with \\(\\theta\\)) or when estimating an upper interval bound (with \\(E[Y]\\) decreasing with \\(\\theta\\)). (For the binomial and negative binomial distributions, this means changing \\(y_{\\rm obs}\\) to \\(y_{\\rm obs}-1\\).) What are the discreteness corrections that we need to make when performing hypothesis tests? First, there are none when computing rejection-region boundaries. Second, when computing \\(p\\)-values, we change the observed statistic value to the next lower value in the domain of the sampling distribution when performing upper-tail tests where \\(E[Y]\\) increases with \\(\\theta\\) and lower-tail tests where \\(E[Y]\\) decreases with \\(\\theta\\). Last, when computing test power, we change the derived rejection-region boundary to the next lower value in the domain of the sampling distribution when performing upper-tail tests where \\(E[Y]\\) decreases with \\(\\theta\\) and lower-tail tests where \\(E[Y]\\) increases with \\(\\theta\\). 3.8.1 UMP Test: Exponential Distribution Let’s suppose we sample \\(n = 3\\) iid data from the exponential distribution \\[ f_X(x) = \\frac{1}{\\theta} e^{-x/\\theta} \\,, \\] for \\(x \\geq 0\\) and \\(\\theta &gt; 0\\), and we wish to define the most powerful test of the simple hypotheses \\(H_o : \\theta_o = 2\\) and \\(H_a : \\theta_a = 1\\). We observe the values \\(\\mathbf{x}_{\\rm obs} = \\{0.215,1.131,2.064\\}\\), and we assume \\(\\alpha = 0.05\\). We begin by determining a sufficient statistic: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^3 \\frac{1}{\\theta} e^{-x_i/\\theta} = \\frac{1}{\\theta^3} e^{(\\sum_{i=1}^3 x_i)/\\theta} \\,. \\] We identify \\(Y = \\sum_{i=1}^3 X_i\\) as a sufficient statistic. The next question is whether we can determine the sampling distribution for \\(Y\\). The moment-generating function for each of the \\(X_i\\)’s is \\[ m_{X_i}(t) = (1 - \\theta t)^{-1} \\,, \\] and so the mgf for \\(Y\\) is \\[ m_Y(t) = \\prod_{i=1}^3 m_{X_i}(t) = (1 - \\theta t)^{-3} \\,, \\] We (might!) recognize this as the mgf for a Gamma(3,\\(\\theta\\)) distribution. (The gamma distribution will be officially introduced in Chapter 4.) So: we know the sampling distribution for \\(Y\\), and we can use it to define the most powerful test. To reiterate: the only thing that the NP lemma is doing for us here is guiding our selection of a test statistic. Beyond that, we construct the test using the framework we already learned in Chapters 1 and 2. Because \\(\\theta_a &lt; \\theta_o\\), we define a lower-tail test. And since \\(E[Y] = 3\\theta\\) increases with \\(\\theta\\), we utilize the formulae from the hypothesis test reference tables that are on the “yes” line. The rejection-region boundary is thus \\[ y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\theta_o) \\,, \\] which in code is x.obs &lt;- c(0.215,1.131,2.064) (y.obs &lt;- sum(x.obs)) ## [1] 3.41 alpha &lt;- 0.05 theta.o &lt;- 2 (y.rr &lt;- qgamma(alpha,shape=3,scale=theta.o)) ## [1] 1.635383 Our observed statistic is \\(y_{\\rm obs} = 3.410\\) and the rejection-region boundary is 1.635: we fail to reject the null and conclude that \\(\\theta_o = 2\\) is a plausible value of \\(\\theta\\). We note that because \\(y_{\\rm RR}\\) is not a function of \\(\\theta_a\\), we have not only defined the most powerful test, but we have also defined a uniformly most powerful test for all alternative hypotheses \\(\\theta_a &lt; \\theta_o\\). What is the \\(p\\)-value, and what is the power of the test if \\(\\theta = 1.5\\)? According to the hypothesis test reference tables, the \\(p\\)-value is \\[ p = F_Y(y_{\\rm obs} \\vert \\theta_o) \\,, \\] which in code is pgamma(y.obs,shape=3,scale=theta.o) ## [1] 0.2440973 The \\(p\\)-value is 0.244, which is greater than \\(\\alpha\\), as we expect. The test power is \\[ {\\rm power}(\\theta) = F_Y(y_{\\rm RR} \\vert \\theta) \\,, \\] which in code is theta &lt;- 1.5 pgamma(y.rr,shape=3,scale=theta) ## [1] 0.09762911 The power is 0.097…only 9.7% of the time will we reject the null hypothesis \\(\\theta_o = 2\\) when \\(\\theta\\) is actually 1.5. 3.8.2 UMP Test: Negative Binomial Distribution Let’s assume that we sample \\(n = 5\\) data from a negative binomial distribution \\[ p_X(x) = \\binom{x+s-1}{x} p^s (1-p)^x \\,, \\] with \\(x \\in \\{0,1,\\ldots,\\infty\\}\\) being the observed number of failures prior to observed the \\(s^{\\rm th}\\) success, \\(p \\in (0,1]\\), and \\(s = 3\\) successes. We wish to define the most powerful test of the simple hypotheses \\(H_o : p_o = 0.5\\) and \\(H_a : p_a = 0.25\\). We observe the values \\(\\mathbf{x}_{\\rm obs} = \\{5,3,10,12,4\\}\\), and we assume \\(\\alpha = 0.05\\). As in the last example, we begin by determining a sufficient statistic: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^5 \\binom{x_i+s-1}{x_i} p^s (1-p)^x_i \\propto \\prod_{i=1}^5 p^s (1-p)^x_i = p^{ns} (1-p)^{\\sum_{i=1}^5 x_i} \\,. \\] We identify \\(Y = \\sum_{i=1}^5 X_i\\) as a sufficient statistic. Earlier in the chapter, we determined that if the \\(X_i\\)’s are iid draws from a negative binomial distribution with parameters \\(p\\) and \\(s\\), then the sum is also negative binomially distributed, with parameters \\(p\\) and \\(ns\\). So: we know the sampling distribution for \\(Y\\), and we can use it to define the most powerful test. Because \\(p_a &lt; p_o\\), we will define a lower-tail test. And since \\(E[Y] = s(1-p)/p\\) decreases with \\(p\\), we will utilize the formulae from the hypothesis test reference tables that are on the “no” line (and we will reject the null hypothesis if \\(y_{\\rm obs} &gt; y_{\\rm RR}\\)). The rejection-region boundary is \\[ y_{\\rm RR} = F_Y^{-1}(1-\\alpha \\vert p_o) \\,, \\] which in code is x.obs &lt;- c(5,3,10,12,4) n &lt;- length(x.obs) (y.obs &lt;- sum(x.obs)) ## [1] 34 alpha &lt;- 0.05 p.o &lt;- 0.5 (y.rr &lt;- qnbinom(1-alpha,size=3*n,prob=p.o)) ## [1] 25 Our observed statistic is \\(y_{\\rm obs} = 34\\) and the rejection-region boundary is 25: we reject the null hypothesis and state that there is sufficient evidence to conclude that \\(p &lt; 0.5\\). We note that because \\(y_{\\rm RR}\\) is not a function of \\(p_a\\), we have not only defined the most powerful test, but we have also defined a uniformly most powerful test for all alternative hypotheses \\(p_a &lt; p_o\\). What is the \\(p\\)-value, and what is the power of the test if \\(p = 0.3\\)? According to the hypothesis test reference tables (and the description of discreteness corrections given above), the \\(p\\)-value is \\[ p = 1 - F_Y(y_{\\rm obs}-1 \\vert p_o) \\,, \\] which in code is 1 - pnbinom(y.obs-1,size=3*n,prob=p.o) ## [1] 0.002757601 The \\(p\\)-value is 0.0028, which is less than \\(\\alpha\\): we would decide to reject the null hypothesis. The test power (a calculation that in this instance does not require a discreteness correction) is \\[ {\\rm power}(\\theta) = 1 - F_Y(y_{\\rm RR} \\vert p) \\,, \\] which in code is p &lt;- 0.3 1 - pnbinom(y.rr,size=3*n,prob=p) ## [1] 0.8074482 We find that the power is 0.807…80.7% of the time will we reject the null hypothesis \\(p = 0.5\\) when \\(p\\) is actually 0.3 (and when \\(n = 5\\) and \\(s = 3\\)). 3.8.3 Defining a UMP Test for the Normal Population Mean In Chapter 2, we use the statistic \\(\\bar{X}\\) as the basis for testing hypotheses about the normal population mean, \\(\\mu\\). We justified this choice because, at the very least, \\(\\hat{\\mu}_{MLE} = \\bar{X}\\), so it is a principled choice. However, beyond that, we did not (and indeed could not) provide any further justification. Is \\(\\bar{X}\\) the basis of the UMP for \\(\\mu\\)? Recall that the NP lemma does not apply if there are two freely varying parameters. The NP lemma applies to simple hypotheses, where the hypotheses uniquely specify the population from which data are drawn. Here, even if we set \\(H_o: \\mu = \\mu_o\\) and \\(H_a: \\mu = \\mu_a\\), \\(\\sigma^2\\) can still freely vary. (So, technically, these hypotheses are composite hypotheses.) So to go further here, we must assume \\(\\sigma^2\\) is known. When \\(\\sigma^2\\) is known, we can factorize the likelihood as \\[ \\mathcal{L}(\\mu,\\sigma^2 \\vert \\mathbf{x}) = \\underbrace{\\exp\\left(\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^n x_i\\right)\\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)}_{g(\\sum x_i,\\mu)} \\cdot \\underbrace{(2 \\pi \\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right)}_{h(\\mathbf{x})} \\,, \\] and identify \\(Y = \\sum_{i=1}^n X_i\\) as a sufficient statistic. We know from using the method of moment-generating functions that the sum of \\(n\\) iid normal random variables is itself a normal random variable with mean \\(n\\mu\\) and variance \\(n\\sigma^2\\), so \\(Y \\sim \\mathcal{N}(n\\mu,n\\sigma^2)\\). Let’s assume that \\(\\mu_a &gt; \\mu_o\\), or, in other words, that we are performing an upper-tail test. Given that \\(E[Y] = n\\mu\\), we know that we are on the “yes” line of the hypothesis test reference tables, so the rejection region is \\[ Y &gt; y_{\\rm RR} = F_Y^{-1}(1-\\alpha,n\\mu_o,n\\sigma^2) \\,, \\] or, in code, qnorm(1-alpha,n*mu.o,n*sigma2) The rejection-region boundary does not depend on \\(\\mu_a\\), so we have defined a uniformly most powerful test of \\(\\mu = \\mu_o\\) versus \\(\\mu = \\mu_a &gt; \\mu_o\\). “But wait. What about \\(\\bar{X}\\)?” Recall that a function of a sufficient statistic is itself a sufficient statistic, so we could also use \\(Y&#39; = \\bar{X} = Y/n\\) as our statistic, particularly as we do know its sampling distribution: \\(\\mathcal{N}(\\mu,\\sigma^2/n)\\). Thus \\[ Y&#39; &gt; y_{\\rm RR}&#39; = F_Y^{-1}(1-\\alpha,\\mu_o,\\sigma^2/n) \\,, \\] or, in code, qnorm(1-alpha,mu.o,sigma2/n) “But…\\(y_{\\rm RR}\\) and \\(y_{\\rm RR}&#39;\\) do not have the same value!” This is fine: \\(Y\\) and \\(Y&#39;\\) don’t have the same value either. The key point is that when we compute, e.g., \\(p\\)-values given observed data, they will be the same in both cases. And the test power will be the same. We can use any function of a sufficient statistic to define our test; in practice, we will use any function of a sufficient statistic for which we know the sampling distribution. Here we know the distributions for both the sample sum and the sample mean; in other situations (like when we are working with binomially distributed data), we will not. Now, what if \\(\\sigma^2\\) is unknown? We discuss this possibility in the next chapter, when we introduce the likelihood ratio test. 3.8.4 Defining a Test That is Not Uniformly Most Powerful Let’s assume we draw one datum \\(X\\) from a normal distribution with mean \\(\\mu\\) and variance \\(\\mu^2\\), and we wish to test \\(H_o : \\mu = \\mu_o = 1\\) versus \\(H_a : \\mu = \\mu_a\\). Can we define a uniformly most powerful test of these hypotheses? To try to do this, we utilize the NP lemma. However, there is an immediate issue that arises: here, there are two sufficient statistics that are identified via factorization (\\(\\sum_{i=1}^n X_i\\) and \\(\\sum_{i=1}^n X_i^2\\)), but just one parameter (\\(\\mu\\)). So we cannot circumvent working directly with the likelihood ratio: \\[\\begin{align*} \\frac{\\mathcal{L}(\\theta_o \\vert \\mathbf{x})}{\\mathcal{L}(\\theta_a \\vert \\mathbf{x})} = \\frac{f_X(x \\vert \\theta_o)}{f_X(x \\vert \\theta_a)} = \\mu_a \\exp\\left(-\\frac{(x-1)^2}{2}+\\frac{(x-\\mu_a)^2}{2\\mu_a^2}\\right) \\,. \\end{align*}\\] To reject the null hypothesis, the ratio must be less than some constant \\(c(\\alpha)\\). First, we can divide both sides of the inequality by \\(\\mu_a\\) (which is a set constant), so that now we reject the null if \\[\\begin{align*} \\exp\\left(-\\frac{(x-1)^2}{2}+\\frac{(x-\\mu_a)^2}{2\\mu_a^2}\\right) &lt; c&#39;(\\alpha) \\,. \\end{align*}\\] We then take the natural logarithm of each side; we would reject the null if \\[\\begin{align*} -\\frac{(x-1)^2}{2}+\\frac{(x-\\mu_a)^2}{2\\mu_a^2} &lt; c&#39;&#39;(\\alpha) \\,. \\end{align*}\\] It turns out that for the test to be uniformly most powerful, we would have to perform further manipulations so as to isolate \\(x\\) on the left-hand side of the inequality, with only constants appearing on the right-hand side (i.e., we would need to simplify this expression such that it achieves the form \\(x &lt; k\\) or \\(x &gt; k\\)). Here, that is impossible to do, meaning that the value of the rejection region boundary changes as \\(\\mu_a\\) changes. Thus while we can define the most powerful test of \\(H_o : \\mu = \\mu_o = 1\\) versus \\(H_a : \\mu = \\mu_a\\), that test is not a uniformly most powerful one. 3.8.5 Estimating a \\(p\\)-Value via Simulation Let’s assume that we sample \\(n\\) iid data from a distribution whose probability density function is \\[\\begin{align*} f_X(x \\vert \\theta) = \\theta x^{\\theta-1} \\,, \\end{align*}\\] where \\(x \\in [0,1]\\) and \\(\\theta &gt; 0\\). Furthermore, let’s assume we wish to test \\(H_o : \\theta = \\theta_o = 1\\) versus \\(H_a : \\theta = \\theta_a = 2\\). The Neyman-Pearson lemma tells us that a sufficient statistic will provide the basis for the most powerful test. Likelihood factorization indicates that one such statistic is \\[\\begin{align*} Y = \\prod_{i=1}^n X_i \\,, \\end{align*}\\] but, since functions of sufficient statistics are themselves sufficient, we could also utilize \\[\\begin{align*} Y = \\log \\prod_{i=1}^n X_i = \\sum_{i=1}^n \\log X_i \\,. \\end{align*}\\] We will choose to utilize \\(Y&#39;\\) because in general, summations are computationally easier to work with than products. However…we do not know the sampling distribution for \\(Y\\). So we simulate. In a simulation, we do the following: we sample data from the original distribution given the null; we form the statistic and store its value; we repeat steps 1 and 2 a large number of times; and we see how often the simulated statistic value is (in this case) greater than the observed value…that proportion is our estimated \\(p\\)-value. Below we carry out a simulation of 100,000 datasets where the true value of \\(\\theta\\) is set (arbitrarily) to 1.75. set.seed(236) num.sim &lt;- 100000 n &lt;- 10 theta &lt;- 1.75 # arbitrarily chosen true value X.obs &lt;- rbeta(n,shape1=theta+1,shape2=1) y.obs &lt;- sum(log(X.obs)) cat(&quot;The observed statistic is &quot;,y.obs,&quot;\\n&quot;) ## The observed statistic is -4.758327 X &lt;- runif(n*num.sim) # the null is equivalent to Uniform(0,1) X &lt;- matrix(X,nrow=num.sim) # each row a dataset of size n Y &lt;- apply(X,1,function(x){sum(log(x))}) # the simulated statistics p &lt;- sum(Y&gt;=y.obs)/num.sim # the p-value cat(&quot;The estimated p-value is &quot;,p,&quot;\\n&quot;) ## The estimated p-value is 0.02413 We can visualize the empirical sampling distribution using a histogram. See Figure 3.11, in which the \\(p\\)-value is the “area under the curve” to the right of the observed statistic value (the vertical red line). To ensure that the \\(p\\)-value estimate is as precise as possible, we should run as many simulations as our computer and our time will allow! Figure 3.11: The empirical sampling distribution for the statistic \\(Y = \\sum_{i=1}^n \\log X_i\\), determined by simulating 100,000 datasets drawn from the distribution \\(f_X(x \\vert \\theta) = \\theta x^{\\theta-1}\\). Here, \\(n = 10\\) and \\(\\theta = 1.75\\). The observed statistic value, \\(y_{\\rm obs}\\), is indicated by the vertical red line, and the estimated \\(p\\)-value is the proportion of simulation statistic values falling to the right of the line. The estimated \\(p\\)-value is a point estimate. We can add to this estimate by constructing a confidence interval on the unknown true \\(p\\)-value. We can do this because our simulation is like a binomial experiment, in which the number of trials \\(k\\) is the number of simulations (num.sim); and the probability of success \\(p\\) is the probability of sampling a value of \\(Y \\geq y_{\\rm obs}\\) (given that this is an upper-tail test with \\(E[Y]\\) increasing with \\(\\theta\\)). In the code below, we denote the observed statistic (here, the number of times we observe \\(Y \\geq y_{\\rm obs}\\)) as u.obs. f &lt;- function(p,k,u.obs,q) { pbinom(u.obs,size=k,prob=p) - q } uniroot(f,interval=c(0,1),k=num.sim,u.obs=sum(Y&gt;=y.obs)-1,q=0.975)$root ## [1] 0.02320476 uniroot(f,interval=c(0,1),k=num.sim,u.obs=sum(Y&gt;=y.obs),q=0.025)$root ## [1] 0.02511073 (Note how we apply a discreteness correction when computing the lower bound.) Our estimated 95% confidence interval for the true \\(p\\)-value is [0.0232,0.0251]; we can feel “confident” when we decide, in this situation, to reject the null hypothesis that \\(\\theta = \\theta_o = 1\\). 3.8.6 Utilizing Simulations When Data Reduction is Not Possible The logistic distribution, whose probability density function is \\[\\begin{align*} f_X(x \\vert \\mu) = \\frac{\\exp(\\mu-x)}{[1+\\exp(\\mu-x)]^2} \\end{align*}\\] for \\(x \\in (-\\infty,\\infty)\\) and \\(\\mu \\in (-\\infty,\\infty)\\), is not a member of the exponential family of distributions. This means that if we sample \\(n\\) iid data according to this distribution, we cannot reduce them so as to define a single-number sufficient statistic for \\(\\mu\\). What we can do, however, is the following: we can simulate a large number of datasets under the null hypothesis \\(\\mu = \\mu_o\\); for each dataset, we can record \\(\\mathcal{L}(\\mu_o \\vert \\mathbf{x})\\) and \\(\\mathcal{L}(\\mu_a \\vert \\mathbf{x})\\), and their difference (which is \\(\\log \\lambda_{NP}\\)); and we can determine the \\(\\alpha^{\\rm th}\\) percentile of the distribution of \\(\\log \\lambda_{NP}\\) values. The \\(\\alpha^{\\rm th}\\) percentile is \\(c&#39;(\\alpha) = \\log c(\\alpha)\\), i.e., it is the rejection-region boundary for the most powerful test of the simple hypotheses \\(\\mu = \\mu_o\\) and \\(\\mu = \\mu_a\\). (However, note that because \\(\\mu_a\\) factors directly into the simulations, what we define is not a uniformly most powerful test, which is to say as we change \\(\\mu_a\\), the value of \\(c&#39;(\\alpha)\\) will also change, unlike the rejection-region boundaries that we derive in single-value sufficient statistic/exponential family distribution contexts. Below, we show how to carry out simulations to determine the rejection-region boundary, the \\(p\\)-value, and the test power for a situation in which we sample \\(n = 10\\) iid data according to a logistic distribution for which \\(\\mu_o = 1\\), with our alternative hypothesis being \\(\\mu_a = 3\\). (See Figure 3.12.) set.seed(36236) alpha &lt;- 0.05 num.sim &lt;- 100000 n &lt;- 10 mu.o &lt;- 1 mu.a &lt;- 2 X &lt;- matrix(rlogis(n*num.sim,location=mu.o),nrow=num.sim) f &lt;- function(x,mu.o) { sum(log(dlogis(x,location=mu.o))) } log.like.o &lt;- apply(X,1,f,mu.o=mu.o) log.like.a &lt;- apply(X,1,f,mu.o=mu.a) delta.log.like &lt;- log.like.o - log.like.a # The rejection-region boundary rr &lt;- quantile(delta.log.like,probs=alpha) cat(&quot;The rejection-region boundary is&quot;,round(rr,3),&quot;\\n&quot;) ## The rejection-region boundary is -1.362 # The p-value for a dataset sampled under the null X &lt;- rlogis(n,location=mu.o) delta.log.like.obs &lt;- f(X,mu.o) - f(X,mu.a) p &lt;- sum(delta.log.like&lt;=delta.log.like.obs)/num.sim cat(&quot;The p-value for the statistic&quot;,round(delta.log.like.obs,3),&quot;is&quot;,round(p,4),&quot;\\n&quot;) ## The p-value for the statistic 0.834 is 0.3226 # The test power for mu = mu.a X &lt;- matrix(rlogis(n*num.sim,location=mu.a),nrow=num.sim) log.like.o.p &lt;- apply(X,1,f,mu.o=mu.o) log.like.a.p &lt;- apply(X,1,f,mu.o=mu.a) delta.log.like.p &lt;- log.like.o.p - log.like.a.p power &lt;- sum(delta.log.like.p&lt;rr)/num.sim cat(&quot;The test power for mu =&quot;,mu.a,&quot;is&quot;,round(power,3),&quot;\\n&quot;) ## The test power for mu = 2 is 0.568 Figure 3.12: The empirical sampling distribution for the statistic \\(Y = \\log\\lambda_{NP}\\), determined by simulating 100,000 datasets of size \\(n = 10\\) drawn from a logistic distribution with mean \\(\\mu_o = 1\\). The estimated rejection-region boundary \\(\\log c(\\alpha)\\) is indicated by the vertical red line; if the \\(y_{\\rm obs} &lt; \\log c(\\alpha)\\) is less than the boundary value, we would reject the null hypothesis that \\(\\mu_o = 1\\). 3.8.7 Large-Sample Tests of the Binomial Success Probability Earlier in the chapter, we saw that if \\[ k &gt; 9\\left(\\frac{\\mbox{max}(p,1-p)}{\\mbox{min}(p,1-p)}\\right) \\,, \\] then, given a random variable \\(X \\sim\\) Binomial(\\(k\\),\\(p\\)), we can assume \\[ X \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}(kp,kp(1-p)) \\,, \\] or that \\[ \\hat{p} = \\hat{p}_{MLE} = \\frac{X}{k} \\stackrel{d}{\\rightarrow} X&#39; \\sim \\mathcal{N}(p,p(1-p)/k) \\,. \\] In hypothesis testing, this approximation forms the basis of two different tests: \\[\\begin{align*} \\mbox{Score}~\\mbox{Test:} ~~ &amp; \\hat{p} \\sim \\mathcal{N}(p_o,p_o(1-p_o)/k) \\\\ \\mbox{Wald}~\\mbox{Test:} ~~ &amp; \\hat{p} \\sim \\mathcal{N}(p_o,\\hat{p}(1-\\hat{p})/k) \\,. \\end{align*}\\] The only difference between the two is in how the variance is estimated under the null. To carry out these tests, we can simply adapt the expressions for the rejection regions, \\(p\\)-values, and power given in Chapter 2 for tests of the normal population mean with variance known, with the primary change being the definition of the variance. We are including the details of the Wald and Score tests here for completeness, in that they are ubiquitous in introductory statistics settings. As they yield only approximate results, we would encourage the reader to always use the exact testing mechanisms described above, while being mindful of the effects of the discreteness of the probability mass function, especially when \\(n\\) and \\(k\\) are both small. 3.9 Logistic Regression In the last chapter, we introduced simple linear regression, in which we model the data-generating process as \\[ Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\,. \\] To perform hypothesis testing (e.g., \\(H_o : \\beta_1 = 0\\) vs. \\(H_a : \\beta_1 \\neq 0\\)), we make the assumption that \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\), or equivalently, that \\(Y_i \\vert x_i \\sim \\mathcal{N}(\\beta_0+\\beta_1x_i,\\sigma^2)\\). When we make this assumption, we are implicitly stating that the response variable is continuous and that it can take on any value between \\(-\\infty\\) and \\(\\infty\\). But what if this doesn’t actually correctly represent the response variable? Maybe the \\(Y_i\\)’s are distances with values \\(\\geq 0\\). Maybe each \\(Y_i\\) belongs to one of several categories, and thus the \\(Y_i\\)’s are discretely valued. When provided data such as these, it is possible, though not optimal, to utilize simple linear regression. A better choice is to generalize the concept of linear regression, and to utilize this generalization to implement a more appropriate statistical model. To implement a generalized linear model (or GLM), we need to do two things: examine the \\(Y_i\\) values and select an appropriate distribution for them (discrete or continuous? what is the functional domain?); and define a link function \\(g(\\theta \\vert x)\\) that maps the line \\(\\beta_0 + \\beta_1 x_i\\), which has infinite range, into a more limited range (e.g., \\([0,\\infty)\\)). Suppose that, in an experiment, the response variable can take on the values “false” and “true.” To generalize linear regression so as to handle these data, we map “false” to 0 and “true” to 1, and assume that we can model the data-generating process using a Bernoulli distribution (i.e., a binomial distribution with \\(k = 1\\)): \\(Y \\sim\\) Bernoulli(\\(p\\)). We know that \\(0 &lt; p &lt; 1\\), so we adopt a link function that maps \\(\\beta_0 + \\beta_1 x\\) to the range \\((0,1)\\). There is no unique choice, but a conventional one is the logit function: \\[ g(p \\vert x) = \\log\\left[\\frac{p \\vert x}{1-p \\vert x}\\right] = \\beta_0 + \\beta_1 x ~\\implies~ p \\vert x = \\frac{\\exp\\left(\\beta_0+\\beta_1x\\right)}{1+\\exp\\left(\\beta_0+\\beta_1x\\right)} \\,. \\] Using the logit function to model dichotomous data is dubbed logistic regression. See Figure 3.13. Figure 3.13: An example of an estimated logistic regression line. The blue line is a sigmoid function; it represents the probability that we would sample a datum of Class 1 as a function of \\(x\\). The red points are the observed data. How do we learn a logistic regression model? In linear regression, we can determine values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by formulae, the ones we derive when we minimize the sum of squared errors (SSE). For logistic regression, such simple formulae do not exist, and so we estimate \\(\\beta_0\\) and \\(\\beta_1\\) via numerical optimization of the likelihood function \\[ \\mathcal{L}(\\beta_0,\\beta_1 \\vert \\mathbf{y}) = \\prod_{i=1}^n p_{Y \\vert \\beta_0,\\beta_1}(y_i \\vert \\beta_0,\\beta_1) = \\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i} = p^{\\sum_{i=1}^n y_i} (1-p)^{n-\\sum_{i=1}^n y_i} \\,, \\] where \\(p_Y(\\cdot)\\) is the Bernoulli probability mass function. Specifically, we would take the first partial derivatives of \\(\\mathcal{L}(\\beta_0,\\beta_1 \\vert \\mathbf{y})\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\), respectively, set them both to zero (thus allowing us to equate the derivative expressions), and determine the values \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that satisfy the equated derivatives. These are the values at which the bivariate likelihood surface achieves its maximum value. Note that because numerical optimization is an iterative process, logistic regression models are learned more slowly than simple linear regression models. Let’s step back for a moment. Why would we want to learn a logistic regression model in the first place? After all, it is a relatively inflexible model that might lack the ability to mimic the true behavior of \\(p \\vert x\\). The reason is that because we specify the mathematical form of the model, we can after the fact examine the estimated coefficients and perform inference: we can examine how the response is affected by changes in the predictor variables’ values. This can be important in, e.g., scientific contexts, where explaining how a model works can be as important, if not more important, than its ability to generate accurate and precise predictions. (This is in constrast to commonly used machine learning models, whose mathematical forms cannot be specified a priori and are thus less interpretable after they are learned.) In simple linear regression, if we increase \\(x\\) by one unit, we can immediately infer how the response value changes: \\[ \\hat{Y}&#39; = \\hat{\\beta}_0 + \\hat{\\beta}_1 (x + 1) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_1 = \\hat{Y} + \\hat{\\beta}_1 \\,. \\] For logistic regression, the situation is not as straightforward, because \\(p\\) changes non-linearly as a function of \\(x\\). So we fall back on the concept of odds: \\[ O(x) = \\frac{p(x)}{1-p(x)} = \\exp\\left(\\hat{\\beta}_0+\\hat{\\beta}_1x\\right) \\,. \\] If, e.g., \\(O(x) = 4\\), then that means that, given \\(x\\), we are four times more likely to sample a success (1) than a failure (0). How does the odds change if we add one unit to \\(x\\)? \\[ O(x+1) = e^{\\hat{\\beta}_0+\\hat{\\beta}_1(x+1)} = \\exp\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1x\\right) \\times \\exp\\left(\\hat{\\beta}_1\\right) = O(x) \\exp\\left(\\hat{\\beta}_1\\right) \\,. \\] The odds change by a factor of \\(\\exp\\left(\\hat{\\beta}_1\\right)\\), which can be greater than or less than one, depending on the sign of \\(\\hat{\\beta}_1\\). It is important to note that when we perform logistic regression, we are simply estimating \\(p \\vert x\\), which is the probability that we would sample a datum belonging to Class 1, given \\(x\\). How we choose to map \\(p \\vert x\\) to either 0 or 1, if we choose to make that mapping, is not actually part of the logistic regression framework. Classification is a broad topic within statistical learning whose details are beyond the scope of this book. We direct the interested reader to, e.g., Introduction to Statistical Learning by James et al. 3.9.1 Logistic Regression in R When observed with a telescope, a star is a point-like object, as opposed to a galaxy, which has some angular extent. Telling stars and galaxies apart visually is thus, in relative terms, easy. However, if the galaxy has an inordinately bright core because the supermassive black hole at its center is ingesting matter at a high rate, that core can outshine the rest of the galaxy so much that the galaxy appears to be point-like. Such galaxies with bright cores are dubbed “quasars,” or “quasi-stellar objects,” and they are much harder to tell apart from stars. Let’s say we are given a dataset showing the difference in magnitude (a logarithmic measure of brightness) at two wavelengths, for 500 stars and 500 quasars. The response variable is a factor variable with two levels, which we dub “Class 0” (here, QSO) and “Class 1” (STAR). (The mapping of qualitative factors to quantitative levels is by default alphabetical; as STAR comes after QSO, STAR gets mapped to Class 1. One can always override this default behavior.) We learn a simple logistic regression model using the R function glm(), rather than lm(), and we specify family=binomial, which means “do logistic regression.” (We can add additional arguments to, e.g., change the link function.) glm.out &lt;- glm(class~r,data=df,family=binomial) print(summary(glm.out),show.residuals=TRUE) ## ## Call: ## glm(formula = class ~ r, family = binomial, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6465 -0.8950 0.0331 0.8436 3.9163 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 23.44801 1.65428 14.17 &lt;2e-16 *** ## r -1.25386 0.08817 -14.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1386.3 on 999 degrees of freedom ## Residual deviance: 1040.9 on 998 degrees of freedom ## AIC: 1044.9 ## ## Number of Fisher Scoring iterations: 5 The summary() of a learned logistic regression model is similar to that for a linear regression model. The “deviance residuals,” which show the residuals for each datum, are defined as \\[ d_i = \\mbox{sign}(Y_i - \\hat{Y}_i) \\sqrt{-2[Y_i \\log \\hat{Y}_i + (1-Y_i)\\log(1-\\hat{Y}_i)]} \\,, \\] where \\[ \\hat{Y}_i = \\hat{p}_i \\vert x_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i)}{1 + \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i)} \\,. \\] The sum of the squared deviance residuals is equal to \\(-2 \\log \\mathcal{L}_{\\rm max}\\), where \\(\\mathcal{L}_{\\rm max}\\) is the maximum value of the joint likelihood of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given \\(\\mathbf{x}\\). Here, we observe that the deviance residuals are seemingly well-balanced around zero. The coefficients can be translated to \\(y\\)-coordinate values as follows: the intercept is \\(e^{23.448}/(1+e^{23.448}) \\rightarrow 1\\) (this is the probability the a sampled datum with r equal to zero is a STAR), while the odds ratio is \\(O_{new}/O_{old} = e^{-1.254}\\) (so that every time r increases by one unit, the probability that a sampled datum is a star is 0.285 times what it had been before: the higher the value of r, the less and less likely that a sampled datum is actually a star, and the more and more likely that it is a quasar…at least, according to the logistic regression model. The numbers in the other three columns of the coefficients section are estimated numerically using the behavior of the likelihood function (specifically, the rates at which it curves downward away from the maximum). Some details about how the numbers are calculated are given in an example in the “Covariance and Correlation” section of Chapter 6. It suffices to say here that if we assume that \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are both normally distributed random variables, we reject the null hypothesis that the intercept is 0 (or equivalently \\(y = 1/2\\)), and that \\(\\beta_1 = 0\\). The null deviance is \\(-2 \\log \\mathcal{L}_{max,o}\\), where \\(\\mathcal{L}_{max,o}\\) is the maximum likelihood when \\(\\beta_1\\) is set to zero. The residual deviance is \\(-2 \\log \\mathcal{L}_{\\rm max}\\). The difference between these values (here, 1386.3-1040.9 = 345.4), under the null hypothesis that \\(\\beta_1 = 0\\), is assumed to be sampled from a chi-square distribution for 999-998 = 1 degree of freedom. The \\(p\\)-value is thus 1 - pchisq(345.4,1) or effectively zero: we emphatically reject the null hypothesis that \\(\\beta_1 = 0\\). While this hypothesis test clearly indicates that \\(\\beta_1\\) is not equal to zero, how can we know whether the model itself fits to the data well in an absolute sense? This is a model’s “goodness of fit,” a concept we introduce at the end of this chapter. There is no unique answer to this question in the context of logistic regression; the interested reader should look up, e.g., the Hosmer-Lemeshow statistic. (Note that the test carried out here is analogous to the \\(F\\)-test in linear regression, and is testing \\(H_o: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\) versus \\(H_a:\\) at least one of the \\(\\beta_i\\)’s is non-zero. Because the sampling distribution here is the chi-square distribution as opposed to the normal distribution, the \\(p\\)-value here will not match the \\(p\\)-value seen in the coefficients section in general.) Last, the AIC, or Akaike Information Criterion, is \\(-2\\) times the model likelihood (or here, the deviance) plus two times the number of variables (here, 2, as the intercept is counted as a variable). Adding the number of variables acts to penalize those models with more variables: the improvement in the maximum likelihood has to be sufficient to justify added model complexity. Discussion of the mathematical details of AIC is beyond the scope of this book; it suffices to say that if we compute it when learning a suite of different models, we would select the model with the smallest value. We wrap up this example by showing how one would start moving from estimating the Class 1 probabilities for each datum towards predicting classes for each datum. Logistic regression is not in and of itself a “classifier”: it simply outputs probabilities. Naively, we would classify an object with an estimated probability below 0.5 as being an object of Class 0 and one with an estimated probability above 0.5 as being an object of Class 1, but that only works in general if the classes are balanced, i.e., if there are the same number of data of Class 1 as of Class 0. (Here, “works” means “acts to minimize misclassification rates across both classes at the same time.”) If there is class imbalance, then we will almost certainly have to change 0.5 to some other value for optimal classification. In the current example, the classes are balanced, and so putting the “dividing line” at 0.5, as we do in Figure 3.14, is an optimal choice. glm.predictions &lt;- predict.glm(glm.out,type=&quot;response&quot;) &lt;img src=“_main_files/figure-html/glmpred-1.png” alt=“Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one would”draw” a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications.” width=“50%” /&gt; Figure 3.14: Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one would “draw” a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications. 3.10 Naive Bayes Regression The Naive Bayes model is the basis for perhaps the simplest probabilitic classifier, one that is used to, e.g., detect spam emails. The meanings of the words “Naive” and “Bayes” will become more clear below. Note that one would often see this model referred to as the “Naive Bayes classifier.” However, as noted in the last section, there are actually two steps in classification, the first being the estimation of probabilities that a given datum belongs to each class, and the second being the mapping of those probabilities to class predictions. In the last section and here, we are only focusing on the generation of predicted probabilities…hence the title of this section. (Many would argue that the Naive Bayes model is a machine-learning model. We would argue that it actually is not: the final form of a machine-learning model is unknown before we start the modeling process [e.g., the number of branches that will appear in a classification tree model is unknown and is learned by the machine], whereas with Naive Bayes the mathematical form of the model is fully specified and all we have to do is estimate unknown probabilities. This is akin to the situation with linear regression, where the mathematical form is set and all we have to do is estimate the coefficients…and no one would argue that linear regression is a machine-learning model!) Let’s assume that we are in a similar setting as the one for logistic regression, but instead of having a response that is a two-level factor variable (i.e., one the represents two classes), the number of levels is \\(K \\geq 2\\). (So instead of just, e.g., “chocolate” and “vanilla” as our response variable values, we can add “strawberry” and other ice cream flavors too!) The ultimate goal of the model is to assign conditional probabilities for each response class, given a datum \\(\\mathbf{x}\\): \\(p(C_k \\vert \\mathbf{x})\\), where \\(C_k\\) denotes “class \\(k\\).” How do we derive this quantity? The first step is to apply Bayes’ rule (hence the “Bayes” in “Naive Bayes”): \\[ p(C_k \\vert \\mathbf{x}) = \\frac{p(C_k) p(\\mathbf{x} \\vert C_k)}{p(\\mathbf{x})} \\,. \\] The next step is to expand \\(p(\\mathbf{x} \\vert C_k)\\): \\[ p(\\mathbf{x} \\vert C_k) = p(x_1,\\ldots,x_p \\vert C_k) = p(x_1 \\vert x_2,\\ldots,x_p,C_k) p(x_2 \\vert x_3,\\ldots,x_p,C_k) \\cdots p(x_p \\vert C_k) \\,. \\] (This is the multiplicative law of probability in action, as applied to a conditional probability. See section 1.4.) The right-most expression above is one that is difficult to evaluate in practice, given all the conditions that must be jointly applied…so this is where the “Naive” aspect of the model comes in. We simplify the expression by assuming (most often incorrectly!) that the predictor variables are all mutually independent, so that \\[ p(x_1 \\vert x_2,\\ldots,x_p,C_k) p(x_2 \\vert x_3,\\ldots,x_p,C_k) \\cdots p(x_p \\vert C_k) ~~ \\rightarrow ~~ p(x_1 \\vert C_k) \\cdots p(x_p \\vert C_k) \\] and thus \\[ p(C_k \\vert \\mathbf{x}) = \\frac{p(C_k) \\prod_{i=1}^p p(x_i \\vert C_k)}{p(\\mathbf{x})} \\,. \\] OK…where do we go from here? We need to make further assumptions! We need to assign “prior probabilities” \\(p(C_k)\\) to each class. Common choices are \\(1/K\\) (we view each class as equally probable before we gather data) and \\(n_k/n\\) (the number of observed data of class \\(k\\) divided by the overall sample size). We also need to assume probability mass and density functions \\(p(x_i \\vert C_k)\\) for each predictor variable (a pmf if \\(x_i\\) is discrete and a pdf if \\(x_i\\) is continuous). It is convention to use binomial or multinomial pmfs, depending on the number of levels, with the observed proportions in each level informing the category probability estimate, and normal pdfs, with \\(\\hat{\\mu} = \\bar{x_i}\\) and \\(\\hat{\\sigma^2} = s_i^2\\) (the sample variance). Ultimately, this model depends on a number of (perhaps unjustified) assumptions. Why would we ever use it? Because the assumption of mutual independence makes model evaluation fast. Naive Bayes is rarely the model underlying the best classifier for any given problem, but when speed is needed (such as in the identification of spam email), one’s choices are limited. (As as general rule: if a model is simple to implement, one should implement it, even if the a priori expectation is that another model will ultimately generate better results. One never knows…) 3.10.1 Naive Bayes Regression With Categorical Predictors Let’s assume that we have collected the following data: \\(x_1\\) \\(x_2\\) \\(Y\\) Yes Chocolate True Yes Chocolate False No Vanilla True No Chocolate True Yes Vanilla False No Chocolate False No Vanilla False We learn a Naive Bayes model given these data, in which we assume “False” is Class 0 and “True” is Class 1. If we have a new datum \\(\\mathbf{x}\\) = (“Yes”,“Chocolate”), what is the probability that the response is “True”? We seek the quantity \\[ p(C_1 \\vert \\mathbf{x}) = \\frac{p(C_1) p(\\mathbf{x} \\vert C_1)}{p(\\mathbf{x})} = \\frac{p(C_1) p(x_1 \\vert C_1) p(x_2 \\vert C_1)}{p(x_1,x_2)} \\,. \\] When we examine the data, we see that \\(p(C_1) = 3/7\\), as there are three data out of seven with the value \\(Y\\) = “True.” Now, given \\(C_1\\), at what rate do we observe “Yes”? (One time out of three…so \\(p(x_1 \\vert C_1) = 1/3\\).) What about “Chocolate”? (Two times out of three…so \\(p(x_2 \\vert C_1) = 2/3\\).) The numerator is thus \\(3/7 \\times 1/3 \\times 2/3 = 2/21\\). The value of the denominator, \\(p(x_1,x_2)\\), is determined utilizing the Law of Total Probability: \\[\\begin{align*} p(x_1,x_2) &amp;= p(x_1 \\vert C_1) p(x_2 \\vert C_1) p(C_1) + p(x_1 \\vert C_2) p(x_2 \\vert C_2) p(C_2) \\\\ &amp;= 2/21 + 2/4 \\times 2/4 \\times 4/7 = 2/21 + 4/28 = 2/21 + 3/21 = 5/21 \\,. \\end{align*}\\] And so now we know that \\[ p(\\mbox{True} \\vert \\mbox{Yes,Chocolate}) = \\frac{2/21}{5/21} = \\frac{2}{5} \\,. \\] 3.10.2 Naive Bayes Regression With Continuous Predictors Let’s assume we have the following data regarding credit-card defaults, where \\(x\\) represents the credit-card balance and \\(Y\\) is a categorical variable indicating whether a default has occurred: \\(x\\) 1487.00 324.74 988.21 836.30 2205.80 927.89 712.28 706.16 1774.69 \\(Y\\) Yes No No No Yes No No No Yes Let’s now suppose someone came along with a credit balance of $1,200. According to the Naive Bayes model, given this balance, what is the probability of a credit default? The Naive Bayes model in this particular case is \\[ p(Y \\vert x) = \\frac{p(x \\vert Y) p(Y)}{p(x \\vert Y) p(Y) + p(x \\vert N) p(N)} \\,, \\] where we can observe immediately that \\(p(Y) = 3/9 = 1/3\\) and \\(p(N) = 6/9 = 2/3\\). To compute, e.g., \\(p(x \\vert Y)\\), we first compute the sample mean and sample standard deviation for the observed data for which \\(Y\\) is “Yes”: x &lt;- c(1487.00,2205.80,1774.69) round(mean(x),2) ## [1] 1822.5 round(sd(x),2) ## [1] 361.78 The mean is $1,822.50 and the standard deviation is $361.78, so the probability density associated with observing $1,200 is dnorm(1200,mean=mean(x),sd=sd(x)) ## [1] 0.0002509367 The density is 2.51 \\(\\times\\) 10\\(^{-4}\\). As far as for those data for which \\(Y\\) is “No”: x &lt;- c(324.74,988.21,836.30,927.89,712.28,706.16) dnorm(1200,mean=mean(x),sd=sd(x)) ## [1] 0.0002748351 The density is 2.75 \\(\\times\\) 10\\(^{-4}\\). Thus \\[ p(Y \\vert x) = \\frac{2.51 \\cdot 0.333}{2.51 \\cdot 0.333 + 2.75 \\cdot 0.667} = \\frac{0.834}{0.834 + 1.834} = 0.313 \\,. \\] We would predict that there is roughly a 1 in 3 chance that a person with a credit balance of $1,200 will default on their debt. 3.10.3 Naive Bayes Applied to Star-Quasar Data Here, we learn a Naive Bayes model that we can use to differentiate between stars and quasars, utilizing functions from R’s e1071 package. library(e1071) naiveBayes(class~r,data=df) ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## QSO STAR ## 0.5 0.5 ## ## Conditional probabilities: ## r ## Y [,1] [,2] ## QSO 19.35770 0.8282726 ## STAR 17.96389 1.3651572 The summary of the model shows that the classes are balanced (as the A-priori probabilities are 0.5 for each class). It then shows the estimated distributions of r values for quasars (a normal with estimated mean 19.358 and standard deviation 0.828) and stars (mean 17.964 and standard deviation 1.365). See Figure 3.15. Figure 3.15: An illustration of the Naive Bayes regression model, as applied to the star-quasar dataset. The red curve is the estimated normal probability density function for stars, while the green curve is the estimated pdf for quasars. Because the two classes in this example are balanced, we can state that where the amplitude of the red curve is higher, the predicted class would be STAR; otherwise, it would be QUASAR. 3.11 The Beta Distribution The beta distribution is a continuous distribution that is commonly used to model random variables with finite, bounded domains. Its probability density function is given by \\[ f_X(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\,, \\] where \\(x \\in [0,1]\\), \\(\\alpha\\) and \\(\\beta\\) are both \\(&gt; 0\\), and the normalization constant \\(B(\\alpha,\\beta)\\) is \\[ B(\\alpha,\\beta) = \\int_0^1 x^{\\alpha-1}(1-x)^{\\beta-1} dx = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\,. \\] (See Figure 3.16.) We have seen the gamma function, \\(\\Gamma(\\alpha)\\), before; it is defined as \\[ \\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha-1} e^{-x} dx \\,. \\] There are two things to note about the gamma function. The first is its recursive property: \\(\\Gamma(\\alpha+1) = \\alpha \\Gamma(\\alpha)\\). (This can be shown by applying integration by parts.) The second is that when \\(\\alpha\\) is a positive integer, the gamma function takes on the value \\((\\alpha-1)! = (\\alpha-1)(\\alpha-2) \\cdots 1\\). (Note that \\(\\Gamma(1) = 0! = 1\\).) Regarding the statement above about “model[ing] random variables with finite, bounded domains”: if a set of \\(n\\) iid random variables \\(\\mathbf{X}\\) has domain \\([a,b]\\), we can define a new set of random variables \\(\\mathbf{Y}\\) via the transformation \\[ \\mathbf{Y} = \\frac{\\mathbf{X}-a}{b-a} \\] such that the domain becomes \\([0,1]\\). We can model these newly defined data with the beta distribution. (Note the word can: we can model these data with the beta distribution, but we don’t have to, and it may be the case that there is another distribution bounded on the interval \\([0,1]\\) that ultimately better describes the data-generating process. The beta distribution just happens to be commonly used.) But…in the end, what does this all have to do with the binomial distribution, the subject of this chapter? We know the binomial has a parameter \\(p \\in [0,1]\\) and the domain of the beta distribution is \\([0,1]\\), but is there more? Let’s write down the binomial pmf: \\[ \\binom{k}{x} p^x (1-p)^{k-x} = \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\,. \\] This pmf dictates the probability of observing a particular value of \\(x\\) given \\(k\\) and \\(p\\). But what if we turn this around a bit…and examine this function if we fix \\(k\\) and \\(x\\) and vary \\(p\\) instead? In other words, let’s examine the likelihood function \\[ \\mathcal{L}(p \\vert k,x) = \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\,. \\] We can see immediately that the likelihood has the form of a beta distribution if we set \\(\\alpha = x+1\\) and \\(\\beta = k-x+1\\): \\[ \\mathcal{L}(p \\vert k,x) = \\frac{\\Gamma(\\alpha+\\beta-1)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\,, \\] except that the normalization term is not quite right: here we have \\(\\Gamma(\\alpha+\\beta-1)\\) instead of \\(\\Gamma(\\alpha+\\beta)\\). But that’s fine: there is no requirement that a likelihood function integrate to one over its domain. (Here, as the interested reader can verify, the likelihood function integrates to \\(1/(k+1)\\).) So, in the end, if we observe a random variable \\(X \\sim\\) Binomial(\\(k,p\\)), then the likelihood function \\(\\mathcal{L}(p \\vert k,x)\\) has the shape (if not the normalization) of a Beta(\\(x+1,k-x+1\\)) distribution. Figure 3.16: Three examples of beta probability density functions: Beta(2,2) (solid red line), Beta(4,2) (dashed green line), and Beta(2,3) (dotted blue line). 3.11.1 The Expected Value of a Beta Random Variable The expected value of a random variable sampled from a Beta(\\(\\alpha,\\beta\\)) distribution is \\[\\begin{align*} E[X] = \\int_0^1 x f_X(x) dx &amp;= \\int_0^1 x \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha+1,\\beta)} \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha+1,\\beta)} dx \\\\ &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\,. \\end{align*}\\] The last result follows from the fact that the integrand is the pdf for a Beta(\\(\\alpha+1,\\beta\\)) distribution, and the integral is over the entire domain, hence the integral evaluates to 1. Continuing, \\[\\begin{align*} E[X] &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\\\ &amp;= \\frac{\\Gamma(\\alpha+1) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta+1)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{(\\alpha+\\beta)\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)} \\\\ &amp;= \\frac{\\alpha}{\\alpha+\\beta} \\,. \\end{align*}\\] Here, we take advantage of the recursive property of the gamma function. We can utilize a similar strategy to determine the variance of a beta random variable, starting by computing \\(E[X^2]\\) and utilizing the shortcut formula \\(V[X] = E[X]^2 - (E[X])^2\\). The final result is \\[ V[X] = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\,. \\] 3.11.2 The Sample Median of a Uniform(0,1) Distribution The Uniform(0,1) distribution is \\[ f_X(x) = 1 ~~~ x \\in [0,1] \\,. \\] Let’s assume that we draw \\(n\\) iid data from this distribution, with \\(n\\) odd. Then we can utilize a result from order statistics to write down the pdf of the sample median, which is the \\(j^{\\rm th}\\) order statstic (where \\(j = (n+1)/2\\)): \\[ f_{((n+1)/2)}(x) = \\frac{n!}{\\left(\\frac{n-1}{2}\\right)! \\left(\\frac{n-1}{2}\\right)!} f_X(x) \\left[ F_X(x) \\right]^{(n-1)/2} \\left[ 1 - F_X(x) \\right]^{(n-1)/2} \\,. \\] Given that \\[ F_X(x) = \\int_0^x dy = x ~~~ x \\in [0,1] \\,, \\] we can write \\[ f_{((n+1)/2)}(x) = \\frac{n!}{\\left(\\frac{n-1}{2}\\right)! \\left(\\frac{n-1}{2}\\right)!} x^{(n-1)/2} (1-x)^{(n-1)/2} \\,, \\] for \\(x \\in [0,1]\\). This function has both the form of a beta distribution (with \\(\\alpha = \\beta = (n+1)/2\\)) and the domain of a beta distribution, so \\(\\tilde{X} \\sim\\) Beta\\(\\left(\\frac{n+1}{2},\\frac{n+1}{2}\\right)\\). (In fact, we can go further and state a more general result: \\(X_{(j)} \\sim\\) Beta(\\(j,n-j+1\\)): all the order statistics for data drawn from a Uniform(0,1) distribution are beta-distributed random variables! See Figure 3.17.) Figure 3.17: The order statistic probability density functions \\(f_{(j)}(x)\\) for, from left to right, \\(j = 1\\) through \\(j = 5\\), for the situation in which \\(n = 5\\) iid data are drawn from a Uniform(0,1) distribution (overlaid in red). Each pdf is itself a beta distribution, with parameter values \\(j\\) and \\(n-j+1\\). 3.12 The Multinomial Distribution Let’s suppose we are in a situation in which we are gathering categorical data. For instance, we might be throwing a ball and recording which of bins numbered 1 through \\(m\\) it lands in; categorizing the condition of old coins as “mint,” “very good,” “fine,” etc.; or classifying a galaxy as being a spiral galaxy, an elliptical galaxy, or an irregular galaxy. These are examples of multinomial trials, which is a generalization of the concept of binomial trials to situations where the number of possible outcomes is \\(m &gt; 2\\) rather than \\(m = 2\\). Earlier in this chapter, we wrote down the five properties of binomial trials. The analogous properties of a multinomial experiment are the following. It consists of \\(k\\) trials, with \\(k\\) chosen in advance. There are \\(m\\) possible outcomes for each trial. A trial may have no more than one realized outcome. The outcomes of each trial are independent. The probability of achieving the \\(i^{th}\\) outcome is \\(p_i\\), a constant quantity. The probabilities of each outcome sum to one: \\(\\sum_{i=1}^m p_i = 1\\). The number of trials that achieve a particular outcome is \\(X_i\\), with \\(\\sum_{i=1}^m X_i = k\\). The probability of any given outcome \\(\\{X_1,\\ldots,X_m\\}\\) is given by the multinomial probability mass function: \\[ p(x_1,\\ldots,x_m \\vert p_1,\\ldots,p_m) = \\frac{k!}{x_1! \\cdots x_m!}p_1^{x_1}\\cdots p_m^{x_m} \\,. \\] The distribution for any given \\(X_i\\) itself is binomial, with \\(E[X_i] = kp_i\\) and \\(V[X_i] = kp_i(1-p_i)\\). This makes intuitive sense, as one either observes \\(i\\) as a trial outcome (success), or something else (failure). However, the \\(X_i\\)’s are not independent random variables; the covariance between \\(X_i\\) and \\(X_j\\), a metric of linear dependence, is Cov(\\(X_i\\),\\(X_j\\)) = \\(-kp_ip_j\\) if \\(i \\neq j\\). (We will discuss the concept of covariance in Chapter 6.) This also makes intuitive sense: given that the number of trials \\(k\\) is fixed, observing more data that achieve one outcome will usually mean we will observe fewer data achieving any other given outcome. 3.13 Chi-Square-Based Hypothesis Testing Imagine that we are scientists, sitting on a platform in the middle of a plain. We are recording the number of animals of a particular species that we see, and the azimuthal angle for each one. (An azimuthal angle is, e.g., \\(0^\\circ\\) when looking directly north, and 90\\(^\\circ\\), 180\\(^\\circ\\), and 270\\(^\\circ\\) as we look east, south, and west, etc.) The question we want to answer is, are the animals uniformly distributed as a function of angle? There is no unique way to answer this question. However, a very common approach is to bin the data and use a chi-square goodness-of-fit test. Let’s assume we’ve observed 100 animals and that we record the numbers seen in each of four quadrants: angle range 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) number of animals 28 32 17 23 There is nothing “special” about the choice of four quadrants\\(-\\)we could have chosen eight, etc.\\(-\\)but as we’ll see below, the chi-square GoF test is an “approximate” test and its results become more precise as the numbers of data in each bin get larger. So there is a tradeoff if we increase the number of quadrants: we might be able to detect smaller-scale non-uniformities, but are test results will become less precise. To perform a chi-square GoF test with these data, we first specify are null and alternative hypotheses: \\[ H_o : p_1 = p_{1,o},\\cdots,p_m = p_{m,o} ~~ vs. ~~ H_a : \\mbox{at least two of the probabilities differ} \\,, \\] where \\(p_{i,o}\\) is the null hypothesis proportion in bin \\(i\\). Here, \\(k = 100\\), \\(m = 4\\), and \\(p_{1,o} = p_{2,o} = p_{3,o} = p_{4,o} = 0.25\\)…we expect, under the null, to see 25 animals in each quadrant. The null hypothesis is that the data are multinomially distributed with specified proportions being expected in each of the \\(m\\) defined bins. As we might imagine, performing a hypothesis test of the form given above that utilizes the multinomial distribution would be difficult to do by hand; multinomial distributions are intrinsically high-dimensional and the data (the numbers of counts in each bin) are not iid. In 1900, the statistician Karl Pearson proposed a “workaround” for testing multinomial hypotheses. He noted that as \\(k \\rightarrow \\infty\\), multinomial random variables converge in distribution to multivariate normal random variables (with the latter being something we will discuss in Chapter 6), and that the statistic \\[ W = \\sum_{i=1}^m \\frac{(X_i-E[X_i])^2}{E[X_i]} = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} \\] converges in distribution to the chi-square random variable for \\(m-1\\) degrees of freedom. (We subtract 1 because only \\(m-1\\) of the multinomial probabilities can freely vary; the \\(m^{th}\\) one is constrained by the fact that the probabilities must sum to 1. This constraint is what makes multinomial data not iid.) The computation of \\(W\\) is thus the basis of the chi-square goodness-of-fit test, or chi-square GoF test. For this test, we reject the null hypothesis if \\(W &gt; w_{RR}\\); the rejection region boundary is \\(w_{RR} = F_{W(m-1)}^{-1}(1-\\alpha)\\) (in R, qchisq(1-alpha,m-1)); the \\(p\\)-value is \\(1 - F_{W(m-1)}(w_{\\rm obs})\\) (e.g., 1-pchisq(w.obs,m-1)); and by convention, \\(kp_i\\) must be \\(\\geq\\) 5 in each bin for the test to yield a valid result. This last point underscores the tradeoff between splitting the data over more bins and test precision! In a chi-square GoF test, the inputs are the observed data and the hypothesized proportions. There are variations on this test in which the inputs are tables of observed data, ones that differ depending upon how the data are collected: chi-square test of independence: the question is whether two variables are associated with each other in a population; subjects are selected and the values of two variables are recorded for each. For instance, we might select \\(k\\) people at random and record whether or not they have had Covid-19, and also record whether or not they initially had zero, one, or two vaccine shots, and put these data into a table with, e.g., “yes” and “no” defining the rows and 0, 1, and 2 defining the columns. If we reject the null, we are stating that the distributions of data along, e.g., each row are statistically significantly different from each other. chi-square test of homogeneity: the question is whether the distribution of a single variable is the same for two subgroups of a population, with subjects being selected randomly from each subgroup separately. For instance, we might select \\(k\\) people under 20 years of age and ask if they prefer vanilla, chocolate, or strawberry ice cream, and then repeat the process for people of age 20 or over, and put the data into a table similar to that described for the test of independence above. Whether we perform a test of independence versus one of homogeneity affects the interpretation of results, but algorithmically the tests are identical. Under the null hypothesis, \\[ \\widehat{E[X_{ij}]} = \\frac{r_i c_j}{k} \\,, \\] i.e., the expected value in the cell in row \\(i\\) and column \\(j\\) is the product of the total number of data in row \\(i\\) and column \\(j\\), divided by the total number of data overall. Then, \\[ W = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(X_{ij} - \\widehat{E[X_{ij}]})^2}{\\widehat{E[X_{ij}]}} \\mathrel{\\dot\\sim} \\chi_{(r-1)(c-1)}^2 \\,, \\] i.e., the test statistic \\(W\\) is, under the null, assumed to be chi-square distributed for \\((r-1) \\times (c-1)\\) degrees of freedom. 3.13.1 Chi-Square Goodness of Fit Test Let’s work with the data presented above at the beginning of this section: angle range 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) number of animals 28 32 17 23 As stated above, we expect 25 counts in each bin. Given this expectation, are these data plausible, at level \\(\\alpha = 0.05\\)? We first compute the test statistic: \\[\\begin{align*} W &amp;= \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} = \\frac{(28-25)^2}{25} + \\frac{(32-25)^2}{25} + \\frac{(17-25)^2}{25} + \\frac{(23-25)^2}{25} \\\\ &amp;= \\frac{9}{25} + \\frac{49}{25} + \\frac{64}{25} + \\frac{4}{25} = \\frac{126}{25} = 5.04 \\,. \\end{align*}\\] The number of degrees of freedom is \\(m-1 = 3\\), so the rejection region boundary is \\(F_{W}^{-1}(1-\\alpha)\\) = qchisq(0.95,3) = 7.815. Since 5.04 \\(&lt;\\) 7.815, we fail to reject the null hypothesis that the animals are distributed uniformly as a function of azimuthal angle. (The \\(p\\)-value is 1-pchisq(5.04,3) = 0.169.) See Figure 3.18. Figure 3.18: An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, hence we fail to reject the null hypothesis. 3.13.2 Simulating an Exact Multinomial Test Let’s assume the same data as in the last example. One reason\\(-\\)in fact, the reason\\(-\\)why we utilize the chi-square GoF test when analyzing these data is that “it has always been done this way.” Stated differently, we have historically not worked with the multinomial distribution directly because we couldn’t…at least, not until computers came along. But now we can estimate the \\(p\\)-value for the hypothesis in the last example via simulation. Will we achieve a result very different from that above, \\(p = 0.169\\)? set.seed(101) O &lt;- c(28,32,17,23) p &lt;- rep(1/4,4) num.sim &lt;- 100000 k &lt;- sum(O) m &lt;- length(O) pmf.obs &lt;- dmultinom(O,prob=p) # the observed multinomial pmf X &lt;- rmultinom(num.sim,k,p) # generates an m x num.sim matrix # (m determined as the length of p) pmf &lt;- apply(X,2,function(x){dmultinom(x,prob=p)}) # simulated pmf&#39;s sum(pmf&lt;pmf.obs)/num.sim ## [1] 0.15974 What does apply() do? It applies the function given as the third argument (which evaluates the multinomial probability mass function given a set of four data \\(\\mathbf{x}\\) and a a set of four probabilities \\(\\mathbf{p}\\)) to each column (hence the “2” as the second argument…“1” would denote rows) of the matrix \\(\\mathbf{X}\\). It is a convenient function that allows us to not have to embed the evaluation of the multinomial pmf into a for loop that would iterate over all the rows of the matrix. We observe \\(p = 0.160\\). This is close to, but at the same time still substantially less than, 0.169. In case the reader is to say “but, the computation time must be much longer than for the chi-square GoF test,” the above computation takes \\(\\sim\\) 1 CPU second. The chi-square test is definitely important to know, in part because testing hypotheses about multinomial probabilities “has always been done this way”…but we would argue that when a simulation of the exact test is possible, one should code that simulation! (And run as many simulations as possible, to reduce uncertainty in the final result. Here, we can take our estimated \\(p\\)-value of 0.160 and state that our one standard error uncertainty is approximately \\(\\sqrt{kp(1-p)}/k = 0.001\\), i.e., we expect the true \\(p\\)-value to be in the range \\(0.160 \\pm 3 \\cdot 0.001\\) or \\([0.157,0.163]\\). 3.13.3 Chi-Square Test of Independence Let’s go back to our animal data. When we observe the animals in each quadrant, we record their color: black or red. So now are data look like this: 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) black 20 18 5 14 57 red 8 14 12 9 43 28 32 17 23 When we record two attributes for each subject (here, azimuthal angle and color), we can perform a chi-square test of independence to answer the question of whether the attributes are independent random variables. In other words, here, does the coloration depend on angle? The null hypothesis is no. We will test this hypothesis assuming \\(\\alpha = 0.05\\). We first determine the expected number of counts in each bin, \\(\\widehat{E[X_{ij}]} = r_i c_j / k\\): 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) black 57 \\(\\cdot\\) 28/100 = 15.96 57 \\(\\cdot\\) 32/100 = 18.24 57 \\(\\cdot\\) 17/100 = 9.69 57 \\(\\cdot\\) 23/100 = 13.11 57 red 43 \\(\\cdot\\) 28/100 = 12.04 43 \\(\\cdot\\) 32/100 = 13.76 43 \\(\\cdot\\) 17/100 = 7.31 43 \\(\\cdot\\) 23/100 = 9.89 43 28 32 17 23 We can already see that working with the numbers directly is tedious. Can we make a matrix of such numbers using R? r &lt;- c(57,43) c &lt;- c(28,32,17,23) E &lt;- (r %*% t(c))/sum(r) # multiply r and the transpose of c print(E) # much better ## [,1] [,2] [,3] [,4] ## [1,] 15.96 18.24 9.69 13.11 ## [2,] 12.04 13.76 7.31 9.89 If we wish to continue using R, we need to define a matrix of observed data values: O &lt;- matrix(c(20,8,18,14,5,12,14,9),nrow=2) # fills in column-by-column print(O) ## [,1] [,2] [,3] [,4] ## [1,] 20 18 5 14 ## [2,] 8 14 12 9 Now we have what we need. The test statistic is round(sum( (O-E)^2/E ),3) ## [1] 7.805 and the number of degrees of freedom is \\((r-1)(c-1) = 1 \\cdot 3 = 3\\), so the rejection region boundary is round(qchisq(0.95,3),3) ## [1] 7.815 We find that if \\(\\alpha = 0.05\\), we cannot reject the null hypothesis. We might be tempted to do so, as our test statistic very nearly falls into the rejection region, but we cannot. We could, if we were so inclined, remind ourselves that chi-square-based hypothesis tests are approximate, and run a simulation to try to estimate the true distribution of \\(W\\), and see what the rejection region and \\(p\\)-value actually are…that way, we might be able to actually reject the null. But really, at the end of the day, such a result should simply motivate us to gather more data! 3.14 Exercises Let \\(X_1,...,X_n\\) be \\(n\\) data drawn from a \\(\\mathcal{N}(2,4)\\) distribution. Write down the probability that exactly \\(m\\) of the \\(n\\) data have values \\(&gt; 2\\). You are a habitual buyer of raffle tickets. Each ticket costs $1, and each time you buy a ticket, you have a 40% chance of having a winning ticket, for which you will receive $3. You decide to keep buying tickets until you win for the first time. Let the random variable \\(F\\) denote the total number of losing tickets that you buy, and let \\(W\\) denote the total amount of money you win (or lose!). (a) \\(F\\) is sampled from what distribution that has what parameter value(s)? (b) What is the probability that you will end up making money, i.e., what is \\(P(W &gt; 0)\\)? (c) Let’s say instead of buying until you win, you decide you will buy exactly two tickets, then stop. What is the probability of buying a winning ticket exactly once, given that you buy at least one winning ticket? Suppose 50% of licensed drivers in a state are insured. If three licensed drivers are selected at random, what is the expected number of insured drivers given that the number of insured drivers is odd? (Assume the state’s population is effectively infinite, so that the probability of selecting an insured driver stays constant from trial to trial.) In an experiment, you take shots at a target until you hit that target twice. On any given shot, you have a 50% change of hitting the target. (a) What is the probability that you hit the target for the second time on your fourth shot overall? (b) You run your experiment twice. What is the probability of needing to take two shots one of the experiments to hit the target twice, and three shots during the other one to hit the target twice? Assume the results are independent (and of course identically distributed). (c) Let’s suppose you do not actually know the success probability \\(p\\), and you wish to determine a one-sided 90% upper bound on \\(p\\). As you are dealing with a discrete distribution, you need to build a uniroot()-style interval estimator. In your code, what value would you adopt for \\(q\\)? You collect \\(n=4\\) iid samples from the following distribution: \\[\\begin{eqnarray*} f_X(x) = \\left\\{ \\begin{array}{cc} 3x^2 &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Write down the sampling distribution for the minimum observed value, \\(X_{(1)}\\). (b) The sampling distribution for the maximum observed value is \\(g_{(4)}(x) = 12x^{11}\\). What is \\(E[X_{(4)}]\\)? You are to sample \\(n = 3\\) data from the following distribution: \\[\\begin{eqnarray*} f_X(x) = \\left\\{ \\begin{array}{cc} x &amp; 0 \\leq x \\leq \\sqrt{2} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Specify the distribution from which \\(X_{(3)}\\) is to be sampled. (b) Specify the variance of the distribution you derived in part (a). If we sample 3 iid data from a Uniform(0,1) distribution, what is the probability that the median value lies between 1/3 and 2/3? You sample three data from a Exp(1) distribution. What is the expected value for the median datum, i.e., the second ordered datum? Let the cdf for a given distribution be \\(F_X(x) = x^3\\) for \\(x \\in [0,1]\\), and let’s assume that you have sampled \\(n\\) iid data from this distribution. (a) What is the pdf within the domain \\(x \\in [0,1]\\)? (b) What is the pdf for \\(X_{(n)}\\) within the domain \\(x \\in [0,1]\\)? (c) What is the cdf for \\(X_{(n)}\\) within the domain \\(x \\in [0,1]\\)? (d) What is \\(E[X_{(n)}]\\)? You sample \\(n = 2\\) iid random variables from a distribution with pdf \\[\\begin{eqnarray*} f_X(x) = \\frac12 x \\,, \\end{eqnarray*}\\] for \\(x \\in [0,2]\\). (a) What is \\(F_X(x)\\) within the domain \\([0,2]\\)? (b) What is \\(f_{(2)}(x)\\)? (c) What is \\(E[X_{(2)}]\\)? (d) Are \\(X_{(1)}\\) and \\(X_{(2)}\\) independent random variables? Find the asymptotic distribution of the MLE of \\(n\\) i.i.d. samples from the Bernoulli distribution with parameter \\(p\\). (\\(p_X(x) = p^x(1-p)^{1-x}\\) for \\(x = \\{0,1\\}\\) and for \\(p \\in (0,1)\\).) You are given \\(n\\) iid data that are sampled from the following pmf: \\[\\begin{equation*} p_X(x) = (1-p)^{x-1}p \\,, \\end{equation*}\\] with \\(0 &lt; p &lt; 1\\) and \\(x = \\{1,2,3,\\ldots\\}\\). For this distribution, \\(E[X] = 1/p\\) and \\(V[X] = (1-p)/p^2\\). (a) What is the maximum likelihood estimator for \\(1/p\\)? Note: you are not required to confirm that the derivative of the score function is negative at \\(1/p = \\widehat{1/p}_{MLE}\\). Also note: a property of MLEs may help you here. (b) Write down \\(V[\\widehat{1/p}_{MLE}]\\), i.e., the variance of the MLE for \\(1/p\\). The probability mass function for the logarithmic distribution is \\[\\begin{eqnarray*} p_X(x) = -\\frac{1}{\\log(1-p)} \\frac{p^x}{x} \\,, \\end{eqnarray*}\\] where \\(x \\in \\{1,2,3,...\\}\\). You sample \\(n\\) iid data from this distribution. Write down a sufficient statistic for \\(p\\). You are given the following probability density function: \\[\\begin{equation*} f_X(x) = a b x^{a-1} (1-x^a)^{b-1} \\,, \\end{equation*}\\] defined over the domain \\(0 &lt; x &lt; 1\\). \\(a\\) and \\(b\\) are the parameters of the distribution, and both are positive and real-valued. Let \\(\\mathbf{X} = \\{X_1,\\cdots,X_n\\}\\) be \\(n\\) i.i.d.~samples from this distribution. (a) If you are able to define joint sufficient statistics for \\(a\\) and \\(b\\), write them down; otherwise, explain why you cannot. (b) Now, let \\(a=1\\). What is a sufficient statistic for \\(b\\)? You sample \\(n\\) iid data from the following distribution: \\[\\begin{eqnarray*} f_X(x) = \\frac{x}{\\beta^2} e^{-x/\\beta} \\,, \\end{eqnarray*}\\] where \\(x \\geq 0\\) and \\(\\beta &gt; 0\\), and where \\(E[X] = 2\\beta\\) and \\(V[X] = 2\\beta^2\\). (a) Write down a sufficient statistic for \\(\\beta\\) that arises directly from likelihood factorization. (b) Using your answer from part (a), determine the MVUE for \\(\\beta\\). (c) Compute the variance of the MVUE for \\(\\beta\\). (d) Compute the Cramer-Rao Lower Bound for the MVUE for \\(\\beta\\). Let \\(X_1,\\ldots, X_n\\) be \\(n\\) iid samples from the following distribution, where \\(\\theta &gt; 0\\), \\[\\begin{eqnarray*} f_X(x) = \\frac{1}{\\theta} \\exp\\left(x-\\frac{1}{\\theta} e^x\\right) \\,. \\end{eqnarray*}\\] Note that \\(e^X \\sim \\text{Exp}(\\theta)\\) (= Gamma(1,\\(\\theta\\))) and \\(Y = \\sum_{i=1}^n e^{X_i} \\sim \\text{Gamma}(n,\\theta)\\) (with \\(E[Y] = n\\theta\\) and \\(V[Y] = n\\theta^2\\)). (a) Find a sufficient statistic for \\(\\theta\\) using the factorization criterion. (b) Find the MVUE for \\(\\theta\\). (c) Find the MVUE for \\(\\theta^2\\). Let’s assume that we have sampled \\(n\\) iid data, \\(\\{X_1,\\ldots,X_n\\}\\), from a Maxwell-Boltzmann distribution with pdf \\[\\begin{eqnarray*} f_X(x) = \\sqrt{\\frac{2}{\\pi}} \\frac{x^2}{a^3} e^{-x^2/(2a^2)} \\,, \\end{eqnarray*}\\] where \\(x &gt; 0\\) and \\(a &gt; 0\\). The expected value and variance for an MB-distributed random variable are \\[\\begin{eqnarray*} E[X] = 2 a \\sqrt{\\frac{2}{\\pi}} ~~~ \\mbox{and} ~~~ V[X] = a^2 \\frac{(3 \\pi - 8)}{\\pi} \\end{eqnarray*}\\] respectively. (a) Identify a sufficient statistic for \\(a^2\\). (b) Derive \\(E[X^2]\\). (Hint: there is no need for integration here.) (c) Determine the MVUE for \\(a^2\\). (d) Can we use an invariance principle to state that the MVUE for \\(a\\) is \\((\\widehat{a^2}_{MVUE})^{1/2}\\)? You sample a single datum \\(X\\) from the following pdf: \\[\\begin{eqnarray*} f_X(x) = \\frac{\\theta}{2^\\theta} x^{\\theta-1} \\,, \\end{eqnarray*}\\] where \\(x \\in [0,2]\\) and \\(\\theta &gt; 0\\). Construct the most powerful level-\\(\\alpha\\) test of \\(H_o: \\theta = \\theta_o\\) versus \\(H_a: \\theta = \\theta_a\\), where \\(\\theta_a &gt; \\theta_o\\). Is your hypothesis test a uniformly most powerful hypothesis test? A common distribution used for the lengths of life of physical systems is the Weibull. Let \\(\\{X_1,\\ldots,X_n\\}\\) be \\(n\\) iid data sampled from this distribution: \\[\\begin{eqnarray*} f_X(x) = \\frac{\\theta}{\\beta} \\hspace{1mm} x^{\\theta-1} \\hspace{1mm} \\exp\\left(-\\frac{x^\\theta}{\\beta}\\right) \\hspace{5mm} x,\\beta, \\theta &gt;0 \\end{eqnarray*}\\] Assume that \\(\\theta\\) is known and that \\(\\beta\\) is freely varying, and that we are interested in finding the most powerful test of \\(H_o: \\beta = \\beta_o\\) versus \\(H_a: \\beta = \\beta_a\\), where \\(\\beta_a &gt; \\beta_o\\), at the level \\(\\alpha\\). What is a sufficient statistic for \\(\\beta\\)? (Do not include a minus sign.) Working with the Weibull directly is difficult; however, it turns out that \\(X^\\theta \\sim \\text{Exp}(\\beta)\\). What is the distribution of the sufficient statistic found in part (a), and what are the parameter values? (This will require examining the gamma distribution and its properties.) Write, in R code, the rejection-region boundary for the test. Let \\(\\{X_1,\\ldots,X_n\\}\\) be \\(n\\) iid data sampled from the following distribution: \\[\\begin{eqnarray*} f_{X}(x) = \\left\\{ \\begin {array}{ll} \\frac{1}{\\theta}\\exp\\left(-\\frac{1}{\\theta}(x-b)\\right) &amp; x &gt; b, \\theta &gt; 0 \\\\ 0&amp;\\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] Derive the moment-generating function for \\(X\\). Now derive the mgf for \\(\\sum_{i=1}^n X_i\\). The answer for (b) contains the term \\(e^{nbt}\\). Going back to our original discussion of the method of moment-generating functions, this means that we can write that \\(\\sum_{i=1}^n X_i = nb + \\sum_{i=1}^n U_i\\). What distribution is \\(\\sum_{i=1}^n U_i\\) sampled from? (Hint: look at the the rest of the mgf for \\(\\sum_{i=1}^n X_i\\) and, if necessary, refer back to the previous problem.) Write, in R code, the rejection-region boundary for the \\(\\alpha\\)-level test of \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta &lt; \\theta_o\\). Is this a uniformly most powerful test of these hypotheses? In an experiment, you sample \\(n\\) data from the distribution: \\[\\begin{eqnarray*} f_X(x) = \\theta e^{-\\theta x} \\,, \\end{eqnarray*}\\] where \\(x \\geq 0\\), \\(\\theta &gt; 0\\), and \\(E[X] = 1/\\theta\\). You wish to test the null hypothesis \\(H_o : \\theta_o = 4\\) versus \\(H_a : \\theta_a = 2\\). (a) Define the most powerful hypothesis test. By “define,” we mean write down a rejection region of the form \\(Y &lt; y_{\\rm RR}\\) or \\(Y &gt; y_{\\rm RR}\\), where you plug in a specific statistic for \\(Y\\). (You cannot evaluate \\(y_{\\rm RR}\\) given the information you have here…that can stay as is.) (b) The cdf of the sampling distribution for the appropriate statistic in part (a) is \\(\\gamma(n,\\theta y)/(n-1)!\\), where \\(\\gamma(\\cdot,\\cdot)\\) is the lower incomplete gamma function. Given this information, and given what you would have to do to define the rejection region boundary, can you conclude that we are defining a uniformly most powerful test? Let’s assume that we have sampled \\(n\\) iid data from a Binom(\\(k,p\\)) distribution, and that we wish to test \\(H_o: p = p_o\\) versus \\(H_a: p &gt; p_o\\). For our statistic, we will use \\(Y = \\sum_{i=1}^n X_i\\). (a) What is the sampling distribution for \\(Y\\)? Provide the name and the value(s) of the parameters. (b) You code elements of the test in R. Assume you have the initialized variables alpha, n, k, y.obs, p.o, and p.a at your disposal. Write out the (one-line!) function call you’d use to compute the rejection region boundary. (c) Now provide code of the \\(p\\)-value. Include a discreteness correction factor if it is needed. The negative inverse link function is \\(-(Y \\vert x)^{-1}\\). Assume that we are in a simple setting (i.e., that there is one predictor variable), and write the regression line formula \\(Y \\vert x\\) as a function of \\(\\beta_0\\), \\(\\beta_1\\), and \\(x\\). You use R to learn a logistic regression model, and you observe the output shown below: (a) What is the sample size \\(n\\)? (Hint: take into account how many parameter values are being estimated to convert a number shown in the output to the sample size.) (b) What is the value of \\(-2\\log\\mathcal{L}_{\\rm max}\\) for the model in which \\(\\beta_1\\) is set to zero? (c) What is the value of \\(O(x)\\) for \\(x = 0\\)? You may leave your answer in the form \\(e^a\\) or \\(\\log(a)\\) or \\(\\sqrt{a}\\), etc., while being sure to fill in the value of the constant \\(a\\). (d) Do the odds increase as \\(x\\) increases, or do the odds decrease as \\(x\\) increases? Below is the observed output from R’s glm() function when learning a logistic regression model. For these data, Class 0 is No (not a student) and Class 1 is Yes (is a student). (a) If the balance is $589, then the predicted probability that the datum belongs to the Yes class is 0.1. What is the odds ratio for $589 dollars? (b) What is the odds ratio if the balance is $689? (c) For the first datum, the observed response is No and the predicted probability that the datum is of the Yes class is 0.07. Compute the deviance for this datum. (d) Whoops…the null deviance value got expunged from the output. Would this value be higher or lower than the observed residual deviance value? Deviance Residuals: Min 1Q Median 3Q Max -1.1243 -0.5539 -0.4409 -0.3460 2.4901 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0616324 0.3936314 -7.778 7.37e-15 *** Balance 0.0014684 0.0004124 3.561 0.00037 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual deviance: 221.47 on 308 degrees of freedom AIC: 225.47 Refer to the displayed data below. Here, x1 and x2 are the two predictor variables, and \\(Y\\) is the response variable. Use these data to learn a Naive Bayes model and use that model to compute \\(p(0 \\vert \\mbox{Yes},\\mbox{False}) = p(0 \\vert \\mbox{Y},\\mbox{F})\\). Leave your answer as a fraction. Assume that \\(p(C_k) = n_k/n\\), and that you would estimate \\(p(x \\vert C_k)\\) as the proportion of times you observe a particular datum value \\(x\\) (e.g., True) given \\(C_k\\). You are given the following pdf: \\[\\begin{eqnarray*} f_X(x) = \\left\\{ \\begin{array}{cc} 2(1-x) &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] An associated cost is \\(C = 10X\\). (a) Identify the “named” distribution for \\(X\\) and state its parameters. (b) Given your answer for (a), determine \\(E[X]\\) and \\(E[X^2]\\). (c) Now determine \\(E[C]\\) and \\(V[C]\\). You are given the following cdf: \\[\\begin{eqnarray*} F_X(x) = \\left\\{ \\begin{array}{cc} 0 &amp; x &lt; 0 \\\\ 6x^2 - 8x^3 + 3x^4 &amp; 0 \\leq x \\leq 1 \\\\ 1 &amp; x &gt; 1 \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) \\(F_X(x)\\) is the cdf for what “named” distribution? (b) What is the expected value of \\(X\\)? You are given \\(X \\sim\\) Beta(2,3). What is \\(E[X^2]\\)? You sample 3 data from a Beta(3,1) distribution. (a) Write down the pdf for the median of the sampled data. (b) Compute the expected value for the median of the sampled data. You are given the following distribution: \\[\\begin{eqnarray*} f_X(x) = c x (1-x) \\,, \\end{eqnarray*}\\] where \\(x \\in [0,1]\\) and \\(c\\) is a constant. (a) Identify this distribution by name and provide the value(s) of its parameter(s). (b) Determine the numerical value of the constant \\(c\\). (c) Is the distribution symmetric about its mean value or it is a skewed distribution? (d) Compute \\(P(X \\leq 1/4 \\vert X \\leq 1/2)\\). Leave your answer as a fraction. Twenty-one (21) students enter a hallway off of which are three rooms. A researcher expects each student to randomly choose a room to enter (and to stay in!). That researcher observes that 10 students choose Room A, 5 students choose Room B, and 6 students choose Room C, and she decides to use these data to perform a test of the null hypothesis \\(p_1 = p_2 = p_3\\) at the level \\(\\alpha = 0.1\\). (a) Choose an appropriate hypothesis test and evaluate the test statistic. To be clear, your final answer should be a number. (b) What is the number of degrees of freedom associated with the test statistic? (c) The rejection region boundary value is 4.61. Do we reject the null hypothesis? A political scientist has developed four pamphlets with different types of information about climate change – one focused on environmental impact, one on economic impact, one on socio-cultural impact, and one about the origins of climate change. She prints 100 of each and randomly gives them out, asks people to read them, and records their answer to the question: how much money should the government spend on counteracting climate change? The answer options are: [a] more than, [b] as much as, and [c] less than they currently do. She plans to do a chi-square test to check whether willingness to spend government funding is related to which pamphlet the respondent read. (Note that she does not segment the respondents into groups.) (a) What kind of chi-square test is conducted here (goodness-of-fit, independence, or homogeneity)? (b) The observed test statistic turns out to be \\(w_{\\rm obs} = 24.3\\). How many terms did the researcher have to sum over to compute this statistic? (c) The test statistic follows a chi-square distribution with \\(m\\) degrees of freedom. What is the value of \\(m\\)? (d) Which of the following expressions corresponds to the \\(p\\)-value associated with the test carried out in part (a): (i) \\(P(W \\geq w_{\\rm obs})\\), (ii) \\(P(W \\leq w_{\\rm obs})\\), (iii) \\(P(\\frac{(n-1) W }{ \\sigma^2} \\geq w_{\\rm obs})\\), or (iv) \\(P(\\frac{(n-1) W}{\\sigma^2} \\leq w_{\\rm obs})\\)? (\\(W\\) is the random variable, chi-square-distributed for \\(m\\) degrees of freedom; \\(n\\) is the sample size; and \\(\\sigma^2\\) is the population variance.) You are recording the amount of time that elapses between when one person walks through a particular door until the next person walks through that same door. You have a weird stopwatch that changes from 0 to 1 after one minute, and from 1 to 2 after two minutes. That’s it. You take 100 measurements, and your data are given below. You wish to test the null hypothesis that each elapsed time is sampled from an exponential distribution with mean \\(\\beta\\) = 1 minute. (a) Under this null hypothesis, the probability that someone ends up in the category “1-2 minutes” is \\(\\int_1^2 e^{-x} dx\\). Explain why this is and calculate the probability. (b) Carry out an appropriate test to test the null hypothesis, provide either the rejection region or the \\(p\\)-value, and state your conclusion. Assume \\(\\alpha = 0.05\\). (c) A friend of yours decides they will repeat the experiment you’ve done, including the statistical analysis, but he is in a hurry and decides that he will only take 10 measurements overall. Explain to your friend why his experimental “design” is flawed. 0-1 min 1-2 min &gt;2 min 52 23 25 You play a (long) game in which you shoot 180 shots at a 3-by-3 square target, inside of which is a 1-by-1 square bullseye. Of your shots, 30 hit the bullseye while the remainder all hit the target, but hit outside the bullseye. You wish to test the null hypothesis that your shots hit the target at random locations. (a) Specify the number of shots that are expected to hit outside of, and inside of, the bullseye if the null hypothesis is correct. (b) Compute the value of an appropriate statistic for this test. (c) Specify the sampling distribution for this statistic (if the null hypothesis is correct); give the name and the values of any distribution parameters. (d) The rejection region boundary for this test is 3.841. State a conclusion regarding the null hypothesis: “reject” or “fail to reject.” "],["the-poisson-and-related-distributions.html", "4 The Poisson (and Related) Distributions 4.1 Motivation 4.2 Probability Mass Function 4.3 Cumulative Distribution Function 4.4 Linear Functions of Random Variables 4.5 Point Estimation 4.6 Confidence Intervals 4.7 Hypothesis Testing 4.8 The Gamma Distribution 4.9 Poisson Regression 4.10 Chi-Square-Based Hypothesis Testing 4.11 Exercises", " 4 The Poisson (and Related) Distributions 4.1 Motivation One of the challenges of a Prussian soldier’s life in the 19th century was avoiding being kicked by horses. This was no trivial matter: over one 20-year period, 122 soldiers died from horse-kick-related injuries. The data below are from the 1925 R. A. Fisher work Statistical Methods for Research Workers; they are a subset of the original data that were compiled earlier by the statistician Ladislaus Bortkiewicz. \\(x\\) \\(N(x)\\) 0 109 1 65 2 22 3 3 4 1 \\(x\\) is the number of deaths observed in any one Prussian army corp in any one year, and \\(N(x)\\) is the number of corps-years in which \\(x\\) deaths were observed. (\\(N(x)\\) sums to \\(20 \\cdot 10 = 200\\), reflecting that the compiled data represent 10 army corps observed over a 20-year period.) The data presented above are an example of a process, i.e., a sequence of observations, but we can immediately see that unlike the case with coin flips, this process is not a Bernoulli process. That’s because the number of possible outcomes is greater than two (0 and 1); in fact, the number of possible outcomes is countably infinite, so we could not even call this a multinomial process. Well, the reader might say, we could simply discretize the data more finely, so that the number of possible outcomes is at least finite (multinomial) or better yet falls to two (Bernoulli). Let’s get monthly data, or daily data, or hourly data. However, there is no time period \\(\\Delta t\\) for which the number of possible outcomes is limited to some maximum value: in theory, an infinite number of soldiers could die in the same second, or even the same nanosecond, etc. But let’s keep playing with this idea of making the time periods smaller and smaller. Let the number of time periods into which we divide our observation interval, \\(k\\), go to infinity such that the probability of observing a horse-kick death \\(p \\rightarrow 0\\) and such that \\(kp \\rightarrow \\lambda\\), where \\(\\lambda\\) is a constant. Under these conditions, as we will see in the next section, the binomial distribution transforms into the Poisson distribution, which Bortkiewicz dubbed the law of small numbers. Before we go to the section, though, let’s define the Poisson distribution in words: it gives the probability of observing a particular number of events (“counts”) in a fixed interval of space and/or time, assuming there is a constant mean rate of events and the occurrence of any one event is independent of the occurrence of other events. (For those who do not wish to wait until the section on point estimation to know the final answer: \\(\\hat{\\lambda}_{MLE} = 0.61\\), i.e., if the data are plausibly Poisson distributed [which is another question to ask entirely!], the true rate of death is estimated to be 0.61 soldiers per corps per year.) 4.2 Probability Mass Function Recall: a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) \\(0 \\leq p_X(x) \\leq 1\\) and (b) \\(\\sum_x p_X(x) = 1\\), where the sum is over all values of \\(x\\) in the distribution’s domain. To derive the Poisson probability mass function, we start by writing down the binomial distribution, setting \\(p\\) to \\(\\lambda/k\\), where \\(\\lambda\\) is an arbitrarily valued positive constant, and letting \\(k\\) go to infinity: \\[\\begin{align*} P(X=x) &amp;= \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\frac{k!}{x!(k-x)!} \\left(\\frac{\\lambda}{k}\\right)^x \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\\\ &amp;= \\frac{k!}{(k-x)! k^x} \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\\\ &amp;= \\left(\\frac{k}{k}\\right) \\left(\\frac{k-1}{k}\\right) \\cdots \\left(\\frac{k-x+1}{k}\\right) \\left(\\frac{\\lambda^x}{x!}\\right) \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\rightarrow \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} ~\\mbox{as}~~ k \\rightarrow \\infty \\\\ &amp;= \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^k \\left(1-\\frac{\\lambda}{k}\\right)^{-x} \\rightarrow \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^k ~\\mbox{as}~~ k \\rightarrow \\infty \\,. \\end{align*}\\] At this point, we concentrate on the parenthetical term above. Given that \\[ \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k}\\right)^k = e^{-1} \\,, \\] we can state that \\[ \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k/\\lambda}\\right)^{k/\\lambda} = e^{-1} \\implies \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k/\\lambda}\\right)^k = e^{-k} \\,. \\] We are now in a position to write down the probability mass function for a Poisson random variable (see Figure 4.1): \\[ P(X=x) = p_X(x) = \\frac{\\lambda^x}{x!} e^{-\\lambda} ~\\mbox{where}~ \\lambda &gt; 0 ~\\mbox{and}~ x \\in [0,\\infty) \\,. \\] A Poisson random variable converges in distribution to a normal random variable as \\(\\lambda \\rightarrow \\infty\\), a result which affects how Poisson-distributed data have historically been treated in, e.g., hypothesis test settings. (We will elaborate on this point when we return to the chi-square goodness of fit test later in the chapter.) To indicate that we have sampled a datum from a Poisson distribution, we write \\(X \\sim\\) Poisson(\\(\\lambda\\)). The expected value and variance of the Poisson distribution are \\(E[X] = \\lambda\\) and \\(V[X] = \\lambda\\), respectively; the former is derived below in an example. Figure 4.1: Poisson probability mass functions for \\(\\lambda = 1\\) (red), 5 (green), and 10 (blue). 4.2.1 The Poisson Distribution as Part of the Exponential Family Recall that the exponential family of distributions, introduced in Chapter 2, comprises distributions whose probability mass or density functions can be written in the form \\[\\begin{align*} h(x) \\exp\\left( \\eta(\\theta)T(x) - A(\\theta) \\right) \\,. \\end{align*}\\] Is the Poisson distribution a member of the larger exponential family of distributions? We will start our answer to this question by noting that \\[\\begin{align*} \\lambda^x = \\exp\\left(\\log \\lambda^x\\right) = \\exp\\left(x \\log \\lambda\\right) \\,. \\end{align*}\\] Thus \\[\\begin{align*} p_X(x \\vert \\lambda) = \\frac{1}{x!} \\exp\\left(x \\log \\lambda - \\lambda\\right) \\,. \\end{align*}\\] and \\[\\begin{align*} h(x) &amp;= \\frac{1}{x!} \\\\ \\eta(\\lambda) &amp;= \\log \\lambda \\\\ T(x) &amp;= x \\\\ A(\\lambda) &amp;= \\lambda \\,. \\end{align*}\\] The Poisson distribution is indeed a member of the exponential family. 4.2.2 The Expected Value of a Poisson Random Variable Recall: the expected value of a discretely distributed random variable is \\[ E[X] = \\sum_x x p_X(x) \\,, \\] where the sum is over all values of \\(x\\) within the domain of the pmf p_X(x). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(p_X(x)\\). For a Poisson distribution, the expected value is \\[ E[X] = \\sum_{x=0}^\\infty x \\frac{\\lambda^x}{x!} e^{-\\lambda} = \\sum_{x=1}^\\infty x \\frac{\\lambda^x}{x!} e^{-\\lambda} = \\sum_{x=1}^\\infty \\frac{\\lambda^x}{(x-1)!} e^{-\\lambda} \\,. \\] The goal is to move constants into or out of the summation so that the summation becomes one of a probability mass function over its entire domain. Here, we move \\(\\lambda\\) out of the summation, and make the substitution \\(y = x-1\\); the summand then takes on the form of a Poisson pmf, summed over its entire domain: \\[ E[X] = \\lambda \\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!} e^{-\\lambda} = \\lambda \\sum_{y=0}^\\infty \\frac{\\lambda^{y}}{y!} e^{-\\lambda} = \\lambda \\,. \\] Note that a similar calculation that starts with the derivation of \\(E[X(X-1)]\\) yields the Poisson variance. 4.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as \\(F_X(x) = \\sum_{y\\leq x} p_Y(y)\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). For the Poisson distribution, the cdf is \\[ F_X(x) = \\sum_{y=0}^{\\lfloor x \\rfloor} p_Y(y) = \\sum_{y=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^y}{y!} \\exp(-\\lambda) = \\frac{\\Gamma(\\lfloor x+1 \\rfloor,\\lambda)}{\\lfloor x \\rfloor !} \\,, \\] where \\(\\lfloor x \\rfloor\\) denotes the floor function, which returns the largest integer that is less than or equal to \\(x\\) (e.g., if \\(x\\) = 8.33, \\(\\lfloor x \\rfloor\\) = 8), and where \\(\\Gamma(\\cdot,\\cdot)\\) is the upper incomplete gamma function \\[ \\Gamma(\\lfloor x+1 \\rfloor,\\lambda) = \\int_{\\lambda}^\\infty u^{\\lfloor x \\rfloor} e^{-u} du \\,. \\] (An example of an R function which computes the upper incomplete gamma function is incgam() in the pracma package.) As we are dealing with a probability mass function, the cdf is a step function, as illustrated in the left panel of Figure 4.2. Recall that because of the step-function nature of the cdf, the form of inequalities in a probabilistic statement matter: e.g., \\(P(X &lt; x)\\) and \\(P(X \\leq x)\\) will not be the same if \\(x\\) is zero or a positive integer. Recall: an inverse cdf function \\(x = F_X^{-1}(q)\\) takes as input a distribution quantile \\(q \\in [0,1]\\) and returns the value of \\(x\\). A discrete distribution has no unique inverse cdf; it is convention to utilize the generalized inverse cdf, \\(x = \\mbox{inf}\\{x : F_X(x) \\geq q\\}\\), where “inf” indicates that the function is to return the smallest value of \\(x\\) such that \\(F_X(x) \\geq q\\). In the right panel of Figure 4.2, we display the inverse cdf for the same distribution used to generate the figure in the left panel (\\(\\lambda = 2\\)). Like the cdf, the inverse cdf for a discrete distribution is a step function. Figure 4.2: Illustration of the cumulative distribution function \\(F_X(x)\\) (left) and inverse cumulative distribution function \\(F_X^{-1}(q)\\) (right) for a Poisson distribution with \\(\\lambda=2\\). Note that because the domain of the Poisson distribution is countably infinite, we do observe any values of \\(x\\) for which \\(F_X(x) = 1\\). 4.3.1 Computing Probabilities If \\(X \\sim\\) Poisson(5), which is \\(P(4 \\leq X &lt; 6)\\)? We first note that due to the form of the inequality, we do not include \\(X=6\\) in the computation. Thus \\(P(4 \\leq X &lt; 6) = p_X(4) + p_X(5)\\), which equals \\[ \\frac{5^4}{4!}e^{-5} + \\frac{5^5}{5!}e^{-5} = \\frac{5^4}{4!}e^{-5} \\left( 1 + \\frac{5}{5} \\right) = 2\\frac{5^4}{4!}e^{-5} = 0.351\\,. \\] If we utilize R: dpois(4,lambda=5) + dpois(5,lambda=5) ## [1] 0.3509347 Note that we can take advantage of R’s vectorization capabilities by writing sum(dpois(4:5,lambda=5)) ## [1] 0.3509347 This approach is far more convenient than writing out a string of calls to dpois(), particularly when the number of values of \\(x\\) to sum over becomes large. We can also utilize cdf functions here: \\(P(4 \\leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \\leq 5) - P(X \\leq 3) = F_X(5) - F_X(3)\\), which in R is computed as follows: ppois(5,lambda=5) - ppois(3,lambda=5) ## [1] 0.3509347 If \\(X \\sim\\) Poisson(5), what is the value of \\(a\\) such that \\(P(X \\leq a) = 0.9\\)? First, we set up the inverse cdf formula: \\[ P(X \\leq a) = F_X(a) = 0.9 ~~ \\Rightarrow ~~ a = F_X^{-1}(0.9) \\] Note that we didn’t do anything differently here than we would have done in a continuous distribution setting…and we can proceed directly to R because it utilizes the generalized inverse cdf algorithm. qpois(0.9,lambda=5) ## [1] 8 4.4 Linear Functions of Random Variables Let’s assume we are given \\(n\\) iid Poisson random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Poisson(\\(\\lambda\\)). What is the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(m_X(t) = E[e^{tX}]\\). Also, if \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) as the mgf for a known family of distributions, then we can immediately identify the distribution for \\(Y\\) and the parameters of that distribution. The mgf for a Poisson random variable \\(X\\) is \\[ m_X(t) = \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] \\,. \\] (We derive this in an example below.) Thus the mgf for \\(Y = \\sum_{i=1}^n X_i\\) is \\[ m_Y(t) = \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] \\cdots \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] = \\exp\\left[(\\lambda+\\cdots+\\lambda)(e^t-1)\\right] = \\exp\\left[n\\lambda(e^t-1)\\right] \\,. \\] This mgf retains the form of a Poisson mgf. We thus see that the sum of Poisson-distributed random variables is itself Poisson distributed with parameter \\(n\\lambda\\), i.e., \\(Y = \\sum_{i=1}^n X_i \\sim\\) Poisson(\\(n\\lambda\\)). While we can identify the distribution of the sum by name, we cannot say the same about the sample mean. If \\(\\bar{X} = \\left(\\sum_{i=1}^n X_i\\right)/n\\), then \\[ m_{\\bar{X}}(t) = \\exp\\left[n\\lambda(e^{t/n}-1)\\right] \\,. \\] We cannot identify a named family of distributions with this specific mgf. However, we do know the distribution: it has a pmf that is identical in form to that of the Poisson distribution, but has the domain \\(\\{0,1/n,2/n,...\\}\\). (We can derive this result mathematically by making the transformation \\(\\sum_{i=1}^n X_i \\rightarrow (\\sum_{i=1}^n X_i)/n\\).) We could define the pmf ourselves using our own R function, but there is no real need to: if we wish to construct a confidence interval for \\(\\lambda\\), we can simply construct one for \\(n\\lambda\\) using the sum of the data as a statistic, and then divide the bounds by \\(n\\). (We could also, in theory, utilize the Central Limit Theorem if \\(n \\gtrsim 30\\), but there is absolutely no reason to do that to make inferences about \\(\\lambda\\): we know the distribution of the sum of the data exactly, and thus there is no need to fall back upon approximations.) 4.4.1 The Moment-Generating Function of a Poisson Random Variable The moment-generating function for a random variable \\(X\\) is found by utilizing the Law of the Unconscious Statistician and computing \\(E[e^{tX}]\\). If \\(X\\) is a Poisson random variable, then \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^\\infty e^{tx} p_X(x) = \\sum_{x=0}^\\infty e^{tx} \\frac{\\lambda^x}{x!} e^{-\\lambda} = e^{-\\lambda} \\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} e^{tx} \\\\ &amp;= e^{-\\lambda} \\left[ 1 + \\lambda e^t + \\frac{\\lambda^2}{2!}e^{2t} + \\ldots \\right] = e^{-\\lambda} \\left[ 1 + y + \\frac{y^2}{2!} + \\ldots \\right] \\\\ &amp;= e^{-\\lambda} e^y = \\exp(-\\lambda) \\exp(\\lambda e^t) = \\exp[\\lambda(e^t-1)] \\,. \\end{align*}\\] 4.4.2 The Distribution of the Difference of Two Poisson Random Variables Let’s assume that we are pointing a camera at an object, such as a star. A star gives off photons at a particular average rate \\(\\alpha_S\\) (with units of, e.g., photons per second). Thus if we open the shutter for a length of time \\(t\\), the number of photons we observe from the star is a Poisson random variable \\(S \\sim\\) Poisson(\\(\\lambda_S=\\alpha_St\\)). But the star is not the only object in the field of view; there may be other objects in the background that give off photons at a rate \\(\\alpha_B\\), and the number of photons we observe from the background will be \\(B \\sim\\) Poisson(\\(\\lambda_B=\\alpha_Bt\\)). Thus what we record is not \\(S\\), but \\(T = S+B\\)…so how can we make statistical inferences about \\(S\\) itself? One possibility is to point the camera to an “empty” field near the star, and record some number of photons \\(B\\) over the same amount of time. Then we can estimate \\(S\\) using \\(S = T - B\\). What is the distribution of \\(S\\)? We can utilize the method of moment-generating functions and write that \\[\\begin{align*} m_S(t) = m_T(t) m_B(-t) &amp;= \\exp[\\lambda_T(e^t-1)] \\exp[\\lambda_B(e^{-t}-1)] \\\\ &amp;= \\exp[\\lambda_T(e^t-1) + \\lambda_B(e^{-t}-1)] \\\\ &amp;= \\exp[-(\\lambda_T+\\lambda_B) + \\lambda_Te^t + \\lambda_Be^{-t}] \\,, \\end{align*}\\] where \\(\\lambda_T = \\lambda_S+\\lambda_B = (\\alpha_S+\\alpha_B)t\\). At first, utilizing the method of mgfs appears to be a fool’s errand: this is not an mgf we know. But it turns out that the family of distributions associated with this mgf does have a name: \\(S = T-B\\) is a Skellam-distributed random variable, with mean \\(\\lambda_T-\\lambda_B\\) and variance \\(\\lambda_T+\\lambda_B\\). We can work with this distribution to, e.g., construct confidence intervals for \\(\\mu_S\\), etc., if we so choose. 4.5 Point Estimation In previous chapters, we describe two commonly used point estimators: the maximum likelihood estimator and the minimum variance unbiased estimator. We review both below, in the context of estimating the Poisson \\(\\lambda\\) parameter, and then for completeness introduce one last, less-commonly used approach, the so-called method of moments. Recall: the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If \\(E[\\hat{\\theta}-\\theta] = 0\\), then the estimator \\(\\hat{\\theta}\\) is said to be unbiased. Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). First, let’s take the logarithm of the likelihood function written out above: \\[ \\ell(\\lambda \\vert \\mathbf{x}) = \\left(\\sum_{i=1}^n x_i\\right) \\log \\lambda - n \\lambda - \\log\\left(\\prod_{i=1}^n x_i!\\right) \\,. \\] (Note that we technically need not write down any terms that do not include \\(\\lambda\\), like the last term, as such terms will disappear entirely during differentiation.) The derivative of \\(\\ell(\\lambda \\vert \\mathbf{x})\\) with respect to \\(\\lambda\\) is \\[ \\frac{d\\ell}{d\\lambda} = \\left(\\frac{1}{\\lambda}\\sum_{i=1}^n x_i \\right) - n \\,. \\] Setting the derivative to zero and rearranging terms, we find that \\[ \\hat{\\lambda}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\] is the MLE for \\(\\lambda\\). By the general rule introduced in Chapter 1, \\(E[\\hat{\\lambda}_{MLE}] = E[\\bar{X}] = \\lambda\\) (so \\(\\hat{\\lambda}_{MLE}\\) is an unbiased estimator), and \\(V[\\hat{\\lambda}_{MLE}] = \\lambda/n\\) (so \\(\\hat{\\lambda}_{MLE}\\) is a consistent estimator, since \\(\\hat{\\lambda}_{MLE} \\rightarrow \\lambda\\) as \\(n \\rightarrow \\infty\\). (There is no guarantee that the MLE is an unbiased estimator; it just happens to be so here. Recall that the MLE will always be at least asymptotically unbiased, and it will always be a consistent estimator.) If we wish to find the MLE for a function of the parameter, e.g., \\(\\lambda^2\\), we simply apply that function to \\(\\hat{\\theta}_{MLE}\\). Hence \\(\\hat{\\lambda^2}_{MLE}\\) is \\(\\bar{X}^2\\). This is the invariance property of the MLE. Also, recall that the MLE converges in distribution to a normal random variable with mean \\(\\theta\\) and variance \\(1/I_n(\\theta)\\), where \\(I_n(\\theta)\\) is the Fisher information content of the data sample. Here, that means that \\[ \\hat{\\lambda} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(\\lambda,\\frac{\\lambda}{n}\\right) \\,. \\] Recall: deriving the minimum variance unbiased estimator involves two steps: factorizing the likelihood function to uncover a sufficient statistic \\(U\\) (that we assume is both minimal and complete); and finding a function \\(h(U)\\) such that \\(E[h(U)] = \\lambda\\). A sufficient statistic for a parameter (or parameters) \\(\\theta\\) captures all information about \\(\\theta\\) contained in the sample. The likelihood function is \\[ \\mathcal{L}(\\lambda \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda} = \\left(\\prod_{i=1}^n \\frac{1}{x_i!}\\right) \\left(\\prod_{i=1}^n \\lambda^{x_i}e^{-\\lambda}\\right) = \\underbrace{\\left(\\prod_{i=1}^n \\frac{1}{x_i!}\\right)}_{h(\\mathbf{x})} \\underbrace{\\lambda^{\\sum_{i=1}^n x_i}e^{-n\\lambda}}_{g\\left(\\sum_{i=1}^n x_i,\\lambda\\right)} \\,. \\] We see that a sufficient statistic is \\(U = \\sum_{i=1}^n X_i\\). Let’s determine the expected value for \\(U\\): \\[ E[U] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n \\lambda = n\\lambda \\,. \\] Thus \\(h(U) = U/n = \\bar{X}\\) is the MVUE for \\(\\lambda\\). As this matches the MLE, we know already that the MVUE is an unbiased (by definition) and consistent estimator. The next question is whether the variance of the MVUE achieves the Cramer-Rao Lower Bound on the variance of an unbiased estimator. We show that it does in an example below. Note: the MVUE does not possess the invariance property, and it may be the case that it does not achieve the CRLB. Its primary advantage over the MLE is that the MVUE is the best estimator among those that are always unbiased, regardless of sample size. The method of moments is a classic means by which to define estimators that has been historically useful when we are faced with the following situation: (a) the likelihood function is not easily differentiated, because it contains terms like \\(\\Gamma(\\theta)\\); and (b) the distribution has two or more free parameters, making it hard to define the MVUE (since we would have joint sufficient statistics). For instance, we face this situation when we work with the beta distribution from Chapter 3. But why do we say “historically useful”? Because in the age of computers, we can always determine MLEs numerically, using an optimization function. We will show an example of such a numerical computation below. Recall that by definition, \\(E[X^k]\\) is the \\(k^{\\rm th}\\) moment of the distribution of the random variable \\(X\\). We can define analagous sample moments, e.g., \\[ m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} ~~ \\mbox{and} ~~ m_2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = \\overline{X^2} \\,. \\] (For \\(m_2\\): note that the average of \\(X_i^2\\) is not the same as the average of \\(X_i\\), squared.) Let’s suppose that we have \\(p\\) parameters that we are trying to estimate. In method of moments estimation, we generally set the first \\(p\\) population moments equal to the first \\(p\\) sample moments and solve the system of equations to determine parameter estimates. These estimates are generally consistent, but also may be biased. (Situations may exist where higher-order moments may be preferable to use, such as when the one parameter of a distribution is \\(\\sigma^2\\) and thus we might derive a better estimator using second moments, but typically we will use the first \\(p\\) moments.) For the Poisson distribution, there is one parameter to estimate and thus we set \\(\\mu_1&#39; = E[X] = \\lambda = m_1&#39; = \\bar{X}\\). We thus find that \\(\\hat{\\lambda}_{MoM} = \\bar{X}\\). For a more relevant example of method of moments usage, see below. 4.5.1 Revisiting the Death-by-Horse-Kick Example We begin this chapter by displaying the number of deaths per Prussian army corps per year resulting from horse kicks. Leaving aside the question of whether the data are truly Poisson distributed (a question we will try to answer later in this chapter), what is the estimated rate of death per corps per year? The total number of events observed are \\[ 0 \\times 109 + 1 \\times 65 + 2 \\times 22 + 3 \\times 3 + 4 \\times 1 = 65 + 44 + 9 + 4 = 122 \\,, \\] and the total sample size is \\(n = 200\\), so \\[ \\hat{\\lambda} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{122}{200} = 0.61 \\,. \\] This is the MLE, the MVUE, and the MoM estimate for \\(\\lambda\\). In the next section, we will use these data to estimate a 95% confidence interval for \\(\\lambda\\). 4.5.2 The CRLB on the Variance of \\(\\lambda\\) Estimators Recall: the Cramer-Rao Lower Bound (or CRLB) is the lower bound on the variance of any unbiased estimator. If an unbiased estimator achieves the CRLB, it is the MVUE…but it can be the case that the MVUE does not achieve the CRLB. For a discrete distribution, the CRLB is given by \\[ V[\\hat{\\theta}] \\geq -\\left(nE\\left[\\frac{d^2}{d\\theta^2} \\log p_X(X \\vert p) \\right]\\right)^{-1} = \\frac{1}{nI(\\theta)} \\] where \\(I(\\theta)\\) is the Fisher information. For the Poisson distribution, \\[\\begin{align*} p_X(x \\vert \\lambda) &amp;= \\frac{\\lambda^x}{x!}e^{-\\lambda} \\\\ \\log p_X(x \\vert \\lambda) &amp;= x \\log \\lambda - \\lambda - \\log x! \\\\ \\frac{d}{d\\lambda} \\log p_X(x \\vert \\lambda) &amp;= \\frac{x}{\\lambda} - 1 \\\\ \\frac{d^2}{d\\lambda^2} \\log p_X(x \\vert \\lambda) &amp;= -\\frac{x}{\\lambda^2} \\\\ E \\left[ \\frac{d^2}{d\\lambda^2} \\log p_X(X \\vert \\lambda) \\right] &amp;= -\\frac{1}{\\lambda^2} E[X] \\\\ &amp;= -\\frac{1}{\\lambda^2} \\lambda = -\\frac{1}{\\lambda} \\end{align*}\\] and thus \\[ V[\\hat{\\lambda}] \\geq -\\frac{1}{-n/\\lambda} = \\frac{\\lambda}{n} \\,. \\] Thus \\(\\hat{\\lambda}_{MLE}\\), \\(\\hat{\\lambda}_{MVUE}\\), and \\(\\hat{\\lambda}_{MoM}\\) all achieve the CRLB. 4.5.3 Minimum Variance Unbiased Estimation and the Invariance Property As stated above, the MVUE does not possess the property of invariance. To demonstrate the lack of invariance, we will define the MVUE for \\(\\lambda^2\\). The first thing to notice is that we cannot fall back on factorization to determine an appropriate sufficient statistic, since \\(\\lambda^2\\) does not appear directly in the likelihood function. So we iterate: we make an initial guess and see where that guess takes us, and we guess again if our initial guess is wrong, etc. An appropriate “guess” for \\(\\lambda^2\\) is \\(\\bar{X}^2\\): \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{\\lambda}{n} + \\lambda^2 \\] We do get the term \\(\\lambda^2\\) here…but we also get \\(\\lambda/n\\). Hmm…so let’s try \\(\\bar{X}^2 - \\bar{X}/n\\) instead: \\[ E\\left[\\bar{X}^2 - \\frac{\\bar{X}}{n}\\right] = E[\\bar{X}^2] - \\frac{1}{n}E[\\bar{X}] = \\frac{\\lambda}{n} + \\lambda^2 - \\frac{\\lambda}{n} = \\lambda^2 \\,. \\] Done! The MVUE for \\(\\lambda^2\\) is thus \\(\\hat{\\lambda^2}_{MVUE} = \\bar{X}^2-\\bar{X}/n\\), which is not equal to \\(\\hat{\\lambda^2}_{MLE} = \\bar{X}^2\\) (except in the limit \\(n \\rightarrow \\infty\\)). 4.5.4 Method of Moments Estimation for the Gamma Distribution We will not officially introduce the gamma distribution until later in this chapter, but it is a good one to look at when exploring method of moments estimation. The probability density function for a gamma random variable \\(X\\) is \\[ f_X(x) = \\frac{x^{\\alpha-1}}{\\beta^{\\alpha}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} \\,, \\] for \\(x \\geq 0\\) and \\(\\alpha,\\beta &gt; 0\\). The expected value is \\(E[X] = \\alpha \\beta\\) while the variance is \\(V[X] = \\alpha \\beta^2\\) (and thus \\(E[X^2] = \\alpha \\beta^2 + \\alpha^2 \\beta^2\\)). Let’s assume we have \\(n\\) iid gamma-distributed random variables. Because there are two parameters, we match the first two moments: \\[\\begin{align*} \\mu_1&#39; = E[X] = \\alpha \\beta &amp;= m_1&#39; = \\bar{X} \\\\ \\mu_2&#39; = E[X^2] = \\alpha \\beta^2 + \\alpha^2 \\beta^2 &amp;= m_2&#39; = \\frac{1}{n}\\sum_{i=1}^n X_i^2 = \\overline{X^2} \\,. \\end{align*}\\] Let \\(\\beta = \\bar{X}/\\alpha\\). Then \\[\\begin{align*} \\alpha \\left( \\frac{\\bar{X}}{\\alpha} \\right)^2 + \\alpha^2 \\left( \\frac{\\bar{X}}{\\alpha} \\right)^2 &amp;= \\overline{X^2} \\\\ \\frac{(\\bar{X})^2}{\\alpha} &amp;= \\overline{X^2} - (\\bar{X})^2 \\\\ \\Rightarrow ~~ \\hat{\\alpha}_{MoM} &amp;= \\frac{(\\bar{X})^2}{\\overline{X^2} - (\\bar{X})^2} \\,, \\end{align*}\\] and thus \\[ \\hat{\\beta}_{MoM} = \\frac{\\bar{X}}{\\hat{\\alpha}_{MoM}} = \\frac{\\overline{X^2} - (\\bar{X})^2}{\\bar{X}} \\,. \\] 4.5.5 Maximum Likelihood Estimation via Numerical Optimization As noted above, a historical reason for using the method of moments to make point estimates is that it will “work” in those situations where we cannot derive the MVUE or the MLE. For instance, let’s suppose that we draw \\(n\\) iid data from a beta distribution, which has the following probability density function: \\[\\begin{align*} f_X(x) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)} \\,, \\end{align*}\\] where \\(x \\in [0,1]\\) and \\(\\alpha,\\beta &gt; 0\\). If \\(\\alpha\\) and \\(\\beta\\) are both freely varying, then we are in a situation in which we have joint sufficient statistics…and thus we are not able to compute the MVUEs. We then fall back on trying to compute the MLEs…but what is the partial derivative of \\(n \\log B(\\alpha,\\beta)\\) with respect to, e.g., \\(\\alpha\\)? With computers, we can circumvent this last issue by attempting to maximize the value of the likelihood, as a function of \\(\\alpha\\) and \\(\\beta\\), using a numerical optimizer. The subject of numerical optimization is far too vast for us to be able to cover all the important details here. It suffices to say that our goal is to (a) define an objective function (here, the log-likelihood), and (b) pass that function into an optimizer that explores the space of free parameters (here, \\(\\alpha\\) and \\(\\beta\\)) and returns the parameter values that optimize the objective function’s value (here, the ones that maximize the log-likelihood). Let’s start with the objective function. If the pdf or pmf of our random variables is already coded in R, we can use the coded function rather than write out the function mathematically. Recall that when we have sampled iid (continuously valued) data, \\[\\begin{align*} \\ell(\\theta \\vert \\mathbf{x}) = \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta) \\end{align*}\\] For the specific case of the beta distribution, we can convert this statement into code: sum(log(dbeta(x,shape1=alpha,shape2=beta))) where dbeta() is the beta pdf function. This is the objective function, except for one tweak that we will make: because R’s optim() function minimizes the objective function value by default, and we want to maximize the log-likelihood value, we will use -sum(log(dbeta(x,shape1=alpha,shape2=beta))) instead. So let’s now write down the full function: f &lt;- function(par,x) { -sum(log(dbeta(x,shape1=par[1],shape2=par[2]))) } Note how R expects the parameter values to be contained within a single vector, which here we call par. The rest of the coding is straightforward: (a) we initialize the vector par with initial (good!) guesses for the values of the parameters, (b) pass par and the observed data into optim(), and (c) access the par element of the list output by optim(). set.seed(236) # generate observed data n &lt;- 40 alpha &lt;- 2.44 # arbitrarily chosen true values beta &lt;- 5.39 X &lt;- rbeta(n,shape1=alpha,shape2=beta) # compute MLEs via optimization f &lt;- function(par,x) { -sum(log(dbeta(x,shape1=par[1],shape2=par[2]))) } par &lt;- c(3,3) opt &lt;- optim(par,f,x=X) # need to specify x via an extra argument opt$par ## [1] 2.510399 5.168113 We thus find that \\(\\hat{\\alpha}_{\\rm MLE} = 2.510\\) and \\(\\hat{\\beta}_{\\rm MLE} = 5.168\\). These values are close to, but not equal to, the true values; because these are MLEs, we know that our estimates will get closer and closer to the true values as \\(n \\rightarrow \\infty\\). 4.6 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}\\) by solving the following equation: \\[ F_Y(y_{\\rm obs} \\vert \\theta) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\), \\(y_{\\rm obs}\\) is the observed value of the statistic, and \\(q\\) is an appropriate quantile value that is determined using the confidence interval reference table introduced in section 16 of Chapter 1. As far as the construction of confidence intervals given a discrete sampling distribution goes, nothing changes algorithmically from where we were in Chapter 3; in an example below, we review how to construct such an interval for the Poisson parameter \\(\\lambda\\). In the context of statistical inference, there is one more important question to answer. What do we do if we neither know nor are willing to assume the distribution from which our data are sampled? Our root-finding algorithm relies upon knowing the sampling distribution of an observed statistic, and that in turn relies on knowing the distribution from which we draw each of our \\(n\\) iid data. One thing we can do is fall back upon bootstrapping. We demonstrate bootstrap confidence interval estimation in an example below. 4.6.1 Confidence Interval for the Poisson Parameter \\(\\lambda\\) Assume that we sample \\(n\\) iid data. We know that the sum \\(Y = \\sum_{i=1}^n X_i\\) is a sufficient statistic for \\(\\lambda\\) and that \\(Y \\sim\\) Poisson(\\(n\\lambda\\)). For this statistic, \\(E[Y] = n\\lambda\\), which increases with \\(\\lambda\\), so we know that we will utilize the “yes” lines of the confidence interval reference table. Our observed test statistic is \\(y_{\\rm obs} = \\sum_{i=1}^n x_i\\). # Let&#39;s assume we observe ten years of data in a Poisson process set.seed(101) alpha &lt;- 0.05 n &lt;- 10 lambda &lt;- 8 X &lt;- rpois(n,lambda=lambda) f &lt;- function(nlambda,y.obs,q) { ppois(y.obs,lambda=nlambda)-q } uniroot(f,interval=c(0.001,10000),y.obs=sum(X)-1,1-alpha/2)$root/n ## [1] 5.722035 uniroot(f,interval=c(0.001,10000),y.obs=sum(X),alpha/2)$root/n ## [1] 9.178654 Note the discreteness correction that we apply when deriving the lower bound, and note the division by \\(n\\) after the calls to uniroot(): this converts the interval bounds from being bounds on \\(n\\lambda\\) to being bounds on \\(\\lambda\\) itself. The interval is \\([\\hat{\\lambda}_L,\\hat{\\lambda}_U] = [5.72,9.18]\\), which overlaps the true value of 8. (See Figure 4.3.) Note that the interval over which we search for the root is [0.001,10000], which is (effectively) the range of possible values for \\(\\lambda\\). (Recall that \\(\\lambda &gt; 0\\), hence the small but non-zero lower bound.) Figure 4.3: Probability mass functions for Poisson distributions for which (left) \\(n\\lambda=57.2\\), and (right) \\(n\\lambda=91.8\\). We assume that we observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 73\\) events in total and that we want to construct a 95% confidence interval for \\(\\lambda\\). \\(n\\lambda=57.2\\) is the smallest value of \\(n\\lambda\\) such that \\(F_Y^{-1}(0.975) = 73\\), while \\(n\\lambda=91.8\\) is the largest value of \\(n\\lambda\\) such that \\(F_Y^{-1}(0.025) = 73\\). In Chapter 3, we discussed how when we work with discrete sampling distributions, the coverage of the confidence intervals that we construct, given a value \\(\\theta = \\theta_o\\), will be equal to the probability of sampling a datum in the acceptance region given a null hypothesis \\(H_o : \\theta = \\theta_o\\), and thus the coverage will be, in general, \\(&gt; 1-\\alpha\\). Let’s verify that that is the case here. y.rr.lo &lt;- qpois(alpha/2,lambda=lambda) y.rr.hi &lt;- qpois(1-alpha/2,lambda=lambda) sum(dpois(y.rr.lo:y.rr.hi,lambda=lambda)) ## [1] 0.968989 In this particular situation, the coverage is 96.9%. 4.6.2 Revisiting the Death-by-Horse-Kick Example In the last section above, we determined that the rate of death from horse kicks per Prussian army corps per year was \\(\\hat{\\lambda} = 0.61\\). Here, we determine a 95% interval estimate for \\(\\lambda\\). X &lt;- c(rep(0,109),rep(1,65),rep(2,22),rep(3,3),rep(4,1)) n &lt;- length(X) alpha &lt;- 0.05 f &lt;- function(nlambda,y.obs,q) { ppois(y.obs,lambda=nlambda)-q } uniroot(f,interval=c(0.001,10000),y.obs=sum(X)-1,1-alpha/2)$root/n ## [1] 0.5065682 uniroot(f,interval=c(0.001,10000),y.obs=sum(X),alpha/2)$root/n ## [1] 0.7283408 The 95% confidence interval is \\([0.507,0.728]\\). 4.6.3 Determining a Confidence Interval Using the Bootstrap The bootstrap, invented by Bradley Efron in 1979, uses the observed data themselves to build up empirical sampling distributions for statistics. Let’s suppose we are handed the following data: \\[ \\mathbf{X} = \\{X_1,X_2,\\ldots,X_n\\} \\overset{iid}{\\sim} P \\,, \\] where the distribution \\(P\\) is unknown. Now, let’s suppose further that from these data we compute a statistic: a single number. How can we build up an empirical sampling distribution from a single number? The answer is to repeatedly resample the data we observe, with replacement. For instance, if we have as data the numbers \\(\\{1,2,3\\}\\), a bootstrap sample might be \\(\\{1,1,3\\}\\) or \\(\\{2,3,3\\}\\), etc. Every time we resample the data, we compute the statistic we are interested in and record its value. Voila: we have an empirical sampling distribution. And if we can link the elements of that sampling distribution to a population parameter, we can immediately write down a confidence interval. For instance, if we have the \\(n_{\\rm boot}\\) statistics \\(\\{\\bar{X}_1,\\ldots,\\bar{X}_k\\}\\), we can put bounds on the population mean \\(\\mu\\): \\[\\begin{align*} \\hat{\\mu}_L &amp;= \\bar{X}_{\\alpha/2} \\\\ \\hat{\\mu}_U &amp;= \\bar{X}_{1-\\alpha/2} \\,, \\end{align*}\\] where \\(\\alpha/2\\) and \\(1-\\alpha/2\\) represent sample percentiles, e.g., the 2.5\\(^{\\rm th}\\) and 97.5\\(^{\\rm th}\\) percentiles. Now, let’s assume we have the same data as in the first example above. set.seed(101) n &lt;- 10 lambda &lt;- 8 X &lt;- rpois(n,lambda=lambda) print(X) ## [1] 7 4 9 9 6 6 8 7 9 8 The confidence interval that we construct for \\(\\lambda\\), which is the distribution mean, is \\([5.72,9.18]\\). How does the bootstrap estimate of the mean compare? n.boot &lt;- 10000 x.bar &lt;- rep(NA,n.boot) for ( ii in 1:n.boot ) { s &lt;- sample(length(X),length(X),replace=TRUE) x.bar[ii] &lt;- mean(X[s]) } quantile(x.bar,probs=c(0.025,0.975)) ## 2.5% 97.5% ## 6.3 8.2 The estimated interval is \\([6.3,8.2]\\). This is substantially smaller than what we found above, and that makes sense: for instance, the largest observed datum is 9, so the largest possible value of the bootstrap sample mean is 9…which is smaller than the previously determined upper bound of 9.18. What we are seeing is the effect of a small sample size: in the limit of small \\(n\\), the length of bootstrap confidence intervals is on average smaller than that of exact ones, with greater variability in lengths. As \\(n\\) increases, the mean lengths converge, but the variability in lengths remains larger for bootstrap intervals than for exact ones. What this means is that when we can propose a plausible distribution for our data, we should do so, as we will get more meaningful confidence intervals than if we were to fall back upon bootstrapping. 4.6.4 The Proportion of Observed Data in a Bootstrap Sample Let’s assume that we sample \\(n\\) iid data from some distribution \\(P\\). When we create a bootstrap sample of these data, some of the observed data appear multiple times, while other data do not appear at all. What is the average proportion of observed data in any given bootstrap sample? Let \\(i\\) be the index of an arbitrary datum, where the indices are \\(\\{1,2,\\ldots,n-1,n\\}\\). Let \\(X\\) be the number of times \\(i\\) is chosen when we construct a bootstrap sample of size \\(n\\): \\(X \\sim\\) Binom(\\(n,1/n\\)). \\(P(X \\geq 1)\\) then represents the average proportion of observed data in a bootstrap sample: \\[ P(X \\geq 1) = 1 - P(X = 0) = 1 - (1-1/n)^n \\,, \\] which, as \\(n \\rightarrow \\infty\\), approaches \\(1-1/e = 0.632\\). Thus, for a sufficiently large sample, 63.2% of the observed data will appear at least once in a bootstrapped dataset. 4.6.5 Confidence Interval Estimation via Simulation Let’s suppose that we sample \\(n\\) iid data from a Beta\\((a,2)\\) distribution: \\[\\begin{align*} f_X(x) = \\frac{x^{a-1}(1-x)}{B(a,2)} \\,, \\end{align*}\\] for \\(x \\in [0,1]\\) and \\(a &gt; 0\\). A sufficient statistic for \\(a\\), found via factorization, is \\(\\prod_{i=1}^n X_i\\); another, more easy to work with, one is \\(Y = -\\sum_{i=1}^n \\log X_i\\). (Why do we include a minus sign? For no other reason that when we include it, \\(Y\\) generally increases as \\(n\\) increases.) The expected value of the random variable \\(X\\) is \\(E[X] = a/(a+2)\\), which increases as \\(a\\) increases. However, as \\(X \\rightarrow 1\\), \\(-\\log X \\rightarrow 0\\), and so \\(E[Y]\\) will decrease with \\(a\\). We are now all set to construct, e.g., 95% confidence intervals for \\(a\\). Except…we cannot identify the sampling distribution for \\(Y\\). So we are stuck…but, in actuality, we are not. We just need to numerically estimate sampling distributions for \\(Y \\vert a\\) via simulation, and find the values of \\(a\\) that make the estimated \\(F_Y(y_{\\rm obs} \\vert a)\\) (approximately) equal to 0.025 and 0.975. Let’s first build up the code that we would need to estimate the sampling distribution and estimate the cdf for the value \\(y_{\\rm obs}\\). X &lt;- matrix(rbeta(n*num.sim,shape1=a,shape2=2),nrow=num.sim) Y &lt;- apply(X,1,function(x){-sum(log(x))}) sum(Y &lt;= y.obs)/num.sim On the first line, we create num.sim separate datasets given a parameter value, and store them row-by-row in a matrix. On the second line, we use R’s apply() function, row-by-row (as indicated by the argument 1), to determine the statistic value for each dataset. Then, on the third line, we determine the proportion of statistic values that are less than or equal to \\(y_{\\rm obs}\\): this is the empirical cumulative distribution function value \\(\\hat{F}_n(y_{\\rm obs} \\vert a)\\). We then find the value of \\(a\\) for which \\(\\hat{F}_n(y_{\\rm obs} \\vert a) - q = 0\\) using uniroot(). Let’s put this all together in an example where \\(n = 5\\) and \\(y_{\\rm obs} = 10\\): n &lt;- 5 y.obs &lt;- 10 alpha &lt;- 0.05 f &lt;- function(a,n,y.obs,q,num.sim=1000000,seed=236) { set.seed(seed) X &lt;- matrix(rbeta(n*num.sim,shape1=a,shape2=2),nrow=num.sim) Y &lt;- apply(X,1,function(x){-sum(log(x))}) sum(Y &lt;= y.obs)/num.sim-q } uniroot(f,c(0.1,3),n=n,y.obs=y.obs,q=alpha/2)$root ## [1] 0.248806 uniroot(f,c(0.1,3),n=n,y.obs=y.obs,q=1-alpha/2)$root ## [1] 1.374306 Our estimated 95% confidence interval for \\(a\\) is [0.249,1.374]. We note that the default number of simulated datasets in the code above is \\(10^6\\). This choice is driven a desire to make the result as precise as possible while keeping the runtime reasonable. (Here, the runtime is \\(\\lesssim\\) 1 CPU minute per uniroot() call on a standard desktop or laptop computer.) The empirical cdf is an estimate, i.e., even if we hold \\(a\\) constant, sum(Y &lt;= y.obs) is a random variable. Let’s denote the sum as \\(S \\vert a\\), the total number of simulated datasets as \\(k\\), and the true cdf value (given \\(y_{\\rm obs}\\)) as \\(p\\). Then \\[\\begin{align*} S \\vert a \\sim \\text{Binomial}(k,p) \\,. \\end{align*}\\] The standard error for the empirical cdf is thus \\[\\begin{align*} \\sqrt{V\\left[\\frac{1}{k}(S \\vert a)\\right]} = \\sqrt{\\frac{p(1-p)}{k}} \\,. \\end{align*}\\] If \\(k = 10^6\\) and, e.g., \\(p = 0.025\\), the standard error is \\(\\sim 10^{-4}\\), which is on par with typical uncertainties of estimates generated by uniroot(). We thus expect the uncertainties of our interval bound estimates to be \\(\\sim 10^{-4}\\). 4.7 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-tail test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bound given by \\(y_{RR}\\), which is a solution to the equation \\[ F_Y(y_{RR} \\vert \\theta_o) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\) and \\(q\\) is an appropriate quantile value that is determined using the hypothesis test reference table introduced in section 17 of Chapter 1. Note that the hypothesis test framework only allows us to make a decision about a null hypothesis; nothing is proven. In Chapter 3, we built upon the framework described above by introducing the Neyman-Pearson lemma. This result allows us to bypass the “guesswork” that goes into defining a hypothesis test statistic, by defining for us the most powerful test of a simple null hypothesis versus a simple specified alternative. Recall: when we test the simple hypotheses \\(H_o: \\theta = \\theta_o\\) versus \\(H_a: \\theta = \\theta_a\\), the Neyman-Pearson lemma allows us to state that the hypothesis test with maximum power has a rejection region of the form \\[ \\frac{\\mathcal{L}(\\theta_o \\vert \\mathbf{x})}{\\mathcal{L}(\\theta_a \\vert \\mathbf{x})} &lt; c(\\alpha) \\,, \\] where \\(c(\\alpha)\\) is a constant whose value depends on the specified Type I error \\(\\alpha\\). When we sample data from an exponential-family distribution, we would simply determine a sufficient statistic \\(Y\\), and develop a hypothesis test as we have done previously using that statistic (or a function of it), assuming we know or can derive its sampling distribution. If the rejection region does not depend on \\(\\theta_a\\), then the test is said to be a uniformly most powerful (UMP) test. In an example, we demonstrate how to apply the NP lemma to construct a hypothesis test for the Poisson parameter \\(\\lambda\\), given a sample of \\(n\\) iid data. Here we describe a more general hypothesis test framework, dubbed the likelihood ratio test (or LRT). “Wait…the NP lemma had a likelihood ratio. How is the LRT different?” That is a good question. It differs in how we specify the null and alternative hypotheses: \\[ H_o: \\theta \\in \\Theta_o ~~\\mbox{vs.}~~ H_a: \\theta \\in \\Theta_o^c \\,, \\] where \\(\\Theta_o\\) (“capital theta naught”) represents a set of possible null values for \\(\\theta\\), while \\(\\Theta_o^c\\) is the complement of that set. For instance, for tests involving the Poisson parameter \\(\\lambda\\), \\(\\Theta_o\\) could be \\(\\lambda \\in [5,10]\\), so that \\(\\Theta_o^c\\) is \\(\\lambda &lt; 5\\) or \\(\\lambda &gt; 10\\). (The null hypothesis in this example is a composite hypothesis, although it can be specified as a simple one, and usually is.) Let \\(\\Theta = \\Theta_o \\cup \\Theta_o^c\\), i.e., the union of the null and alternative sets. The rejection region for the LRT is \\[ \\lambda_{LR} = \\frac{\\mbox{sup}_{\\theta \\in \\Theta_o} \\mathcal{L}(\\theta \\vert \\mathbf{x})}{\\mbox{sup}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta \\vert \\mathbf{x})} &lt; c(\\alpha) \\,, \\] where, like it is in the context of the NP lemma, \\(c(\\alpha)\\) is a constant that depends on the specified Type I error \\(\\alpha\\). “Since the LRT is more general, why would we ever utilize the NP lemma?” That is another good question. The primary point to make is that when we construct a test of two simple hypothesis within the framework of the NP lemma, we know that we are constructing the most powerful test of those hypotheses. On the other hand, while the LRT is generally a powerful test, given the composite nature of one (or both) of the hypotheses it comes with no guarantee of being the most powerful test. For instance, perhaps an alternative like the score test (also known as the Lagrange multiplier test) or the Wald test would provide the most powerful test in a given analysis situation. Diving into the details of these alternatives is beyond the scope of this book; those who are interested in learning more about them should start by looking at Buse (1982). How does using the likelihood ratio test play out in practice? Let’s look at some possible use cases. (For simplicity, below we will assume that we have sampled data from an exponential-family distribution, and thus that we can reduce the data to, e.g., a single-number sufficient statistic for \\(\\theta\\). If we were to sample data from non-exponential-family distributions, we could, as we did in Chapter 3, fall back upon the use of simulations to determine the empirical distribution of the statistic \\(\\lambda_{LR}\\) [or \\(\\log\\lambda_{LR}\\)] under the null and use that distribution to estimate rejection-region boundaries, \\(p\\)-values, and test power.) We have one freely varying parameter \\(\\theta\\) and we wish to test \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta \\neq \\theta_o\\). In this situation, we identify a sufficient statistic (or a function of it) for which we know the sampling distribution and use it to define a two-tail test as we have done previously, utilizing the hypothesis test reference tables. We have one freely varying parameter \\(\\theta\\) and we wish to define a one-sided alternative hypothesis, e.g., \\(H_a : \\theta &lt; \\theta_o\\). Here, the null hypothesis would be the complement of \\(H_a\\), i.e., \\(H_o : \\theta \\geq \\theta_o\\). The distribution of the ratio \\[ \\lambda_{LR} = \\frac{\\mbox{sup}_{\\theta \\geq \\theta_o} \\mathcal{L}(\\theta \\vert \\mathbf{x})}{\\mathcal{L}(\\hat{\\theta}_{MLE} \\vert \\mathbf{x})} \\] is thus not uniquely specifiable: it is a function of \\(\\theta\\), and our null does not state a specific value for \\(\\theta\\). So how would we determine \\(y_{\\rm RR}\\)? It turns out the most conservative rejection-region boundary we can specify is the one that we would derive assuming that \\(\\theta = \\theta_o\\)…which means that to define an LRT, we would simply assume \\(\\theta_o\\) as our null value, identify a sufficient statistic (or a function of it) for which we know the sampling distribution, and define a one-tail test as we have done previously. We have two (or more) freely varying parameters. In this context, we will have joint sufficient statistics that are sampled from a multivariate sampling distribution that may be difficult to work with directly. Thus we fall back upon Wilks’ theorem. Let \\(r_o\\) denote the number of free parameters in \\(H_o: \\theta \\in \\Theta_o\\) and let \\(r\\) denote the number of free parameters in \\(\\theta \\in \\Theta = \\Theta_o \\cup \\Theta_a\\). (To reiterate, \\(\\Theta\\) must include all possible values of the parameters. For instance, if \\(H_o\\) is \\(\\theta = \\theta_o\\), then \\(H_a\\) must be \\(\\theta \\neq \\theta_o\\) and not \\(\\theta &lt; \\theta_o\\) or \\(\\theta &gt; \\theta_o\\).) For large \\(n\\), \\[ -2\\log \\lambda_{LR} \\stackrel{d}{\\rightarrow} W \\sim \\chi_{r-r_o}^2 \\,, \\] and we would reject the null hypothesis if \\(-2\\log \\lambda_{LR} &gt; w_{\\rm RR} = F_{W(r-r_o)}^{-1}(1-\\alpha)\\). Since this result is related to the central limit theorem, large \\(n\\) would be, by rule-of-thumb, 30 or more. 4.7.1 The Uniformly Most Powerful Test of Poisson \\(\\lambda\\) Let’s say that we are counting the number of students that enter a classroom each minute. We assume that the entry of students is a homogeneous Poisson process (i.e., \\(\\lambda\\), the expected number of entering students per time period, is constant). We think that five students, on average, will pass through the door each minute, while someone else thinks the number will be three. We collect data during five independent one-minute intervals: 4, 4, 3, 2, 3. Can we reject our null hypothesis at the level \\(\\alpha = 0.05\\)? What is the \\(p\\)-value? And what is the power of the test for \\(\\lambda_a = 3\\)? We test the simple hypotheses \\(H_o: \\lambda_o = 5\\) and \\(H_a: \\lambda_a = 3\\). The factorized likelihood for our data sample is \\[ \\mathcal{L}(\\lambda \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda} = \\underbrace{\\lambda^{\\sum_{i=1}^n x_i} e^{-n\\lambda}}_{g(\\sum x_i,\\lambda)} \\cdot \\underbrace{\\frac{1}{\\prod_{i=1}^n x_i!}}_{h(\\mathbf{x})} \\,. \\] A sufficient statistic is thus \\(Y = \\sum_{i=1}^n X_i \\sim\\) Poisson(\\(n\\lambda\\)). Since \\(E[Y] = n\\lambda\\) increases with \\(\\lambda\\), we find ourselves on the “yes” line of the hypothesis test reference tables, and specifically the “yes” line for a lower-tail test, and the rejection region is \\(Y &lt; y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\lambda_o)\\). We determine \\(y_{\\rm RR}\\) via the R function call y.rr &lt;- qpois(alpha,lambda=n*lambda.o). Since \\(y_{\\rm obs} = 16\\) and \\(y_{\\rm RR} = 17\\), we reject the null hypothesis that \\(\\lambda = 5\\). We can further write down that the \\(p\\)-value is ppois(y.obs,lambda=n*lambda.o), which is 0.038. (Note that since this is a lower-tail/yes test, we need not make a discreteness correction when computing the \\(p\\)-value, i.e., we do not need to subtract 1 from y.obs in the function call above.) Since the rejection region does not depend on \\(\\lambda_a\\), we have defined the uniformly most powerful test of \\(\\lambda_o\\) versus \\(\\lambda_a\\). Thus far, the only way that we’ve utilized the alternative hypothesis \\(H_a: \\lambda_a = 3\\) is when determining that we are conducting a lower-tail test. We will now use this value to determine the power of the test. The power is the probability of rejecting the null hypothesis given a specific value of \\(\\lambda\\), i.e., \\(P(Y &lt; y_{\\rm RR} \\vert \\lambda_a)\\). Here, that value is given by ppois(y.rr-1,n*lambda.a); since this is a lower-tail/yes test, we do need to make a discreteness correction when computing the test power. The power is 0.664: if \\(\\lambda\\) is truly equal to 3, we would reject the null hypothesis that \\(\\lambda_o = 5\\) after collecting five data some two-thirds of the time. 4.7.2 Likelihood Ratio Test of the Poisson Parameter \\(\\lambda\\) Let’s assume the same setting as for the last example, but here, let’s say that we will test \\(H_o: \\lambda = \\lambda_o = 5\\) versus \\(H_a: \\lambda \\neq \\lambda_o\\). The alternative hypothesis is a composite hypothesis, because it does not uniquely specify the shape of the probability mass function. Because we know the sampling distribution for the sufficient statistic \\(Y = \\sum_{i=1}^n X_i\\) in the Poisson distribution with parameter \\(n\\lambda\\), we can directly derive each of the two rejection region boundaries, which are \\[\\begin{align*} y_{\\rm RR,lo} &amp;= F_Y^{-1}(\\alpha/2 \\vert n\\lambda_o) \\\\ y_{\\rm RR,hi} &amp;= F_Y^{-1}(1-\\alpha/2 \\vert n\\lambda_o) \\,. \\end{align*}\\] In R, we determine the boundaries as y.rr.lo &lt;- qpois(0.025,25), or 16, and y.rr.hi &lt;- qpois(0.975,25), or 35. Our observed statistic is \\(y_{\\rm obs} = 16\\), thus we fail to reject the null hypothesis. As a reminder, because the alternative hypothesis is a composite hypothesis, the NP lemma does not apply here, and thus we cannot guarantee that the test we have just constructed is the most powerful of all possible tests of \\(H_o: \\lambda = \\lambda_o\\) versus \\(H_a: \\lambda \\neq \\lambda_o\\). 4.7.3 Using Wilks’ Theorem to Test Hypotheses About the Normal Mean We have collected \\(n\\) iid data from a normal distribution and we wish to test the hypothesis \\(H_o: \\mu = \\mu_o\\) versus the hypothesis \\(H_a: \\mu \\neq \\mu_o\\) using the likelihood ratio test. (We assume the variance is unknown.) For this problem, \\[\\begin{align*} \\Theta_o &amp;= \\{ \\mu,\\sigma^2 : \\mu = \\mu_o, \\sigma^2 &gt; 0 \\} \\\\ \\Theta_a &amp;= \\{ \\mu,\\sigma^2 : \\mu \\neq \\mu_o, \\sigma^2 &gt; 0 \\} \\,, \\end{align*}\\] and so \\(\\Theta = \\Theta_o \\cup \\Theta_a = \\{ \\mu,\\sigma^2 : \\mu \\in (-\\infty,\\infty), \\sigma^2 &gt; 0 \\}\\), with \\(r_o = 1\\) (\\(\\sigma^2\\)) and \\(r = 2\\) (\\(\\mu,\\sigma^2\\)). The likelihood for the normal pdf is \\[ \\mathcal{L}(\\mu,\\sigma \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp\\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right) \\] and the log-likelihood is \\[ \\ell(\\mu,\\sigma \\vert \\mathbf{x}) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2 \\,. \\] The test statistic for Wilks’ theorem is \\[ W = -2 \\left[ \\ell(\\mu_o,\\widehat{\\sigma^2}_{MLE,o} \\vert \\mathbf{x}) - \\ell(\\hat{\\mu}_{MLE},\\widehat{\\sigma^2}_{MLE} \\vert \\mathbf{x}) \\right] \\,, \\] where \\(\\hat{\\mu}_{MLE}\\) is the MLE for \\(\\mu\\) and where \\(\\widehat{\\sigma^2}_{MLE,o}\\) and \\(\\widehat{\\sigma^2}_{MLE}\\) are the MLEs for \\(\\sigma\\) given the restricted and full parameter spaces, respectively We know these results from previous derivations: \\[\\begin{align*} \\hat{\\mu}_{MLE} &amp;= \\bar{X} \\\\ \\widehat{\\sigma^2}_{MLE,o} &amp;= \\frac{n-1}{n}S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\mu_o)^2 \\\\ \\widehat{\\sigma^2}_{MLE} &amp;= \\frac{n-1}{n}S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\,. \\end{align*}\\] Ultimately, we compare the value of \\(W\\) against a chi-square distribution for \\(r-r_o = 1\\) degree of freedom, and thus we reject the null (at level \\(\\alpha = 0.05\\)) if \\(W &gt; 3.841\\) (= qchisq(0.95,1)). 4.7.4 Simulating the Likelihood Ratio Test Wilks’ theorem generates an approximate result\\(-\\)it assumes that the test statistic is chi-square-distributed\\(-\\)and a major issue is that we do not know how good the approximation is. For instance, let’s say \\(n = 20\\)…this might be an insufficient sample size for the Wilks’ theorem machinery to yield an accurate and precise result (at least in terms of a rejection-region boundary). To get a sense as to how well Wilks’ theorem works for us, we can run simulations. We simulate sets of data under the null (\\(\\mu = 5\\)), and for each, we compute \\(W\\). The empirical rejection region boundary is then the, e.g., 95\\(^{\\rm th}\\) percentile of the values of \\(W\\). set.seed(101) alpha &lt;- 0.05 num.sim &lt;- 10000 n &lt;- 20 mu.o &lt;- 5 sigma2 &lt;- 9 # an arbitrary value alpha &lt;- 0.05 W &lt;- rep(NA,num.sim) f &lt;- function(X,n,mu.o) { hat.mu.mle &lt;- mean(X) hat.sigma2.mle &lt;- (n-1)*var(X)/n logL.o &lt;- -(n/2)*log(2*pi*hat.sigma2.mle)-(1/2/hat.sigma2.mle)*sum((X-mu.o)^2) logL &lt;- -(n/2)*log(2*pi*hat.sigma2.mle)-(1/2/hat.sigma2.mle)*sum((X-hat.mu.mle)^2) return(-2*(logL.o - logL)) } for ( ii in 1:num.sim ) { X &lt;- rnorm(n,mean=mu.o,sd=sqrt(sigma2)) W[ii] &lt;- f(X,n,mu.o) } Figure 4.4: The empirical distribution of the statistic \\(-2(\\log\\mathcal{L}_o - \\log\\mathcal{L}_a)\\), with the chi-square distribution for \\(\\Delta r = 1\\) degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks’ theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks’ theorem at best provides approximate results and that simulations can provide more accurate and precise results. In this simulation, we find that the empirical rejection-region boundary is 4.584, which is sufficiently far from the value 3.841 derived from Wilks’ theorem to be concerning. We also find that if we adopt 3.841 as our boundary value, our Type I error is actually 0.071 instead of the expected value of 0.05. The upshot: if \\(n\\) is small, it is best not to assume that \\(W\\) is chi-square-distributed; run simulations to determine rejection region boundaries and \\(p\\)-values instead. 4.8 The Gamma Distribution The gamma distribution is a continuous distribution that is commonly used to, e.g., model the waiting times between discrete events. Its probability density function is given by \\[ f_X(x) = \\frac{x^{\\alpha-1}}{\\beta^\\alpha} \\frac{\\exp\\left(-x/\\beta\\right)}{\\Gamma(\\alpha)} \\,, \\] where \\(x \\in [0,\\infty)\\), \\(\\alpha\\) and \\(\\beta\\) are both \\(&gt;\\) 0, and \\(\\Gamma(\\alpha)\\) is the gamma function: \\[ \\Gamma(\\alpha) = \\int_0^\\infty u^{\\alpha-1} e^{-u} du \\,. \\] (See Figure 4.5.) \\(\\alpha\\) and \\(\\beta\\) are referred to as “shape” and “scale” parameters, respectively. The gamma family of distributions exhibits a wide variety of functional shapes and it is the parent family to a number of other distributions, some of which we have met before. One in particular is the exponential distribution, \\[ f_X(x) = \\frac{1}{\\beta} \\exp\\left(-\\frac{x}{\\beta}\\right) \\,, \\] which is a gamma distribution with \\(\\alpha = 1\\). Note how we lead off above by saying that the gamma distribution is commonly used to model the waiting times between discrete events. The exponential distribution specifically models the waiting time between one event and the next in a Poisson process. The number of strong earthquakes that occur in California in one year? That can be modeled as a Poisson random variable. The time that elapses between two consecutive strong earthquakes in California? That can be modeled using the exponential distribution. (For completeness: the Erlang distribution is a generalization of the exponential distribution, in the sense that we can use it to model the waiting time between the \\(i^{\\rm th}\\) and \\((i+\\alpha)^{\\rm th}\\) events, where \\(\\alpha\\) is a positive integer, in a Poisson process.) Distributions Within the Gamma Family of Distributions distribution \\(\\alpha\\) \\(\\beta\\) exponential 1 \\((0,\\infty)\\) Erlang \\(\\{1,2,3,\\ldots\\}\\) \\((0,\\infty)\\) chi-square \\(\\{1/2,1,3/2,\\ldots\\}\\) 2 Let’s conclude this section by repeating the exercise we did in the last chapter while discussing the beta distribution, the one in which we examined the functional form of the likelihood function \\(\\mathcal{L}(p \\vert k,x)\\). Here, we write down the Poisson likelihood function \\[ \\mathcal{L}(\\lambda \\vert x) = \\frac{\\lambda^x}{x!} e^{-\\lambda} \\] and compare it with the gamma pdf. We can match the gamma pdf if we map the Poisson \\(\\lambda\\) to the gamma \\(x\\), the Poisson \\(x\\) to the gamma \\(\\alpha-1\\), and we set \\(\\beta\\) to 1. But because the Poisson \\(x\\) is an integer with values \\(\\{0,1,2,\\ldots\\}\\), we find that the integrand specifically matches the Erlang pdf, for which \\(\\alpha = \\{1,2,3,\\ldots\\}\\). So, if we observe a random variable \\(X \\sim\\) Poisson(\\(\\lambda\\)), then the likelihood function \\(\\mathcal{L}(\\lambda \\vert x)\\) has the shape (and normalization!) of a Gamma(\\(x+1,1\\)) (or Erlang(\\(x+1\\))) distribution. (About the normalization: if we integrate the likelihood function over its domain, we find that \\[ \\frac{1}{x!} \\int_0^\\infty \\lambda^x e^{-\\lambda} d\\lambda = \\frac{1}{x!} \\Gamma(x+1) = \\frac{x!}{x!} = 1 \\,. \\] The mathematics works out because \\(x\\) is integer valued and thus \\(\\Gamma(x+1) = x!\\).) Figure 4.5: Three examples of gamma probability density functions: Gamma(2,2) (solid red line), Gamma(4,2) (dashed green line), and Gamma(2,3) (dotted blue line). 4.8.1 The Expected Value of a Gamma Random Variable The expected value of a gamma random variable is found by introducing constants into the expected value integral so that a gamma pdf integrand is formed. Specifically \\[\\begin{align*} E[X] = \\int_0^\\infty x f_X(x) dx &amp;= \\int_0^\\infty x \\frac{x^{\\alpha-1}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha+1)} \\frac{\\beta^{\\alpha+1}}{\\beta^{\\alpha+1}} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^{\\alpha+1}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha+1)} \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)} \\frac{\\beta^{\\alpha+1}}{\\beta^\\alpha} dx \\\\ &amp;= \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)} \\frac{\\beta^{\\alpha+1}}{\\beta^\\alpha} \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^{\\alpha+1}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha+1)} dx \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{\\Gamma(\\alpha)} \\beta \\times 1 \\\\ &amp;= \\alpha \\beta \\,. \\end{align*}\\] By introducing the constants, we are able to transform the integrand to that of a Gamma(\\(\\alpha+1,\\beta\\)) distribution, and because the integral is over the entire domain of a gamma distribution, the integral evaluates to 1. A similar calculation involving the derivation of \\(E[X^2]\\) allows us to determine that the variance of a gamma random variable is \\(V[X] = \\alpha \\beta^2\\). 4.8.2 The Distribution of the Sum of Exponential Random Variables As stated above, the exponential distribution, i.e., the gamma distribution with \\(\\alpha = 1\\), is used to model the waiting time between two successive events in a Poisson process. Let’s assume that we have recorded \\(n\\) separate times between \\(n\\) separate pairs of events. What is the distribution of \\(T = T_1 + \\cdots + T_n\\)? As we do when faced with a linear function of \\(n\\) iid random variables, we utilize the method of moment-generating functions: \\[ m_T(t) = \\prod_{i=1}^n m_{T_i}(t) \\,, \\] where \\(m_{T_i}(t) = (1-\\beta t)^{-1}\\) is the mgf for the exponential distribution. Thus \\[ m_T(t) = \\prod_{i=1}^n (1-\\beta t)^{-1} = (1-\\beta t)^{-n} \\,. \\] This has the form of the mgf for a Gamma(\\(n,\\beta\\)) distribution, or, equivalently, an Erlang(\\(n,\\beta\\)) distribution. In other words the sum of \\(n\\) iid waiting times has the same distribution as the waiting time between the \\(i^{\\rm th}\\) and \\((i+n)^{\\rm th}\\) events of a Poisson process. 4.8.3 Memorylessness and the Exponential Distribution An important feature of the exponential distribution is that when we use it to model, e.g., the lifetimes of components in a system, it exhibits memorylessness. In other words, if \\(T\\) is the random variable representing a component’s lifetime, where \\(T \\sim\\) Exponential(\\(\\beta\\)) and where \\(E[T] = \\beta\\), it doesn’t matter how old the component is when we first examine it: from that point onward, the average lifetime will be \\(\\beta\\). Let’s demonstrate how this works. We examine a component “born” at time \\(t_0=0\\) at a later time \\(t_1\\), and we wish to determine the probability that it will live beyond an even later time \\(t_2\\). In other words, we wish to compute \\[ P(T \\geq t_2-t_0 \\vert T \\geq t_1-t_0) \\,. \\] (We know the component lived to time \\(t_1\\), hence the added condition.) Let \\(T \\sim\\) Exponential(\\(\\beta\\)). Then \\[\\begin{align*} P(T \\geq t_2-t_0 \\vert T \\geq t_1-t_0) &amp;= \\frac{P(T \\geq t_2-t_0 \\cap T \\geq t_1-t_0)}{P(T \\geq t_1-t_0)} \\\\ &amp;= \\frac{P(T \\geq t_2-t_0)}{P(T \\geq t_1-t_0)} \\\\ &amp;= \\frac{\\int_{t_2-t_0}^\\infty (1/\\beta) \\exp(-t/\\beta) dt}{\\int_{t_1-t_0}^\\infty (1/\\beta) \\exp(-t/\\beta) dt} \\\\ &amp;= \\frac{\\left. -\\exp(-t/\\beta) \\right|_{t_2-t_0}^\\infty}{\\left. -\\exp(-t/\\beta) \\right|_{t_1-t_0}^\\infty} \\\\ &amp;= \\frac{0 + \\exp(-(t_2-t_0)/\\beta)}{0 + \\exp(-(t_1-t_0)/\\beta)} \\\\ &amp;= \\exp[-(t_2-t_1)/\\beta] = P(T \\geq t_2-t_1) \\,. \\end{align*}\\] Note that \\(t_0\\) drops out of the final result: no matter how long ago \\(t_0\\) might have been, the probability that the component will live \\(t_2-t_1\\) units longer is the same, and the average additional lifetime is still \\(\\beta\\). 4.9 Poisson Regression Suppose that for a given measurement \\(x\\), we record a random variable \\(Y\\) that is a number of counts. For instance, \\(x\\) might be the time of day, and \\(Y\\) might be the observed number of cars parked in a lot at that time. Because \\(Y\\) is (a) integer valued, and (b) non-negative, an appropriate distribution for the random variable \\(Y \\vert x\\) might be the Poisson distribution, and thus to model these data, we may want to pursue the use of Poisson regression. Recall: To implement a generalized linear model (or GLM), we need to do two things: examine the \\(Y_i\\) values and select an appropriate distribution for them (discrete or continuous? what is the functional domain?); and define a link function \\(g(\\theta \\vert x)\\) that maps the line \\(\\beta_0 + \\beta_1 x_i\\), which has infinite range, into a more limited range (e.g., \\([0,\\infty)\\)). Because \\(\\lambda &gt; 0\\), in Poisson regression we adopt a link function that maps \\(\\beta_0 + \\beta_1 x\\) from the range \\((-\\infty,\\infty)\\) to \\((0,\\infty)\\). There is no unique choice of link function, but the conventionally applied one is the logarithm: \\[ g(\\lambda \\vert x) = \\log (\\lambda \\vert x) = \\beta_0 + \\beta_1 x ~~\\implies~~ \\lambda \\vert x = e^{\\beta_0 + \\beta_1 x} \\,. \\] Similar to logistic regression, our goal is to estimate \\(\\beta_0\\) and \\(\\beta_1\\), which is done via numerical optimization of the likelihood function \\[ \\mathcal{L}(\\beta_0,\\beta_1 \\vert \\mathbf{y}) = \\prod_{i=1}^n p_{Y \\vert \\beta_0,\\beta_1}(y_i \\vert \\beta_0,\\beta_1) = \\prod_{i=1}^n \\frac{\\exp(\\beta_0+\\beta_1x_i)^{x_i}}{x_i!} e^{-\\exp(\\beta_0+\\beta_1x_i)} \\,. \\] For the Poisson distribution, \\(E[X] = V[X] = \\lambda\\), so the expectation is that for any given value \\(x\\), \\(E[Y \\vert x] = V[Y \\vert x]\\). However, it is commonly seen in real-life data that the sample variance of \\(Y \\vert x\\) exceeds the sample mean. This is dubbed overdispersion, and it can arise when, e.g., the observed Poisson process is inhomogeneous…or differently stated, when \\(\\lambda\\) varies as a function of space and/or time. A standard way to deal with overdispersion is to move from Poisson regression to negative binomial regression. Before we say more…we note that while the name “negative binomial regression” is technically correct (in the sense that the model assumes that the response data are negative binomially distributed), it can be very confusing for those new to regression, who might view the negative binomial as a distribution that is only useful when, e.g., modeling failures in Bernoulli trials. How could that distribution possibly apply here? The answer is that the negative binomial probability mass function is “just” a function (and as such, it is “allowed” to have more general use than just modeling failures), but more to the point, it is a function that arises naturally when we apply the Law of Total Probability to the Poisson pmf. Let’s suppose that \\(Y \\vert x \\sim\\) Poisson(\\(\\lambda\\)), but that \\(\\lambda\\) itself is a random variable. There is no unique way to model the distribution of \\(\\lambda\\), but the gamma distribution provides a flexible means by which to do so (since the family encompasses a wide variety of shapes, unlike, say, the normal distribution, which can never be skew). Let’s assume that \\(\\lambda \\sim\\) Gamma(\\(\\theta,p/\\theta\\)). The distribution of \\(Y\\) is found with the LoTP: \\[\\begin{align*} p_{Y \\vert \\theta,p}(y \\vert \\theta,p) &amp;= \\int_0^\\infty p_{Y \\vert \\lambda}(y \\vert \\lambda) f_{\\lambda}(\\lambda \\vert \\theta,p) d\\lambda \\\\ &amp;= \\int_0^\\infty \\frac{\\lambda^y}{y!} e^{-\\lambda} \\frac{\\lambda^{\\theta-1}}{(p/\\theta)^\\theta} \\frac{e^{-\\lambda/(p/\\theta)}}{\\Gamma(\\theta)} d\\lambda \\\\ &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\int_0^\\infty \\lambda^{y+\\theta+1} e^{-\\lambda(1+\\theta/p)} d\\lambda \\,. \\end{align*}\\] The integrand looks suspiciously like the integrand of a gamma function integral, but we have to change \\(\\lambda(1+\\theta/p)\\) in the exponential to just \\(\\lambda&#39;\\): \\[ \\lambda&#39; = \\lambda(1+\\theta/p) ~~ \\mbox{and} ~~ d\\lambda&#39; = d\\lambda (1+\\theta/p) \\,. \\] The bounds of the integral do not change. The integral now becomes \\[\\begin{align*} p_{Y \\vert \\theta,p}(y \\vert \\theta,p) &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\frac{1}{(1+\\theta/p)^{y+\\theta}} \\int_0^\\infty (\\lambda&#39;)^{y+\\theta-1} e^{-\\lambda&#39;} d\\lambda&#39; \\\\ &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\frac{1}{(1+\\theta/p)^{y+\\theta}} \\Gamma(y+\\theta) \\\\ &amp;= \\frac{(y+\\theta-1)!}{y! (\\theta-1)!} \\left(\\frac{\\theta}{p}\\right)^\\theta \\left(\\frac{p}{p+\\theta}\\right)^{y+\\theta} \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{p}{p+\\theta}\\right)^y \\left(\\frac{\\theta}{p}\\right)^\\theta \\left(\\frac{p}{p+\\theta}\\right)^\\theta \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{p}{p+\\theta}\\right)^y \\left(\\frac{\\theta}{p+\\theta}\\right)^\\theta \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{\\theta}{p+\\theta}\\right)^\\theta \\left(1 - \\frac{\\theta}{p+\\theta}\\right)^y \\,. \\end{align*}\\] This has the functional form of a negative binomial pmf in which \\(y\\) represents the number of “failures,” \\(\\theta\\) is the number of “successes,” and \\(\\theta/(p+\\theta)\\) is the probability of success. Now, why do we choose this form of the negative binomial distribution? We do so because it so happens that \\[\\begin{align*} E[Y] &amp;= p \\\\ V[Y] &amp;= p + \\frac{p^2}{\\theta} \\,. \\end{align*}\\] (One can derive these quantities starting with, e.g., \\(E[Y] = \\theta(1-p&#39;)/p&#39;\\) and \\(V[Y] = \\theta(1-p&#39;)/(p&#39;)^2\\) and plugging in \\(p&#39; = \\theta/(p+\\theta)\\).) Varying \\(\\theta\\) thus allows us to model the overdispersion with a single variable. (We assume that \\(p\\) takes the place of \\(\\lambda\\), in the sense that now \\(p \\vert x = \\exp(\\beta_0 + \\beta_1x\\)).) In the limit that \\(\\theta \\rightarrow \\infty\\), negative binomial regression becomes Poisson regression. Because the overdispersion is represented in a single variable, we can examine the results of learning both Poisson and negative binomial regression models to determine whether or not the quality of fit improves enough to justify the extra model complexity. 4.9.1 Revisiting the Death-by-Horse-Kick Example Modeling Bortkiewicz’s horse-kick dataset provides a simple example of the use of Poisson regression. x &lt;- 0:4 Y &lt;- c(109,65,22,3,1) poi.out &lt;- glm(Y~x,family=poisson) summary(poi.out) ## ## Call: ## glm(formula = Y ~ x, family = poisson) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.80136 0.08490 56.55 &lt;2e-16 *** ## x -0.92213 0.07704 -11.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 232.430 on 4 degrees of freedom ## Residual deviance: 12.437 on 3 degrees of freedom ## AIC: 38.911 ## ## Number of Fisher Scoring iterations: 5 The summary output from the Poisson regression model is, in its structure, identical to that of logistic regression. But there are some differences in how values are defined. For instance, the deviance residual is \\[ d_i = \\mbox{sign}(Y_i - \\hat{Y}_i) \\sqrt{2[Y_i \\log (Y_i/\\hat{Y}_i) - (Y_i - \\hat{Y}_i)]} \\,, \\] where \\[ \\hat{Y}_i = \\hat{\\lambda}_i = \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i) \\,. \\] (Note that when \\(Y_i = 0\\), \\(Y_i \\log (Y_i/\\hat{Y}_i)\\) is assumed to be zero.) Because there are only five data points in the fit, all the deviance residual values are displayed, rather than a five-number summary. Also, the residual deviance (here, 12.437) is not \\(-2 \\log \\mathcal{L}_{max}\\), as it was for logistic regression. There are two ways to determine \\(\\mathcal{L}_{max}\\); one is to take the AIC value (38.911), subtract 2 times the number of model terms (2 here, thus yielding 34.911), and then dividing by \\(-2\\). The more straightforward way, however, is to utilize the logLik() function: logLik(poi.out) ## &#39;log Lik.&#39; -17.45551 (df=2) As we did in the context of logistic regression, we can use the the difference in the values of the null and residual deviances (here, 232.430-12.437 = 219.993) to test the null hypothesis that \\(\\beta_1 = 0\\). The difference is assumed to be sampled from a chi-square distribution for 4-3 = 1 degree of freedom. The \\(p\\)-value is 1 - pchisq(220.0,1) or effectively zero: we emphatically reject the null hypothesis that \\(\\beta_1 = 0\\). As a final note, unlike the case of logistic regression where determining the quality of fit of the learned model is not particularly straightforward, for Poisson regression we can simply assume that the residual deviance is chi-square-distributed for the given number of degrees of freedom. Here, the \\(p\\)-value is 1 - pchisq(12.437,3) ## [1] 0.006026712 or 0.0060. Because this value is less than, e.g., \\(\\alpha = 0.05\\), we would (in this instance) reject the null hypothesis that the observed data are truly Poisson distributed. 4.9.2 Negative Binomial Regression Example In the code chunk below, we simulate 100 data at each of four different values of \\(x\\): 1, 2, 3, and 4. The data are simulated with a Poisson overdispersion factor of 2. set.seed(236) n &lt;- 100 # 100 data per x value x &lt;- rep(c(1,2,3,4),n) Y &lt;- rep(NA,length(x)) for ( ii in 1:length(x) ) { Y[ii] &lt;- rpois(1,rgamma(1,2,scale=x[ii]/2)) } For these data, \\(E[Y \\vert x] = x\\) and \\(V[Y \\vert x] = x + x^2/2\\), meaning that the overdispersion factor is, again, \\(\\theta = 2\\). Let’s see how overdispersion affects the learning of a Poisson regression model. First, the Poisson regression model itself: summary(glm(Y~x,family=poisson)) ## ## Call: ## glm(formula = Y ~ x, family = poisson) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.08336 0.09240 -0.902 0.367 ## x 0.38014 0.02944 12.913 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1107 on 399 degrees of freedom ## Residual deviance: 930 on 398 degrees of freedom ## AIC: 1805 ## ## Number of Fisher Scoring iterations: 5 …and second, the negative binomial regression model, as learned using the glm.nb() function of the MASS package. (Note that MASS does not represent the state of Massachusetts, but rather stands for “Modern Applied Statistics with S”…with S being the precursor software to R.) library(MASS) summary(glm.nb(Y~x)) ## ## Call: ## glm.nb(formula = Y ~ x, init.theta = 1.84882707, link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.10769 0.13101 -0.822 0.411 ## x 0.38908 0.04483 8.679 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.8488) family taken to be 1) ## ## Null deviance: 530.52 on 399 degrees of freedom ## Residual deviance: 454.45 on 398 degrees of freedom ## AIC: 1633.9 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.849 ## Std. Err.: 0.258 ## ## 2 x log-likelihood: -1627.868 The negative binomial model is displayed in Figure 4.6. When we compare the output, we first look for the lines beginning with AIC: AIC: 1805 [Poisson regression] AIC: 1633.9 [negative binomial regression] The Akaike Information Criterion, or AIC, as we will recall, is a quality-of-fit metric that penalizes model complexity. If we learn a suite of models, we would generally adopt that associated with the lowest AIC value. Here, the negative binomial model has a much lower AIC value than the Poisson model, so we would definitely adopt it! But is the negative binomial model a good model in an absolute sense? To determine that, we would compare the residual deviance value of 454.45 against a chi-square distribution with 398 degrees of freedom; the result is 1 - pchisq(454.45,398) = 0.026 We would reject the null hypothesis that the negative binomial model is the correct representation of the data-generating process, but we note that the \\(p\\)-value is only slightly smaller than 0.05: the model may not be technically “correct,” but it appears to be at least approximately correct! The estimated value of the overdispersion parameter is \\(\\hat{\\theta} = 1.849\\), with estimated standard error \\(0.258\\). This is consistent with the true value \\(\\theta = 2\\) and is definitely inconsistent with \\(\\theta = \\infty\\) (the value for truly Poisson-distributed data). (Note that when the data are not overdispersed, \\(\\hat{\\theta}\\) will usually be a large number, but not actually infinity.) Figure 4.6: The negative binomial regression line superimposed on the simulated data. The jitter() function is applied to the \\(x\\) values to allow us to more easily see the number of counts as a function of \\(x\\) and \\(Y\\). 4.10 Chi-Square-Based Hypothesis Testing In the previous chapter, we introduced the chi-square goodness-of-fit test as a way of conducting hypothesis tests given multinomial data. Given \\(k\\) data recorded in \\(m\\) separate bins, we can compute the test statistic \\[ W = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} \\,, \\] where \\(X_i\\) is the number of data observed in bin \\(i\\), and where \\(p_i\\) is the probability of any one datum being observed in that bin under the null. If \\(k\\) is sufficiently large, then \\(W\\) converges in distribution to a chi-square random variable for \\(m-1\\) degrees of freedom. (Hence the name of the test!) But: what if the overall observed number of data \\(X\\) is a random variable instead of being a set constant \\(X=k\\)? Imagine a very simple digital camera that has four light collecting bins, each of the same size, and we point it towards a light source that emits an average of \\(\\lambda\\) photons in a particular time period. If we open the shutter for that time period, what we would observe is \\(X \\sim\\) Poisson(\\(\\lambda\\)) photons, with the numbers in each bin being \\(\\{X_1,X_2,X_3,X_4\\}\\). Now let’s say we want to test the hypothesis that the probability of a photon falling into each bin is the same, i.e., \\(H_o: p_1 = p_2 = p_3 = p_4 = 1/4\\). If we were to use the chi-square GoF test directly, we would compute \\[ w_{obs} = \\sum_{i=1}^4 \\frac{(X_i - \\lambda p_i)^2}{\\lambda p_i} \\] and then compute the \\(p\\)-value \\(1-F_W(w_{obs})\\) assume 3 degrees of freedom. (In R, this would be computed via 1 - pchisq(w.obs,3).) We can do this\\(-\\)the computer will not stop us from doing so\\(-\\)but is this valid? The answer is that this is valid so long as each \\(X_i\\) converges in distribution to a normal random variable. And Poisson random variables do converge in distribution to normal random variables. Thus, in short, yes, use of the chi-square GoF test is valid if \\(\\lambda p_i\\) is large. The question is, how large? The Poisson probability mass function is \\[ p_X(x \\vert \\lambda) = \\frac{\\lambda^x}{x!} e^{-\\lambda} \\,. \\] We note that the normal probability density function does not have a factorial in it, so we will start by using Stirling’s approximation: \\[ x! \\approx \\sqrt{2 \\pi x} x^x e^{-x} \\,. \\] This approximation has an error of 1.65% for \\(x = 5\\) and 0.83% for \\(x = 10\\), with the percentage error continuing to shrink as \\(x \\rightarrow \\infty\\). With this approximation, we can write that \\[ p_X(x \\vert \\lambda) \\approx \\frac{\\lambda^x}{\\sqrt{2 \\pi x}} x^{-x} e^{x-\\lambda} \\,. \\] This still does not quite look like a normal pdf. So there is more work to do. We compute the logarithm of this quantity: \\[ \\log p_X(x \\vert \\lambda) \\approx x \\log \\lambda - \\frac{1}{2} \\log (2 \\pi x) - x \\log x + x - \\lambda = -x ( \\log x - \\log \\lambda ) - \\frac{1}{2} \\log (2 \\pi x) + x - \\lambda \\,, \\] and then look at \\(\\log x - \\log \\lambda\\): \\[ \\log x - \\log \\lambda = \\log \\frac{x}{\\lambda} = \\log \\left( 1 - \\frac{\\lambda-x}{\\lambda}\\right) \\approx -\\frac{\\delta}{\\sqrt{\\lambda}} - \\frac{\\delta^2}{2 \\lambda} - \\cdots \\,. \\] Here, \\(\\delta = (\\lambda - x)/\\sqrt{\\lambda}\\). Plugging this result into the expression for \\(\\log p_X(x \\vert \\lambda)\\), we find that \\[ \\log p_X(x \\vert \\lambda) \\approx -\\frac{1}{2} \\log (2 \\pi x) + x \\left( \\frac{\\delta}{\\sqrt{\\lambda}} + \\frac{\\delta^2}{2\\lambda} \\right) - \\delta \\sqrt{\\lambda} \\,. \\] The next step is plug in \\(x = \\lambda - \\sqrt{\\lambda}\\delta\\): \\[\\begin{align*} \\log p_X(x \\vert \\lambda) &amp;\\approx -\\frac{1}{2} \\log (2 \\pi x) + (\\lambda - \\sqrt{\\lambda}\\delta)\\left( \\frac{\\delta}{\\sqrt{\\lambda}} + \\frac{\\delta^2}{2\\lambda} \\right) - \\delta \\sqrt{\\lambda} \\\\ &amp;= -\\frac{1}{2} \\log (2 \\pi x) + \\sqrt{\\lambda}\\delta - \\delta^2 + \\frac{\\delta^2}{2} - \\frac{\\delta^3}{2 \\lambda^{3/2}} - \\sqrt{\\lambda}\\delta \\\\ &amp;\\approx -\\frac{1}{2} \\log (2 \\pi x) - \\frac{\\delta^2}{2} \\,, \\end{align*}\\] where we drop the \\(\\mathcal{O}(\\delta^3)\\) term. When we exponentiate both sides, the final result is \\[ p_X(x \\vert \\lambda) \\approx \\frac{1}{\\sqrt{2 \\pi x}} \\exp\\left( -\\frac{\\delta^2}{2} \\right) = \\frac{1}{\\sqrt{2 \\pi x}} \\exp\\left( -\\frac{(x-\\lambda)^2}{2\\sqrt{\\lambda}} \\right) \\,. \\] This has the (approximate) form of a normal pdf, at least for values \\(x \\approx \\lambda\\). So, in the end, the Poisson probability mass function \\(p_X(x \\vert \\lambda)\\) has approximately the same shape as the normal probability density function \\(f_X(x \\vert \\mu=\\lambda,\\sigma^2=\\lambda)\\) for \\(x \\gg 1\\) and \\(x \\approx \\lambda\\). The conventional rule-of-thumb is that one can utilize the chi-square GoF test with Poisson data so long as \\(\\lambda p_i \\geq 5\\) counts in each bin. 4.10.1 Revisiting the Death-by-Horse-Kick Example In previous examples, we have shown that over the course of 20 years, in 10 separate Prussian army corps, soldiers died as a result of horse kicks at a rate of \\(\\hat{\\lambda} = \\bar{X} = 0.61\\) deaths per corps per year, and that a 95% confidence interval for \\(\\lambda\\) is [0.51,0.73]. However, we never examined what is perhaps the most important question of all: is it plausible that the data are Poisson-distributed in the first place? We last looked at the idea of performing hypothesis tests regarding distributions in Chapter 2, where we introduce the Kolmogorov-Smirnov test for use with arbitrary distributions and the Shapiro-Wilk test to assess the plausibility that our data are normally distributed. So it would seem that here we should work with the KS test, as the Poisson distribution is not the normal distribution…but we can only use the KS test in the context of continuous distributions. So we need a new method! We can utilize the chi-square goodness-of-fit test. Let’s assume \\(\\lambda = 0.61\\). Then the probabilities \\(P(X=x)\\) are as follows: x P(X=x) \\(E_x\\) \\(O_x\\) 0 0.543 108.67 109 1 0.331 66.29 65 2 0.101 20.29 22 3 0.021 4.11 3 4 0.003 0.63 1 The conventional rule-of-thumb is that the expected number of counts in each bin must be \\(\\geq 5\\). Here, we will break that rule slightly by combining bins 3 and 4 into one bin with expected counts 4.11 + 0.63 = 4.74. The chi-square GOF test statistic is thus \\[ W = \\frac{(109-108.67)^2}{108.67} + \\frac{(65-66.29)^2}{66.29} + \\frac{(22-20.29)^2}{20.29} + \\frac{(4-4.74)^2}{4.74} = 0.298 \\,. \\] This figure can be found using R: e &lt;- 200*dpois(0:4,0.61) e[4] &lt;- e[4]+e[5] e &lt;- e[-5] o &lt;- c(109,65,22,4) (W = sum((o-e)^2/e)) ## [1] 0.2980418 When using the chi-square GOF test to assess the viability of a distribution whose parameters are estimated, we lose additional degrees of freedom, so the number of degrees of freedom here is 4 - 2 = 2. The \\(p\\)-value is 1 - pchisq(W,2) ## [1] 0.8615511 We find that we fail to reject the null hypothesis and thus that it is plausible that Bortkiewicz’s horse-kick data are indeed Poisson-distributed. 4.11 Exercises Suppose that you receive phone calls at a rate of 2 per hour. (a) Let \\(X\\) be the number of phone calls received in a randomly selected 15-minute period. Compute \\(P(\\mu \\leq X \\leq \\mu+2\\sigma)\\). (b) Let \\(Y\\) be the length of time between when the phone first rings from one call and when the phone first rings for the next call. What is \\(P(Y &gt; 1/2 \\, \\vert \\, Y &gt; 1/4)\\)? (c) When the phone rings, there is probability 0.2 that the caller is your friend, who you always talk to for 10 minutes. (All other calls are one minute in length.) What is the expected overall time spent on the phone during any given set of 10 phone calls? A particular basketball player shoots, on average, one time every two minutes, and exactly half of her shots are successful. In an upcoming game, she is to play for exactly eight minutes. (a) What is the probability that the player will have one successful shot, or less? (b) What is the probability that the number of successful shots will be within one standard deviation of the mean? A particular event happens, on average, three times per year in California, four times per year in Nevada, and one time per year in Arizona. Instances of these events are independent of each other. What the mean and variance for the total number of events, summed over the three states, that would be observed in a single two-year window of time? We sample \\(n\\) iid data from the following distribution: \\[\\begin{align*} f_X(x) = \\frac{x}{\\sigma^2} e^{-x^2/(2\\sigma^2)} \\,, \\end{align*}\\] where \\(x \\geq 0\\) and \\(\\sigma &gt; 0\\). For this distribution, \\(E[X] = \\sigma\\sqrt{\\pi/2}\\) and \\(V[X] = (4-\\pi)\\sigma^2/2\\). (a) What is \\(E[X^2]\\)? (b) What is the method of moments estimator for \\(\\sigma\\) that utilizes the first population and sample moments? (c) What is the bias of \\(\\hat{\\sigma}_{MoM}\\)? (d) What is the variance of \\(\\hat{\\sigma}_{MoM}\\)? (e) What is the method of moments estimator of \\(\\sigma^2\\) that utilizes the second population and sample moments? We sample a datum \\(X\\) from a geometric distribution. (a) Find a method-of-moments estimator of \\(p\\), based on the first population and sample moments. (b) Find a second method-of-moments estimator of \\(p\\), using the second moments. We sample \\(n\\) iid data from a beta distribution with known value \\(\\alpha\\). (a) Write down the method-of-moments estimator for \\(\\beta\\) that is based on the first sample moment. (b) Can we use our answer to part (a) to write down the method-of-moments estimator for \\(\\sqrt{\\beta}\\)? If yes, write down that estimator. We have been given the code shown below for computing a 95% one-sided lower bound on \\(\\lambda\\), given \\(n\\) iid data sampled from a Poisson distribution. Apparently someone was in a hurry. (a) Is the argument list for f valid as is? If not, specify how it should be fixed. (b) Is the input to ppois() valid as is? If not, specify how it should be fixed. (c) The code writer forgot to “FIX” the input value of q in the uniroot() call. Specify the number that should go here. (d) Will the coverage of the confidence intervals found with a properly fixed code be exactly 95%, greater than or equal to 95%, or less than or equal to 95%? X &lt;- ... # this is a vector of n iid Poisson r.v.&#39;s n &lt;- length(X) f &lt;- function(X,lambda,n,q) { ppois(mean(X),lambda) - q } uniroot(f,interval=c(0.001,1000),X=X,n=n,q=FIX)$root The Bernoulli probability mass function is \\[\\begin{align*} p_X(x) = p^x (1-p)^{1-x} \\,, \\end{align*}\\] where \\(x \\in \\{0,1\\}\\) and \\(p \\in (0,1)\\). Assume that we draw \\(n\\) iid data from this distribution. (a) Write down the likelihood ratio test statistic \\(\\lambda_{LR}\\) that arises when testing the hypothesis \\(H_o : p = p_o\\) versus the hypothesis \\(H_a : p \\neq p_o\\). (b) In this situation, is the LRT the most powerful test? In an experiment, you sample one datum from a Poisson distribution; the value of that datum is \\(x_{\\rm obs}\\). You wish to test \\(H_o : \\lambda = \\lambda_o\\) versus the alternative \\(H_a : \\lambda \\neq \\lambda_o\\), and you will do this by utilizing the likelihood ratio test. However, instead of identifying a sufficient statistic and constructing an exact test, you decide to use Wilks’ theorem. (In real life: don’t do this…construct an exact test.) (a) Write down \\(\\hat{\\lambda}_{MLE}\\). (b) Write down an expression for \\(\\lambda_{LR}\\). (c) Use the answer for part (a) to write down an expression for the Wilks’ theorem test statistic. (d) Regarding the statistic written down for part (c): what is its sampling distribution? Give the name and the values of its parameters. In an experiment, you sample \\(n\\) iid data from a Poisson distribution with unknown mean \\(\\lambda\\). You wish to conduct a likelihood-ratio test for which \\(H_a : \\lambda &lt; \\lambda_o\\). Assume \\(\\alpha = 0.05\\). (a) Write down the sufficient statistic for \\(\\lambda\\) that appears directly in likelihood factorization. (b) Write down the sampling distribution for the statistic you found in part (a), the name and the value(s) of its parameters. (c) Write down the appropriate null hypothesis, given the stated alternative (and given that you are performing an LRT). (d) Write down the rejection region, using R code (e.g., qnorm(0.975,mean=mu.o,sd=sqrt(sigma2))). Assume the null value is to be written as lambda.o, and that a number should be provided for \\(q\\). You are given \\(n\\) iid data sampled from an exponential distribution with pdf \\[\\begin{align*} f_X(x \\vert \\beta) = \\frac{1}{\\beta}e^{-x/\\beta} \\,, \\end{align*}\\] where \\(x &gt; 0\\) and \\(\\beta &gt; 0\\). (a) We wish to test \\(H_o : \\beta = 1\\) versus \\(H_a : \\beta \\neq 1\\) at level \\(\\alpha = 0.05\\). Write down an expression for the likelihood ratio test statistic \\(\\lambda\\). Recall that the MLE for \\(\\beta\\) is \\(\\bar{X}\\). (b) Assume that you do not know the sampling distribution for any sufficient statistic that you can define. Use Wilks’ theorem to write down a rejection region for this test. Suppose you flipped a coin 1000 times and observe heads 550 times. You wish to conduct a likelihood-ratio test to check whether this coin is fair, at the level \\(\\alpha = 0.05\\). (a) Specify the null and alternative hypotheses. (b) What are the corresponding sets \\(\\Theta_0\\) and \\(\\Theta_a\\)? (c) Determine the number of free parameters \\(r_0\\) and \\(r\\) in \\(\\Theta_0\\) and \\(\\Theta = \\Theta_0\\, \\cup\\, \\Theta_a\\), respectively. (d) Determine the likelihood ratio test statistic, and its value for the observed data. (e) Assume that you do not know the sampling distribution for any sufficient statistic that you can define, and use Wilks’ theorem to determine the corresponding \\(p\\)-value. (f) Give your conclusion. Is the coin fair? You are given the following probability density function: \\[\\begin{align*} f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{27}{16} x^2 e^{-\\frac{3x}{2}} &amp; x &gt; 0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] What are \\(E[X]\\) and \\(V[X]\\)? Assume you are working with a gamma distribution for which \\(E[X] = 5\\) and \\(V[X] = 10\\). (a) What are the values of \\(\\alpha\\) and \\(\\beta\\)? (b) What other name is given to this particular distribution, given the values of \\(\\alpha\\) and \\(\\beta\\) you derived in (a)? Compute \\(E[X^{-1}]\\) for a chi-square distribution, leaving your answer in terms of the number of degrees of freedom \\(\\nu\\). The inverse-gamma distribution has the probability density function \\[\\begin{align*} f_X(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{1}{x^{\\alpha+1}} e^{-\\beta/x} \\,, \\end{align*}\\] for \\(x &gt; 0\\) and \\(\\alpha,\\beta &gt; 0\\). Derive \\(E[X]\\) for this distribution. Your final answer should be in terms of \\(\\alpha\\) and \\(\\beta\\) only (with no gamma or beta functions appearing in the answer). Let \\(X\\) be a random variable sampled from a Gamma(3/2,\\(\\beta\\)) distribution. (a) If \\(\\beta = 2\\), what sub-family of gamma distributions is \\(X\\) sampled from? (b) Now, assume \\(\\beta\\) is unknown, and that we sample \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\). What is a sufficient statistic for \\(\\beta\\)? (c) What is the MVUE for \\(\\beta\\)? (d) What is the method of moments estimator for \\(\\beta\\) that utilizes the first population and sample moments? (e) What is the bias of \\(\\hat{\\beta}_{MoM}\\)? What is \\(E[X^{1/2}]\\) for the random variable \\(X \\sim\\) Gamma(\\(\\alpha,\\beta\\))? Below is (edited) output from the application of Poisson and negative binomial regression to a dataset in R: (a) Identify the value for the overdispersion parameter. (b) Is the value of the overdispersion parameter such that we can conclude that the data are indeed overdispersed? (c) What procedure utilizes the two deviance values in each model to make inferences about one or more of the model coefficients? (d) What specific conclusion can we make given both the procedure in (c) and the deviance values that are observed? Recall what the null and alternative hypotheses are, and state a conclusion based on these and the observed deviance values. "],["the-uniform-distribution.html", "5 The Uniform Distribution 5.1 Properties 5.2 Linear Functions of Uniform Random Variables 5.3 Sufficient Statistics and the Minimum Variance Unbiased Estimator 5.4 Maximum Likelihood Estimation 5.5 Confidence Intervals 5.6 Hypothesis Testing 5.7 Exercises", " 5 The Uniform Distribution Let \\(\\{X_1,\\ldots,X_n\\}\\) be \\(n\\) independent and identically distributed data sampled according to a distribution \\(P(\\theta)\\). \\(\\theta\\) is a domain-specifying parameter if, e.g., all the sampled data have values \\(\\leq \\theta\\). This chapter is all about the quirks that we observe when we work with domain-specifying parameters, ones that affect the determination of maximum-likelihood and minimum-variance unbiased estimators and how we go about constructing hypothesis tests, etc. 5.1 Properties The uniform distribution is often used within the realm of probability, in part because of its utility and in part because of its simplicity. We briefly touched upon this distribution at times earlier in this book, such as, for instance, when we talked about hypothesis test \\(p\\)-values (which are distributed uniformly between 0 and 1 when the null hypothesis is correct). Why do we return to the uniform distribution now? Because it is slightly different from other distributions: its two parameters, often denoted \\(a\\) and \\(b\\) (where \\(b &gt; a\\)), do not dictate the shape of its probability density function, but rather its domain. This affects aspects of estimation, such as determining sufficient statistics and deriving maximum likelihood estimates, etc. We highlight these “quirks” of the uniform pdf throughout this chapter. Recall: a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) \\(f_X(x) \\geq 0\\) and (b) \\(\\int_x f_X(x) dx = 1\\), where the integral is over all values of \\(x\\) in the dist ribution’s domain. The uniform pdf is defined as \\[ f_X(x) = \\frac{1}{b-a} ~~\\mbox{where}~~ x \\in [a,b] \\,. \\] \\(f_X(x)\\) is thus constant between \\(a\\) and \\(b\\). (See Figure 5.1.) This means that we can think of the uniform distribution “geometrically,” as the following is true: \\[ \\underbrace{(b-a)}_{\\mbox{domain}} \\cdot \\underbrace{\\frac{1}{b-a}}_{f_X(x)} = 1 \\] If we know the domain of the pdf, we immediately know \\(f_X(x)\\); conversely, if we know \\(f_X(x)\\), we immediately know the width of the domain (but not \\(a\\) and \\(b\\) themselves). Figure 5.1: Three examples of uniform probability mass functions: Uniform(0,1) (solid red line), Uniform(0.5,2) (dashed green line), and Uniform(-1.5,1.5) (dotted blue line). Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as \\(F_X(x) = \\int_{y \\leq x} f_Y(y) dy\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). The cdf for a uniformly distributed random variable is \\[ F_X(x) = \\int_a^x f_Y(y) dy = \\int_a^x \\frac{1}{b-a} dy = \\frac{x-a}{b-a} ~~ x \\in [a,b] \\,, \\] with a value of 0 for \\(x &lt; a\\) and 1 for \\(x &gt; b\\). (We can quickly confirm that the derivative of the cdf yields the pdf. Recall that for continuous distributions, \\(f_X(x) = dF_X(x)/dx\\).) Recall: an inverse cdf function \\(F_X^{-1}(q)\\) takes as input a distribution quantile \\(q \\in [0,1]\\) and returns the value of \\(x\\) such that \\(q = F_X(x)\\). The inverse cdf is exceptionally simple to compute: \\[ q = \\frac{x-a}{b-a} ~~ \\Rightarrow ~~ x = (b-a)q + a \\,. \\] Technically the inverse cdf has no unique solution when \\(q = 0\\) or \\(q = 1\\). However, it is convention (for instance, within R) that the inverse cdf output for continuous distributions be the largest value for which \\(q = 0\\) and the smallest value for which \\(q = 1\\). Thus, for a Uniform(\\(a,b\\)) distribution, when \\(q = 0\\), then \\(x = a\\), and when \\(q = 1\\), \\(x = b\\). A discrete analogue to the uniform distribution is the discrete uniform distribution, which is defined over a range of integers \\([a,b]\\). (The rolls of a fair, six-sided die would, for instance, be governed by the discrete uniform distribution.) The pmf for the discrete uniform is \\[ p_X(x) = \\frac{1}{n} ~~ x \\in [a,b] \\,, \\] where \\(n = b - a + 1\\) is the number of possible experimental outcomes. The cdf is \\[ F_X(x) = \\frac{\\lfloor x \\rfloor - a + 1}{n} ~~ x \\in [a,b] \\,, \\] where \\(\\lfloor x \\rfloor\\) is the largest integer that is smaller than or equal to \\(x\\), while the inverse cdf is given by the generalized inverse cdf formalism that we’ve previously seen for discrete distributions. Note that there are no standard R functions of the form xdiscunif() for computing the pmf or cdf of the discrete uniform distribution, or for sampling from it. We will show how one can create such functions in an example below. 5.1.1 The Expected Value and Variance of a Uniform Random Variable Recall: the expected value of a continuously distributed random variable is \\[ E[X] = \\int_x x f_X(x) dx\\,, \\] where the integral is over all values of \\(x\\) within the domain of the pdf \\(f_X(x)\\). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(f_X(x)\\). The expected value of a random variable drawn from a Uniform(\\(a,b\\)) distribution is \\[ E[X] = \\int_a^b x f_X(x) dx = \\int_a^b \\frac{x}{b-a} dx = \\frac{1}{b-a} \\left. \\frac{x^2}{2} \\right|_a^b = \\frac{1}{b-a} \\frac{b^2-a^2}{2} = \\frac{1}{b-a} \\frac{(b-a)(b+a)}{2} = \\frac{a+b}{2} \\,. \\] Recall: the variance of a continuously distributed random variable is \\[ V[X] = \\int_x (x-\\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\\,, \\] where the integral is over all values of \\(x\\) within the domain of the pdf \\(f_X(x)\\). The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of \\(x\\) for which \\(f_X(x)\\) is effectively non-zero. To find the variance, we work with the shortcut formula: \\(V[X] = E[X^2] - (E[X])^2\\). We know \\(E[X]\\) already; as for \\(E[X^2]\\), we utilize the Law of the Unconscious Statistician: \\[\\begin{align*} E[X^2] = \\int_a^b x^2 f_X(x) dx &amp;= \\int_a^b \\frac{x^2}{b-a} dx \\\\ &amp;= \\frac{1}{b-a} \\left. \\frac{x^3}{3} \\right|_a^b \\\\ &amp;= \\frac{b^3-a^3}{3(b-a)} \\\\ &amp;= \\frac{(b-a)(a^2+ab+b^2)}{3(b-a)} = \\frac{1}{3}\\left(a^2 + ab + b^2\\right) \\,. \\end{align*}\\] Thus \\[\\begin{align*} V[X] &amp;= \\frac{1}{3}\\left(a^2 + ab + b^2\\right) - \\left(\\frac{a+b}{2}\\right)^2 \\\\ &amp;= \\frac{1}{3}\\left(a^2 + ab + b^2\\right) - \\frac{1}{4}\\left(a^2+2ab+b^2\\right) \\\\ &amp;= \\frac{1}{12}\\left(4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\\right) \\\\ &amp;= \\frac{1}{12}\\left(a^2 - 2ab + b^2 \\right) \\\\ &amp;= \\frac{(a-b)^2}{12} \\,. \\end{align*}\\] 5.1.2 Coding R-Style Functions for the Discrete Uniform Distribution There are four standard functions associated with any distribution: the one prefaced by d that returns the output of the probability mass function or probability density function, given a coordinate \\(x\\); the one prefaced by p that returns the output of the cumulative distribution function, given \\(x\\); the one prefaced q that returns the output of the inverse cdf, given a quantile \\(q \\in [0,1]\\); and the random sampler, a function prefaced by r. For the discrete uniform distribution, one can code the probability mass function as follows: ddiscunif &lt;- function(x,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) if ( x %in% y ) return(1/length(y)) return(0) } ddiscunif(4,min=1,max=6) # assume a fair six-sided die ## [1] 0.1666667 As for the cumulative distribution function: pdiscunif &lt;- function(x,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) w &lt;- which(y&lt;=x) if ( length(w) == 0 ) return(0) return(length(w)/length(y)) } pdiscunif(4,min=1,max=6) ## [1] 0.6666667 The inverse cdf implements the generalized inverse algorithm: qdiscunif &lt;- function(q,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) if ( q == 0 ) return(min(y)) if ( q == 1 ) return(max(y)) cdf &lt;- (1:length(y))/length(y) w &lt;- which(cdf&gt;=q) if ( length(w) == 0 ) return(max(y)) return(y[min(w)]) } qdiscunif(0.55,min=1,max=6) ## [1] 4 And last, the random data generator: rdiscunif &lt;- function(n,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) s &lt;- sample(length(y),n,replace=TRUE) return(y[s]) } set.seed(235) # set to ensure consistent output rdiscunif(10,min=1,max=6) ## [1] 6 5 5 6 2 1 5 1 3 6 5.2 Linear Functions of Uniform Random Variables Let’s assume that we are given \\(n\\) iid Uniform random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Uniform(\\(a,b\\)). What is the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(E[e^{tX}]\\). If \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) os the mgf for a known family of distributions, then we can immediately identify the distribution of \\(Y\\) and the parameters of that distribution. The mgf for the uniform distribution is \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\int_a^b \\frac{e^{tx}}{b-a} dx \\\\ &amp;= \\frac{1}{b-a} \\left. \\frac{1}{t}e^{tx} \\right|_a^b \\\\ &amp;= \\frac{e^{tb}-e^{ta}}{t(b-a)} \\,. \\end{align*}\\] The mgf for the sum \\(Y = \\sum_{i=1}^n X_i\\) is thus \\[ m_Y(t) = \\prod_{i=1}^n m_{X_i}(t) = \\left( \\frac{e^{tb}-e^{ta}}{t(b-a)} \\right)^n \\,. \\] This expression does not simplify such that we recognize the distribution of \\(Y\\). If \\(a = 0\\) and \\(b = 1\\), it turns out that the mgf does take on the form of that for an Irwin-Hall distribution. An Irwin-Hall random variable converges in distribution to a normal random variable as \\(n \\rightarrow \\infty\\). We are placed in a similar situation if we look at the sample mean \\(\\bar{X} = Y/n\\): \\[ m_{\\bar{X}}(t) = \\prod_{i=1}^n m_{X_i}\\left(\\frac{t}{n}\\right) = \\left( \\frac{n(e^{tb/n}-e^{ta/n})}{t(b-a)} \\right)^n \\,. \\] If \\(a = 0\\) and \\(b = 1\\), \\(\\bar{X}\\) is sampled from the Bates distribution. A Bates random variable converges in distribution to a normal random variable as \\(n \\rightarrow \\infty\\). For all other combinations of \\(a\\) and \\(b\\), we cannot write down a specific functional form for the sampling distribution of \\(\\bar{X}\\) and thus we would have to perform simulations to test hypotheses, etc. (However, we note that because statistical inference for a uniform distribution involves determining the lower and/or upper bounds, we can utilize order statistics for inference instead of \\(\\bar{X}\\). See the next section below.) 5.2.1 The Moment-Generating Function for a Discrete Uniform Distribution The mgf for a discrete uniform random variable is \\[ E[e^{tX}] = \\sum_{x=a}^b e^{tx} p_X(x) = \\frac{1}{n} \\sum_{x=a}^b e^{tx} \\,. \\] We cannot say anything further without making an assumption. If we say that \\(x \\in [a,a+1,\\ldots,b-1,b]\\), i.e., that there are integer steps between the probability masses, then \\[ E[e^{tX}] = \\frac{1}{n} \\sum_{x=a}^b e^{tx} = \\frac{1}{n}e^{ta} \\left( 1 + e^{t(a+1)} + \\cdots + e^{t(b-a)} \\right) \\,. \\] If \\(t\\) is negative, then we can make use of a geometric sum: \\[ 1 + e^t + \\cdots = \\frac{1}{1-e^t} = \\underbrace{1 + \\cdots + e^{t(b-a)}}_{} + \\underbrace{e^{t(b-a+1)} + \\cdots}_{} \\,, \\] where the first underbraced quantity is what appears above in the expected value. Thus we can rearrange terms and write \\[\\begin{align*} 1 + e^{t(a+1)} + \\cdots + e^{t(b-a)} &amp;= \\frac{1}{1-e^t} - \\left( e^{t(b-a+1)} + \\cdots \\right) \\\\ &amp;= \\frac{1}{1-e^t} - e^{t(b-a+1)}\\left(1 + e^t + \\cdots\\right) \\\\ &amp;= \\frac{1}{1-e^t} - \\frac{e^{t(b-a+1)}}{1-e^t} = \\frac{1-e^{t(b-a+1)}}{1-e^t} \\,. \\end{align*}\\] Putting everything together, we find that \\[ m_X(t) = \\frac{1}{n}e^{ta} \\frac{1-e^{t(b-a+1)}}{1-e^t} = \\frac{e^{ta}-e^t(b+1)}{n(1-e^t)} \\,. \\] This is the usual form of the mgf presented for the discrete uniform distribution, but again, this is only valid if the masses are separated by one unit: \\(x \\in [a,a+1,\\ldots,b-1,b]\\). 5.3 Sufficient Statistics and the Minimum Variance Unbiased Estimator Recall: a sufficient statistic for a population parameter \\(\\theta\\) captures all information about \\(\\theta\\) contained in a data sample; no additional statistic will provide more information about \\(\\theta\\). Sufficient statistics are not unique: functions of sufficient statistics are themselves sufficient statistics. Before we discuss sufficient statistics in the context of the uniform distribution, it is useful to (re-)introduce the indicator function. This function, mentioned briefly in Chapter 1, takes on the value 1 if a specified condition is met and 0 otherwise. For instance, \\[ \\mathbb{I}_{x_i \\in [0,1]} = \\left\\{ \\begin{array}{cl} 1 &amp; x_i \\in [0,1] \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] One use for the indicator function is to, well, indicate the domain of a pmf or pdf. For instance, we can write \\[ f_X(x) = \\left\\{ \\begin{array}{ll} e^{-x} &amp; x \\geq 0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\] to express that the exponential distribution with rate \\(\\beta = 1\\) is defined within the domain \\(x \\in [0,\\infty)\\), or, equivalently, we can write \\[ f_X(x) = e^{-x} \\mathbb{I}_{x \\in [0,\\infty)} \\,. \\] The latter form expresses the same information in a more condensed fashion. So…what do indicator functions have to do with uniform distributions? Let’s suppose we sample \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) from a uniform distribution with lower bound 0 and upper bound \\(\\theta\\), and our goal is to define a sufficient statistic for \\(\\theta\\). Let’s work with the factorization criterion: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = g(\\mathbf{x},\\theta) \\cdot h(\\mathbf{x}) \\,. \\] The likelihood is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} = \\frac{1}{\\theta^n} \\,. \\] OK…no…wait, there are no data in this expression, so we cannot define a sufficient statistic. The way around this is to re-express the pdf as \\[ f_X(x) = \\frac{1}{\\theta} \\mathbb{I}_{x \\in [0,\\theta]} \\] and to re-write the likelihood as \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\frac{1}{\\theta^n} \\prod_{i=1}^n \\mathbb{I}_{x \\in [0,\\theta]} \\,. \\] A product of indicator functions will equal 1 if and only if all data lie in the domain \\(x \\in [0,\\theta]\\). This is equivalent to saying that \\(\\theta \\geq X_{(n)}\\), the order statistic representing the maximum observed datum. Thus \\(X_{(n)}\\) is a sufficient statistic for \\(\\theta\\): we know \\(\\theta\\) is greater than this statistic’s value, and none of the data aside from \\(X_{(n)}\\) provide any additional information about \\(\\theta\\). The upshot: when the parameter \\(\\theta\\) dictates (at least in part) the domain of a distribution, the sufficient statistics for \\(\\theta\\) will be functions of an order statistic. When we first introduced the factorization criterion and sufficient statistics back in Chapter 3, we did it so that ultimately we could write down the minimum variance unbiased estimator (or MVUE). Recall: the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If \\(E[\\hat{\\theta}-\\theta] = 0\\), then the estimator \\(\\hat{\\theta}\\) is said to be unbiased. Recall: deriving the minimum variance unbiased estimator involves two steps: factorizing the likelihood function to uncover a sufficient statistic \\(U\\) (that we assume is both minimal and complete); and finding a function \\(h(U)\\) such that \\(E[h(U)] = \\theta\\). When \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim}\\) Uniform(\\(0,\\theta\\)), can we define an MVUE for \\(\\theta\\)? The answer is yes…but we have to recall how we define the pdf of \\(X_{(n)}\\) first. Recall: the maximum of \\(n\\) iid random variables sampled from a pdf \\(f_X(x)\\) has a sampling distribution given by \\[ f_{(n)}(x) = n f_X(x) [ F_X(x) ]^{n-1} \\,, \\] where \\(F_X(x)\\) is the associated cdf. For the Uniform(\\(0,\\theta\\)) distribution, \\[ f_X(x) = \\frac{1}{\\theta} ~~\\mbox{and}~~ F_X(x) = \\int_0^x f_Y(y) dy = \\int_0^x \\frac{1}{\\theta} dy = \\frac{x}{\\theta} \\,, \\] so \\[ f_{(n)}(x) = n \\frac{1}{\\theta} \\left[ \\frac{x}{\\theta} \\right]^{n-1} = n \\frac{x^{n-1}}{\\theta^n} \\,. \\] To find the MVUE, we first compute the expected value of the sufficient statistic \\(X_{(n)}\\): \\[ E[X_{(n)}] = \\int_0^\\theta x n \\frac{x^{n-1}}{\\theta^n} dx = \\left. \\frac{n}{(n+1)\\theta^n} x^{n+1} \\right|_0^\\theta = \\frac{n}{n+1} \\theta \\,, \\] and then rearrange terms: \\[ E\\left[\\frac{n+1}{n}X_{(n)}\\right] = \\theta \\,. \\] Thus \\[ \\hat{\\theta}_{MVUE} = \\frac{n+1}{n}X_{(n)} \\] is the MVUE for \\(\\theta\\). (Note that a similar calculation to this one can be used to determine, e.g., the MVUE for the \\(\\theta\\) when the data are sampled from a Uniform(\\(\\theta,b\\)) distribution.) 5.3.1 The Sufficient Statistic for the Domain Parameter of the Pareto Distribution The Pareto [puh-RAY-toh] distribution, also known in some quarters as the power-law, is \\[ f_X(x) = \\frac{\\alpha \\beta^\\alpha}{x^{\\alpha+1}} \\,, \\] where \\(\\alpha &gt; 0\\) is the shape parameter and \\(x \\in [\\beta,\\infty)\\), where \\(\\beta\\) is the scale (or location) parameter. Let’s assume \\(\\alpha\\) is known. A sufficient statistic for \\(\\beta\\), found via likelihood factorization, is \\[ \\mathcal{L}(\\beta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i) = \\underbrace{\\beta^{n\\alpha}}_{g(\\mathbf{x},\\beta)} \\cdot \\underbrace{\\frac{\\alpha^n}{(\\prod_{i=1}^n x_i)^{\\alpha+1}}}_{h(\\mathbf{x})} \\,. \\] Wait…again, as is the case for the uniform distribution, no data appear in the expression \\(g(\\cdot)\\). So we would go back and introduce an indicator function into the pdf; it should be clear that when we do so, \\(g(\\mathbf{x},\\beta)\\) changes to \\[ g(\\mathbf{x},\\beta) = \\beta^{n\\alpha} \\prod_{i=1}^n \\mathbb{I}_{x_i \\in [\\beta,\\infty)} \\] and thus that because all data have to be larger than \\(\\beta\\), the sufficient statistic is the minimum observed datum, \\(X_{(1)}\\). 5.3.2 MVUE Properties for Uniform Distribution Bounds The properties of estimators that we have examined thus far include the bias (are our estimates offset from the truth, on average?), the variance (over how large a range do our estimates vary?), etc. Let’s look at some of these properties here, assuming we sample \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution. Bias: the MVUE is by definition unbiased, since \\(E[\\frac{n+1}{n}X_{(n)}] = \\theta\\). Variance: the variance of \\(\\hat{\\theta}_{MVUE}\\) is \\[\\begin{align*} V[\\hat{\\theta}_{MVUE}] &amp;= E\\left[\\left(\\hat{\\theta}_{MVUE}\\right)^2\\right] - \\left( E\\left[ \\hat{\\theta}_{MVUE} \\right] \\right)^2 \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( E[X_{(n)}^2] - (E[X_{(n)}])^2 \\right) \\,, \\end{align*}\\] where \\[ E[X_{(n)}^2] = \\int_0^\\theta x^2 n \\frac{x^{n-1}}{\\theta^n} dx = \\left. \\frac{n}{(n+2)\\theta^n} x^{n+2} \\right|_0^\\theta = \\frac{n}{n+2} \\theta^2 \\,. \\] Thus \\[\\begin{align*} V[\\hat{\\theta}_{MVUE}] &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n}{n+2} \\theta^2 - \\frac{n^2}{(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n(n+1)^2 - n^2(n+2)}{(n+2)(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n}{(n+2)(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{1}{n(n+2)} \\theta^2 \\rightarrow \\frac{\\theta^2}{n^2} ~~\\mbox{as}~~ n \\rightarrow \\infty \\,. \\end{align*}\\] We observe that since the variance goes to zero as \\(n \\rightarrow \\infty\\), the MVUE is a consistent estimator… …but does the MVUE achieve the Cramer-Rao Lower Bound (CRLB), the theoretical lower bound on the variance of unbiased estimators? It turns out that not only does it achieve the lower bound (which one can show equals \\(\\theta^2/n\\)), but it even surpasses that bound! This seemingly worrisome result is actually fine, however, because the CRLB theorem is not applicable in situations where the domain of a pmf or pdf depends on \\(\\theta\\). 5.4 Maximum Likelihood Estimation Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is, thus far, found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). Now that we have recalled how maximum likelihood estimation works, we can state that this is not how the MLE is found for a domain-affecting parameter! (Hence the “thus far” in the recall statement above.) Let’s assume, for instance, that we sample \\(n\\) iid random variables from a Uniform(\\(0,\\theta\\)) distribution. As stated above (without the indicator function), the likelihood is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\frac{1}{\\theta^n} \\,. \\] This means that the smaller \\(\\theta\\) is, the larger the likelihood will be. So how small can \\(\\theta\\) be? We can answer this intuitively: the domain \\([0,\\theta]\\) has to just encompass all the observed data, i.e., \\[ \\hat{\\theta}_{MLE} = X_{(n)} \\,. \\] If \\(\\theta\\) were smaller, \\(X_{(n)}\\) would lie outside the domain. It is fine for \\(\\theta\\) to be larger, since then all the data lie in the domain \\([0,\\theta]\\)…but the larger \\(\\theta\\) is, the smaller the likelihood. We plot an example likelihood function in Figure 5.2. We observe immediately that the usual MLE algorithm will not work here, as the likelihood function is discontinuous at \\(\\theta = X_{(n)}\\) and thus we cannot compute a first derivative. All we can do is, e.g., plot the likelihood and identify the MLE as that value for which the likelihood is maximized (or identify the value intuitively as we do above). Figure 5.2: The likelihood function given \\(n=5\\) data drawn from a Uniform(0,\\(\\theta\\)) distribution, with \\(\\theta = 1\\). As \\(\\theta\\) cannot be smaller than the maximum observed value, the likelihood is zero for \\(\\theta &lt; X_{(n)}\\); it is \\(1/\\theta^n\\) for \\(\\theta \\geq X_{(n)}\\). The maximum likelihood estimate is thus \\(X_{(n)}\\) itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters. 5.4.1 The MLE for the Domain Parameter of the Pareto Distribution In the last section above, we introduce the Pareto distribution, \\[ f_X(x) = \\frac{\\alpha\\beta^\\alpha}{x^{\\alpha+1}} \\,, \\] where \\(\\alpha &gt; 0\\) and \\(x \\in [\\beta,\\infty)\\), and we show that the sufficient statistic for \\(\\beta\\) (with \\(\\alpha\\) fixed) is the smallest observed datum, \\(X_{(1)}\\). Because \\(\\beta\\) is a parameter that dictates the domain, we find the MLE not via differentiation but rather by identifying that the likelihood is maximized when \\(\\beta\\) is exactly equal to \\(X_{(1)}\\), i.e., \\(\\hat{\\beta}_{MLE} = X_{(1)}\\). See Figure 5.3. Figure 5.3: The likelihood function given \\(n=5\\) data drawn from a Pareto(1,\\(\\beta\\)) distribution, with \\(\\beta = 1\\). As \\(\\beta\\) cannot be larger than the minimum observed value, the likelihood is zero for \\(\\beta \\geq X_{(1)}\\); it is \\(\\theta^n(1/\\prod_{i=1}^n x_i)^2\\) for \\(\\beta &lt; X_{(n)}\\). The maximum likelihood estimate is thus \\(X_{(1)}\\) itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters. 5.4.2 MLE Properties for Uniform Distribution Bounds In this example, we will mimic what we do above when discussing the properties of the MVUE, and look at estimator bias and variance, etc., assuming that \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim}\\) Uniform(\\(0,\\theta\\)). Bias: we know, from our derivation of the MVUE, that \\[ E[\\hat{\\theta}_{MLE}] = E[X_{(n)}] = \\frac{n}{n+1}\\theta \\,, \\] and thus the estimator bias is \\[ B[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}] - \\theta = \\frac{n}{n+1}\\theta - \\theta = \\frac{1}{n+1}\\theta \\,. \\] As we expect for the MLE, the estimator is at least asymptotically unbiased, as the bias goes to zero as the sample size \\(n \\rightarrow \\infty\\). Variance: the variance of the MLE is \\[ V[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}^2] - \\left(E[\\hat{\\theta}_{MLE}\\right)^2 = E[X_{(n)}^2] - (E[X_{(n)}])^2 \\,. \\] We derived both \\(E[X_{(n)}]\\) and \\(E[X_{(n)}^2]\\) above when discussing the MVUE, so we can write down immediately that \\[ V[\\hat{\\theta}_{MLE}] = \\frac{n}{n+2}\\theta^2 - \\left( \\frac{n}{n+1}\\theta\\right)^2 = \\frac{n}{(n+2)(n+1)^2}\\theta^2 \\rightarrow \\frac{\\theta^2}{n^2} ~~\\mbox{as}~~ n \\rightarrow \\infty\\,. \\] We observe that because the variance goes to zero as \\(n \\rightarrow \\infty\\), the MLE is a consistent estimator. The variance of the MLE is similar to, but not exactly the same as, the variance for the MVUE, although the variances converge to the same value in asymtopia. 5.5 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_U\\) by, e.g., solving for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The construction of confidence intervals thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) to \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_U\\) by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 14 of Chapter 1.) Here, we will recall something else about confidence interval construction: we can choose the statistic that we use. This is important, because as we have seen, if we are given \\(n\\) iid data drawn from, e.g., a Uniform(\\(0,\\theta\\)) distribution, we do not know the distribution of \\(\\bar{X}\\) (unless \\(\\theta=1\\))…while we do know the distribution of the order statistic \\(Y = X_{(n)}\\). We derive it above: \\[ f_{(n)}(x) = n \\frac{x^{n-1}}{\\theta^n} \\,. \\] The cdf is thus \\(F_Y(y) = F_{(n)}(x) = (x/\\theta)^n\\). We work with this expression in an example below, noting that \\(E[Y] = (n-1)\\theta/n\\), i.e., that \\(E[Y]\\) increases with \\(\\theta\\). We conclude our coverage (so to speak) of confidence intervals by going back to the notion of the confidence coefficient \\(1 - \\alpha\\). In a footnote in Chapter 1, we make the point that technically, the confidence coefficient is the infimum, or minimum value, of the probability \\(P(\\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U)\\). What does this actually mean? Let’s suppose that we have sampled \\(n\\) iid data from a normal distribution, and that we are going to construct a confidence interval of the form \\[ P(S^2 - a \\leq \\sigma^2 \\leq S^2 + a) \\] for the population variance \\(\\sigma^2\\). We can do this, right? Let’s see… \\[\\begin{align*} P(S^2 - a \\leq \\sigma^2 \\leq S^2 + a) &amp;= P(-a \\leq S^2-\\sigma^2 \\leq a) \\\\ &amp;= P\\left(1 - \\frac{a}{\\sigma^2} \\leq \\frac{S^2}{\\sigma^2} \\leq 1 + \\frac{a}{\\sigma^2}\\right) \\\\ &amp;= P\\left( (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\leq \\frac{(n-1)S^2}{\\sigma^2} \\leq (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right)\\\\ &amp;= F_{W(n-1)}\\left( (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right) - F_{W(n-1)}\\left( (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\right) \\,. \\end{align*}\\] The key to interpreting the last line above is that \\(\\sigma^2\\) is unknown (otherwise, why would we be constructing a confidence interval for it in the first place?), and thus can take on any positive value. What if \\(\\sigma^2\\) is very large? \\[ \\lim_{\\sigma^2 \\to \\infty} F_{W(n-1)}\\left[ (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right] - F_{W(n-1)}\\left[ (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\right] = F_{W(n-1)}(n-1) - F_{W(n-1)}(n-1) = 0 \\,. \\] Thus the confidence coefficient for the interval \\(S^2 - a \\leq \\sigma^2 \\leq S^2 + a\\) is \\(1 - \\alpha = 0\\) (or, we have that \\(\\alpha = 1\\)). The upshot: one cannot write down just any interval and assume that it is a valid one! 5.5.1 Interval Estimation Given Order Statistics We assume that we are given \\(n\\) iid data drawn from a Uniform(\\(0,\\theta\\)) distribution. Above, we note that the cdf for the maximum observed datum, \\(X_{(n)}\\), is \\(F_{(n)}(x) = (x/\\theta)^n\\). To find the lower and upper bounds on \\(\\theta\\), respectively, we solve for \\(\\theta\\) in the expressions \\[\\begin{align*} \\left(\\frac{X_{(n)}}{\\theta_{1-\\alpha/2}}\\right)^n &amp;= 1 - \\frac{\\alpha}{2} ~~ \\mbox{(lower)} \\\\ \\left(\\frac{X_{(n)}}{\\theta_{\\alpha/2}}\\right)^n &amp;= \\frac{\\alpha}{2} ~~ \\mbox{(upper)} \\,. \\end{align*}\\] (We assume that we are constructing a two-sided interval. Similar expressions would yield the lower or upper bound.) We thus find that \\[ \\hat{\\theta}_L = \\theta_{1-\\alpha/2} = \\frac{X_{(n)}}{(1-\\alpha/2)^{1/n}} ~~\\mbox{and}~~ \\hat{\\theta}_U = \\theta_{\\alpha/2} = \\frac{X_{(n)}}{(\\alpha/2)^{1/n}} \\,. \\] 5.5.2 Confidence Coefficient for a Uniform-Based Interval Estimator In the example above, we show that the interval estimate with confidence coefficient \\(1-\\alpha\\) for the uniform upper bound \\(\\theta\\) has the form \\([aX_{(n)},bX_{(n)}]\\). Can we also define an appropriate interval estimator if, for instance, it has the form \\([X_{(n)} + a,X_{(n)} + b]\\)? The short answer is no…because the confidence coefficient will be zero! To see why, let’s expand out and solve: \\[\\begin{align*} P(X_{(n)} + a \\leq \\theta \\leq X_{(n)} + b) &amp;= P(\\theta - b \\leq X_{(n)} \\leq \\theta - a)\\\\ &amp;= P(X_{(n)} \\leq \\theta - a) - P(X_{(n)} \\leq \\theta - b)\\\\ &amp;= F_{(n)}(\\theta-a) - F_{(n)}(\\theta-b)\\\\ &amp;= \\left(\\frac{\\theta-a}{\\theta}\\right)^2 - \\left(\\frac{\\theta-b}{\\theta}\\right)^2\\\\ &amp;= \\left(1-\\frac{a}{\\theta}\\right)^2 - \\left(1-\\frac{b}{\\theta}\\right)^2 \\,. \\end{align*}\\] The confidence coefficient is the infimum (or minimum value) that this expression can take on. For an interval of the form \\([aX_{(n)},bX_{(n)}]\\), \\(\\theta\\) does not appear, and thus the infimum is a constant. Here, however, \\[ \\lim_{\\theta \\to \\infty} P(X_{(n)} + a \\leq \\theta \\leq X_{(n)} + b) = 0 \\,, \\] and thus the confidence coefficient is (i.e., the proportion of computed intervals that overlap the true value \\(\\theta\\) goes to zero). Thus an interval estimator of the form \\([aX_{(n)},bX_{(n)}]\\) is a better one than one of the form \\([X_{(n)} + a,X_{(n)} + b]\\). 5.6 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-tail test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bound given by \\(y_{RR}\\), which is a solution to the equation \\[ F_Y(y_{RR} \\vert \\theta_o) - q = 0 \\,, \\] where \\(F_Y(\\cdot)\\) is the cumulative distribution function for the statistic \\(Y\\) and \\(q\\) is an appropriate quantile value that is determined using the hypothesis test reference table introduced in section 17 of Chapter 1. Note that the hypothesis test framework only allows us to make a decision about a null hypothesis; nothing is proven. One aspect of hypothesis testing that we reiterate here is that the hypotheses are always to be established, along with the level of the test, before we collect data. This should be obvious\\(-\\)looking at the data prior to establishing hypotheses and test levels can (and often will) lead to bias\\(-\\)so why are we reiterating this now? We are making this point because when we perform tests involving domain-specifying parameters, there are some quirks that we observe when we establish rejection regions. Let’s look at an example: we sample \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution, and we use these data to test the hypothesis \\(H_o : \\theta = \\theta_o\\) versus the hypothesis \\(H_a : \\theta \\neq \\theta_o\\) at level \\(\\alpha\\). We know that the sufficient statistic upon which we will build our test is \\(X_{(n)}\\). Given this, what can we say about the rejection region right away? We can say that we will reject the null if \\(X_{(n)} &gt; \\theta_o\\). This is a “trivial” statement, as no mathematics is involved. Initially, this might seem off-putting: we would, of course, never set \\(\\theta_o\\) to be less than the maximum datum, would we? (That would be silly.) But…that’s an incorrect way of looking at this situation, since that implies that we looked at the data first and only established the hypotheses afterwards. If we do things in the proper order, then it can be very much the case that the maximum datum will exceed \\(\\theta_o\\). If we observe this, then life is easy: we simply reject the null and move on. But…how do we establish the part of the rejection region involving values of \\(X_{(n)}\\) that are less than \\(\\theta_o\\)? That seems simple enough: we are performing a two-tail test, so we set the cdf for \\(X_{(n)}\\) to \\(\\alpha/2\\) and invert and…oh, but there’s a problem. If the null is actually correct, then the power of the test for \\(\\theta = \\theta_o\\) would be \\(\\alpha/2\\) and not \\(\\alpha\\). (If the null is correct, it is impossible to observe \\(X_{(n)} &gt; \\theta_o\\), so all rejections would happen “to the left” of \\(\\theta_o\\)!) So this is quirk number two: we have to be careful about whether we use, e.g., \\(\\alpha/2\\) or \\(\\alpha\\) we finding the rejection region boundary. If in doubt, think about the test power and how we can reject the null if \\(\\theta = \\theta_o\\), and make sure the power is actually \\(\\alpha\\). The last hypothesis-test-related topic that we will touch upon is the concept of multiple comparisons. This is a somewhat opaque term that denotes the situation in which we perform many hypothesis tests simultaneously and need to correct for the fact that if the null is correct in all cases, it becomes more and more likely that we will observe (multiple) instances in which we decide to reject the null. We can illustrate this using the binomial distribution: if we collect \\(k\\) sets of data (e.g., \\(k\\) separate sets of \\(n\\) iid data sampled from a Uniform(0,\\(\\theta\\)) distribution), and perform level-\\(\\alpha\\) hypothesis tests for each, then the number of tests results in which we reject the null is \\[ X \\sim \\mbox{Binom}(k,\\alpha) \\,, \\] The expected value of \\(X\\) is \\(E[X] = k\\alpha\\), which increases with \\(k\\). The family-wise error rate, or FWER, is the probability that at least one test will result in a rejection when the null is correct: \\[ FWER = P(X &gt; 0) = 1 - P(X = 0) = 1 - \\binom{k}{x} \\alpha^0 (1-\\alpha)^k = 1 - (1-\\alpha)^k \\,. \\] For instance, if \\(k = 10\\) and \\(\\alpha = 0.05\\), the family-wise error rate is 0.401: for every 10 tests we perform, the probability of erroneously rejecting one or more null hypotheses (i.e., detecting one or more false positives) is about 40 percent. This increase in the FWER with \\(k\\) is not good, and is well-known to be associated with a commonly seen data analysis issue dubbed data dredging or p-hacking. \\(p\\)-hacking greatly increases the probability that researchers will make incorrect claims about what their data say, and worse yet, that they will publish papers purporting these claims. To mitigate this issue, we can attempt to change the test level for individual tests such that the overall FWER is reduced to \\(\\alpha\\). There are many procedures for how we might go about changing the test level for individual tests, but the most commonly used one is the Bonferroni correction: \\[ \\alpha \\rightarrow \\frac{\\alpha}{k} \\,. \\] What is the FWER given this correction? Let’s assume the null is correct for all \\(k\\) tests. Then \\[ FWER = P\\left( p_1 \\leq \\frac{\\alpha}{k} \\cup \\cdots \\cup p_k \\leq \\frac{\\alpha}{k} \\right) = \\sum_{i=1}^k P\\left( p_i \\leq \\frac{\\alpha}{k}\\right) = \\sum_{i=1}^k \\frac{\\alpha}{k} = \\alpha \\,. \\] This works! Except…what happens if actually only \\(k&#39;\\) out of the \\(k\\) are actually true? The FWER becomes \\(k&#39; \\alpha / k \\leq \\alpha\\). Thus when there are incorrect nulls sprinkled into the mix, the FWER is too low…which means that the Bonferroni correction is unduly conservative. Using it will lead to us possibly not detecting false nulls that we should have detected! We illustrate this issue in an example below. An alternative to the Bonferroni correction and related procedures is to not focus upon the FWER, but to attempt to limit the false discovery rate, or FDR, instead. The simplest and most often used FDR-based procedure is the one of Benjamini and Hochberg (1995): compute all \\(k\\) \\(p\\)-values; sort the \\(p\\)-values into ascending order: \\(p_{(1)},\\ldots,p_{(k)}\\); determine the largest value \\(k&#39;\\) such that \\(p_{(k&#39;)} \\leq k&#39; \\alpha / k\\); and reject the null for all tests that map to \\(p_{(1)},\\ldots,p_{(k&#39;)}\\). In an example below, we illustrate the use of the BH procedure. To be clear: \\(\\alpha\\) here represents the proportion of rejected null hypotheses that are actually correct. (“We reject the null 20 times. Assuming \\(\\alpha = 0.05\\), then we expect that we were right to reject the null 19 times, and that we’d be mistaken once.”) This is different from the FWER setting, where \\(\\alpha\\) represents the probability of erroneously rejecting one or more null hypotheses. (“We perform 100 independent tests. Assuming \\(\\alpha = 0.05\\), we expect to erroneously reject the null five percent of the time when the null is correct…but we can say nothing about how often we correctly reject the null.”) We can further illustrate this point with the following table: Null Correct Null False Total Null Rejected V S R Fail to Reject U T k-R Total k’ k-k’ k The only observable random variable here is \\(R\\), the total number of rejected null hypotheses. In the FDR procedure, we focus on the first row. We know that \\[ E[V] = \\alpha k&#39; \\leq \\alpha k \\] and we know that \\(V+S \\leq k\\), so \\[ E\\left[\\frac{V}{V+S}\\right] = E\\left[\\frac{V}{R}\\right] \\leq \\frac{\\alpha k&#39;}{k} \\leq \\alpha \\,. \\] The FWER procedure, on the other hand, focuses on the first column, with \\[ E\\left[\\frac{V}{V+U}\\right] = \\frac{\\alpha k&#39;}{k&#39;} = \\alpha \\,. \\] 5.6.1 The Power Curve for Testing the Uniform Distribution Upper Bound Assume, as we do above, that we have sampled \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution, and that we will use these data to test the hypotheses \\[ H_o: \\theta = \\theta_o ~~\\mbox{versus}~~ H_a: \\theta \\neq \\theta_o \\,. \\] The sufficient statistic is the maximum datum \\(X_{(n)}\\). As stated above, we know that we will reject the null when \\(X_{(n)} &gt; \\theta_o\\); that’s a “trivial” statement. As for the rejection region boundary when \\(X_{(n)} \\leq \\theta_o\\): we know that the cdf for \\(X_{(n)}\\) is \\(F_{(n)}(x) = (x/\\theta)^n\\), so the lower boundary is \\[ \\left(\\frac{x_{\\alpha/2}}{\\theta_o}\\right)^n = \\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ \\ldots \\,. \\] Except, this is wrong: what would be the power if \\(\\theta = \\theta_o\\)? It would be \\(\\alpha/2\\) and not \\(\\alpha\\). So despite the fact that we are carrying out a two-tail test, all the \\(\\alpha\\) “goes to the left” of \\(\\theta_o\\) (because it is impossible to reject “to the right”: if the null is correct, \\(X_{(n)} &gt; \\theta_o\\) is impossible. So we have that \\[ \\left(\\frac{x_{\\alpha}}{\\theta_o}\\right)^n = \\alpha \\] and thus that \\[ x_{\\alpha} = \\theta_o \\alpha^{1/n} \\,. \\] If \\(X_{(n)} &lt; x_{\\alpha}\\), we reject the null. Full stop. The power of this test is \\[\\begin{align*} P(\\mbox{reject}~\\mbox{null} \\vert \\theta) &amp;= P(X_{(n)} &lt; x_\\alpha \\cup X_{(n)} &gt; \\theta_o \\vert \\theta) \\\\ &amp;= F_{(n)}(x_\\alpha \\vert \\theta) + [1 - F_{(n)}(\\theta_o \\vert \\theta)] \\\\ &amp;= \\left\\{ \\begin{array}{rl} 1 &amp; \\theta &lt; x_\\alpha \\\\ \\left(\\frac{x_\\alpha}{\\theta}\\right)^n &amp; x_\\alpha &lt; \\theta \\leq \\theta_o \\\\ 1 + \\left(\\frac{x_\\alpha}{\\theta}\\right)^n - \\left(\\frac{\\theta_o}{\\theta}\\right)^n &amp; \\theta &gt; \\theta_o \\end{array} \\right. \\,. \\end{align*}\\] For the first condition above: if \\(\\theta &lt; x_\\alpha\\), then \\(X_{(n)} &lt; x_\\alpha\\), so every test will result in a rejection, and the power is thus 1. We plot out the power curve for \\(\\theta_o = 1\\) and \\(n = 10\\) in Figure 5.4. Figure 5.4: The power curve for the test of \\(H_o : \\theta = \\theta_o = 1\\) versus \\(H_a : \\theta \\neq \\theta_o\\). The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, \\(\\alpha\\), at \\(\\theta = 1\\). 5.6.2 An Illustration of Multiple Comparisons When Testing for the Normal Mean In the code chunk below, we generate \\(k = 100\\) independent datasets of size \\(n = 40\\); for \\(k&#39; = 80\\) datasets, \\(\\mu = 0\\), and for the remainder, \\(\\mu = 0.5\\). For simplicity, we assume \\(\\sigma^2\\) is known and is equal to one. For each dataset, we test the hypotheses \\(H_o : \\mu = 0\\) versus \\(H_a : \\mu &gt; 0\\). set.seed(101) n &lt;- 40 k &lt;- 100 k.p &lt;- 80 mu &lt;- c(rep(0,k.p),rep(0.5,k-k.p)) mu.o &lt;- 0 sigma2 &lt;- 1 p &lt;- rep(NA,k) for ( ii in 1:k ) { X &lt;- rnorm(n,mean=mu[ii],sd=sigma2) p[ii] &lt;- 1 - pnorm(mean(X),mean=mu.o,sd=sqrt(sigma2/n)) } Below, we try two separate corrections for multiple comparisons: the Bonferroni correction (controlling FWER), and the Benjamini-Hochberg procedure (controlling FDR). alpha &lt;- 0.05 cat(&quot;The number of rejected null hypotheses for Bonferroni: &quot;, sum(p &lt; alpha/k),&quot;\\n&quot;) ## The number of rejected null hypotheses for Bonferroni: 9 w &lt;- which(p &lt; alpha/k) cat(&quot;The number of falsely rejected null hypotheses is: &quot;,sum(w&lt;=k.p),&quot;\\n&quot;) ## The number of falsely rejected null hypotheses is: 0 p.sort &lt;- sort(p) cat(&quot;The number of rejected null hypotheses for FDR: &quot;, sum(p.sort &lt; (1:k)*alpha/k),&quot;\\n&quot;) ## The number of rejected null hypotheses for FDR: 17 p.rej &lt;- p.sort[p.sort&lt;(1:k)*alpha/k] w &lt;- p %in% p.rej w &lt;- which(w==TRUE) cat(&quot;The number of falsely rejected null hypotheses is: &quot;,sum(w&lt;=k.p),&quot;\\n&quot;) ## The number of falsely rejected null hypotheses is: 0 With the Bonferroni correction, we reject nine null hypotheses (with the guarantee that there is, on average, a five percent chance that we erroneously reject one or more of the true nulls…here, we reject no correct null hypotheses. See Figure 5.5. With the BH procedure, we reject 17 null hypotheses (with the guarantee that on average, five percent of these 17 [meaning, effectively, 1] is an erroneous rejection…here, we reject no correct null hypotheses). Figure 5.5: An illustration of the difference between the Bonferroni correction and the Benjamini-Hochberg procedure. The blue dots represent sorted \\(p\\)-values resulting from a simulation in which 80 of 100 null hypotheses are correct (so that a perfect disambiguation between null and non-null hypotheses would result in 20 rejected nulls, with none falsely rejected. The Bonferroni correction shifts \\(\\alpha = 0.05\\) downwards to the green short-dashed line; 9 \\(p\\)-values lie below the line, so 9 (true) null hypotheses are rejected in all. The BH procedure looks for the number of \\(p\\)-values lying below the red dashed line; that number is 17 (with no false rejections). 5.7 Exercises Let \\(X_1, X_2, \\ldots, X_n\\) denote independent and identically distributed uniform random variables on the interval \\([0, 3\\theta]\\). Derive the method-of-moments estimator for \\(\\theta\\). Compute \\(P(X &gt; a+b \\vert X &gt; b)\\) for a Uniform(0,1) distribution. (Assume \\(0 &lt; b &lt; a+b &lt; 1\\).) Does the Uniform(0,1) distribution exhibit the property of memorylessness? Why or why not? A woman goes to her local bus stop every day at a random time between noon and 1 PM, for five days total. If a bus doesn’t appear to pick her up within 10 minutes, she immediately hops into a waiting Uber and is driven off. On every day, there is only one bus that will arrive between noon and 1:10 PM, and it will arrive at a random time \\(X\\) minutes after noon. \\(X\\) is sampled from the following distribution: \\[\\begin{eqnarray*} f_X(x) = \\left\\{ \\begin{array}{ll} 1/70 &amp; x \\in [0,70] \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) On any one day, what is the probability that the woman catches the bus? (b) Over the five days, what is the probability that the woman catches the bus one or more times? (You may leave fractions raised to powers in your final answer, such as \\((3/4)^3\\) or \\((7/15)^5\\), if they are part of your answer.) (Also, if you are in doubt about your answer to (a), just use the variable \\(p\\) in place of your answer for (a) in part (b).) You sample a datum \\(X\\) from a Uniform(0,1) distribution. What is \\(P(X \\leq 2u \\vert X \\geq u])\\), where \\(0 \\leq u \\leq 0.5\\)? Let \\(X_1\\) and \\(X_2\\) be two iid random variables sampled from a Uniform(0,1) distribution. What is \\(P(X_1 &lt; 2X_2 \\vert X_2 &lt; 1/2)\\)? (Note: \\(X_1\\) and \\(X_2\\) are not order statistics, so do not treat them as such!) Assume that we have sampled \\(n\\) iid random variables from a Uniform(\\(\\theta,0\\)) distribution, where \\(\\theta &lt; 0\\). (a) What is a sufficient statistic for \\(\\theta\\)? (b) What is the cdf for this sufficient statistic? Be careful when deriving \\(F_X(x)\\): the pdf \\(f_X(x)\\) is \\(1/(0-\\theta) = -1/\\theta\\) and not \\(1/\\theta\\). Also, take care when writing down the integral bounds. (c) We wish to test \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta \\neq \\theta_o\\). Recall that hypothesis tests are written down (in theory!) before the collection of data. Given that factoid, write down the trivial part of the test rejection region, i.e., the part of the overall rejection region that one can write down without having to work with the sufficient statistic cdf. (d) To derive the other part of the rejection region, do we set the cdf for the sufficient statistic to \\(1-\\alpha\\) or \\(1-\\alpha/2\\)? Choose one and write it in the answer box. Recall that the power of the hypothesis test when \\(\\theta = \\theta_o\\) is exactly \\(\\alpha\\). (e) Given your answers for (b) and (d), derive the boundary of the other part of the rejection region (the non-trivial part). You sample \\(n\\) iid data from the following (unnamed) distribution: \\[\\begin{eqnarray*} f_X(x) = \\frac{2}{\\theta^2} x ~~~ x \\in [0,\\theta] \\,. \\end{eqnarray*}\\] The cdf for this distribution is \\(F_X(x) = (x/\\theta)^2\\). (a) What is the MLE for \\(\\theta\\)? (b) What is \\(E[X_{(n)}]\\)? (c) What is the MVUE for \\(\\theta\\)? Let’s assume we have sampled \\(n\\) iid data from the following distribution: \\[\\begin{align*} f_X(x) = e^{-(x-\\theta)} ~~~ x \\in [\\theta,\\infty) \\end{align*}\\] where \\(\\theta &gt; 0\\). The cdf for this distribution, for \\(x \\geq \\theta\\), is \\[\\begin{align*} F_X(x) = 1-e^{-(x-\\theta)} \\,. \\end{align*}\\] (a) Identify a sufficient statistic for \\(\\theta\\). (b) Identify the maximum likelihood estimator for \\(\\theta\\). No work need be shown. (c) Determine the sampling distribution (specifically, the pdf, and not the cdf) for the sufficient statistic identified in part (a). (d) Determine the minimum variance unbiased estimator for \\(\\theta\\). You will want to utilize a variable subsitution here. Recall that \\[\\begin{align*} \\Gamma(a+1) = a! = \\int_0^\\infty u^a e^{-u} du \\,, \\end{align*}\\] assuming that \\(a\\) is a non-negative integer. (Also recall that 0! = 1.) We sample two iid data, \\(X_1\\) and \\(X_2\\), from a Uniform(0,1) distribution. (a) What is \\(P(X_1 &gt; 1/2 \\vert X_2 &lt; 1/2)\\)? (b) What is \\(P(X_1 &gt; 1/2 \\vert X_1 &lt; 3/4)\\)? (c) What is \\(P(X_1 &lt; 3X_2)\\)? (Hint: draw this out in a 1 \\(\\times\\) 1 box. Do the same for (d).) (d) What is \\(P(X_2 &lt; X_1 \\vert X_2 &lt; 1/2)\\)? Let’s assume that we have sampled \\(n\\) iid data from a particular distribution with domain \\([\\theta,\\infty)\\), and let the cdf of the sampling distribution of the appropriate statistic \\(Y\\) to use to construct confidence intervals and perform hypothesis tests be \\[\\begin{align*} F_Y(y) = 1 - e^{-n(y-\\theta)} \\,. \\end{align*}\\] Assume the observed statistic value is \\(y_{\\rm obs}\\), and that \\(E[Y] = \\theta + 1/n\\). (Note that it is not necessary to know what \\(Y\\) actually represents to answer the questions below.) (a) Determine a \\(100(1-\\alpha)\\)-percent lower bound on \\(\\theta\\). (b) Assume we wish to test \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta \\neq \\theta_o\\). Derive the rejection-region boundary (or boundaries) \\(y_{\\rm RR}\\) in terms of \\(\\theta_o\\), the Type I error \\(\\alpha\\), and \\(n\\). "],["multivariate-distributions.html", "6 Multivariate Distributions 6.1 Independence of Random Variables 6.2 Properties of Multivariate Distributions 6.3 Covariance and Correlation 6.4 Marginal and Conditional Distributions 6.5 Conditional Expected Value and Variance 6.6 The Multivariate Normal Distribution 6.7 Exercises", " 6 Multivariate Distributions Thus far we have looked at univariate probability distributions, i.e., we have assumed that there is a single random variable \\(X\\) that maps the events in a sample space to the real-number line. The quantity represented by \\(X\\) might be, for instance, height or weight. In this chapter, we shift to the multivariate case, wherein we might have \\(p\\) random variables \\(\\mathbf{X} = \\{X_1,\\ldots,X_p\\}\\) representing, e.g., height and weight (if \\(p = 2\\)). When there are a set of \\(p\\) random variables, they are sampled from a joint \\(p\\)-dimensional probability mass or density function that encapsulates how the random variables are jointly distributed. Both joint pmfs \\(p_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p)\\) and joint pdfs \\(f_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p)\\) have similar properties as their univariate counterparts: they are non-negative (with \\(p_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p) \\leq 1\\)) and they either sum or integrate to 1. They is nothing “special” about these functions; they are simply more mathematically complex and thus often less easy to work with. However, there are new concepts for us to cover that only arise in a multivariate context. Independence of Random Variables: do the sampled values for one random variable (e.g., heights) depend on the sampled values for the others (e.g., weights)? If not, the random variables are independent. (In this simple example, we expect that heights and weights to be very much dependent: on average, taller people are heavier.) Covariance and Correlation: these metrics build upon the concept of variance and indicate the amount of linear dependence between two random variables. Marginal Distributions: these show how a subset of the random variables is (jointly) distributed, without regard to the values taken on by the other random variables. For instance, if \\(f_{X_h,X_W}(x_h,x_w)\\) represents the joint distribution of heights and weights in a population, then the marginal distribution \\(f_{X_h}(x_h)\\) represents how heights are distributed, without regard to weight. Conditional Distributions: these show how a subset of the random variables is (jointly) distributed, conditional on the other random variables taking on specific values. For instance, the conditional distribution \\(f_{X_h \\vert X_w}(x_h \\vert x_w)\\) indicates the distribution of heights given a specific weight. Conditional Expectation and Variance: these metrics represent the mean and “width” of conditional distributions. Note that throughout this chapter, we will illustrate concepts using bivariate distributions, as adding mathematical complexity by increasing dimensionality provides no additional benefit in terms of conceptual understanding. The one exception to this is in the last section, where we discuss the multivariate normal distribution. 6.1 Independence of Random Variables In Chapter 1, we describe at a high level the concept of two or more random variables being independent of each other. In that chapter, we give the example of a bivariate probability density function, i.e., a function which outputs the probability density given two inputs, \\(x_1\\) and \\(x_2\\): \\(f_{X_1,X_2}(x_1,x_2)\\). To test for independence, we need only inspect the functional form of \\(f_{X_1,X_2}(x_1,x_2)\\) and its domain; \\(X_1\\) and \\(X_2\\) are independent if and only if the boundaries of the domain depend on either \\(x_1\\) or \\(x_2\\) but not both at the same time (i.e., the domain is “rectangular”); and \\(f_{X_1,X_2}(x_1,x_2)\\) can be factored into the product of functions that only depend on \\(x_1\\) and on \\(x_2\\), respectively: \\(f_{X_1}(x_1) \\times f_{X_2}(x_2)\\). (Furthermore, we state that if \\(f_{X_1}(x_1) = f_{X_2}(x_2)\\), then \\(X_1\\) and \\(X_2\\) are iid random variables.) If either condition given above does not hold, then we conclude that \\(X_1\\) and \\(X_2\\) are dependent random variables. 6.1.1 Determining Whether Two Random Variables are Independent Let \\(X_1\\) and \\(X_2\\) have the following joint pdf: \\[ f_{X_1,X_2}(x_1,x_2) = c(1-x_2) \\] for \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). For now, we are not concerned with the value of the constant \\(c\\). Are \\(X_1\\) and \\(X_2\\) independent random variables? The first thing to do is inspect the joint domain. This can be tricky, and it is often best to break the statement of the domain up into multiple “pieces.” Here, we can say that first, we know that \\(x_1 \\in [0,1]\\) and that \\(x_2 \\in [0,1]\\). This limits the domain to a box on the \\(x_1\\)-\\(x_2\\) plane. (See the axes and the red dashed lines in Figure 6.1.) Furthermore, we know that \\(x_1 \\leq x_2\\), or, equivalently, that \\(x_2 \\geq x_1 + 0\\), i.e., that \\(x_2\\) lies above a line with slope one and intercept zero. (See the orange short-dashed line in Figure 6.1.) Given that the domain is triangular, we can state unequivocally here that \\(X_1\\) and \\(X_2\\) are not independent random variables. In particular, we do not need to check whether \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) \\times f_{X_2}(x_2)\\). Figure 6.1: The domain of \\(f_{X_1,X_2}(x_1,x_2)\\), expressed mathematically as \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). The red dashed lines indicate that \\(0 \\leq x_1 \\leq 1\\) and \\(0 \\leq x_2 \\leq 1\\), and the orange short-dashed line indicates that \\(x_2 \\geq x_1\\). The blue triangle is the domain of the function. As a second example, let \\(X_1\\) and \\(X_2\\) have the following joint pdf: \\[ f_{X_1,X_2}(x_1,x_2) = ce^{-x_1x_2} \\,, \\] where \\(0 \\leq x_1 \\leq 1\\) and \\(0 \\leq x_2 \\leq 1\\). Again, we are not concerned about the value of \\(c\\), at least for now. Are \\(X_1\\) and \\(X_2\\) independent random variables? We can see by inspection that the domain is “rectangular,” specifically a square with vertices (0,0), (0,1), (1,0), and (1,1). So far, so good. Can we factor \\(f_{X_1,X_2}(x_1,x_2)\\) into two separate functions that depend only on \\(x_1\\) or only on \\(x_2\\)? The answer is no. (As a reminder, \\(e^{ab} \\neq e^a e^b\\)!) Hence we can state that \\(X_1\\) and \\(X_2\\) are not independent random variables. 6.2 Properties of Multivariate Distributions As stated in Chapter 1, a probability distribution is a mapping \\(P : \\Omega \\rightarrow \\mathbb{R}^n\\), where \\(\\Omega\\) is the sample space for an experiment. This mapping describes how probabilities are distributed across the values of a random variable. In the first five chapters, \\(n = 1\\), meaning that we focus on univariate distributions. In this chapter, however, \\(n &gt; 1\\), with the bulk of our discussion focusing upon the case \\(n = 2\\). Nothing changes, fundamentally, when we work with multivariate distributions: (a) they are non-negative; and (b) they sum or integrate to 1. Assuming \\(n = 2\\), we can write that joint pmf joint pdf \\(p_{X_1,X_2}(x_1,x_2) \\in [0,1]\\) \\(f_{X_1,X_2}(x_1,x_2) \\in [0,\\infty)\\) \\(\\sum_{x_1} \\sum_{x_2} p_{X_1,X_2}(x_1,x_2) = 1\\) \\(\\int_{x_1} \\int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = 1\\) We will reiterate that the joint pdf \\(f_{X_1,X_2}(x_1,x_2)\\) is not the probability of sampling the tuple \\((x_1,x_2)\\) but rather is a probability density; to determine probabilities, we would invoke multi-dimensional integration: \\[ P(a_1 \\leq X_1 \\leq b_1,a_2 \\leq X_2 \\leq b_2) = \\int_{a_1}^{b_1} \\int_{a_2}^{b_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\,. \\] As for the joint cumulative distribution function, or joint cdf, the convention is to treat each axis separately from the others, i.e., to define it as \\[ F_{X_1,X_2}(x_1,x_2) = P(X_1 \\leq x_1 \\cap X_2 \\leq x_2) \\,. \\] The joint cdf satisfies the following properties: if either \\(X_i = -\\infty\\), the joint cdf is zero; if both \\(X_i = \\infty\\), the joint cdf is one; it increases monotonically along each coordinate axis; and if \\(X_1\\) and \\(X_2\\) are independent random variables, then \\(F_{X_1,X_2}(x_1,x_2) = F_{X_1}(x_1) F_{X_2}(x_2)\\). To characterize bivariate distributions (and multivariate ones in general), we compute distribution moments by utilizing the Law of the Unconscious Statistician: \\[\\begin{align*} E[g(X_1,X_2)] &amp;= \\sum_{x_1} \\sum_{x_2} g(x_1,x_2) p_{X_1,X_2}(x_1,x_2) \\qquad \\mbox{discrete}\\\\ &amp;= \\int_{x_1} \\int_{x_2} g(x_1,x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\qquad \\mbox{continuous} \\,, \\end{align*}\\] with the shortcut formula continuing to hold in the multivariate case: \\[ V[g(X_1,X_2)] = E\\left[ g(X_1,X_2)^2 \\right] - (E[g(X_1,X_2)])^2 \\,. \\] We note that if we write \\(g(x_1,x_2)\\) as \\(g_1(x_1)g_2(x_2)\\) and if \\(X_1\\) and \\(X_2\\) are independent random variables, then \\[\\begin{align*} E[g_1(X_1)g_2(X_2)] &amp;= \\int_{x_1} \\int_{x_2} g_1(x_1) g_2(x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\\\ &amp;= \\int_{x_1} \\int_{x_2} g_1(x_1) g_2(x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_2 dx_1 \\\\ &amp;= \\left[ \\int_{x_1} g_1(x_1) f_{X_1}(x_1) \\right] \\cdot \\left[ \\int_{x_2} g_2(x_2) f_{X_2}(x_2) \\right] \\\\ &amp;= E[g_1(X_1)] E[g_2(X_2)] \\,. \\end{align*}\\] So, for instance, \\(E[X_1X_2] = E[X_1]E[X_2]\\) if \\(X_1\\) and \\(X_2\\) are independent, but not generally. 6.2.1 Characterizing a Discrete Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is \\(E[X_1X_2]\\)? We utilize the Law of the Unconscious Statistician: \\[\\begin{align*} E[X_1X_2] &amp;= \\sum_{x_1} \\sum_{x_2} x_1 x_2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0 \\times 0.20 + 2 \\times 0 \\times 0.40 + 1 \\times 1 \\times 0.30 + 2 \\times 2 \\times 0.10 \\\\ &amp;= 0.30 + 0.40 = 0.70 \\,. \\end{align*}\\] What is \\(F_{X_1,X_2}(3/2,3/2)\\)? The joint cdf is \\[\\begin{align*} P(X_1 \\leq 3/2 \\cap X_2 \\leq 3/2) &amp;= P(X_1 = 1 \\cap X_2 = 0) + P(X_1 = 1 \\cap X_2 = 1) \\\\ &amp;= 0.20 + 0.30 = 0.50 \\,. \\end{align*}\\] 6.2.2 Characterizing a Continuous Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = c(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). In an example above, we determined that \\(X_1\\) and \\(X_2\\) are dependent random variables due to the domain of the pdf not being “rectangular” on the \\(x_1\\)-\\(x_2\\) plane (see Figures 6.1 and 6.2). Here, we will first determine the value of \\(c\\) that makes this function a valid joint pdf. Carrying out this calculation will involve double integration; for a short review of double integration, see Chapter 8. We note here that we can integrate in either order (i.e., along the \\(x_1\\) axis first, or the \\(x_2\\) axis first), as the final result will be the same. Here, we will integrate along \\(x_1\\) first, since the lower bound along this axis is zero (a choice which often simplifies the overall calculation) and because \\(x_1\\) does not appear in the integrand: \\[\\begin{align*} \\int_0^1 \\left[ \\int_0^{x_1 = x_2} c (1-x_2) dx_1 \\right] dx_2 &amp;= c \\int_0^1 (1-x_2) \\left[ \\int_0^{x_1 = x_2} dx_1 \\right] dx_2 \\\\ &amp;= c \\int_0^1 (1-x_2) \\left[ \\left. x_1 \\right|_0^{x^2} \\right] dx_2 \\\\ &amp;= c \\int_0^1 x_2 (1-x_2) dx_2 \\\\ &amp;= c B(2,2) = c \\Gamma(2) \\Gamma(2) / \\Gamma(4) = c \\times 1! \\times 1! / 3! = c/6 = 1 \\,. \\end{align*}\\] Thus \\(c = 6\\). Note that we take advantage of the fact that \\(x_2 (1-x_2)\\) integrated from 0 to 1 is a Beta(2,2) distribution. If we had not made that association, we would still have observed the same final solution after integrating the polynomial in the integrand. Figure 6.2: The probability density function \\(6(1-x_2)\\) as a function of \\(x_1\\) (axis pointing to upper right) and \\(x_2\\) (axis pointing to upper left). The pdf peaks at \\((x_1,x_2) = (0,0)\\) and falls off towards \\(x_2 = 1\\) as a plane, with value greater than zero only within the domain \\(0 \\leq x_1 \\leq x_2 \\leq 1\\); Figure 6.1 shows that domain. Now we will ask, what are \\(E[X_2]\\) and \\(V[X_2]\\)? \\[\\begin{align*} E[X_2] = \\int_0^1 \\left[ \\int_0^{x_1 = x_2} 6 x_2 (1-x_2) dx_1 \\right] dx_2 &amp;= 6 \\int_0^1 x_2 (1-x_2) \\left[ \\int_0^{x_1 = x_2} dx_1 \\right] dx_2 \\\\ &amp;= 6 \\int_0^1 x_2 (1-x_2) \\left[ \\left. x_1 \\right|_0^{x^2} \\right] dx_2 \\\\ &amp;= 6 \\int_0^1 x_2^2 (1-x_2) dx_2 \\\\ &amp;= 6 B(3,2) = 6 \\Gamma(3) \\Gamma(2) / \\Gamma(5) = 6 \\times 2! \\times 1! / 4! = 1/2 \\,. \\end{align*}\\] In a similar manner, we can determine that \\(E[X_2^2] = 6 B(4,2) = 6 \\times 3! \\times 1! / 5! = 36/120 = 3/10\\) and that \\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 3/10 - 1/4 = 1/20\\). 6.2.3 The Bivariate Uniform Distribution The bivariate uniform distribution is defined as \\[ f_{X_1,X_2}(x_1,x_2) = \\frac{1}{A} \\,, \\] where \\(A\\) is the area of the domain. (Thus the integral of the bivariate function is \\(A \\times 1/A = 1\\).) Let \\(f_{X_1,X_2}(x_1,x_2) = 1\\) over the square defined by the vertices (0,0), (0,1), (1,0), and (1,1). What is \\(P(X_1 &gt; 2X_2)\\)? A nice feature of the bivariate uniform is that we can work with it “geometrically”: if we can determine the fraction of the domain that abides by the stated condition, then we have our answer. (This is because the joint pdf is flat, so integration is unnecessary.) We can rewrite \\(x_1 &gt; 2x_2\\) as \\(x_2 &lt; x_1/2\\), i.e., the region of interest in the domain is the region below the line with intercept zero and slope 1/2. (See Figure 6.3.) This region is a triangle with vertices (0,0), (1,1/2), and (1,0), which has the area (recall: one-half times base times height) \\(1/2 \\times 1 \\times 1/2 = 1/4\\). Done! \\(P(X_1 &gt; 2X_2) = 1/4\\). Figure 6.3: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region \\(x_1 &gt; 2x_2\\) is indicated by the blue triangle. 6.3 Covariance and Correlation The covariance between two random variables \\(X_1\\) and \\(X_2\\) is a metric that quantifies the amount of linear dependence between them. It is defined as \\[ {\\rm Cov}(X_1,X_2) = E[(X_1-\\mu_1)(X_2-\\mu_2)] \\,, \\] but one rarely uses this expression, as there is a shortcut formula: \\[\\begin{align*} {\\rm Cov}(X_1,X_2) &amp;= E[X_1X_2 - X_1\\mu_2 - X_2\\mu_1 + \\mu_1\\mu_2] \\\\ &amp;= E[X_1X_2] - \\mu_2E[X_1] - \\mu_1E[X_2] + \\mu_1\\mu_2 \\\\ &amp;= E[X_1X_2] - E[X_2]E[X_1] - E[X_1]E[X_2] + E[X_1]E[X_2] \\\\ &amp;= E[X_1X_2] - E[X_1]E[X_2] \\,. \\end{align*}\\] In the previous section, we saw that \\(E[X_1X_2] = E[X_1]E[X_2]\\) if \\(X_1\\) and \\(X_2\\) are independent random variables. Thus independent random variables have no covariance, which makes sense: independent random variables would have by definition no linear dependence. However, it is not true that \\(E[X_1X_2] = E[X_1]E[X_2]\\) implies that \\(X_1\\) and \\(X_2\\) are independent random variables…it just implies there is no linear dependence between the two variables. See Figure 6.4. Figure 6.4: Examples of data with negative covariance (left), no covariance (center), and positive covariance (right). Covariance is not necessarily an optimal metric for expressing linear dependence, as its value is not readily interpretable. To see this, assume we have the expression \\((tX_1 - X_2)\\) for some constant \\(t\\). Then \\((tX_1-X_2)^2 \\geq 0\\) and \\(E[(tX_1-X_2)^2] \\geq 0\\). Thus \\[\\begin{align*} E[(tX_1-X_2)^2] &amp;= E[t^2X_1^2 - 2tX_1X_2 + X_2^2] \\\\ &amp;= E[X_1^2] t^2 - 2E[X_1X_2] t + E[X_2^2] \\\\ &amp;= a t^2 + b t + c \\geq 0 \\,. \\end{align*}\\] (Recall that expected values are constants, and not themselves random variables.) The key here is that if \\(a t^2 + b t + c = 0\\), there is one real root to this quadratic equation, while if \\(a t^2 + b t + c &gt; 0\\), there are no real roots. Thus the discriminant, \\(b^2 - 4ac\\), must be \\(\\leq 0\\), and so \\[\\begin{align*} b^2 - 4ac &amp;= 4 (E[X_1X_2])^2 - 4 E[X_1^2] E[X_2^2] \\leq 0 \\\\ \\Rightarrow&amp; (E[X_1X_2])^2 \\leq E[X_1^2] E[X_2^2] \\,. \\end{align*}\\] At this point, the reader might ask “well, what about this?” The expression to the left above, \\((E[X_1X_2])^2\\), is \\([{\\rm Cov}(X_1,X_2)]^2\\) when \\(\\mu_1 = \\mu_2 = 0\\), while the expression to the right, \\(E[X_1^2] E[X_2^2]\\), is \\(V[X_1]V[X_2]\\) when \\(\\mu_1 = \\mu_2 = 0\\). Thus \\[ \\vert {\\rm Cov}(X_1,X_2) \\vert \\leq \\sqrt{V[X_1]V[X_2]} = \\sigma_1 \\sigma_2 \\,, \\] or \\(- \\sigma_1 \\sigma_2 \\leq {\\rm Cov}(X_1,X_2) \\leq \\sigma_1 \\sigma_2\\). The fact that we may not know immediately the value of \\(\\sigma_1 \\sigma_2\\) is what makes any numerical value of \\({\\rm Cov}(X_1,X_2)\\) hard to interpret. Thus we conventionally turn to an alternate expression of linear dependence, the correlation coefficient: \\[ \\rho_{X_1,X_2} = \\frac{{\\rm Cov}(X_1,X_2)}{\\sigma_1\\sigma_2} \\Rightarrow -1 \\leq \\rho \\leq 1 \\,. \\] If \\(\\rho_{X_1,X_2} &lt; 0\\), then an increase in the sampled value of \\(X_1\\) is associated with, on average, a smaller sampled value of \\(X_2\\), i.e., \\(X_1\\) and \\(X_2\\) are negatively correlated…whereas if \\(\\rho_{X_1,X_2} &gt; 0\\), larger sampled values for \\(X_1\\) are associated on average with larger sampled values of \\(X_2\\), i.e., \\(X_1\\) and \\(X_2\\) are positively correlated. (We note that we have actually seen \\(\\rho_{X_1,X_2}\\) before, in Chapter 2, when we introduced correlation as a metric of the strength of linear association as measured in simple linear regression. Recall that we estimate \\(\\rho_{X_1,X_2}\\) with, e.g., the Pearson correlation coefficient \\(R\\), and that we use coefficient of determination \\(R^2\\) to quantify the usefulness of the linear regression model. See Figure 6.5.) Figure 6.5: The same data as shown in Figure 6.4, with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right. Let’s suppose we are given \\(n\\) correlated random variables \\(\\{X_1,\\ldots,X_n\\}\\), and we define the sum \\(Y = \\sum_{i=1}^n a_i X_i\\). Furthermore, let’s assume we know the pdf/pmf, expected value, and variance of each of the \\(X_i\\)’s. What do we know about the distribution of \\(Y\\)? With work, we might be able to determine its pdf or pmf, but we would have to go beyond the method of moment-generating functions because that method requires the random variables to be independent of each other. We will not do that here. However, we can determine the expected value of \\(Y\\): \\[ E[Y] = E\\left[ \\sum_{i=1}^n a_i X_i \\right] = \\sum_{i=1}^n a_i E[X_i] \\,. \\] Dependencies between the \\(X_i\\)’s does not affect this equality. As for the variance of \\(Y\\)… We start by encapsulating the information about the covariances between each variable pair into a covariance matrix. (See Chapter 08 for a short introduction to matrices and their basic use.) For \\(n\\) random variables, the \\(n \\times n\\) covariance matrix \\(\\boldsymbol{\\Sigma}\\) is \\[ \\boldsymbol{\\Sigma} = \\left[ \\begin{array}{ccc} {\\rm Cov}(X_1,X_1) &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; {\\rm Cov}(X_n,X_n) \\end{array} \\right] = \\left[ \\begin{array}{ccc} V[X_1] &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; V[X_n] \\end{array} \\right] \\,. \\] We note that the diagonal of this matrix (the elements from the upper left corner to the lower right corner) contains the individual variances, since Cov(\\(X_i,X_i\\)) = \\(V[X_i]\\), and that because Cov(\\(X_i,X_j\\)) = Cov(\\(X_j,X_i\\)), the matrix is symmetric about the diagonal. Now, let \\(a^T = [ a_1 ~ a_2 ~ \\ldots a_n ]\\) be the \\(1 \\times n\\) matrix (or transposed vector) of coefficients. Then: \\[ V[Y] = a^T \\boldsymbol{\\Sigma} a \\,. \\] Nice and compact! Let’s compare this to the equivalent expression that does not utilize matrices: \\[\\begin{align*} V[Y] &amp;= \\left[ a_1 ~ \\ldots ~ a_n \\right] \\left[ \\begin{array}{ccc} V[X_1] &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; V[X_n] \\end{array} \\right] \\left[ \\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array} \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j {\\rm Cov}(X_i,X_j) \\,. \\end{align*}\\] Note that we can derive this expression directly using the Law of the Unconscious Statistician: \\[\\begin{align*} V[Y] = E[Y^2] - (E[Y])^2 = E[(Y - E[Y])^2] &amp;= E\\left[ \\left( \\sum_{i=1}^n a_i X_i - \\sum_{i=1}^n a_i \\mu_i \\right)^2 \\right] \\\\ &amp;= E\\left[ \\left(\\sum_{i=1}^n a_i(X_i - \\mu_i) \\right)^2 \\right] \\,. \\end{align*}\\] Let’s look at this for a moment. If we have, e.g., \\(Y = a_1X_1+a_2X_2\\), then \\[\\begin{align*} (a_1X_1+a_2X_2)^2 &amp;= a_1^2X_1^2 + a_2^2X_2^2 + a_1a_2X_1X_2 + a_2a_1X_2X_1 \\\\ &amp;= \\sum_{i=1}^2 a_i^2 X_i^2 + 2a_1a_2X_1X_2 \\\\ &amp;= \\sum_{i=1}^2 a_i^2 X_i^2 + 2 \\sum_{i=1}^{2-1} \\sum_{j=i+1}^2 a_i a_j X_i X_j \\,. \\end{align*}\\] Recognizing that the 2’s in the summation bounds would be the sample size \\(n\\) in general, we can write that \\[ \\left(\\sum_{i=1}^n a_i X_i\\right)^2 = \\sum_{i=1}^n a_i^2 X_i^2 + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j X_i X_j \\,. \\] So, picking up where we left off… \\[\\begin{align*} V[Y] &amp;= E\\left[ \\left(\\sum_{i=1}^n a_i(X_i - \\mu_i) \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n a_i^2(X_i-\\mu_i)^2 + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j (X_i - \\mu_i) (X_j - \\mu_2) \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 E\\left[(X_i-\\mu_i)^2\\right] + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j E\\left[(X_i - \\mu_i) (X_j - \\mu_2) \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j \\mbox{Cov}(X_i,X_j) \\,. \\end{align*}\\] 6.3.1 Correlation of Two Discrete Random Variables Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is the correlation between \\(X_1\\) and \\(X_2\\)? In an example above, we determined that \\(E[X_1X_2] = 0.7\\). This is but one piece of the correlation puzzle: we also need to determine \\(E[X_1]\\) and \\(E[X_2]\\) along with \\(V[X_1]\\) and \\(V[X_2]\\). \\[\\begin{align*} E[X_1] &amp;= \\sum_{x_1} \\sum_{x_2} x_1 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0.20 + 1 \\times 0.30 + 2 \\times 0.40 + 2 \\times 0.10 = 1.50 \\\\ \\\\ E[X_1^2] &amp;= \\sum_{x_1} \\sum_{x_2} x_1^2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1^2 \\times 0.20 + 1^2 \\times 0.30 + 2^2 \\times 0.40 + 2^2 \\times 0.10 = 2.50 \\\\ \\\\ E[X_2] &amp;= \\sum_{x_2} \\sum_{x_2} x_2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0.30 + 2 \\times 0.10 = 0.50 \\\\ \\\\ E[X_2^2] &amp;= \\sum_{x_2} \\sum_{x_2} x_2^2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1^2 \\times 0.30 + 2^2 \\times 0.10 = 0.70 \\,. \\end{align*}\\] Hence Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2] = 0.70 - 0.75 = -0.05\\), while \\(V[X_1] = E[X_1^2] - (E[X_1])^2 = 2.50 - 2.25 = 0.25\\) and \\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 0.70 - 0.25 = 0.45\\). Thus the correlation between \\(X_1\\) and \\(X_2\\) is \\[ \\rho_{X_1,X_2} = \\frac{\\mbox{Cov}(X_1,X_2)}{\\sqrt{V[X_1]V[X_2]}} = \\frac{-0.05}{\\sqrt{0.25 \\cdot 0.45}} = -0.149 \\,. \\] \\(X_1\\) and \\(X_2\\) are (relatively weakly) negatively correlated: increasing \\(X_1\\) leads to slightly decreased values of \\(X_2\\), on average. 6.3.2 Correlation of the Sum of Two Discrete Random Variables Let \\(X_1\\) and \\(X_2\\) be the random variables defined in the previous example, and let \\(Y = X_1 - X_2\\). What is \\(V[Y]\\)? Because only two random variables are involved, we will first do the calculation using the summation form given at the end of the section: \\[\\begin{align*} V[Y] &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j {\\rm Cov}(X_i,X_j) \\\\ &amp;= 1^2 V[X_1] + (-1)^2 V[X_2] + 2 (1) (-1) \\mbox{Cov}(X_1,X_2) \\\\ &amp;= V[X_1] + V[X_2] - 2 \\mbox{Cov}(X_1,X_2) \\\\ &amp;= 0.25 + 0.45 - 2(-0.05) = 0.80 \\,. \\end{align*}\\] Here is the same calculation using R. a &lt;- c(1,-1) Sigma &lt;- matrix(c(0.25,-0.05,-0.05,0.45),nrow=2) # fill column-by-column t(a) %*% Sigma %*% a ## [,1] ## [1,] 0.8 Note that a is by default a column vector, and thus must be transposed via the t() function, and that %*% is the matrix multiplication operator. 6.3.3 Uncorrelated is Not the Same as Independent: a Demonstration Let the region shown in Figure 6.6 be the domain of a bivariate uniform distribution. (The area of the domain is 1, hence the amplitude of the bivariate pdf is 1/1 = 1.) Let \\(X_1\\) and \\(X_2\\) be random variables drawn from this distribution. Because the domain is not “rectangular,” we can state that \\(X_1\\) and \\(X_2\\) are dependent random variables. What is the correlation between them? The definition of covariance is Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2]\\). If we examine the figure, we can convince ourselves that \\(E[X_1] = 0\\) due to the uniform nature of the pdf and the symmetry of the domain about \\(x_1 = 0\\). Hence Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2]\\), which is \\[\\begin{align*} E[X_1X_2] &amp;= \\int_0^1 x_2 \\left[ \\int_{x_2-1}^{1-x_2} x_1 dx_1 \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\left. \\frac{x_1^2}{2} \\right|_{x_2-1}^{1-x_2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\frac{(x_2-1)^2}{2} - \\frac{(1-x_2)^2}{2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\frac{(x_2-1)^2}{2} - \\frac{(x_2-1)^2}{2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ 0 \\right] dx_2 = 0 \\,. \\end{align*}\\] Hence Cov(\\(X_1,X_2\\)) = 0 (and the correlation between \\(X_1\\) and \\(X_2\\) is zero). This result demonstrates that uncorrelated random variables are not necessarily independent random variables: they are simply random variables that exhibit no linear dependence. (One way to view this intuitively is to imagine that we sample \\(n\\) data from this distribution and then regress \\(X_2\\) (as \\(Y\\)) against \\(X_1\\) (as \\(x\\)) using linear regression. The linear regression line would, on average, pass flat through the points, i.e., would, on average, have a slope of zero, indicating no linear association between \\(X_1\\) and \\(X_2\\). Figure 6.6: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region \\(x_1 &gt; 2x_2\\) is indicated by the blue triangle. 6.3.4 Covariance of Multinomial Random Variables In Chapter 3, we introduce the multinomial distribution, which governs experiments in which we sample data that can \\(m\\) different discrete values. The probability mass function is \\[ p_{X_1,\\ldots,X_m}(x_1,\\ldots,x_m \\vert p_1,\\ldots,p_m) = \\frac{k!}{x_1! \\cdots x_m!}p_1^{x_1}\\cdots p_m^{x_m} \\,, \\] where \\(x_i\\) represents the number of times outcome \\(i\\) is observed in \\(k\\) trials, and where \\(p_i\\) is the probability of observing outcome \\(i\\). As stated in Chapter 3, the distribution for each random variable \\(X_i\\) is Binomial(\\(k,p_i\\)), but the \\(X_i\\)’s are not independent; because \\(\\sum_{i=1}^m X_i = k\\), increase the value of one of the random variables would lead the values of the others to decrease on average. Here we will derive the result that was quoted in Chapter 3, namely that Cov(\\(X_i\\),\\(X_j\\)) = \\(-kp_ip_j\\) for \\(i \\neq j\\). Because \\(X_i\\) and \\(X_j\\) are each binomially distributed random variables, we can view them as sums of Bernoulli distributed random variables, i.e., we can write \\[ X_i = \\sum_{l=1}^k U_l ~~\\mbox{and}~~ X_j = \\sum_{l=1}^k V_l \\,, \\] where \\(U_l \\sim\\) Bernoulli(\\(p_i\\)) and \\(V_l \\sim\\) Bernoulli(\\(p_j\\)), and where \\(l\\) is an index representing the \\(l^{\\rm th}\\) trial. The covariance of \\(X_i\\) and \\(X_j\\) is thus \\[\\begin{align*} \\mbox{Cov}(X_i,X_j) &amp;= E[X_iX_j] - E[X_i]E[X_j] \\\\ &amp;= E[X_iX_j] - (k p_i)(k p_j) \\\\ &amp;= E[X_iX_j] - k^2p_ip_j \\,, \\end{align*}\\] where \\[\\begin{align*} E[X_iX_j] &amp;= E[U_1V_1 + U_1V_2 + \\cdots + U_1V_k + U_2V_1 + U_2V_2 + \\cdots + U_2V_k + \\cdots + U_kV_k] \\\\ &amp;= E[U_1V_1] + E[U_1V_2] + \\cdots + E[U_kV_k] \\,. \\end{align*}\\] There are \\(k^2\\) terms in this summation overall. Because we cannot observe two different outcomes in the same trial, \\(E[U_iV_i] = 0\\) for all indices \\(i \\in [1,k]\\). As for the other \\(k^2 - k\\) terms…let’s starting by looking at one of them: \\[ E[U_1V_2] = E[U_1]E[V_2] = p_ip_j \\,. \\] This result holds because the results of any two trials in a multinomial experiment are independent random variables. Thus the sum of the remaining \\(k^2-k\\) terms is \\((k^2-k)p_ip_j\\), and thus \\[ \\mbox{Cov}(X_i,X_j) = (k^2-k)p_ip_j - k^2p_ip_j = -kp_ip_j \\,. \\] 6.3.5 Tying Covariance Back to Simple Linear Regression Above, we point out how \\(R^2\\) is related to the correlation coefficient between the \\(x_i\\)’s and \\(Y_i\\)’s. Here, we extend that discussion to the determinination of the correlation coefficient between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in simple linear regression. When discussing simple linear regression in Chapter 2, we give formulae for the variances of both \\(V[\\hat{\\beta}_0]\\) and \\(V[\\hat{\\beta}_1]\\). However, now that we know about the concept of covariance, we can ask a question that we did not ask then: are \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) independent random variables? The answer is: of course they are not; if we have a set of data that we are trying to draw a line through, it should be intuitively obvious that changing the intercept of the line will lead to its slope having to change as well in order to (re-)optimize the sum of squared errors. So: the covariance between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is non-zero…but how do we determine it? The standard method of determining covariance involves the calculation of the so-called variance-covariance matrix. (To be clear, this is nomenclature that is specific to regression contexts.) Stated without derivation or proof, that matrix is given by the following: \\[ \\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\,, \\] where \\(\\sigma^2\\) is the true variance (which, as you will recall, we assume does not vary as a function of \\(x\\)), \\(\\mathbf{X}\\) is the design matrix \\[ \\mathbf{X} = \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\,, \\] the superscript \\(T\\) denotes the matrix transpose, and where the superscript \\(-1\\) denotes matrix inversion. The column of 1’s in the design matrix represents what we multiply \\(\\beta_0\\) by in the model, while the column of \\(x_i\\)’s represents what we multiply \\(\\beta_1\\) by. As far as \\(\\sigma^2\\) is concerned…we generally never know this quantity, so we plug in \\(\\widehat{\\sigma^2}\\) in place of \\(\\sigma^2\\) above. Let’s demonstrate how this works using the same data as we used to talk through the output from the R function lm() in Chapter 2. set.seed(202) x &lt;- runif(40,min=0,max=10) Y &lt;- 4 + 0.5*x + rnorm(40) lm.out &lt;- lm(Y~x) summary(lm.out) ## ## Call: ## lm(formula = Y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 Recall that the estimate of \\(\\sigma\\) is given by the Residual standard error, which here is 0.9792, so \\(\\widehat{\\sigma^2} = 0.9587\\). We can extract this from the object lm.out: hat.sigma &lt;- summary(lm.out)$sigma hat.sigma2 &lt;- hat.sigma^2 (To determine what quantities are available in an R list output by a function such as summary(lm), we can type names(summary(lm)). Here, names() indicates that sigma is an accessible list element.) We can set up the design matrix as follows: X &lt;- cbind(rep(1,40),x) “cbind” means “column bind”: the first argument is a vector of 1’s that is bound to the second argument, which is the vector of \\(x_i\\)’s, thus creating a matrix with 40 rows and 2 columns. We can now compute the variance-covariance matrix: hat.sigma2 * solve(t(X) %*% X) ## x ## 0.10344444 -0.01652116 ## x -0.01652116 0.00343436 The function t() returns the transpose of \\(X\\) (a matrix with 2 rows and 40 columns), while solve() is the matrix inversion function. What do we see above? First, if we take the square roots of the matrix elements at upper left and lower right (i.e., along the matrix diagonal), we get \\[ se(\\hat{\\beta}_0) = \\sqrt{0.1034} = 0.3216 ~~~ \\mbox{and} ~~~ se(\\hat{\\beta}_0) = \\sqrt{0.0034} = 0.0586 \\,. \\] These quantities match the values in the Std. Error column of the coefficient table output by summary(). Good! In addition, however, we see the off-diagonal element \\(-0.0165\\): this is Cov(\\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)). The negative sign makes sense: if we increase the intercept, we have to make the slope smaller (i.e., more negative) to optimize that coefficient. The correlation is given by \\[ \\frac{\\mbox{Cov}(\\hat{\\beta}_0,\\hat{\\beta}_1)}{se(\\hat{\\beta}_0) \\cdot se(\\hat{\\beta}_1)} = \\frac{-0.0165}{0.3216 \\cdot 0.0586} = -0.8755 \\,. \\] We see immediately that the intercept and slope are strongly (and negatively) correlated. In “real-life” situations, do we need to carry out the chain of computations we carry out above? No…because R provides wrapper functions to help us: (Sigma &lt;- vcov(lm.out)) # compute the variance-covariance matrix ## (Intercept) x ## (Intercept) 0.10344444 -0.01652116 ## x -0.01652116 0.00343436 cov2cor(Sigma) # convert the covariance to correlation ## (Intercept) x ## (Intercept) 1.0000000 -0.8765245 ## x -0.8765245 1.0000000 (We note that the difference between the correlation coefficient computed above and that output by cov2cor() is entirely due to round-off error.) 6.3.6 Tying Covariance to Simple Logistic Regression In the last example, we state that the variance-covariance matrix for simple linear regression is given by \\[ \\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\,. \\] A similar expression exists for logistic regression: \\[ \\boldsymbol{\\Sigma} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\,, \\] where the diagonal weight matrix \\(\\mathbf{W}\\) is given by \\[ \\mathbf{W} = \\left( \\begin{array}{cccc} \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_1)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_1))^2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_2)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_2))^2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_n)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_n))^2} \\end{array} \\right) \\,. \\] Note that if we were to replace every non-zero matrix entry above with \\(1/\\sigma^2\\), we would recover the expression \\(\\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Below, we show how to populate the \\(\\mathbf{W}\\) matrix and use it to determine the variance-covariance matrix. # run simple logistic regression on training dataset log.out &lt;- glm(class~col.iz,data=df.train,family=binomial) summary(log.out) ## ## Call: ## glm(formula = class ~ col.iz, family = binomial, data = df.train) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.16841 0.09265 1.818 0.06911 . ## col.iz -0.96729 0.31051 -3.115 0.00184 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 970.41 on 699 degrees of freedom ## Residual deviance: 958.16 on 698 degrees of freedom ## AIC: 962.16 ## ## Number of Fisher Scoring iterations: 4 hat.beta0 &lt;- log.out$coefficients[1] hat.beta1 &lt;- log.out$coefficients[2] e &lt;- exp(hat.beta0 + hat.beta1*df.train$col.iz) W &lt;- diag(e/(1+e)^2) X &lt;- cbind(rep(1,nrow(df.train)),df.train$col.iz) (Sigma &lt;- solve(t(X) %*% W %*% X)) ## [,1] [,2] ## [1,] 0.008584293 -0.01635715 ## [2,] -0.016357147 0.09641920 sqrt(Sigma[1,1]) ## [1] 0.09265146 sqrt(Sigma[2,2]) ## [1] 0.3105144 We see that the diagonal elements match the standard errors output by the summary() function. We can find the correlation between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by using the function cov2cor(): cov2cor(Sigma) ## [,1] [,2] ## [1,] 1.0000000 -0.5685564 ## [2,] -0.5685564 1.0000000 The correlation is not as strong as we observed above in our simple linear regression example, but in absolute terms we would still say that \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are strongly negatively correlated. 6.4 Marginal and Conditional Distributions Let’s say that we perform an experiment where we sample a pair of random variables \\((X_1,X_2)\\) from a bivariate distribution, where \\(X_1\\) and \\(X_2\\) are dependent random variables. Despite doing this, we might ultimately find that we are only interested in how \\(X_1\\) is distributed, regardless of the sampled value of \\(X_2\\) (or vice-versa). The distribution that we’d like to derive is dubbed a marginal distribution. To compute the marginal distribution for \\(X_1\\) when given a bivariate pdf \\(f_{X_1,X_2}(x_1,x_2)\\), we integrate over all values of \\(x_2\\): \\[ f_{X_1}(x_1) = \\int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 \\,, \\] while for a bivariate pmf, we simply replace integration with summation, e.g., \\[ p_{X_1}(x_1) = \\sum_{x_2} p_{X_1,X_2}(x_1,x_2) \\,. \\] (If we are interested in computing the marginal distribution for \\(X_2\\), we would simply swap the indices 1 and 2 above.) An important thing to keep in mind is that a marginal pdf (or pmf) is a pdf (or pmf), meaning that it is like any other pdf (or pmf): it is non-negative and it integrates (or sums) to one, it has an expected value and a standard deviation, etc. A related, albeit narrower question to the one motivating the computation of marginals is: what is the distribution of \\(X_1\\) when \\(X_2 = x_2\\)? (It is narrower in the sense that for a marginal, we do not care about the sampled value of \\(X_2\\), while here, we do.) This distribution is dubbed a conditional distribution, and it is defined as follows: \\[ f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} \\,. \\] For a bivariate pmf, the expression is similar. As was the case with a marginal distribution, a conditional pdf (or pmf) is a pdf (or pmf). One might notice immediately the similarity between this expression and the one defining conditional probability in Chapter 1: \\(p(A \\vert B) = p(A \\cap B)/p(B)\\). This is not coincidental. For marginals, the analogous probability expression is \\(p(A) = \\sum_i p(A \\vert B_i) p(B_i)\\), i.e., the law of total probability! If this is not immediately evident, note that we can replace the \\(f_{X_1,X_2}(x_1,x_2)\\) in the marginal integral with \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) f_{X_2}(x_2)\\). Then we can see how \\(x_1\\) takes the place of \\(A\\) and \\(x_2\\) takes the places of \\(B_i\\).) Why do we divide by a marginal distribution when deriving a conditional distribution? The answer is simple: if we do not, then there is no guarantee that the conditional pdf or pmf will integrate or sum to one. Finding the conditional distribution is akin to chopping through the bivariate pdf at a particular value of \\(x_1\\) or \\(x_2\\), and tracing the pdf along the chopped edge. This traced-out function will be non-negative, but it will not necessarily integrate to one. The division by the marginal acts to “normalize” the traced-out function, i.e., the division raises or lowers it such that it subsequently will integrate to one. As a final note, we will mention that if \\(X_1\\) and \\(X_2\\) are independent random variables, then, e.g., \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\\); and \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) = f_{X_1}(x_1)\\) and \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1) = f_{X_2}(x_2)\\) The first bullet point above shows the result that we look for when establishing the independence of two random variables mathematically: we compute the marginals for each variable, take the product of marginals, and see if it matches the original bivariate function. However, this conventional textbook approach to establishing independence is unnecessary, as it is sufficient to examine the domain of the bivariate function and to determine if we can factorize it into separate functions of \\(x_1\\) and \\(x_2\\). 6.4.1 Marginal and Conditional Distributions for a Bivariate PMF Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is the marginal distribution \\(p_{X_2}(x_2)\\) and the conditional distribution \\(p_{X_1 \\vert X_2}(x_1 \\vert x_2)\\)? When probability masses are involved, the marginal distribution is derived by summing over an axis (here, \\(x_1\\)): \\[ p_{X_2}(x_2) = \\sum_{x_1} p_{X_1,X_2}(x_1,x_2) \\,. \\] For this problem, the summation can be done by inspection, yielding the following marginal distribution: \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) 0.60 0.30 0.10 As stated in the text above, a marginal pmf is a pmf, meaning that for instance the values sum to 1, and we can compute quantities like \\(E[X_2] = 0 \\times 0.6 + 1 \\times 0.3 + 2 \\times 0.1 = 0.5\\). As for the conditional distribution, we have the following expression \\[ p_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)} \\,, \\] which in practice means we take the original bivariate table and divide each row by the marginal for \\(x_2\\), if we are conditioning on \\(x_2\\), or divide each column by the marginal for \\(x_1\\), if we are conditioning on \\(x_1\\). Here, we condition on \\(x_2\\), so our result is \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20/0.60 = 0.33 0.30/0.30 = 1.00 0.00/0.10 = 0.00 \\(x_1 = 2\\) 0.40/0.60 = 0.67 0.00/0.30 = 0.00 0.10/0.10 = 1.00 In such a table, the values in the individual columns all need to sum to one: given some value of \\(x_2\\), we will with probability 1 sample a value of \\(x_1\\). (As should be clear, if we condition on \\(x_1\\) instead, we’d generate a table in which the values in each row would sum to 1.) 6.4.2 Marginal and Conditional Distributions for a Bivariate PDF Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). What is the marginal distribution \\(f_{X_1}(x_1)\\)? What is the conditional distribution \\(f_{X_2 \\vert X_l}(x_2 \\vert x_1)\\)? Refer back to Figure 6.1. The marginal distribution is defined as a function of \\(x_1\\), so we integrate over \\(x_2\\)…which means that for any given value of \\(x_1\\), the bounds of integration are \\(x_1 = x_2\\) to 1: \\[ f_{X_1}(x_1) = \\int_{x_1}^1 6(1-x_2) dx_2 \\,. \\] Why is the lower bound \\(x_1\\) instead of \\(x_2\\)? They are interchangable, but if we use \\(x_2\\), then our final result would not be a function of \\(x_1\\)! In general, if we integrate along one axis, the integral bounds should be expressed in terms of the other axis. To continue: \\[\\begin{align*} f_{X_1}(x_1) &amp;= \\int_{x_1}^1 6(1-x_2) dx_2 \\\\ &amp;= 6 \\left[ \\int_{x_1}^1 dx_2 - \\int_{x_1}^1 x_2 dx_2 \\right] \\\\ &amp;= 6 \\left[ \\left. x_2 \\right|_{x_1}^1 - \\left. \\frac{x_2^2}{2} \\right|_{x_1}^1 \\right] \\\\ &amp;= 6 \\left[ 1 - x_1 - \\left(\\frac{1}{2}-\\frac{x_1^2}{2} \\right) \\right] \\\\ &amp;= 3 - 6x_1 + 3x_1^2 = 3(1-x_1)^2 \\,, \\end{align*}\\] for \\(0 \\leq x_1 \\leq 1\\). See Figure 6.7. Given the functional form and domain, we can recognize this as a Beta(1,3) distribution. (Such recognition is helpful if, for instance, we were to ask for the expected value of \\(X_1\\). Rather than doing yet another integral, we’d write that \\(E[X_1] = \\alpha/(\\alpha+\\beta) = 1/4\\).) As for the conditional distribution…once we have the marginal, this is easy to write down: \\[ f_{X_2 \\vert X_1}(x_2 \\vert x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \\frac{6(1-x_2)}{3(1-x_1)^2} = \\frac{2(1-x_2)}{(1-x_1)^2} \\,. \\] Refering back to Figure @(fig:mulfig1), we note that this conditional expression can only be non-zero along the line from \\(x_2 = x_1\\) to 1. So the domain of this conditional distribution is \\(x_2 \\vert x_1 \\in [x_1,1]\\). See Figure 6.7. Figure 6.7: The marginal distribution \\(f_{X_1}(x_1)\\) and conditional distribution \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1=0.3)\\) for the bivariate function \\(f_{X_1,X_2}(x_1,x_2) = 6(1-x_2)\\) with domain \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). 6.5 Conditional Expected Value and Variance As was stated above, a conditional pdf or pmf is a pdf or pmf…meaning that like a pdf or pmf, it has an expected value and a variance. For instance, the expected value of a conditional pdf \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\), or the conditional expected value, is \\[ E[X_1 \\vert X_2] = \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\,. \\] As you might expect, an analogous expression exists for bivariate pmfs: \\(E[X_1 \\vert X_2] = \\sum_{x_1} p_{X_1 \\vert X_2}(x_1 \\vert x_2)\\). Also, there are two important points to make here: While, e.g., \\(E[X_1]\\) is a constant, \\(E[X_1 \\vert X_2]\\) is a random variable due to the randomness of \\(X_2\\)…that is, unless one specifies \\(E[X_1 \\vert X_2=x_2]\\), which is constant because \\(X_2\\) is no longer random. Beware notation! One can generalize \\(E[X_1 \\vert X_2]\\) in the manner of the Law of the Unconscious Statistician: \\(E[g(X_1) \\vert X_2] = \\int_{x_1} g(x_1) f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1\\). The definition of conditional variance builds off of that of the conditional expected value: \\[\\begin{align*} V[X_1 \\vert X_2] &amp;= E[(X_1-\\mu_1)^2 \\vert X_2] \\\\ &amp;= E[X_1^2 \\vert X_2] - (E[X_1 \\vert X_2])^2 \\\\ &amp;= \\int_{x_1} x_1^2 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 - \\left[ \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\right]^2 \\,. \\end{align*}\\] As we can see in the second line above, the fact that we are dealing with a condition does not fundamentally change how variance is computed: the form of the shortcut formula is the same as before, just with the condition added. Can one go from a conditional expected value to an unconditional expected value \\(E[X_1]\\)? Yes: intuitively, this involves averaging the values found for \\(E[X_1 \\vert X_2]\\) over all possible values of \\(x_2\\), weighting each of these possible values by how relatively likely it is in the first place (i.e., in the case of a bivariate pdf, weighting each value of \\(E[X_1 \\vert X_2]\\) by \\(f_{X_2}(x_2)\\)): \\[\\begin{align*} E[X_1] = E[E[X_1 \\vert X_2]] &amp;= \\int_{x_2} f_{X_2}(x_2) E[X_1 \\vert X_2] dx_2 \\\\ &amp;= \\int_{x_2} f_{X_2}(x_2) \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 dx_2 \\\\ &amp;= \\int_{x_2} \\int_{x_1} x_1 f_{X_2}(x_2) f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 dx_2 \\\\ &amp;= \\int_{x_1} \\int_{x_2} x_1 f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = E[X_1] \\,. \\end{align*}\\] As you might expect, given the conditional variance \\(V[X_1 \\vert X_2]\\), we can compute the unconditional variance \\(V[X_1]\\)…and for this, it is simplest to begin by writing down that \\[ V[X_1] = V[E[X_1 \\vert X_2]] + E[V[X_1 \\vert X_2]] \\,. \\] The intuitive interpretation of this equation is that it reflects that \\(X_1\\) can vary because the mean of \\(X_1 \\vert X_2\\) can shift as a function of \\(X_2\\) (so we want to quantify how much \\(E[X_1 \\vert X_2]\\) varies…giving the first term on the right above), but it can also vary due to being randomly distributed about the mean (so we want to quantify how much, on average, is that random variation…which gives the second term above). As one might guess, writing out the above expression in terms of integrals or summations over bivariate functions is going to be messy and thus we forego doing this here. (This might lead one to ask “how will we solve unconditional variance problems if expressions with integrals or summations are not provided?” It turns out there is an entire class of problems that we can solve without integration or summation, and we provide an example of a problem from that class below.) Note that if \\(X_1\\) and \\(X_2\\) are independent, it follows that, e.g., \\(E[X_1 \\vert X_2] = E[X_1]\\) and \\(V[X_1 \\vert X_2] = V[X_1]\\). Let’s look at the latter expression. Above, we wrote that \\(V[X_1] = V[E[X_1 \\vert X_2]] + E[V[X_1 \\vert X_2]]\\). If \\(X_1\\) and \\(X_2\\) are independent, then \\(E[X_1 \\vert X_2]\\) is a constant, and thus \\(V[E[X_1 \\vert X_2]] = 0\\) (because a constant does not vary). In addition, \\(V[X_1 \\vert X_2]\\) is a constant (it doesn’t vary as \\(X_2\\) changes) and thus \\(E[V[X_1 \\vert X_2]] = V[X_1 \\vert X_2]\\). So in the end, \\(V[X_1] = 0 + V[X_1 \\vert X_2] = V[X_1 \\vert X_2]\\). 6.5.1 Conditional and Unconditional Expected Value Given a Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\) (see Figure 6.1). For this distribution, the marginal \\(f_{X_2}(x_2)\\) is \\(6x_2(1-x_2)\\) for \\(x_2 \\in [0,1]\\), or a Beta(2,2) distribution, while the conditional distribution \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\) is \\[ f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \\frac{6(1-x_2)}{6(1-x_2)x_2} = \\frac{1}{x_2} \\,, \\] for \\(x_1 \\in [0,x_2]\\). (The interested reader can verify these results!) What is \\(E[X_1 \\vert X_2]\\) and what is \\(E[X_1]\\)? For the conditional expected value, we have that \\[\\begin{align*} E[X_1 \\vert X_2] &amp;= \\int_0^{x_2} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\\\ &amp;= \\int_0^{x_2} \\frac{x_1}{x_2} dx_1 \\\\ &amp;= \\frac{1}{x_2} \\int_0^{x_2} x_1 dx_1 \\\\ &amp;= \\frac{1}{x_2} \\left. \\frac{x_1^2}{2} \\right|_0^{x_2} = \\frac{1}{x_2} \\frac{x_2^2}{2} = \\frac{x_2}{2} \\,. \\end{align*}\\] It turns out that we did not necessarily have to do this integral, however. Note that the conditional expression is \\(1/x_2\\) for \\(x_1 \\in [0,x_2]\\)…this is a uniform distribution! So we’d see, by inspection, that the conditional expected value is \\(x_2/2\\). To determine the unconditional expected value \\(E[X_1]\\), we weight every possible value of \\(E[X_1 \\vert X_2]\\) by the probability (density) that we would even observe \\(X_2\\) in the first place (which is the marginal distribution for \\(X_2\\)): \\[\\begin{align*} E[X_1] &amp;= \\int_0^1 f_{X_2}(x_2) E[X_1 \\vert X_2] dx_2 \\\\ &amp;= \\int_0^1 6 x_2 (1 - x_2) \\frac{x_2}{2} dx_2 \\\\ &amp;= 3 \\int_0^1 x_2^2 (1 - x_2) dx_2 \\\\ &amp;= 3 B(3,2) = 3 \\times 2! \\times 1! / 4! = 1/4 \\,. \\end{align*}\\] 6.5.2 Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions Let’s look at the following problem: the number of homework assignments due in a given week, \\(N\\), varies from week to week and its value is sampled from a Poisson distribution with mean \\(\\lambda\\). The time to complete any one homework assignment, \\(X\\), also varies, and its value is sampled from a Gamma distribution with parameter \\(\\alpha\\) and \\(\\beta\\) (and is the form of the Gamma with expected value \\(\\alpha\\beta\\) and variance \\(\\alpha\\beta^2\\)). Let \\(T \\vert N = \\sum_{i=1}^N X_i\\) be the total time spent completing \\(N\\) homework assignments. (Assume all the \\(X_i\\)’s are independent random variables.) What are \\(E[T]\\) and \\(V[T]\\)? We first note that because we are working with two univariate distributions for which the expected value and variance are known, there is no need to, e.g., integrate. We simply need to identify \\(E[T \\vert N]\\) and \\(V[T \\vert N]\\) and work with those expressions directly. We’ll start with the expected value: \\[ E[T] = E[E[T \\vert N]] = E\\left[E\\left[\\sum_{i=1}^N X_i\\right]\\right] = E\\left[\\sum_{i=1}^N E[X_i]\\right] = E[N\\alpha\\beta] = \\alpha \\beta E[N] = \\alpha\\beta\\lambda \\,. \\] Somewhat more complicated is the computation of the variance: \\[\\begin{align*} V[T] &amp;= V[E[T \\vert N]] + E[V[T \\vert N]] \\\\ &amp;= V[N \\alpha \\beta] + E\\left[ V \\left[ \\sum_{i=1}^N X_i \\right] \\right] \\\\ &amp;= \\alpha^2 \\beta^2 V[N] + E \\left[ \\sum_{i=1}^N V[X_i] \\right] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + E \\left[ \\sum_{i=1}^N \\alpha \\beta^2 \\right] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + E[N\\alpha \\beta^2 ] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + \\alpha \\beta^2 E[N] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + \\alpha \\beta^2 \\lambda = \\alpha (\\alpha + 1) \\beta^2 \\lambda \\,. \\end{align*}\\] Thus in any randomly chosen week, the time needed to complete homework is \\(\\alpha\\beta\\lambda\\) on average, with standard deviation \\(\\sqrt{\\alpha (\\alpha + 1) \\beta^2 \\lambda}\\). 6.6 The Multivariate Normal Distribution The multivariate normal distribution is the most important one in multivariate settings, due in part to its role in the multivariate analogue of the central limit theorem: if we have a collection of \\(n\\) iid random vectors, where \\(n\\) is sufficiently large then the sample mean vector is going to be approximately multivariate normally distributed. The joint probability density function for the multivariate normal is given by \\[ f_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^p \\vert \\boldsymbol{\\Sigma} \\vert}} \\exp\\left(-\\frac12 (\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\right) \\,. \\] Here, \\(\\mathbf{x} = \\{x_1,\\ldots,x_p\\}\\), and \\(\\boldsymbol{\\mu} = \\{\\mu_1,\\ldots,\\mu_p\\}\\) are the centroids of the normal along each of the \\(p\\) coordinate axes. \\(\\boldsymbol{\\Sigma}^{-1}\\) is the inverse of the covariance matrix \\(\\boldsymbol{\\Sigma}\\), while \\(\\vert \\boldsymbol{\\Sigma} \\vert\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\). (If the reader is unfamiliar with these terms, see the short description of matrices in Chapter 8.) We denote sampling from this distribution with the notation \\[ \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\,, \\] where \\(\\mathbf{X}\\) is a \\(p\\)-dimensional vector. To be clear: the multivariate normal is not a distribution that we work with by hand! We would conventionally use, e.g., R to do any and all calculations. That said, there are two qualitative facts about the multivariate normal that are useful to know. A marginal distribution of the multivariate normal is itself a univariate or multivariate normal (depending on \\(p\\) and how many axes are being integrated over). To derive a marginal distribution, one simply needs to remove elements from the mean vector and from the covariance matrix. For instance, if \\(p = 2\\) and we wish to derive the marginal distribution for \\(X_1\\), then \\[\\begin{align*} \\boldsymbol{\\mu} &amp;= [ \\mu_1 ~ \\mu_2 ] ~~\\rightarrow \\mu_{\\rm marg} = \\mu_1 \\\\ \\boldsymbol{\\Sigma} &amp;= \\left( \\begin{array}{cc} V[X_1] &amp; \\mbox{Cov}(X_1,X_2) \\\\ \\mbox{Cov}(X_2,X_1) &amp; V[X_2] \\end{array} \\right) ~~\\rightarrow~~ \\Sigma_{\\rm marg} = V[X_1] \\,. \\end{align*}\\] Here, we removed the second element of \\(\\boldsymbol{\\mu}\\) and the second row and second column of \\(\\boldsymbol{\\Sigma}\\). Note that \\(\\Sigma_{\\rm marg}^{-1}\\) is trivially \\(1/V[X_1] = 1/\\sigma_1^2\\) and that the determinant of \\(\\Sigma_{\\rm marg}\\) is trivially \\(V[X_1] = \\sigma_1^2\\). Plugging in these values, we can see that the marginal distribution for \\(X_1\\) has the form of a univariate normal pdf. A conditional distribution of the multivariate normal is itself a univariate or multivariate normal (depending on \\(p\\) and how many axes are being conditioned upon). To derive a conditional distribution, we first identify which axes are being conditioned upon. Call that set of axes \\(v\\); all others we dub \\(u\\). (For instance, perhaps we want to determine the joint pdf of \\(X_1\\) and \\(X_3\\) given that we have set \\(X_2 = x_2\\) and \\(X_4 = x_4\\). Here, \\(u = \\{1,3\\}\\) and \\(v = \\{2,4\\}\\).) We split the mean vector and the covariance matrix into pieces: \\[\\begin{align*} \\boldsymbol{\\mu} &amp;\\rightarrow [ \\boldsymbol{\\mu}_u ~ \\boldsymbol{\\mu}_v ] \\\\ \\boldsymbol{\\Sigma} &amp;\\rightarrow \\left( \\begin{array}{cc} \\boldsymbol{\\Sigma}_{uu} &amp; \\boldsymbol{\\Sigma}_{uv} \\\\ \\boldsymbol{\\Sigma}_{vu} &amp; \\boldsymbol{\\Sigma}_{vv} \\end{array} \\right) \\,. \\end{align*}\\] (So, for instance, \\(\\boldsymbol{\\mu}_v = [ \\mu_2 ~ \\mu_4 ]\\), and \\[ \\boldsymbol{\\Sigma}_{uu} = \\left( \\begin{array}{cc} V[X_1] &amp; \\mbox{Cov}(X_1,X_3) \\\\ \\mbox{Cov}(X_3,X_1) &amp; V[X_3] \\end{array} \\right) \\,, \\] etc.) Given these pieces, we can define the conditional distribution \\(\\mathbf{X}_u \\vert \\mathbf{X}_v = \\mathbf{x}_v\\) as \\[ \\mathbf{X}_u \\vert \\mathbf{X}_v = \\mathbf{x}_v \\sim \\mathcal{N}(\\boldsymbol{\\mu}_c,\\boldsymbol{\\Sigma}_c) \\,, \\] where \\[\\begin{align*} \\boldsymbol{\\mu}_c &amp;= \\boldsymbol{\\mu}_u + \\boldsymbol{\\Sigma}_{uv} \\boldsymbol{\\Sigma}_{vv}^{-1} (\\mathbf{x}_v - \\boldsymbol{\\mu}_v) \\\\ \\boldsymbol{\\Sigma}_c &amp;= \\boldsymbol{\\Sigma}_{uu} - \\boldsymbol{\\Sigma}_{uv} \\boldsymbol{\\Sigma}_{vv}^{-1} \\boldsymbol{\\Sigma}_{vu} \\,. \\end{align*}\\] Multivariate Normal - R Functions (mvtnorm Package) quantity R function call PDF dmvnorm(x,mu,Sigma) CDF pmvnorm(x,mu,Sigma) Inverse CDF qmvnorm(q,mu,Sigma) \\(n\\) iid random samples rmvnorm(n,mu,Sigma) 6.6.1 The Marginal Distribution of a Multivariate Normal Distribution Let’s suppose that we have a three-dimensional normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,4,6\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{ccc} 2 &amp; 0.5 &amp; 1.2 \\\\ 0.5 &amp; 2 &amp; 1 \\\\ 1.2 &amp; 1 &amp; 2 \\end{array} \\right) \\,. \\] What is the marginal distribution for \\((X_1,X_2)\\)? To determine the marginal distribution, we simply remove the third element of \\(\\boldsymbol{\\mu}\\) and the third row and column of \\(\\boldsymbol{\\Sigma}\\); the marginal distribution is a bivariate normal with means \\(\\boldsymbol{\\mu} = \\{2,4\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{cc} 2 &amp; 0.5 \\\\ 0.5 &amp; 2 \\end{array} \\right) \\,. \\] In R, we might visualize the result using the following code: mu &lt;- c(2,4,6) (Sigma &lt;- matrix(c(2,0.5,1.2,0.5,2,1,1.2,1,2),nrow=3)) ## [,1] [,2] [,3] ## [1,] 2.0 0.5 1.2 ## [2,] 0.5 2.0 1.0 ## [3,] 1.2 1.0 2.0 keep &lt;- c(1,2) mu.marg &lt;- mu[keep] Sigma.marg &lt;- Sigma[keep,keep] x1.plot &lt;- seq(-1.5,5.5,by=0.05) x2.plot &lt;- seq(0.5,7.5,by=0.05) x.plot &lt;- expand.grid(x1=x1.plot,x2=x2.plot) library(mvtnorm) library(metR) x.plot$fx &lt;- matrix(dmvnorm(x.plot,mu.marg,Sigma.marg),ncol=1) ggplot(data=x.plot,aes(x=x1,y=x2)) + geom_contour(aes(z=fx),col=&quot;turquoise&quot;,breaks=seq(0.01,0.08,by=0.01)) + geom_text_contour(aes(z=fx),skip=0.75,col=&quot;turquoise&quot;,stroke=0.2,stroke.color=&quot;azure2&quot;) + labs(x=expression(x[1]),y=expression(x[2])) + base_theme Figure 6.8: Contour plot indicating the location and orientation of a bivariate normal distribution with mean \\(\\boldsymbol{\\mu} = \\{2,4\\}\\) and covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center. 6.6.2 The Conditional Distribution of a Multivariate Normal Distribution Let’s assume the same setting as in the previous example. What is the conditional distribution of \\(X_1\\) and \\(X_3\\) given that \\(X_2 = 5\\)? Because of the complexity of the calculation, we will answer this question using R code only, following the mathematical prescription given in the text above. mu &lt;- c(2,4,6) Sigma &lt;- matrix(c(2,0.5,1.2,0.5,2,1,1.2,1,2),nrow=3) u &lt;- c(1,3) v &lt;- c(2) v.coord &lt;- c(5) mu.u &lt;- mu[u] mu.v &lt;- mu[v] Sigma.uu &lt;- Sigma[u,u] Sigma.uv &lt;- Sigma[u,v] Sigma.vu &lt;- Sigma[v,u] Sigma.vv &lt;- Sigma[v,v] mu.c &lt;- mu.u + Sigma.uv %*% solve(Sigma.vv) %*% (v.coord - mu.v) (Sigma.c &lt;- Sigma.uu - Sigma.uv %*% solve(Sigma.vv) %*% Sigma.vu) ## [,1] [,2] ## [1,] 1.875 0.95 ## [2,] 0.950 1.50 x1.plot &lt;- seq(-1.5,5.5,by=0.05) x2.plot &lt;- seq(3,10,by=0.05) x.plot &lt;- expand.grid(x1=x1.plot,x2=x2.plot) library(mvtnorm) library(metR) x.plot$fx &lt;- matrix(dmvnorm(x.plot,mu.c,Sigma.c),ncol=1) ggplot(data=x.plot,aes(x=x1,y=x2)) + geom_contour(aes(z=fx),col=&quot;turquoise&quot;,breaks=seq(0.01,0.11,by=0.01)) + geom_text_contour(aes(z=fx),skip=0.8,col=&quot;turquoise&quot;,stroke=0.2,stroke.color=&quot;azure2&quot;) + labs(x=expression(x[1]),y=expression(x[3])) + base_theme Figure 6.9: Contour plot indicating the location and orientation of a bivariate normal distribution with conditional mean \\(\\boldsymbol{\\mu} = \\{2.25,6.50\\}\\) and conditional covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center. 6.6.3 The Calculation of Sample Covariance Let’s assume that we observe \\(n = 40\\) iid data drawn from a bivariate normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,1\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{cc} 1 &amp; 0.5 \\\\ 0.5 &amp; 2 \\end{array} \\right) \\,. \\] Furthermore, we assume that the covariance matrix is unknown to us. How would we estimate this matrix? set.seed(101) n &lt;- 40 mu &lt;- c(2,1) Sigma &lt;- matrix(c(1,0.5,0.5,2),nrow=2) X &lt;- rmvnorm(n,mu,Sigma) head(round(X,3),3) # x1: column 1 | x2: column2 ## [,1] [,2] ## [1,] 1.798 1.704 ## [2,] 1.385 1.158 ## [3,] 2.551 2.707 df &lt;- data.frame(x=X[,1],y=X[,2]) ggplot(data=df,aes(x=x,y=y)) + geom_point(col=&quot;blue&quot;,size=3) + labs(x=expression(x[1]),y=expression(x[2])) + theme(axis.title=element_text(size = rel(1.25))) Figure 6.10: Sample of \\(n = 40\\) iid data drawn from a bivariate normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,1\\}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) given in the main body of the text. First, let’s sample some data. See Figure 6.10. The sample covariance \\(C_{jk}\\) between variables \\(j\\) and \\(k\\) is given by \\[ C_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (X_{ji} - \\bar{X}_j)(X_{ki} - \\bar{X}_k) \\,. \\] So, for instance, for our data we would determine \\(C_{12}\\) in R as follows: (1/(n-1))*sum((X[,1]-mean(X[,1]))*(X[,2]-mean(X[,2]))) ## [1] 0.389421 This calculation matches that done by the R function cov(): cov(X) ## [,1] [,2] ## [1,] 0.8057418 0.389421 ## [2,] 0.3894210 1.653003 The sample covariance matrix is an unbiased estimator of the true covariance. We mention for completeness that just as the (scaled) sample variance \\((n-1)S^2/\\sigma^2\\) is chi-square-distributed for \\(n-1\\) degrees of freedom, the sample covariance matrix is sampled from a Wishart distribution, a multivariate generalization of the chi-square distribution. 6.7 Exercises We are given the following bivariate pdf: \\[\\begin{align*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} k(x_1+x_2^2) &amp; 0 \\leq x_1 \\leq 1, 0 \\leq x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) Determine the constant \\(k\\). (b) What is the conditional probability density function \\(f_{X_1|X_2}(x_1|x_2)\\)? We are given the probability mass function shown below. (a) Compute Cov(\\(X_1,X_2\\)). (b) Compute \\(\\rho_{X_1,X_2}\\). (c) Write down the conditional expected value \\(E[X_1 \\vert X_2 &lt; 1]\\). (This can be done by inspection.) (d) Write down the conditional variance \\(V[X_2 \\vert X_1 = 1]\\). (This can also be done by inspection.) \\(x_1 = 0\\) \\(x_1 = 1\\) \\(x_2 = 0\\) 0.1 0.4 \\(x_2 = 1\\) 0.4 0.1 Every day, we sample \\(n\\) items built via an assembly line and look for defectives. On any given day, there is a constant probability \\(p\\) of assembling a defective item, but \\(p\\) can change from day to day: it is sampled from a Uniform(0,0.1) distribution. (a) What is the unconditional expected value for \\(X\\), the number of defectives observed per day? (Hint: what is the named distribution for the conditional expected value?) (b) What is the unconditional variance of \\(X\\)? We are given the following bivariate probability density function: \\[\\begin{align*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cl} k x_1^2 x_2 &amp; x_1 \\geq 0,\\,x_2 \\geq 0,\\,x_1+x_2 \\leq 1 \\\\ \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (a) Determine the value of \\(k\\) such that \\(f_{X_1,X_2}(x_1,x_2)\\) is a valid bivariate pdf. (b) Compute \\(P(X_1 &gt; 0.25 \\vert X_2 = 0.5)\\). (c) Are \\(X_1\\) and \\(X_2\\) independent random variables? We are given the probability mass function shown below. (a) Write down the marginal probability mass function \\(p_{X_2}(x_2)\\). (b) Using our answer from (a), write down the marginal cdf \\(F_{X_2}(x_2)\\). (c) Assume \\(p_X(x) = p_{X_2}(x_2)\\) (i.e., forget the table exists…all we have now is a univariate pmf). Compute the mean and standard deviation of \\(p_X(x)\\). \\(x_1 = 0\\) \\(x_1 = 1\\) \\(x_1 = 2\\) \\(x_2 = 0\\) 0 0.2 0.1 0.1 \\(x_2 = 1\\) 1 0.1 0.3 0 \\(x_2 = 2\\) 2 0.05 0.05 0.1 We are given the following bivariate probability density function: \\[\\begin{align*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cl} 60 x_1^2 x_2 &amp; x_1 \\geq 0,\\,x_2 \\geq 0,\\,x_1+x_2 \\leq 1 \\\\ \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] We are also given that \\(E[X_1] = 1/2\\), \\(E[X_2] = 1/3\\), \\(E[X_1^2] = 2/7\\), and \\(E[X_2^2] = 1/7\\). (a) Compute Cov(\\(X_1,X_2\\)). (b) Compute Corr(\\(X_1,X_2\\)). (c) Compute \\(V[X_1-2X_2]\\). The number of eggs laid by a certain insect is Poisson-distributed with mean \\(\\lambda\\), and the probability that any one egg hatches is \\(p\\). Assume that the eggs hatch independently of one another. (a) Compute the expected value for the number of eggs that hatch. (b) Compute the variance for the number of eggs that hatch. We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} k x_1 x_2 e^{-x_1/2} (1-x_2) &amp; 0 \\leq x_1 &lt; \\infty \\,, \\,0 \\leq x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] \\(X_1\\) and \\(X_2\\) are independent random variables. (a) What (numerical) value of \\(k\\) makes this a valid bivariate pdf? (b) Compute \\(V[X_1-X_2]\\). We are given that the random variables \\(X_1\\) and \\(X_2\\) are distributed uniformly within the region bounded by \\(0 \\leq x_2 \\leq 1\\) and \\(0 \\leq x_1 \\leq 2-x_2\\). (a) What is \\(f_{X_1,X_2}(x_1,x_2)\\) in the region specified above? For parts (b)-(d), assume \\(f_{X_1,X_2}(x_1,x_2) = k\\). (b) Write down the marginal distribution for \\(X_2\\). (c) Specify the conditional distribution for \\(X_1\\) given \\(X_2 = x_2\\). (d) What is the expected value of \\(X_1\\)? (e) Are \\(X_1\\) and \\(X_2\\) independent or dependent random variables? A car drives through the same five traffic lights while on its way to work every day. The probability that the car has to stop at any given light on any given day is a beta-distributed random variable with parameters \\(\\alpha = \\beta = 2\\). (To be clear, the probability is \\(p\\) that the car will stop at the first light, then \\(p\\) that it will stop at the second light, etc., up to the fifth light. \\(p\\) changes from one day to the next.) Assume the traffic lights operate independently. (a) If \\(X\\) is the r.v. representing the number of lights the car stops at on any single day, what is \\(E[X]\\)? (b) What is \\(V[X]\\)? We are given the following bivariate pdf: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} 3x_1 &amp; 0 \\leq x_2 \\leq x_1 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] We are also given that \\(E[X_1]\\) = 3/4, \\(E[X_2]\\) = 3/8, \\(V[X_1]\\) = 3/80, and \\(V[X_2]\\) = 19/320. Compute the correlation coefficient \\(\\rho\\) between \\(X_1\\) and \\(X_2\\). We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} ke^{-x_1} &amp; 0 \\leq x_2 \\leq x_1 &lt; \\infty \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) What value of \\(k\\) makes this a valid bivariate pdf? (b) What is \\(P(X_2 &lt; 1)\\)? (c) Derive the marginal distribution for \\(X_2\\). (d) Derive the conditional pdf for \\(X_1\\) given \\(X_2 = x_2\\). (e) Compute \\(P(X_1 &gt; 2 \\vert X_2 = 1)\\). A strangely designed prize machine works as follows: the payout \\(X\\), in dollars, is a Poisson-distributed random variable whose mean is sampled from a negative binomial distribution with \\(r = 4\\) and \\(p = 1/2\\). (a) Compute \\(E[X]\\). (b) Compute \\(V[X]\\). We are given the following bivariate pdf: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} c &amp; -2 \\leq x_1 \\leq 2 \\,, 0 \\leq x_2 \\leq \\vert x_1 \\vert \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Are \\(X_1\\) and \\(X_2\\) independent random variables? (b) Determine \\(c\\). (c) Compute \\(E[X_2]\\). We are given the following bivariate pdf: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} cx_1x_2 &amp; 0 \\leq x_1 \\leq 1 \\,, 0 \\leq x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) What is \\(c\\)? (b) What is \\(P(X_2 &gt; 1/2 \\vert X_1 = 1/2)\\)? (c) What is Cov(\\(X_1,X_2\\))? We are given two r.v.’s, \\(X_1\\) and \\(X_2\\), whose variances are \\(V[X_1] = V[X_2] = 3\\) and whose covariance is 2. We define a new r.v. \\(U = 2X_1 - X_2\\). What is \\(V[U]\\)? In an experiment, we flip a fair coin. If the coin shows heads, we then sample a datum \\(U\\) from a Uniform(1,2) distribution; otherwise we sample \\(U\\) from a Uniform(0,2) distribution. Compute the unconditional variance of the random variable \\(U\\). We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} 12x_1^2(1-x_1) &amp; 0 \\leq x_1 \\leq 1, 0 \\leq x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Are \\(X_1\\) and \\(X_2\\) independent random variables? (b) Write down the marginal distribution \\(f_{X_2}(x_2)\\). (c) Compute \\(E[X_1]\\). We are given the following: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} c &amp; 0 \\leq x_1 \\leq 2, x_2 \\geq 0, x_2 \\leq x_1 ~\\mbox{and}~x_2 \\leq 2-x_1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Determine the value of \\(c\\) that makes \\(f_{X_1,X_2}(x_1,x_2)\\) a valid pdf. (b) Write down the marginal distribution \\(f_{X_2}(x_2)\\). (c) Write down the conditional distribution \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\). We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} 4x_1x_2 &amp; 0 \\leq x_1 \\leq 1, 0 \\leq x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Compute \\(f_{X_1}(x_1)\\). (b) Compute \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1)\\). (c) Compute \\(P(X_2 &lt; 1/2 \\vert X_1=x_1)\\). We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} 2 &amp; x_1 \\geq 0~~x_2 \\geq 0~~x_1+x_2 \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] Compute the variance of \\(X_1\\). We are given the probability mass function shown below. Compute the variance of \\(X_1-X_2\\). It may help to know that \\(E[X_1^2] = 8/9\\), \\(E[X_2^2] = 6/9\\), and \\(E[X_1X_2] = 4/9\\). \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_1 = 0\\) 0 1/9 3/9 \\(x_1 = 1\\) 1 2/9 2/9 \\(x_1 = 2\\) 2 0 1/9 In an experiment, we sample a datum \\(X\\) from a gamma distribution whose \\(\\beta\\) parameter is drawn from an exponential distribution with mean \\(\\gamma\\). (a) What is the unconditional expected value of \\(X\\)? (b) What is \\(V[X]\\)? We are given a bivariate uniform probability density function that is non-zero for \\(0 \\leq x_1 \\leq 1\\) and \\(0 \\leq x_2 \\leq x_1\\). (a) What is the amplitude of the pdf inside the region described above? (b) What is \\(E[X_1]\\)? (c) What is the covariance between \\(X_1\\) and \\(X_2\\)? (Note that \\(E[X_2] = 1/3\\).) We are given the following bivariate probability density function: \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = \\left\\{ \\begin{array}{cc} k e^{-x_1/2} &amp; x_1 \\geq 0 ~{\\rm and}~ 0 \\leq x_2 \\leq 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{eqnarray*}\\] (a) Find the value of \\(k\\) that makes \\(f_{X_1,X_2}(x_1,x_2)\\) a valid pdf. (b) Compute \\(E[X_1]\\). We are given a bivariate uniform probability density function that is non-zero for \\(x_2 \\geq 0\\), \\(x_2 \\leq x_1\\), and \\(x_2 \\leq 2-x_1\\). (a) Compute the marginal distribution for \\(X_2\\). (b) Compute the conditional distribution for \\(X_1\\) given \\(X_2\\). (c) Compute \\(P(X_1 + X_2 \\leq 1)\\). We sample a random variable \\(X\\) from a normal distribution with mean \\(\\mu\\) and variance one, and you are given that \\(\\mu \\sim N(0,1)\\). Write down the variance of \\(X\\). We are given the probability mass function shown below. (a) Compute the conditional expected value \\(E[X_1 \\vert X_2=1]\\). (b) Compute the conditional variance \\(V[X_1 \\vert X_2=1]\\). \\(x_2 = 0\\) \\(x_2=1\\) \\(x_1 = 0\\) 0.2 0.3 \\(x_1 = 1\\) 0.4 0.1 Let \\(X_1\\) and \\(X_2\\) be data that are jointly sampled from the bivariate probability density function \\[\\begin{eqnarray*} f_{X_1,X_2}(x_1,x_2) = k x_1^2 \\,, \\end{eqnarray*}\\] where \\(x_1 \\geq 0\\), \\(x_2 \\geq 0\\), and \\(x_1 + x_2 \\leq 2\\), and \\(k\\) is a constant. (a) Are \\(X_1\\) and \\(X_2\\) independent random variables? (b) What value of \\(k\\) makes \\(f_{X_1,X_2}(x_1,x_2)\\) a valid probability density function? (c) Derive the marginal distribution \\(f_{X_1}(x_1)\\). (d) Derive the conditional distribution \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1)\\). (e) \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1)\\) is a distribution from what named univariate family of distributions? We are given the bivariate probability mass function shown below. (a) What is Cov(\\(X_1,X_2\\))? (b) What is \\(\\rho_{X_1,X_2}\\)? (c) Let \\(Y = 2X_1 - X_2\\). Derive \\(V[Y]\\). \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_1 = 0\\) 0.3 0.1 \\(x_1 = 1\\) 0.1 0.5 "],["further-conceptual-details-optional.html", "7 Further Conceptual Details (Optional) 7.1 Types of Convergence 7.2 The Central Limit Theorem 7.3 Asymptotic Normality of Maximum Likelihood Estimates 7.4 Point Estimation: (Relative) Efficiency 7.5 Sufficient Statistics", " 7 Further Conceptual Details (Optional) 7.1 Types of Convergence There are two primary types of convergence which interest us in this book: convergence in probability and convergence in distribution. Let \\(X_1,X_2,\\ldots\\) be a sequence of random variables, and let \\(Y\\) be some other random variable. (We note that the concept of a sequence can be initially confusing for students: here, \\(X_n\\) is a statistic formed from a set of data with sample size \\(n\\). A classic example of a sequence is \\(\\bar{X}_1,\\bar{X}_2,...\\); the first element is the mean for a sample of size 1 (the datum itself!), the second element is the mean that we compute after we independently sample a second datum from the same underlying distribution to go along with the first, etc.) In addition, let \\(F_{X_n}\\) denote the cumulative distribution function for \\(X_n\\) and \\(F_Y\\) denote the cdf for \\(Y\\). We say that \\(X_n\\) converges in probability to \\(Y\\) if for every \\(\\epsilon &gt; 0\\), \\[ P(\\vert X_n - Y \\vert &gt; \\epsilon) \\rightarrow 0 \\] as \\(n \\rightarrow \\infty\\); i.e., \\(X_n \\stackrel{p}{\\rightarrow} Y\\). \\(X_n\\) converges in distribution to \\(Y\\) if \\[ \\lim_{n \\rightarrow \\infty} F_{X_n}(u) = F_Y(u) \\] for all values of \\(u\\) for which \\(F_Y(u)\\) is continuous; i.e., \\(X_n \\stackrel{d}{\\rightarrow} Y\\). An example of convergence in probability is the weak law of large numbers, which states that \\(\\bar{X}_n \\stackrel{p}{\\rightarrow} \\mu\\). (This is a “weak” statement because it does not invoke “almost sure” convergence, a concept beyond the scope of this book.) What is “weak” about this statement? The statement \\(P(\\vert \\bar{X}_n - Y \\vert &gt; \\epsilon) \\rightarrow 0\\), where \\(Y\\) has mean \\(\\mu\\), is not as restrictive a statement as others that one can make; for any chosen value of \\(\\epsilon\\), \\(\\vert \\bar{X}_n - Y \\vert\\) can stray so as to be greater than \\(\\epsilon\\) an infinite amount of times as \\(n \\rightarrow \\infty\\). In other words, \\(\\bar{X}\\) asymptotically approaches \\(\\mu\\), but is still “allowed” to substantially deviate from that value for any finite \\(n\\). 7.2 The Central Limit Theorem We introduce the Central Limit Theorem in Chapter 2. It states that if we have \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) with mean \\(E[X_i] = \\mu\\) and finite variance \\(V[X_i] = \\sigma^2 &lt; \\infty\\), and if \\(n\\) is sufficiently large, then \\(\\bar{X}\\) converges in distribution to a standard normal random variable: \\[ \\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq z \\right) = \\Phi(z) \\,. \\] We can prove this using moment-generating functions. Let \\(X_i \\stackrel{iid}{\\sim} P(\\mu,\\sigma^2)\\), where \\(P\\) is some distribution with finite variance, and let \\[ Y = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} = \\frac{1}{\\sqrt{n}} \\left(\\frac{n\\bar{X} - n\\mu}{\\sigma}\\right) = \\frac{1}{\\sqrt{n}} \\left(\\frac{\\sum_{i=1}^n X_i - n\\mu}{\\sigma}\\right) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n Z_i \\,. \\] Since the \\(Z_i\\)’s are standardized random variables, by definition \\(E[Z_i] = 0\\) and \\(V[Z_i]\\) = 1. Here’s where things “break down” if we do not know \\(\\sigma\\), but instead plug \\(S\\) in in CLT-related problems: if we use \\(S\\), then \\(V[X_i] \\neq 1\\). However, a theoretical result known as Slutsky’s theorem saves us here. As we are determining here and below, \\[ \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\stackrel{d}{\\rightarrow} Z \\sim \\mathcal{N}(0,1) \\,, \\] and hence \\[ \\sqrt{n}(\\bar{X} - \\mu) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}(0,\\sigma^2) \\,. \\] Furthermore, \\(S^2 \\stackrel{p}{\\rightarrow} \\sigma^2\\) (by the weak law of large numbers), or equivalently \\(S \\stackrel{p}{\\rightarrow} \\sigma\\) (by the continuous mapping theorem). Given these pieces of information, Slutsky’s theorem tells us that \\(\\sqrt{n}(\\bar{X} - \\mu)/S \\stackrel{d}{\\rightarrow} Y/\\sigma\\), a normally distributed random variable with mean 0 and variance 1. Hence eventually the CLT is valid, even when we plug in \\(S\\)! To determine the distribution of \\(Y\\), we use the method of moment-generating functions: \\[ m_Y(t) = m_{Z_1}\\left(\\frac{t}{\\sqrt{n}}\\right) \\cdots m_{Z_n}\\left(\\frac{t}{\\sqrt{n}}\\right) = \\left[m_{Z_i}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^n \\,. \\] “Wait,” one might say. “We don’t know the mgf for the quantity \\(Z_i\\), so how can this possibly be helpful?” It is because we can work with the expected value and variance to get at the final result. First, \\[\\begin{align*} m_{Z_i}\\left(\\frac{t}{\\sqrt{n}}\\right) &amp;= m_{Z_i}\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} + m_{Z_i}&#39;\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} \\frac{t}{\\sqrt{n}} + m_{Z_i}&#39;&#39;\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} \\frac{t^2}{2n} + \\cdots \\\\ &amp;\\approx m_{Z_i}(0) + E[Z_i] \\frac{t}{\\sqrt{n}} + E[Z_i^2] \\frac{t^2}{2n} \\\\ &amp;= E[\\exp(0Z_i)] + 0 + (V[Z_i] + (E[Z_i])^2) \\frac{t^2}{2n} \\\\ &amp;= 1 + V[Z_i] \\frac{t^2}{2n} = 1 + \\frac{t^2}{2n} \\,. \\end{align*}\\] Thus \\[ m_Y(t) \\approx \\left[ 1 + \\frac{t^2}{2n} \\right]^n = \\left[ 1 + \\frac{t^2/2}{n} \\right]^n \\,, \\] and, as \\(n \\rightarrow \\infty\\), \\[ \\lim_{n \\rightarrow \\infty} \\left[ 1 + \\frac{t^2/2}{n} \\right]^n = \\exp\\left(\\frac{t^2}{2}\\right) \\,. \\] This is the moment-generating function for a standard normal…hence \\(Y\\) converges in distribution to a standard normal random variable and \\(\\bar{X}\\) converges in distribution to normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). 7.3 Asymptotic Normality of Maximum Likelihood Estimates When discussing point estimation in Chapter 2, we declared that as \\(n \\rightarrow \\infty\\), the maximum likelihood estimate for a parameter \\(\\theta\\) converges in distribution to a normal random variable: \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) ~\\mbox{or}~ (\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y&#39; \\sim \\mathcal{N}\\left(0,\\frac{1}{nI(\\theta)}\\right) \\,. \\] Here, we sketch out why this is true. As we will see, this result rests upon fundamental concepts covered earlier in this book and chapter: the weak law of large numbers (aka, convergence in probability), and the central limit theorem. We start with the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\). (However, to simplify the notation in what follows, we will drop the \\(\\mathbf{x}\\) for now.) By definition, the MLE for \\(\\theta\\) is that value for which \\(\\ell&#39;(\\theta) = \\ell&#39;(\\hat{\\theta}_{MLE}) = 0\\). The Taylor expansion of \\(\\ell&#39;(\\theta)\\) around \\(\\hat{\\theta}_{MLE}\\) is \\[ \\ell&#39;(\\theta) \\approx \\ell&#39;(\\hat{\\theta}_{MLE}) + (\\theta - \\hat{\\theta}_{MLE})\\ell&#39;&#39;(\\theta) + \\cdots = (\\theta - \\hat{\\theta}_{MLE})\\ell&#39;&#39;(\\theta) + \\cdots \\,. \\] We rearrange terms (and introduce the factor \\(\\sqrt{n}\\)): \\[\\begin{align*} (\\hat{\\theta}_{MLE}-\\theta)\\ell&#39;&#39;(\\theta) &amp;\\approx -\\ell&#39;(\\theta) \\\\ \\sqrt{n} (\\hat{\\theta}_{MLE}-\\theta)\\ell&#39;&#39;(\\theta) &amp;\\approx -\\sqrt{n}\\ell&#39;(\\theta) \\\\ \\sqrt{n} (\\hat{\\theta}_{MLE}-\\theta) &amp;\\approx \\frac{-\\sqrt{n}\\ell&#39;(\\theta)}{\\ell&#39;&#39;(\\theta)} = \\frac{\\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta)}{-\\frac{1}{n}\\ell&#39;&#39;(\\theta)} \\,. \\end{align*}\\] (Note that we reversed \\(\\theta\\) and \\(\\hat{\\theta}_{MLE}\\) in the parentheses!) Let’s look at the numerator and the denominator separately. For the numerator: \\[\\begin{align*} \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) &amp;= \\sqrt{n}\\left(\\frac{\\ell&#39;(\\theta \\vert \\mathbf{x})}{n} - 0\\right) \\\\ &amp;= \\sqrt{n}\\left(\\frac{\\ell&#39;(\\theta \\vert \\mathbf{x})}{n} - E[\\ell&#39;(\\theta \\vert x_i)] \\right) \\\\ &amp;= \\sqrt{n}\\left(\\frac{\\sum_{i=1}^n \\ell&#39;(\\theta \\vert x_i)}{n} - E[\\ell&#39;(\\theta \\vert x_i)] \\right) \\,. \\end{align*}\\] Here we utilize the results that, e.g., \\(\\ell&#39;(\\theta \\vert \\mathbf{x})\\) equals the sum of the \\(\\ell&#39;\\)’s for each datum, and that the average value of the slope for the likelihood function is zero. What we have written is equivalent in form to \\[ \\sqrt{n} (\\bar{X} - \\mu) \\] and given the CLT, we know that as \\(n \\rightarrow \\infty\\), this quantity converges in distribution to normal random variable \\(Y \\sim \\mathcal{N}(0,\\sigma^2)\\). Thus \\[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,V[\\ell&#39;(\\theta \\vert x_i)]\\right) \\,, \\] and, since \\(V[\\ell&#39;(\\theta \\vert x_i)] = E[(\\ell&#39;(\\theta \\vert x_i))^2] - (E[\\ell&#39;(\\theta \\vert x_i)])^2 = E[(\\ell&#39;(\\theta \\vert x_i))^2] = I(\\theta)\\), \\[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,I(\\theta)\\right) \\,. \\] For the denominator, we can utilize the weak law of large numbers: \\[ -\\frac{1}{n} \\ell&#39;&#39;(\\theta \\vert \\mathbf{x}) = -\\frac{1}{n} \\left( \\sum_{i=1}^n \\ell&#39;&#39;(\\theta \\vert x_i) \\right) \\stackrel{p}{\\rightarrow} -E[\\ell&#39;&#39;(\\theta \\vert x_i)] = I(\\theta) \\,. \\] At last, we can determine the variance of the ratio: \\[ \\lim_{n \\rightarrow \\infty} V[\\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta)] = V\\left[ \\frac{\\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta)}{I(\\theta)} \\right] = \\frac{1}{I(\\theta)^2} V\\left[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\right] = \\frac{1}{I(\\theta)^2} I(\\theta) = \\frac{1}{I(\\theta)} \\,. \\] We combine this information with the finding above that the numerator in the ratio is, asymptotically, a normally distributed random variable with mean 0 and variance \\(I(\\theta)\\) to write \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) \\,. \\] 7.4 Point Estimation: (Relative) Efficiency The efficiency of an unbiased estimator of a parameter \\(\\theta\\) is \\[ e(\\hat{\\theta}) = \\frac{1/I(\\theta)}{V[\\hat{\\theta}]} \\,, \\] i.e., the ratio of the minimum possible variance for an unbiased estimator of \\(\\theta\\) (the Cramer-Rao lower bound) to the variance for the estimator in question. If an unbiased estimator attains \\(e(\\hat{\\theta})\\) for all possible values of \\(\\theta\\), the estimator is efficient. (It is also the MVUE!) The relative efficiency is a metric that we can use to compare two unbiased estimators: \\[ e(\\hat{\\theta}_1,\\hat{\\theta}_2) = \\frac{V[\\hat{\\theta}_2]}{V[\\hat{\\theta}_1]} \\,. \\] If \\(e(\\hat{\\theta}_1,\\hat{\\theta}_2) &gt; 1\\), we would opt to use \\(\\hat{\\theta}_1\\); otherwise, if \\(e(\\hat{\\theta}_1,\\hat{\\theta}_2) &lt; 1\\), we would opt to use \\(\\hat{\\theta}_2\\). (We could use either if the relative efficiency is exactly one.) As an example, let \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and let \\(\\hat{\\mu}_1 = \\bar{X}\\) and \\(\\hat{\\mu}_2 = (X_1+X_2)/2\\). What is the relative efficiency of these two estimators? We know the general result that \\(V[\\hat{\\mu}_1] = V[\\bar{X}] = \\sigma^2/n\\), and we know that \\(\\hat{\\mu}_2\\) is simply the sample mean of the first two data, so \\(V[\\hat{\\mu}_2] = V[(X_1+X_2)/2] = \\sigma^2/2\\). The relative efficiency is thus \\[ e(\\hat{\\mu}_1,\\hat{\\mu}_2) = \\frac{V[\\hat{\\mu}_2]}{V[\\hat{\\mu}_1]} = \\frac{\\sigma^2/2}{\\sigma^2/n} = \\frac{n}{2} \\,. \\] This value is \\(&gt; 1\\) for all \\(n &gt; 2\\), and it is never less than 1, so we would choose to use \\(\\hat{\\mu}_1 = \\bar{X}\\) as our estimator of the population mean. 7.5 Sufficient Statistics Let us assume that we are given \\(n\\) independent and identically distributed (iid) data sampled from some distribution \\(P\\) whose shape is (without loss of generality) governed by a single parameter \\(\\theta\\): \\[ \\{X_1,\\ldots,X_n\\} \\overset{iid}{\\sim} P(\\theta) \\] A sufficient statistic \\(U\\) is a function of \\(\\mathbf{X}\\) that encapsulates all the information needed to estimate \\(\\theta\\), i.e., if \\(U\\) is sufficient, no other computed statistic would provide any additional information that would help us estimate \\(\\theta\\). Sufficient statistics are not unique: if \\(U\\) is a sufficient statistic, then every one-to-one function \\(f(U)\\) is as well, so long as \\(f(U)\\) does not depend on \\(\\theta\\). In Chapter 3 we show how one finds a sufficient statistic by factorizing the likelihood function, and uses it to define the minimum variance unbiased estimator (the MVUE). In that chapter, we sweep a number of things under the metaphorical rug, such as the fact that to be used to determine the MVUE, a sufficient statistic has to be both minimal and complete. We elaborate on those points below. However, we start by providing an alternate means by which to determine a sufficient statistic. 7.5.1 A Formal Definition of Sufficiency A statistic \\(U(\\mathbf{X})\\) is a sufficient statistic for the parameter \\(\\theta\\) if the conditional distribution of \\(\\mathbf{X}\\) given \\(U(\\mathbf{X})\\) does not depend on \\(\\theta\\), i.e., if \\(P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}) = k)\\) does not depend on \\(\\theta\\). As an example, let \\(\\{X_1,\\ldots,X_n\\} \\overset{iid}{\\sim}\\) Bernoulli(\\(p\\)), and let us propose \\(U(\\mathbf{X}) = \\sum_{i=1}^n X_i\\) as a sufficient ststistic. To determine if it is, we need to determine if \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{P(X_1=x_1,\\ldots,X_n=x_n,U(\\mathbf{X})=k}{P(U(\\mathbf{X}=k)} \\] does not depend on \\(p\\). The first thing to note is that for the numerator to be non-zero, \\(k = x_1+\\cdots+x_n = \\sum_{i=1}^n x_i\\), and thus \\(U(\\mathbf{Y})=k\\) is, from an information standpoint, completely redundant with respect to \\(X_1=x_1,\\ldots,X_n=x_n\\)…so we can ignore it: \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{P(X_1=x_1,\\ldots,X_n=x_n)}{P(U(\\mathbf{X}=k)} \\,. \\] The next thing to note is that because the data are iid, we can rewrite the numerator as a product of probabilities: \\[ P(X_1=x_1,\\ldots,X_n=x_n) = P(X_1=x_1) \\cdots P(X_n=x_n) = \\prod_{i=1}^n P(X_i=x_i) \\,, \\] which means, in the context of Bernoulli random variables, that the numerator is \\[ \\prod_{i=1}^n P(X_i=x_i) = p^k (1-p)^{n-k} \\,. \\] (Why \\(p^k\\), etc.? We are given that \\(U(\\mathbf{X}) = k\\), i.e., that there are \\(k\\) observed successes (and \\(n-k\\) observed failures) in the sample. That takes care of the numerator. Now for the denominator: \\[ P(U(\\mathbf{X} = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\,, \\] because the sum of \\(n\\) Bernoulli-distributed random variables is binomially distributed (as you can show yourself using the method of moment-generating functions). Thus \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{p^k(1-p)^{n-k}}{\\binom{n}{k} p^k(1-p)^{n-k}} = \\frac{1}{\\binom{n}{k}} \\,, \\] which does not depend on the parameter \\(p\\). Thus \\(U(\\mathbf{X}) = \\sum_{i=1}^n X_i\\) is indeed a sufficient statistic for \\(p\\). (The foregoing clearly illustrates why factorization is a preferred means by which to determine sufficient statistics!) 7.5.2 Minimal Sufficiency A question that naturally arises when dealing with sufficient statistics is: if we have many sufficient statistics, which one is the best one? It would be the one that “reduces” the data the most. In a given context, \\(U(\\mathbf{X})\\) is minimal sufficient if, given any another sufficient statistic \\(T(\\mathbf{X})\\), \\(U(\\mathbf{X}) = f(T(\\mathbf{X}))\\). To generate a minimal sufficient statistic, one can make use of the Lehmann-Scheffe theorem. If we have two iid samples of data of the same size, \\(\\{X_1,\\ldots,X_n\\}\\) and \\(\\{Y_1,\\ldots,Y_n\\}\\), then, if the ratio of likelihoods \\[ \\frac{\\mathcal{L}(x_1,\\ldots,x_n \\vert \\theta)}{\\mathcal{L}(y_1,\\ldots,y_n \\vert \\theta)} \\] is constant as a function of \\(\\theta\\) if and only if \\(g(x_1,\\ldots,x_n) = g(y_1,\\ldots,y_n)\\) for a function \\(g(\\cdot)\\), then \\(g(X_1,\\ldots,X_n)\\) is a minimal sufficient statistic for \\(\\theta\\). As an example, let \\(\\{X_1,\\ldots,X_n\\}\\) and \\(\\{Y_1,\\ldots,Y_n\\} \\overset{iid}{\\sim}\\) Poisson(\\(\\lambda\\)). The ratio of likelihoods is \\[ \\frac{\\mathcal{L}(x_1,\\ldots,x_n \\vert \\theta)}{\\mathcal{L}(y_1,\\ldots,y_n \\vert \\theta)} = \\frac{\\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda}}{\\prod_{i=1}^n \\frac{\\lambda^{y_i}}{y_i!} e^{-\\lambda}} = \\frac{\\prod_{i=1}^n \\frac{1}{x_i!}}{\\prod_{i=1}^n \\frac{1}{y_i!}} \\frac{\\lambda^{\\sum_{i=1}^n x_i}}{\\lambda^{\\sum_{i=1}^n y_i}} = \\frac{\\prod_{i=1}^n \\frac{1}{x_i!}}{\\prod_{i=1}^n \\frac{1}{y_i!}} \\lambda^{\\left(\\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i\\right)} \\,. \\] This is only constant as a function of \\(\\lambda\\) if we can “get rid of” \\(\\lambda\\), by setting its exponent to 0…i.e., by setting \\(\\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i\\). Thus \\(g(\\mathbf{x}) = \\sum_{i=1}^n x_i\\) is a minimal sufficient statistic for \\(\\lambda\\). Note that in the context of the present course, any and all sufficient statistics that we define via, e.g., factorization are minimal sufficient. One might ask “what is an example of a sufficient statistic that is not minimally sufficient?” The answer is simple but may not initially be intuitive, in that we naturally think of a statistic as being a single number (the output from applying a given function to a full dataset). However, e.g., a full dataset is a statistic, and it is a sufficient statistic: there is sufficient information in a full dataset so as to compute the likelihood of a population parameter \\(\\theta\\). However, a full dataset is not minimally sufficient! If \\(T(\\mathbf{X}) = \\bar{X}\\), and \\(U(\\mathbf{X}) = \\{X_1,\\ldots,X_n\\}\\), then we cannot define a function \\(f\\) such that \\(\\{X_1,\\ldots,X_n\\} = f(\\bar{X})\\). 7.5.3 Completeness The concept of the completeness of a statistic is a general concept, i.e., it is not limited to sufficient statistics. One can think of it as the idea that each value of \\(\\theta\\) in a distribution maps to a distinct pmf or pdf. We bring up completeness here because the concept appears in the context of determining an MVUE: the sufficient statistic that we find via, e.g., factorization technically needs to be both minimal and complete. Let \\(f_U(u \\vert \\theta)\\) be the family of pmfs or pdfs for the statistic \\(U(\\mathbf{X})\\). \\(U\\) is dubbed a complete statistic if \\(E[g(U)] = 0\\) for all \\(\\theta\\) implies \\(P(g(U) = 0) = 1\\) for all \\(\\theta\\). As an example, let \\(U \\sim\\) Binomial(\\(n,p\\)), with \\(0 &lt; p &lt; 1\\), and let \\(g(\\cdot)\\) be a function such that \\(E[g(U)] = 0\\). Then \\[ 0 = E[g(U)] = \\sum_{u=0}^n g(u) \\binom{n}{u} p^u (1-p)^{n-u} = (1-p)^n \\sum_{u=0}^n g(u) \\binom{n}{u} \\left( \\frac{p}{1-p}\\right)^u \\,. \\] For any choice of \\(u\\) and \\(n\\), \\(\\binom{n}{u} [p/(1-p)]^u &gt; 0\\). So, in order to have \\(E[g(U)] = 0\\) for all \\(p\\), \\(g(u) = 0\\) for all \\(u\\), i.e., \\(P(g(U) = 0) = 1\\). Therefore \\(U\\) is a complete statistic. Note that a complete statistic is also minimal sufficient, but the converse is not necessarily true: a minimal sufficient statistic may not be complete. 7.5.4 The Rao-Blackwell Theorem The Rao-Blackwell theorem implies that an unbiased estimator for \\(\\theta\\) with a small variance is, or can be made to be, a function of a sufficient statistic: if we have an unbiased estimator for \\(\\theta\\), we might be able to improve it using the prescription of the theorem. In the end, the theorem provides a mathematically more challenging route to defining a minimum variance unbiased estimator than what we show in the main text\\(-\\)likelihood factorization followed by the “de-biasing” of a sufficient statistic\\(-\\)and hence we relegate it here, to the chapter of optional material. Let \\(\\hat\\theta\\) be an unbiased estimator for \\(\\theta\\) such that \\(V[\\hat\\theta] &lt; \\infty\\). If \\(U\\) is a sufficient statistic for \\(\\theta\\), define \\(\\hat\\theta^* = E[\\hat\\theta \\vert U]\\). Then, for all \\(\\theta\\), \\[ E[\\hat\\theta^*] = \\theta ~~~ \\text{and} ~~~ V[\\hat\\theta^*] \\leq V[\\hat\\theta] \\,. \\] Let’s suppose we have sampled \\(n\\) iid data from a Binomial distribution with number of trials \\(k\\) and proportion \\(p\\). We propose an estimator for \\(p\\): \\(X_1/k\\). First, is \\(\\hat{p} = X_1/k\\) unbiased? We have that \\[ E[\\hat{p}-p] = E\\left[\\frac{X_1}{k}\\right] - p = \\frac{1}{k}E[X_1] - p = \\frac{1}{k}kp - p = 0 \\,. \\] So…yes. Second, is \\(V[\\hat{p}] &lt; \\infty\\)? \\(V[X_1/k] = V[X_1]/k^2 = p(1-p)/k\\)…so, also yes. We are thus free to use the Rao-Blackwell theorem to improve upon \\(\\hat{p} = X_1/k\\): \\[ \\hat\\theta^* = E[\\hat\\theta \\vert U] = E\\left[\\frac{X_1}{k} \\left| ~ U = \\sum_{i=1}^n X_i \\right. \\right] \\,. \\] Since we are given the sum of the data, and since the data are iid, we would expect, on average, that \\(X_1\\) has the value \\(U/n\\), and thus that \\(X_1/k\\) has the value \\(U/(nk)\\). Thus \\[ \\hat\\theta^* = \\frac{\\bar{X}}{k} \\] is the MVUE for \\(p\\). 7.5.5 Exponential Family of Distributions The exponential family is a set of distributions whose probability mass or density functions can be expressed as either \\[ f_X(x \\vert \\theta) = a(\\theta) b(x) \\exp[ c(\\theta) t(x) ] \\] or \\[ f_X(x \\vert \\theta) = b(x) \\exp[ c(\\theta) t(x) - d(\\theta)] \\,. \\] These two forms are equivalent, with \\(a(\\theta) = \\exp[-d(\\theta)]\\). Note that the domain of \\(f_X(x \\vert \\theta)\\) cannot depend on \\(\\theta\\), meaning that distributions like the uniform and Pareto cannot be members of the exponential family even if their density functions could be expressed in the form above. There are many concepts related to the exponential family that are beyond the scope of this book. The one factoid that we will note here is that sufficient statistics for the parameter \\(\\theta\\) can be read directly off a distribution’s exponential family form. Assuming we have \\(n\\) iid random variables, we can write the likelihood and factorize it, and isolate the sufficient statistic: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\\\ &amp;= \\underbrace{[a(\\theta)]^n \\exp\\left[ \\sum_{i=1}^n c(\\theta) t(x_i) \\right]}_{g(\\theta,u)} \\underbrace{\\left( \\prod_{i=1}^n b(x_i) \\right)}_{h(\\mathbf{x})} \\,. \\end{align*}\\] The sufficient statistic is thus \\[ U = \\sum_{i=1}^n t(X_i) \\,. \\] As an example, what is the sufficient statistic for the mean of an exponential distribution? The exponential pdf is \\((1/\\beta)\\exp(-x/\\beta)\\) for \\(\\beta &gt; 0\\) and \\(x \\geq 0\\). If we use the first exponential family form above, we can read off that \\[ a(\\beta) = \\frac{1}{\\beta} ~~ b(x) = 1 ~~ c(\\beta) = -\\frac{1}{\\beta} ~~ t(x) = x \\,. \\] Thus, assuming we have an iid sample of size \\(n\\), the sufficient statistic would be \\(U = \\sum_{i=1}^n t(X_i) = \\sum_{i=1}^n X_i\\). "],["appendix-a-table-of-symbols.html", "Appendix A: Table of Symbols", " Appendix A: Table of Symbols Symbol What the Symbol Represents \\(\\theta\\) parameter(s) that dictate the shape/location of a distribution \\(\\hat{\\theta}\\) parameter estimate(s) \\(\\alpha\\) one minus the confidence coefficient, or the hypothesis test Type I error \\(\\beta_i\\) the \\(i^{\\rm th}\\) coefficient in a regression model \\(\\Gamma(x)\\) the gamma function \\((= (x-1)!\\) if \\(x\\) is an integer) \\(\\epsilon\\) the error term in a linear regression model \\(\\mu\\) the distribution mean (or population mean) \\((= E[X])\\) \\(\\mu_k&#39;\\) the \\(k^{\\rm th}\\) distribution moment \\((= E[X^k])\\) \\(\\nu\\) the number of degrees of freedom \\(\\sigma\\) the distribution standard deviation (or population standard deviation) \\(\\sigma^2\\) the distribution variance (or population variance) \\((= V[X])\\) \\(\\boldsymbol{\\Sigma}\\) the covariance matrix \\(\\Omega\\) the sample space corresponding to an experiment \\(A\\),\\(B\\) generic symbols for events in a sample space \\(B(\\alpha,\\beta)\\) the beta function \\(B[\\hat{\\theta}]\\) the bias of the estimator \\(\\hat{\\theta}\\) \\(E[X]\\) the expected value of the random variable \\(X\\) \\((= \\mu)\\) erf(\\(x\\)) the error function \\(f_X(x)\\) the probability density function for the continuous random variable \\(X\\) \\(F_X(x)\\) the cumulative distribution function for the random variable \\(X\\) iid independent and identically distributed random variables MSE mean-squared error (\\(V[\\hat{\\theta}] + B[\\hat{\\theta}]^2\\)) \\(P(a \\leq X \\leq b)\\) the probability that \\(X \\in [a,b]\\) \\(p_X(x)\\) the probability mass function for the discrete random variable \\(X\\) \\(S\\) the standard deviation of \\(n\\) sampled data \\(S^2\\) the variance of \\(n\\) sampled data \\(T\\) a datum sampled from a \\(t\\) distribution \\(U\\) a sufficient statistic, found via likelihood factorization \\(V[X]\\) the variance of the random variable \\(X\\) \\((= \\sigma^2)\\) \\(W\\) a datum sampled from a chi-square distribution \\(X\\) a single datum (or random variable), sampled from a pmf or pdf \\(X_i\\) the \\(i^{\\rm th}\\) datum of \\(n\\) sampled data \\(X_{(i)}\\) the \\(i^{\\rm th}\\) smallest datum of \\(n\\) sampled data \\(\\bar{X}\\) the mean of \\(n\\) sample \\(Y\\) a statistic, a function of the data in a data sample \\(\\mathcal{L}(\\theta \\vert \\mathbf{x}\\)) the likelihood of \\(\\theta\\) given data coordinates \\(\\mathbf{x}\\) \\(\\ell(\\theta \\vert \\mathbf{x}\\)) the log-likelihood \\(\\log \\mathcal{L}(\\theta \\vert \\mathbf{x})\\) \\(\\cup\\) “or” in a probability statment \\(\\cap\\) “and” in a probability statment \\(|\\) a condition, stated to the right of \\(|\\), in a probability statement \\(\\in\\) “in,” as in \\(x \\in [a,b]\\) or “\\(x\\) is in the range \\(a\\) to \\(b\\)” \\(\\prod_{i=1}^n x_i\\) the product \\(x_1 \\cdot x_2 \\cdot \\cdots \\cdot x_n\\) \\(\\sum_{i=1}^n x_i\\) the summation \\(x_1 \\cdot x_2 \\cdot \\cdots \\cdot x_n\\) "],["appendix-b-root-finding-algorithm-for-confidence-intervals.html", "Appendix B: Root-Finding Algorithm for Confidence Intervals", " Appendix B: Root-Finding Algorithm for Confidence Intervals confint &lt;- function(FUN,param,stat,lpb=-1.e+4,upb=1.e+4, bound=&quot;two-sided&quot;,alpha=0.05,tolb=1.e-6,tol=1.e-6,...) { f &lt;- function(x,FUN,param,stat,c,...) { pars &lt;- function(...) { list(...) } args &lt;- pars(...) args &lt;- append(args,stat) names(args)[length(args)] &lt;- &quot;q&quot; if ( is.null(param) == FALSE ) { args &lt;- append(args,x) names(args)[length(args)] &lt;- param } do.call(FUN,args) - c } findRoot &lt;- function(f,lpb,upb,tolb,FUN,param,stat,q,tol,...) { tryCatch( {uniroot(f,interval=c(lpb+tolb,upb-tolb),tol=tol,FUN,param,stat,q,...)$root}, warning = function(w) { cat(&quot;The interval bounds are invalid.\\n&quot;); return(NULL); }, error = function(e) { cat(&quot;Root not computable.\\n&quot;); return(NULL) } ) } if ( bound == &quot;two-sided&quot; ) { lo &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha/2,tol,...) if ( is.null(lo) ) { return(NULL) } hi &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha/2,tol,...) if ( hi &lt; lo ) { tmp = hi; hi = lo; lo = tmp; } return(c(lo,hi)) } else if ( bound == &quot;lower&quot; ) { lo.inc &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha,tol,...) if ( is.null(lo.inc) ) { return(NULL) } lo.dec &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha,tol,...) lo &lt;- min(c(lo.inc,lo.dec)) return(lo) } else if ( bound == &quot;upper&quot; ) { hi.inc &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha,tol,...) if ( is.null(hi.inc) ) { return(NULL) } hi.dec &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha,tol,...) hi &lt;- max(c(hi.inc,hi.dec)) return(hi) } else { stop(&quot;invalid choice for bound&quot;) } return(NULL) } "],["bibliography.html", "Bibliography", " Bibliography Benjamini, Y., and Hochberg, Y. (1995). Controlling the False Discovery Rate: a Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society, Series B 57 289-300. Buse, A. (1982). The Likelihood Ratio, Wald, and Lagrange Multiplier Tests: An Expository Note. The American Statistician 36 153-157. Casella, G., and Berger, R. L. (2002). Statistical Inference, 2nd edition. Belmont, CA: Duxbury. Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics 7 1-26. Kolmogorov, A. (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung (in German; translated as Foundations of the Theory of Probability in 1950). Berlin: Julius Springer. Ross, S. (1988). A First Course in Probability, 3rd edition. New York, NY: Macmillan. Sharpiro, S. S., and Wilk, M. B. (1965). An Analysis of Variance Test for Normality (Complete Samples). Biometrika 52 591-611. Thulin, M. (2014). The Cost of Using Exact Confidence Intervals for a Binomial Proportion. Electronic Journal of Statistics 8 817-840. Wasserman, L. (2004). All of Statistics, 1st edition. New York, NY: Springer. "],["chapter-exercises-solutions.html", "Chapter Exercises: Solutions Chapter 1 Chapter 3 Chapter 4 Chapter 5 Chapter 6", " Chapter Exercises: Solutions Chapter 1 Problem 1 We start with Bayes’ rule: \\[\\begin{align*} P(A \\vert B) &amp;= \\frac{P(B \\vert A)P(A)}{P(B)} \\\\ \\Rightarrow ~~~ \\frac{P(B \\vert A)P(A)}{P(B)} &amp;= \\frac{P(A \\vert B)P(A)}{P(B)} \\\\ \\Rightarrow ~~~ 1 &amp;= \\frac{P(A)}{P(B)} \\\\ \\Rightarrow ~~~ P(A) &amp;= P(B) \\,. \\end{align*}\\] Now let’s look at a 2 \\(\\times\\) 2 table: \\(B\\) \\(\\bar{B}\\) \\(A\\) \\(P(A \\cap B)\\) \\(P(A)-P(A \\cap B)\\) \\(P(A)\\) \\(\\bar{A}\\) \\(P(A)-P(A \\cap B)\\) 0.2 \\(1-P(A)\\) \\(P(A)\\) \\(1-P(A)\\) \\(1\\) So we have that \\[\\begin{align*} P(A \\cap B) + 2 [ P(A)-P(A \\cap B) ] + 0.2 &amp;= 1 \\\\ \\Rightarrow ~~~ P(A \\cap B) + 2 [ P(A)-P(A \\cap B) ] &amp;= 0.8 \\\\ \\Rightarrow ~~~ 2 P(A) - P(A \\cap B) &amp;= 0.8 \\\\ \\Rightarrow ~~~ P(A \\cap B) &amp;= 2 P(A) - 0.8 \\,. \\end{align*}\\] Problem 2 We are given that \\(A \\subset B\\), so \\[\\begin{align*} P(A \\vert B)= \\frac{P(B \\cap A)}{P(B)} = \\frac{P(A)}{P(B)} \\neq P(A) \\,. \\end{align*}\\] Problem 3 We have that \\[\\begin{align*} P(A \\cap B| A\\cup B) = \\frac{P[(A \\cap B) \\cap (A \\cup B)]}{P(A\\cup B)} = \\frac{P(A \\cap B)}{P(A\\cup B)} = \\frac{1 - P(\\bar{A} \\cup \\bar{B})}{1 - P(\\bar{A} \\cap \\bar{B})} \\,. \\end{align*}\\] Problem 4 (a) We have that \\[\\begin{align*} P(B|A) + P(\\bar{B}|A) &amp;= 1 \\\\ \\Rightarrow ~~~ P(B|A) &amp;= \\frac{1}{4} = \\frac{1}{3}P(\\bar{B}|A) \\\\ \\Rightarrow ~~~ \\frac{P(A \\cap B)}{P(A)} &amp;= \\frac{1}{3} \\frac{P(A \\cap \\bar{B})}{P[A]} \\\\ \\Rightarrow ~~~ P(A \\cap B) &amp;= x = \\frac{1}{3} \\frac{1}{6} = \\frac{1}{18} \\,. \\end{align*}\\] (b) Here, we utilize a \\(2 \\times 2\\) table: \\(B\\) \\(\\bar{\\text{B}}\\) \\(A\\) 1/18 1/6 4/18 \\(\\bar{\\text{A}}\\) 1/3 14/18 1/2 1/2 Thus \\[\\begin{align*} p(\\bar{A} \\cap B) = \\frac{14}{18} - \\frac{6}{18} = \\frac{8}{18} \\end{align*}\\] and \\[\\begin{align*} P(B|\\bar{A}) = \\frac{\\bar{A} \\cap B}{P(\\bar{A})} = \\frac{8/18}{14/18} = \\frac{8}{14} = \\frac{4}{7} \\,. \\end{align*}\\] Problem 5 One way to approach this problem is to write \\[\\begin{align*} P(\\bar{A}|\\bar{B}) = \\frac{P(\\bar{A}\\cap\\bar{B})}{P(\\bar{B})} = \\frac{1 - P(A \\cup B)}{1 - P(B)} \\,; \\end{align*}\\] equivalently, we can write \\[\\begin{align*} P(\\bar{A}|\\bar{B}) = 1 - P(A|\\bar{B}) = 1 - \\frac{P(\\bar{B}|A)P(A)}{P(\\bar{B})} = 1 - \\frac{\\left[1 - P(B|A)\\right]P(A)}{1 - P(B)} \\,. \\end{align*}\\] Problem 6 Using a decision tree: \\(P(W_1) = 2/5\\) and \\(P(B_2 \\vert W_1) = 3/4\\), so \\(P(B_2 \\cap W_1) = 3/10\\); and \\(P(B_1) = 3/5\\) and \\(P(B_2 \\vert B_1) = 1/2\\), so \\(P(B_2 \\cap B_1) = 3/10\\). Thus, by adding the probabilities of these disjoint events, we determine that \\(P(B_2) = P(B_2 \\cap W_1) + P(B_2 \\cap B_1) = 3/5\\). Problem 7 If we denote the event of knowing the answer as \\(K\\), \\(P(K) = 0.6\\) and \\(P(\\bar{K}) = 0.4\\), and if we denote the event of getting the question correct as \\(C\\), we know that \\(P(C \\vert \\bar{K}) = 0.2\\). It is implicit in the wording of the question that \\(P(C \\vert K) = 1\\). Ultimately, we want to determine \\(P(K \\vert C)\\): \\[\\begin{align*} P(K \\vert C) = \\frac{P(C \\vert K)P(K)}{P(C)} = \\frac{P(C \\vert K)P(K)}{P(C \\vert K)P(K)+P(C \\vert \\bar{K})P(\\bar{K})} = \\frac{1 \\cdot 0.6}{1 \\cdot 0.6 + 0.2 \\cdot 0.4} = \\frac{60}{68} = \\frac{15}{17} \\,. \\end{align*}\\] Problem 8 We are given that \\(P(S) = 0.15\\), \\(P(D \\vert S) = 10 \\cdot P(D \\vert \\bar{S})\\), and \\(P(D) = 0.01\\). So \\[\\begin{align*} P(S \\vert D) &amp;= \\frac{P(D\\vert S)P(S)}{P(D)} = \\frac{P(D \\vert S)P(S)}{P(D \\vert S)P(S) + P(D \\vert \\bar{S})P(\\bar{S})} \\\\ &amp;= \\frac{P(D \\vert S)P(S)}{P(D \\vert S)P(S) + P(D \\vert S)(1 - P(S))/10} \\\\ &amp;= \\frac{P(S)}{P(S)+1/10-P(S)/10} = \\cdots = 30/47 \\,. \\end{align*}\\] Problem 9 The compound event of winning is \\(W = \\{ 1\\cap3, 1\\cap1\\cap2, 1\\cap1\\cap3, \\ldots \\}\\). Let the event \\(S\\) denote rolling a 1, with \\(P(S) = 1/3\\), and the event \\(F\\) denote rolling a 2 or a 3, with \\(P(F) = 2/3\\). Then \\[\\begin{align*} P(W) &amp;= P(1\\cap3) + P(1\\cap1\\cap2) + \\ldots = \\\\ &amp;= \\frac{1}{2} P(S) P(F) + P(S)^2 P(F) + P(S)^3 P(F) + \\ldots \\\\ &amp;= \\frac{1}{2}\\frac{1}{3}\\frac{2}{3} + P(S)^2 P(F) \\left(1 + P(S) + \\ldots \\right)\\\\ &amp; = \\frac{1}{9} + P(S)^2 P(F) \\frac{1}{1 - P(S)} = \\frac{1}{9} + \\frac{P(S)^2 P(F) }{P(F)} = \\frac{1}{9} + P(S^2) = \\frac{2}{9} \\,. \\end{align*}\\] Alternatively, we can define compound event of losing, \\(L = \\{2, 3, 1\\cap2 \\}\\), and write that \\[\\begin{align*} P(L) &amp;= \\frac{1}{3} + \\frac{1}{3} + \\left(\\frac{1}{3}\\right)^2 = \\frac{7}{9} \\\\ \\Rightarrow ~~~ P(W) &amp;= 1 - P(L) = 1 - \\frac{7}{9} = \\frac{2}{9} \\,. \\end{align*}\\] Problem 10 There are multiple ways to do this. Here is one: \\[\\begin{align*} P(H_1 \\cup T_2) &amp;= P(H_1) + P(T_2) - P(H_1 \\cap T_2) = P(H_1) + P(T_2) - P(T_2 | H_1) P(H_1)\\\\ &amp;= P(H_1) + P(T_2|H_1)P(H_1) + P(T_2|T_1)P(T_1) - P(T_2 | H_1) P(H_1)\\\\ &amp;= P(H_1) + P(T_2|T_1)P(T_1) = 0.5 + 0.4 \\cdot 0.5 = 0.7 \\,. \\end{align*}\\] Note: \\(P(T_2|T_1) = 1 - P(H_2|T_1)= 0.4\\), since \\(H_2\\) is just \\(\\bar{T}_2\\). Problem 11 Let \\(N\\) be the event that a sample contains nitrates and \\(R\\) be the event that the sample burns red. We are given that \\(P(N) = 0.2\\), \\(P(R|N) = 0.9\\), and \\(P(R|\\bar{N}) = 0.15\\). (a) We seek \\(P(R)\\), which is \\[\\begin{align*} P(R|N)P(N) + P(R|\\bar{N})P(\\bar{N}) = 0.9 \\cdot 0.2 + 0.15 \\cdot (1-0.2) = 0.30 \\,. \\end{align*}\\] (b) We seek \\(P(N|R)\\), which is \\[\\begin{align*} \\frac{P(R|N)P(N)}{P(R)} = \\frac{0.9 \\cdot 0.2}{0.3} = 0.60 \\,. \\end{align*}\\] Problem 12 (a) Let \\(M\\), \\(V\\), and \\(O\\) denote the event of flying on a major airline, private plane, or other aircraft, respectively, and let \\(B\\) denote the event of being a business traveler. The Law of Total Probability tells us that \\[\\begin{align*} P(B) &amp;= P(B \\vert M) P(M) + P(B \\vert V)P(V)+ P(B \\vert O)P(O) = 0.5\\cdot 0.6 + 0.6\\cdot 0.3 + 0.9 \\cdot 0.1 = 0.57 \\,. \\end{align*}\\] (b) We use Bayes’ rule to determine that \\[\\begin{align*} P(V \\vert B) &amp;= \\frac{P(B \\vert V)P(V)}{P(B)} = \\frac{0.6 \\cdot 0.3}{0.57} = \\frac{6}{19} \\,. \\end{align*}\\] Problem 13 We can use, e.g., a decision tree to determine that the probability mass function for \\(X\\) is \\(x\\) \\(p_X(x)\\) 1 1/2 2 1/2 \\(\\cdot\\) 2/3 = 1/3 3 1/2 \\(\\cdot\\) 1/3 = 1/6 (a) The expected value is \\[\\begin{align*} E[X] = \\sum_{x=1}^3 x p_X(x) = 1 \\cdot \\frac{1}{2} + 2 \\frac{1}{3} + 3 \\frac{1}{6} = \\frac{5}{3} \\,. \\end{align*}\\] (b) We use the shortcut formula to find the variance: \\[\\begin{align*} E[X^2] &amp;= \\sum_{x=1}^3 x^2 p_X(x) = 1 \\cdot \\frac{1}{2} + 4 \\frac{1}{3} + 9 \\frac{1}{6} = \\frac{10}{3} \\\\ \\Rightarrow ~~~ V[X] &amp;= E[X^2] - E[X]^2 = \\frac{10}{3}- \\left(\\frac{5}{3}\\right)^2 = \\frac{5}{9} \\,. \\end{align*}\\] Problem 14 (a) Let \\(W_2\\) denote the event of observing two white balls, and let 1, 2, and 3 denote the event of choosing Bowl 1, 2, and 3, respectively. We apply the Law of Total Probability here: \\[\\begin{align*} P(W_2) &amp;= P(W_2 \\vert 1)P(1) + P(W_2 \\vert 2)P(2)+ P(W_2 \\vert 3)P(3) \\\\ &amp;= 0 \\cdot 1/3 + P(W_2 \\vert 2) \\cdot 1/3 + 1 \\cdot 1/3 \\,. \\end{align*}\\] Bowl 2 has 2 white and 1 black balls. Thus \\(P(W_2 \\vert 2) = 2/3 \\cdot 1/2 = 1/3\\), since there’s a 2/3 chance of drawing a white ball first, and conditional on that a 1/2 chance of drawing a white ball second. So \\[\\begin{align*} P(W_2) = 1/3 \\cdot 1/3 + 1/3 = 4/9 \\,. \\end{align*}\\] (b) We seek \\(P(3 \\vert W_2)\\). We apply Bayes’ rule here: \\[\\begin{align*} P(3 \\vert W_2) = \\frac{P(W_2 \\vert 3)P(3)}{P(W_2)} = \\frac{1 \\cdot 1/3}{4/9} = 3/4 \\,. \\end{align*}\\] Problem 15 (a) We are given that \\(P(A) = 0.8\\), \\(P(\\bar{A}) = 0.2\\), \\(P(F \\vert A) = 0.2\\), and \\(P(F \\vert \\bar{A}) = 0.1\\). Using the Law of Total Probability, we find that \\[\\begin{align*} P(F) = P(F \\vert A)P(A) + P(F \\vert \\bar{A})P(\\bar{A}) = 0.2 \\cdot 0.8 + 0.1 \\cdot 0.2 = 0.18 \\,. \\end{align*}\\] (b) We seek \\(P(A \\vert F)\\). Using Bayes’ rule, \\[\\begin{align*} P(A \\vert F) = \\frac{P(F \\vert A)P(A)}{P(F)} = \\frac{0.2 \\cdot 0.8}{0.18} = \\frac{0.16}{0.18} = \\frac{8}{9} \\,. \\end{align*}\\] Problem 16 We know that \\[\\begin{align*} E[X] = \\sum x p_X(x) = 0 \\cdot x + 1 \\cdot 0.5 + 2 \\cdot x + 3 \\cdot 0.3 = 0.5 + 2x + 0.9 = 1.6 \\,. \\end{align*}\\] Thus \\(p_X(2) = 0.1\\). Since \\(\\sum p_X(x) = 1\\), we also know \\(p_X(0) = 0.1\\). Next, we compute \\(E[X^2]\\): \\[\\begin{align*} E[X^2] = \\sum x^2 p_X(x) = 1 \\cdot 0.5 + 4 \\cdot 0.1 + 9 \\cdot 0.3 = 3.6 \\,. \\end{align*}\\] So \\(V[X] = 3.6 - (1.6)^2 = 3.6 - 2.56 = 1.04\\), and \\(\\sigma \\approx 1.02\\). Thus \\[\\begin{align*} P(\\mu - \\sigma \\leq X \\leq \\mu+\\sigma) &amp;= P(1.6-1.02 \\leq X \\leq 1.6+1.02) = P(0.58 \\leq X \\leq 2.62) \\\\ &amp;= p_X(1) + p_X(2) = 0.5 + 0.1 = 0.6 \\,. \\end{align*}\\] Problem 17 Note that one can derive the answer without ever determining the value of \\(c\\): \\[\\begin{align*} P(X &gt; 1.25 | X &lt; 1.5) &amp;= \\frac{P(X &gt; 1.25 \\cap X &lt; 1.5)}{P(X &lt; 1.5)} = \\frac{P(1.25 &lt; X &lt; 1.5)}{P(X &lt; 1.5)} \\\\ &amp;= \\frac{\\int_{5/4}^{3/2}\\frac{c}{x^2}dx}{\\int_{1}^{3/2}{\\frac{c}{x^2}}dx} = \\frac{-x^{-1}|_{5/4}^{3/2}}{-x^{-1}|_{1}^{3/2}} = \\frac{(4/5 - 2/3)}{(1-2/3)} = \\frac{12/15 - 10/15}{5/15} = \\frac{2}{5} \\,. \\end{align*}\\] Problem 18 (a) Later, we will recognize that this is a beta distribution and we thus would be able to use its properties to derive, e.g., a value for \\(c\\). In the meantime, we will apply brute-force integration: \\[\\begin{align*} \\frac{1}{c} &amp;= \\left( \\int_0^1 x^2 dx - \\int_0^1 x^3 \\right) = \\left. \\frac{x^3}{3} \\right|_0^1 - \\left. \\frac{x^4}{4} \\right|_0^1 = \\frac13 - \\frac14 = \\frac{1}{12} \\,. \\end{align*}\\] Thus \\(c = 12\\). (b) We have that \\[\\begin{align*} P(X &gt; 1/2) &amp;= 12 \\int_{1/2}^1 x^2(1-x) dx = 12 \\left[ \\frac{x^3}{3}\\bigg|_{1/2}^1 - \\frac{x^4}{4}\\bigg|_{1/2}^1 \\right] = \\left(4 - \\frac{1}{2}\\right) - \\left(3 - \\frac{3}{16}\\right) = \\frac{11}{16} \\,. \\end{align*}\\] (c) The expected value is \\[\\begin{align*} E[X] &amp;= 12 \\left( \\int_0^1 x^3 dx - \\int_0^1 x^4 \\right) = 12 \\left. \\frac{x^4}{4} \\right|_0^1 - \\left. \\frac{x^5}{5} \\right|_0^1 = 12 \\left( \\frac14 - \\frac15 \\right) = \\frac{12}{20} = \\frac35 \\,. \\end{align*}\\] Problem 19 The following 2 \\(\\times\\) 2 table shows the possible outcomes of the experiment, i.e., the values of \\(X = \\vert Y_2 - Y_1 \\vert\\): \\(Y_2 = 0\\) \\(Y_2 = 1\\) \\(Y_1 = 0\\) 0 1 \\(Y_1 = 1\\) 1 0 Each outcome is equally likely. Hence \\(P(X = 0) = 1/2\\) and \\(P(X=1) = 1/2\\). (a) We seek \\(\\sigma = \\sqrt{V[X]} = \\sqrt{E[X^2]-(E[X])^2}\\): \\[\\begin{align*} E[X] &amp;= 0 \\cdot 1/2 + 1 \\cdot 1/2 = 1/2~~\\mbox{and}\\\\ E[X^2] &amp;= 0^2 \\cdot 1/2 + 1^2 \\cdot 1/2 = 1/2 \\,. \\end{align*}\\] So \\(\\sigma = \\sqrt{1/2-1/4} = 1/2\\). (b) The expected value is \\(\\mu = E[X] = 1/2\\), while the standard deviation is \\(\\sigma = 1/2\\), so ultimately we are asking for \\(P(1/2-1/2 &lt; X &lt; 1/2+1/2) = P(0 &lt; X &lt; 1)\\)…which equals zero because there are no probability masses between 0 and 1 exclusive. Problem 20 (a) The mean of the distribution is \\[\\begin{align*} E[X] = \\int_0^1 x f_X(x) dx = \\int_0^1 3x^3 dx = \\left. \\frac{3}{4}x^4 \\right|_0^1 = \\frac{3}{4} \\,. \\end{align*}\\] (b) We first compute \\(E[X^2]\\): \\[\\begin{align*} E[X^2] = \\int_0^1 x^2 f_X(x) dx = \\int_0^1 3x^4 dx = \\left. \\frac{3}{5}x^5 \\right|_0^1 = \\frac{3}{5} \\,. \\end{align*}\\] We then apply the shortcut formula to determine the variance: \\[\\begin{align*} V[X] = E[X^2] - (E[X])^2 = \\frac{3}{5} - \\left(\\frac34\\right)^2 = \\frac{3}{80} \\,. \\end{align*}\\] (c) The expected value of the difference is \\[\\begin{align*} E[2X_1 - 2X_2] = 2E[X_1] - 2E[X_2] = 2 \\cdot \\frac34 - 2 \\cdot \\frac34 = 0 \\,. \\end{align*}\\] (d) The variance of the difference is \\[\\begin{align*} V[2X_1 - 2X_2] = 4V[X_1] + 4V[X_2] = 4\\left(\\frac{3}{80} + \\frac{3}{80}\\right) = \\frac{3}{10} \\,. \\end{align*}\\] Problem 21 (a) We are dealing with a continuous random variable. The pdf is thus \\[\\begin{align*} f_X(x) = \\frac{d}{dx}F_X(x) = 3x^2 ~~~~~~ x \\in [0,1] \\,. \\end{align*}\\] (b) Given \\(f_X(x) = 3x^2\\) from part (a), we can compute \\(V[X]\\): \\[\\begin{align*} V[X] &amp;= E[X^2] - (E[X])^2 \\\\ &amp;= \\int_0^1 x^2 f_X(x) dx - \\left[\\int_0^1 x f_X(x) dx\\right]^2 = \\int_0^1 3 x^4 dx - \\left[\\int_0^1 3 x^3 dx\\right]^2 \\\\ &amp;= \\left.\\frac{3x^5}{5}\\right|_0^1 - \\left[ \\left.\\frac{3x^4}{4}\\right|_0^1 \\right]^2 = \\frac{3}{5} - \\left( \\frac{3}{4} \\right)^2 = \\frac{48}{80} - \\frac{45}{80} = \\frac{3}{80} \\,. \\end{align*}\\] (c) We seek \\(P\\left(X &lt; \\frac{1}{2} \\vert X &gt; \\frac{1}{4}\\right)\\): \\[\\begin{align*} P\\left(X &lt; \\frac{1}{2} \\vert X &gt; \\frac{1}{4}\\right) &amp;= \\frac{P\\left(X &lt; \\frac{1}{2} \\cap X &gt; \\frac{1}{4}\\right)}{P\\left(X &gt; \\frac{1}{4}\\right)} = \\frac{P\\left(\\frac{1}{4} &lt; X &lt; \\frac{1}{2}\\right)}{P\\left(X &gt; \\frac{1}{4}\\right)} \\\\ &amp;= \\frac{F_X\\left(\\frac{1}{2}\\right)-F_X\\left(\\frac{1}{4}\\right)}{1 - F_X\\left(\\frac{1}{4}\\right)} = \\frac{\\left(\\frac{1}{2}\\right)^3 - \\left(\\frac{1}{4}\\right)^3}{1 - \\left(\\frac{1}{4}\\right)^3} = \\frac{\\frac{1}{8} - \\frac{1}{64}}{1 - \\frac{1}{64}} = \\frac{\\frac{7}{64}}{\\frac{63}{64}} = \\frac{1}{9} \\,. \\end{align*}\\] Problem 22 First, we write down the pmf: \\(x\\) 0 1 3 \\(p_X(x)\\) 0.5 0.25 0.25 Second, we compute \\(E[X]\\) and \\(E[X^2]\\): \\[\\begin{align*} E[X] &amp;= \\sum_{x=-a}^{a} x p_X(x) = 0 \\cdot 0.5 + 1\\cdot 0.25 + 3 \\cdot 0.25 = 1 \\,. \\\\ E[X^2] &amp;= \\sum_{x=-a}^{a} x^2 p_X(x) = 0 \\cdot 0.5 + 1\\cdot 0.25 + 3^2 \\cdot 0.25 = 2.5 \\,. \\end{align*}\\] Thus \\(V[X] = E[X^2] - E[X]^2 = 2.5 - 1 = 1.5\\). Problem 23 (a) We know that \\(F_X(1) = 1\\), so \\(c(1^2) = 1 ~~~ \\Rightarrow ~~~ c = 1\\). (b) Given that \\(f_X(x) = 2x\\), for \\(0 \\leq x \\leq 1\\), \\[\\begin{align*} E[X] = \\int_0^1 x 2x dx = \\frac{2}{3}x^3\\bigg|_0^1 = \\frac{2}{3} \\,. \\end{align*}\\] (c) We utilize the shortcut formula to determine the standard deviation: \\[\\begin{align*} E[X^2] &amp;= \\int_0^1 x^2 2x dx = \\frac{2}{4}x^4\\bigg|_0^1 = \\frac{1}{2} \\\\ V[X] &amp;= E[X^2] - E[X]^2 = \\frac{1}{2} - \\frac{4}{9} = \\frac{1}{18} \\\\ \\Rightarrow ~~~ \\sigma &amp;= \\sqrt{V[X]} = \\sqrt{\\frac{1}{18}} = 0.236 \\,. \\end{align*}\\] Problem 24 (a) If \\(b\\) is larger \\(b\\), then \\(a\\) is smaller. Given that the minimum value of \\(a\\) for a valid pdf is zero, we can determine that \\[\\begin{align*} \\int_1^2 bx dx = 1 = b \\frac{x^2}{1}\\bigg|_1^2 = b\\left(\\frac{4}{2} - \\frac{1}{2} \\right) ~~~ \\Rightarrow ~~~ b = \\frac{2}{3} \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} F_X(x) = \\left\\{ \\begin{array}{cc} 0 &amp; x \\leq 0 \\\\ \\int_0^xa dz = az\\bigg|_0^x = ax = \\frac{1}{2}x &amp; x \\in (0,1] \\\\ \\int_0^1 a dz + \\int_1^xbz dz = F_X(1) + b\\frac{z^2}{2}\\bigg|_0^x = \\frac{1}{2} + \\frac{1}{6}(x^2 - 1) &amp; x \\in (1,2] \\\\ 1 &amp; x &gt; 2 \\end{array} \\right. \\,. \\end{align*}\\] Problem 25 (a) \\(E[X] = 2 = 1(0.4) + y (0.2) ~~~ \\Rightarrow ~~~ y = 8\\). (b) \\(E[X] = 1(0.4) + 3 (0.2) = 1\\) and \\(E[X^2] = 1(0.4) + 9 (0.2) = 2.2\\). Therefore \\[\\begin{align*} \\sigma = \\sqrt{E[X^2] - E[X]^2} = \\sqrt{1.2} \\,. \\end{align*}\\] (c) \\(P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) = P(1 -\\sqrt{1.2} \\leq X \\leq 1 +\\sqrt{1.2} ) = p_X(0) + p_X(1) = 0.8\\). Problem 26 (a) \\(\\int_0^1 c \\, dx = 1 - 0.2 - 0.2 = 0.6 = cx\\bigg|_0^1 = c \\, \\Rightarrow \\, c = 0.6\\). (b) \\(E[X] = (-1)(0.2) + \\int_0^1 0.6x dx + 2 (0.2) = 0.2 + 0.3x^2|_0^1 = 0.5\\). (c) The cdf is constantly \\(0\\) before \\(-1\\); constantly \\(0.2\\) between \\([-1,0)\\); a line that connects these two points \\((0, 0.2) - (1,0.8)\\) between \\([0,1)\\); constantly \\(0.8\\) between \\([1,2)\\); then constantly \\(1\\) after \\(2\\). Problem 27 (a) \\(F_X(x) = \\int_0^x f_Z(z) dz =\\int_0^x e^{-z} dz = -e^{-z} \\bigg|_0^x = 1-e^{-x}\\). (b) \\(P(X &gt; 1) = 1 - F_X(1) = 1 - (1 - e^{-1}) = e^{-1}\\). (c) We have that \\[\\begin{align*} P(X &lt; 1/2 \\cup X&gt;2) &amp;= F_X(1/2) + (1 - F_X(2)) \\\\ &amp;= 1 - e^{-1/2} + (1 - (1 - e^{-2})) = 1 - e{-1/2} + e^{-2} \\,. \\end{align*}\\] (d) We have that \\[\\begin{align*} P(X &lt; 2 | X&gt;1) = \\frac{P(1 &lt; X &lt; 2)}{P(X&gt;1)} = \\frac{F_X(2) - F_X(1)}{1 - F_X(1)} = \\frac{(1 - e^{-2}) - (1 - e^{-1})}{1 - (1 - e^{-1})} =1 - e^{-1} \\,. \\end{align*}\\] Problem 28 (a) The pdf is symmetric around zero \\(\\Rightarrow E[X] = 0\\): \\[\\begin{align*} E[X] = \\int_{-1}^0 -x^2 dx + \\int_0^1 x^2 dx = - \\frac{x^3}{3}\\bigg|_{-1}^0 + \\frac{x^3}{3}\\bigg|_{0}^1 = -\\frac{1}{3} + \\frac{1}{3} = 0 \\,. \\end{align*}\\] (b) The variance is \\[\\begin{align*} V[X] = E[X^2] - E[X]^2 = \\int_{-1}^0 -x^3 dx + \\int_0^1 x^3 dx = - \\frac{x^4}{3}\\bigg|_{-1}^0 + \\frac{x^4}{4}\\bigg|_{0}^1 + \\frac{x^4}{4}\\bigg|_{0}^1 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} \\,. \\end{align*}\\] (c) We have that \\(F_X(0) = \\frac{1}{2}\\)…thus: \\[\\begin{align*} F_X(x) \\text{ for } x\\in\\left[0,1\\right] = \\frac{1}{2} + \\int_0^x z dz = \\frac{1}{2} + \\frac{z^2}{2}\\bigg|_0^x = \\frac{1}{2} + \\frac{x^2}{2} \\,. \\end{align*}\\] Problem 29 (a) We have that \\[\\begin{align*} f_X(x) = \\frac{d}{dx}F_X(x) = \\begin{cases} 0 &amp; \\text{ if } x \\leq 0\\\\ 2cx &amp; \\text{ if } x \\in (0, 1]\\\\ 0 &amp;\\text{ if } x&gt;1 \\end{cases} \\,. \\end{align*}\\] At \\(x = 1\\), \\(cx^2 = 1\\), and so \\(c = 1\\). (b) \\(E[X] = \\int_0^1 2x^2 dx = \\frac{2}{3}\\bigg|_0^1 = \\frac{2}{3}\\). Problem 30 (a) A valid pdf has integral 1 over the domain. Thus \\[\\begin{align*} \\int_0^1 c dx + \\int_1^\\infty e^{-x} dx &amp;= 1 \\\\ c \\int_0^1 dx &amp;= 1 - \\int_1^\\infty e^{-x} dx \\\\ c \\cdot 1 &amp;= 1 + \\left. e^{-x}\\right|_1^\\infty \\\\ c &amp;= 1 + (0 - e^{-1}) = 1 - e^{-1} \\,. \\end{align*}\\] (b) The cdf for \\(x \\in [0,1)\\) is \\(c \\int_0^x dz = cx\\). For \\(x \\in [1,\\infty)\\), we have \\[\\begin{align*} \\int_0^x f(z) dz &amp;= \\int_0^1 c dz + \\int_1^x e^{-z} dz \\\\ &amp;= c - \\left. e^{-z}\\right|_1^x = c - (e^{-x} - e^{-1}) = c + (e^{-1} - e^{-x}) \\,. \\end{align*}\\] The cdf is thus \\(x\\) \\((-\\infty,0)\\) \\([0,1)\\) \\([1,\\infty)\\) \\(F_X(x)\\) 0 \\(cx\\) \\(c + (e^{-1} - e^{-x})\\) Problem 31 (a) The sum of the probability masses has to equal 1, so \\(c = 1 - 1/6 - 1/6 = 2/3\\). (b) The cdf is 0 for \\(x \\in (-\\infty,-1)\\), then 1/6 for \\(x \\in [-1,0)\\), then 1/6+2/3 = 5/6 for \\(x \\in [0,1)\\)…thus \\(F_X(0.5) = 5/6\\). (c) The generalized inverse cdf is \\[\\begin{align*} x = F_X^{-1}(q) = \\mbox{inf}\\{ x : F_X(x) \\geq q \\} \\,, \\end{align*}\\] i.e., the smallest value of \\(x\\) such that \\(F_X(x) \\geq q = 0.9\\). Given that the cdf jumps from 5/6 (=0.833) to 1 at \\(x=1\\), the value that we want is \\(x=1\\). (d) The variance is \\(V[X] = E[X^2] - (E[X])^2\\): \\[\\begin{align*} E[X] &amp;= \\sum_{x} x p_X(x) = -1 \\cdot \\frac16 + 0 \\cdot \\frac23 + 1 \\cdot \\frac16 = 0 \\,. \\\\ E[X^2] &amp;= \\sum_{x} x^2 p_X(x) = (-1)^2 \\cdot \\frac16 + (0)^2 \\cdot \\frac23 + (1)^2 \\cdot \\frac16 = \\frac13 \\,. \\end{align*}\\] Thus \\(V[X] = \\frac13\\). Problem 32 (a) In the range \\([0,1]\\), \\(f_X(x)\\) is \\[\\begin{align*} f_X(x) = \\frac{d}{dx} x^3 = 3x^2 \\,. \\end{align*}\\] (b) The probability is \\[\\begin{align*} P\\left(\\frac14 \\leq X \\leq \\frac34\\right) &amp;= F_X\\left(\\frac34\\right) - F_X\\left(\\frac14\\right) = \\left(\\frac34\\right)^3 - \\left(\\frac14\\right)^3 = \\frac{27}{64} - \\frac{1}{64} = \\frac{26}{64} = \\frac{13}{32} \\,. \\end{align*}\\] (c) The conditional probability is \\[\\begin{align*} P\\left(X \\leq \\frac14 \\vert X \\leq \\frac12\\right) &amp;= \\frac{P\\left(X \\leq \\frac14 \\cap X \\leq \\frac12\\right)}{P\\left(X \\leq \\frac12\\right)} = \\frac{P\\left(X \\leq \\frac14\\right)}{P\\left(X \\leq \\frac12\\right)} \\\\ &amp;= \\frac{F_X\\left(\\frac14\\right)}{F_X\\left(\\frac12\\right)} = \\frac{(1/4)^3}{(1/2)^3} = \\frac{8}{64} = \\frac18 \\,. \\end{align*}\\] (d) The inverse cdf is given by \\[\\begin{align*} q = x^3 ~~~ \\Rightarrow ~~~ x = F_X^{-1}(q) = q^{1/3} \\,. \\end{align*}\\] Problem 33 (a) The expected value is \\[\\begin{align*} E[Y] = E[X_1+2X_2-3X_3-4] = E[X_1] + 2E[X_2] - 3E[X_3] - 4 = 1 + 2 - 3 - 4 = -4 \\,. \\end{align*}\\] (b) The variance is \\[\\begin{align*} V[Y] = V[X_1+2X_2-3X_3-4] = V[X_1] + 4V[X_2] + 9V[X_3] = 1 + 4 + 9 = 14 \\,. \\end{align*}\\] (c) By “reversing” the shortcut formula, we find that \\[\\begin{align*} E[Y^2] = V[Y] + (E[Y])^2 = 14 + (-4)^2 = 30 \\,. \\end{align*}\\] Problem 34 (a) We know that \\(F_X(2) = 1 = c \\cdot 2^3\\), so \\(c = 1/8\\). (b) \\(f_X(x) = (d/dx)F_X(x) = 3cx^2\\). (c) The conditional probability is \\[\\begin{align*} P(X &lt; 1/2 \\vert X &lt; 1) &amp;= \\frac{P(X &lt; 1/2 \\cap X &lt; 1)}{P(X &lt; 1)} = \\frac{P(X &lt; 1/2)}{P(X &lt; 1)} = \\frac{F_X(1/2)}{F_X(1)} = \\frac{(1/2)^3}{1^3} = 1/8 \\,. \\end{align*}\\] Problem 35 We are given that \\(X \\vert \\theta\\) is a continuous random variable, and that \\(\\Theta\\) is a discrete random variable, so the appropriate Law of Total Probability expression is \\[\\begin{align*} f_X(x) &amp;= \\sum_{\\theta} f_{X \\vert \\theta}(x \\vert \\theta) p_{\\Theta}(\\theta) \\\\ &amp;= \\sum_{\\theta} \\theta x^{\\theta-1} p_{\\Theta}(\\theta) \\\\ &amp;= 1 \\cdot x^{1-1} \\cdot p_{\\Theta}(\\theta = 1) + 2 \\cdot x^{2-1} \\cdot p_{\\Theta}(\\theta = 2) \\\\ &amp;= 1 \\cdot x^{1-1} \\cdot 1/2 + 2 \\cdot x^{2-1} \\cdot 1/2 \\\\ &amp;= x + 1/2 \\,, \\end{align*}\\] with \\(x \\in [0,1]\\). Problem 36 (a) The expected value \\(E[X]\\) is \\[\\begin{align*} E[X] &amp;= \\int_0^1 x f_X(x) dx = \\int_0^1 x 6 x (1-x) dx \\\\ &amp;= 6 \\left[ \\int_0^1 x^2 dx - \\int_0^1 x^3 dx \\right] = 6 \\left[ \\left.\\frac{x^3}{3}\\right|_0^1 - \\left.\\frac{x^4}{4}\\right|_0^1 \\right] = 6 \\left( \\frac{1}{3} - \\frac{1}{4} \\right) = 6 \\frac{1}{12} = \\frac{1}{2} \\,. \\end{align*}\\] (b) We know that for any distribution, \\(E[\\bar{X}] = \\mu = E[X]\\). Hence \\(E[\\bar{X}] = 1/2\\). Problem 37 (a) The biases are \\[\\begin{align*} B[\\hat{\\mu}_1] &amp;= E[\\hat{\\mu}_1 - \\mu] = E[\\hat{\\mu}_1] = E\\left[X_1-X_2\\right] = E[X_1] - E[X_2] = \\mu - \\mu = 0 \\\\ B[\\hat{\\mu}_2] &amp;= E[\\hat{\\mu}_2 - \\mu] = E[\\hat{\\mu}_2] = E\\left[\\frac{X_1+X_2}{2}\\right] = \\frac12 \\left(E[X_1] + E[X_2]\\right) = \\frac12 \\left(\\mu + \\mu\\right) = 0 \\,. \\end{align*}\\] Both estimators are unbiased. (b) The variances are \\[\\begin{align*} V[\\hat{\\mu}_1] &amp;= V\\left[X_1-X_2\\right] = V[X_1] + V[X_2] = \\sigma^2 + \\sigma^2 = 2\\sigma^2 = \\frac23\\\\ V[\\hat{\\mu}_2] &amp;= V\\left[\\frac{X_1+X_2}{2}\\right] = \\frac14 \\left(V[X_1] + V[X_2]\\right) = \\frac14 \\left(\\sigma^2 + \\sigma^2\\right) = \\frac{\\sigma^2}{2} = \\frac16\\,. \\end{align*}\\] (c) The mean-squared errors are \\[\\begin{align*} MSE[\\hat{\\mu}_1] &amp;= (B[\\hat{\\mu}_1])^2 + V[\\hat{\\mu}_1] = 0^2 + 2\\sigma^2 = 2\\sigma^2 = \\frac23 \\\\ MSE[\\hat{\\mu}_2] &amp;= (B[\\hat{\\mu}_2])^2 + V[\\hat{\\mu}_2] = 0^2 + \\frac{\\sigma^2}{2} = \\frac{\\sigma^2}{2} = \\frac16 \\,. \\end{align*}\\] (d) \\(\\hat{\\mu}_2\\) has the lower mean-squared error, so it is the better estimator. Problem 38 (a) The likelihood function is \\[\\begin{align*} \\mathcal{L}(a \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x_i) = \\prod_{i=1}^n a (1+x_i)^{-(a+1)} = a^n [\\prod_{i=1}^n (1+x_i)]^{-(a+1)} \\,. \\end{align*}\\] The log-likelihood function is thus \\[\\begin{align*} \\ell(a \\vert \\mathbf{x}) &amp;= \\log \\left[ a^n [\\prod_{i=1}^n (1+x_i)]^{-(a+1)} \\right] = \\log a^n + \\log \\left[ [\\prod_{i=1}^n (1+x_i)]^{-(a+1)} \\right] \\\\ &amp;= n \\log a - (a+1) \\log [\\prod_{i=1}^n (1+x_i)] = n \\log a - (a+1) \\sum_{i=1}^n \\log (1+x_i) \\,. \\end{align*}\\] (b) The maximum likelihood estimate is \\[\\begin{align*} \\frac{d}{da} \\ell(a \\vert \\mathbf{x}) &amp;= \\frac{n}{a} - \\sum_{i=1}^n \\log (1+x_i) = 0 \\\\ \\Rightarrow ~~~ \\frac{n}{a} &amp;= \\sum_{i=1}^n \\log (1+x_i) \\\\ \\Rightarrow ~~~ a &amp;= \\frac{n}{\\sum_{i=1}^n \\log (1+x_i)} \\,. \\end{align*}\\] We rewrite this into “probabilistic” notation: \\[\\begin{align*} \\hat{a}_{MLE} = \\frac{n}{\\sum_{i=1}^n \\log (1+X_i)} \\,. \\end{align*}\\] (c) We invoke the invariance property of MLEs to write that \\[\\begin{align*} \\hat{\\mu}_{MLE} &amp;= \\frac{1}{\\hat{a}_{MLE}-1} = \\frac{1}{[n/\\sum_{i=1}^n \\log (1+X_i)]-1} = \\frac{\\sum_{i=1}^n \\log (1+X_i)}{n-\\sum_{i=1}^n \\log (1+X_i)} \\,. \\end{align*}\\] Problem 39 (a) The log-likelihood is \\[\\begin{align*} \\ell(\\theta \\vert \\mathbf{x}) &amp;= \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta) = \\sum_{i=1}^n \\log \\theta + (\\theta-1) \\log(1-x_i) = n \\log \\theta + (\\theta - 1) \\sum_{i=1}^n \\log(1-x_i) \\,. \\end{align*}\\] (b) The first derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) is \\[\\begin{align*} \\frac{d}{d\\theta} \\left[ n \\log \\theta + (\\theta - 1) \\sum_{i=1}^n \\log(1-x_i) \\right] &amp;= \\frac{n}{\\theta} + \\sum_{i=1}^n \\log(1-x_i) \\,. \\end{align*}\\] We set this equal to zero and solve for \\(\\theta\\): \\[\\begin{align*} \\hat{\\theta}_{MLE} = -\\frac{n}{\\sum_{i=1}^n \\log(1-X_i)} \\,. \\end{align*}\\] (c) We utilize the invariance property of the MLE: \\[\\begin{align*} \\hat{\\mu}_{MLE} = \\frac{1}{1 + \\hat{\\theta}_{MLE}} = \\frac{\\sum_{i=1}^n \\log(1-X_i)}{\\left[ \\sum_{i=1}^n \\log(1-X_i) \\right] - n} \\,. \\end{align*}\\] Problem 40 We first derive the likelihood \\[\\begin{align*} \\ell(x_1, x_2, \\dots, x_n | \\alpha) &amp;= \\sum_{i=1}^n \\left[ \\log \\alpha + (\\alpha-1) \\log x_i \\right] = n \\log \\alpha + (\\alpha - 1) \\sum_{i=1}^n \\log x_i\\\\ \\implies ~~~ \\ell&#39;(x_1, x_2, \\dots, x_n | \\alpha) &amp;= \\frac{n}{\\alpha} + \\sum_{i=1}^n \\log x_i \\,. \\end{align*}\\] We set \\(\\frac{n}{\\hat{\\alpha}_{MLE}} + \\sum_{i=1}^n \\log x_i = 0\\) and find that \\[\\begin{align*} \\hat{\\alpha}_{MLE} = \\frac{n}{- \\sum_{i=1}^n \\log X_i)} \\,. \\end{align*}\\] Problem 41 (a) The log-likelihood and its derivative are \\[\\begin{align*} \\ell (x_1,\\dots, x_n | p) &amp;= \\log (1-p) \\sum_{i=1}^n (x_i - 1) + n \\log p\\\\ \\implies \\ell&#39; (x_1,\\dots, x_n | \\hat{p}) &amp;= - \\frac{\\sum_{i=1}^n x_i - n}{1 - \\hat{p}} + \\frac{n}{\\hat{p}} = 0 \\,. \\end{align*}\\] From \\[\\begin{align*} \\frac{\\sum_{i=1}^n x_i - n}{1 - \\hat{p}} = \\frac{n}{\\hat{p}} \\quad \\Rightarrow \\quad \\left(\\sum_{i=1}^n x_i - n\\right) \\hat{p} = n (1 - \\hat{p}) \\quad \\Rightarrow \\quad \\hat{p} \\,\\sum_{i=1}^n x_i = n \\,, \\end{align*}\\] it follows that \\(\\hat{p}_{MLE} = \\frac{n}{\\sum_{i=1}^n X_i}\\). By the invariance property of the MLE, we find that \\[\\begin{align*} \\widehat{1/p}_{MLE} = \\frac{\\sum_{i=1}^n X_i}{n} \\,. \\end{align*}\\] (b) The variance of this estimator is \\[\\begin{align*} V\\left[\\widehat{1/p}_{MLE} \\right] = V\\left[\\frac{\\sum_{i=1}^n X_i}{n} \\right] = \\frac{V(X_1)}{n} = \\frac{1-p}{np^2} \\,. \\end{align*}\\] Problem 42 (a) The bias of the estimator is \\[\\begin{align*} B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta] = E[\\hat{\\theta}]-\\theta = E\\left[\\frac{X_1-X_2}{2}\\right] - \\theta = \\frac12 ( E[X_1] - E[X_2] ) - \\theta = \\frac12 ( \\theta - \\theta ) - \\theta = -\\theta \\,. \\end{align*}\\] (b) The variance of the estimator is \\[\\begin{align*} V[\\hat{\\theta}] = V\\left[\\frac{X_1-X_2}{2}\\right] = \\frac14(V[X_1]+V[X_2]) = \\frac14 \\frac{2\\theta^2}{12} = \\frac{\\theta^2}{6} \\,. \\end{align*}\\] (c) The mean-squared error of the estimator is \\[\\begin{align*} MSE[\\hat{\\theta}] = (B[\\hat{\\theta}])^2 + V[\\hat{\\theta}] = \\theta^2 + \\frac{\\theta^2}{6} = \\frac76\\theta^2 \\,. \\end{align*}\\] (d) Changing the sign does not change the variance! And “unbiased” means the bias is zero. So: \\[\\begin{align*} MSE[\\hat{\\theta}&#39;] = V[\\hat{\\theta}] = \\frac{\\theta}{12} \\,. \\end{align*}\\] Problem 43 (a) The bias of the estimator is \\[\\begin{align*} B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta] = E\\left[X_1 - \\frac{X_2}{n}\\right] - \\theta = E[X_1] - \\frac{E[X_2]}{n} - \\theta = \\theta - \\frac{\\theta}{n} - \\theta = -\\frac{\\theta}{n} \\,. \\end{align*}\\] (b) The variance of the estimator is \\[\\begin{align*} V[\\hat{\\theta}] = V\\left[X_1 - \\frac{X_2}{n}\\right] = V[X_1] + \\frac{1}{n^2}V[X_2] = \\left(1+\\frac{1}{n^2}\\right)\\theta^2 \\,. \\end{align*}\\] Problem 44 (a) The likelihood is \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\prod_{i=1}^n c e^{-\\theta x_i} \\theta^{x_i-1} = c^n e^{-\\theta \\sum_{i=1}^n x_i} \\theta^{\\sum_{i=1}^n x_i - n} \\,. \\end{align*}\\] Thus the log-likelihood is \\[\\begin{align*} \\ell(\\theta \\vert \\mathbf{x}) &amp;= n \\log c - \\theta \\sum_{i=1}^n x_i + (\\sum_{i=1}^n x_i - n) \\log \\theta \\,. \\end{align*}\\] (b) The derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) with respect to \\(\\theta\\) is \\[\\begin{align*} \\ell&#39;(\\theta \\vert \\mathbf{x}) &amp;= - \\sum_{i=1}^n x_i + \\frac{\\sum_{i=1}^n x_i - n}{\\theta} \\,. \\end{align*}\\] Setting this to zero and solving for \\(\\theta\\), we get \\[\\begin{align*} \\hat{\\theta}_{MLE} = \\frac{\\sum_{i=1}^n X_i - n}{\\sum_{i=1}^n X_i} = \\frac{\\bar{X}-1}{\\bar{X}} \\,. \\end{align*}\\] (c) We utilize the invariance property of the MLE: \\[\\begin{align*} \\hat{\\mu}_{MLE} = \\frac{1}{1-\\hat{\\theta}_{MLE}} = \\frac{1}{1-(\\bar{X}-1)/\\bar{X})} = \\frac{\\bar{X}}{\\bar{X}-(\\bar{X}-1)} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\,. \\end{align*}\\] Problem 45 (a) The cdf for the given pdf is \\[\\begin{align*} F_Y(y) = \\int_0^y a(1+v)^{-a-1} dv \\,. \\end{align*}\\] This is a \\(u^n du\\)-style integral, meaning that \\[\\begin{align*} F_Y(y) &amp;= \\int_0^y a(1+v)^{-a-1} dv = a \\frac{1}{-a} \\left. (1+v)^{-a}\\right|_0^y = -\\left((1+y)^{-a} - 1^{-a}\\right) = 1 - (1+y)^{-a} \\,. \\end{align*}\\] (b) \\(E[Y]\\) decreases with \\(a\\); combining that fact with our desire to determine a lower bound, we focus on the fourth line of the confidence interval reference table and solve \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert a) - \\alpha &amp;= 0 \\\\ \\Rightarrow ~~~ 1 - (1+y_{\\rm obs})^{-a} - \\alpha &amp;= 0 \\\\ \\Rightarrow ~~~ (1+y_{\\rm obs})^{-a} &amp;= 1 - \\alpha \\\\ \\Rightarrow ~~~ -a \\log (1+y_{\\rm obs}) &amp;= \\log(1-\\alpha) \\\\ \\Rightarrow ~~~ \\hat{a}_L &amp;= -\\frac{\\log(1-\\alpha)}{\\log(1+y_{\\rm obs})} \\,. \\end{align*}\\] Problem 46 (a) The fact that we wish to perform a lower-tail test, combined with the fact that \\(E[Y]\\) increases with \\(\\mu\\), means that we will focus on the third line of the hypothesis test reference table and solve for \\[\\begin{align*} y_{\\rm RR} = F_Y^{-1}(\\alpha \\vert \\mu_o) \\,. \\end{align*}\\] We have that \\[\\begin{align*} F_Y(y_{\\rm RR} \\vert \\mu_o) - \\alpha &amp;= 0 \\\\ \\Rightarrow ~~~ \\frac{1}{1+\\exp(-y_{\\rm RR})} &amp;= \\alpha \\\\ \\Rightarrow ~~~ 1+\\exp(-y_{\\rm RR}) &amp;= \\frac{1}{\\alpha} \\\\ \\Rightarrow ~~~ \\exp(-y_{\\rm RR}) &amp;= \\frac{1}{\\alpha}-1 \\\\ \\Rightarrow ~~~ y_{\\rm RR} &amp;= -\\log\\left(\\frac{1}{\\alpha}-1\\right) \\,. \\end{align*}\\] Since \\(1/0.05 = 20\\), we find that \\(y_{\\rm RR} = -\\log(19)\\). (b) As stated in the hypothesis test reference table, the rejection region is \\(y_{\\rm obs} &lt; y_{\\rm RR}\\). Thus if \\(y_{\\rm obs} \\geq y_{\\rm RR}\\), we fail to reject the null hypothesis. (c) If we go from a lower-tail test (with \\(E[Y]\\) increasing with \\(\\mu\\)) to a two-tail test, then the lower rejection region boundary will shift because we put \\(\\alpha/2 = 0.025\\) into the equation rather than \\(\\alpha = 0.05\\). This moves the boundary further from \\(\\mu_o = 0\\). (Specifically, \\(1/\\alpha\\) would now be 40, so the new \\(y_{\\rm RR} = -\\log(39)\\), which is further from 0 than \\(-\\log(19)\\).) Chapter 3 Problem 1 The median of a \\(\\mathcal{N}(2,4)\\) is \\(\\tilde{\\mu} = 2\\), thus \\(P(X_i &gt; 2) = 0.5\\) by inspection. Now, let \\(Y\\) be the number of values \\(&gt; 2\\). Then \\[\\begin{align*} P(Y=m) = \\binom{n}{m} \\left(\\frac12\\right)^m \\left(\\frac12\\right)^{n-m} = \\binom{n}{m} \\left(\\frac12\\right)^n \\,. \\end{align*}\\] Problem 2 (a) We fix the number of successes to \\(s=1\\), with the random variable being the number of tickets we need to buy before buying the one that allows us to win the raffle. There are two correct answers here: \\(F\\) is sampled from the geometric distribution, with \\(p=0.4\\), or from the negative binomial distribution, with \\(s=1\\) and \\(p=0.4\\). (b) We have that \\(W = 2-F\\), so \\[\\begin{align*} P(W &gt; 0) = P(2-F &gt; 0) &amp;= P(F &lt; 2) = p_X(0) + p_X(1) \\\\ &amp;= (0.4)^1(1-0.4)^0 + (0.4)^1(1-0.4)^1 = 0.4 + 0.24 = 0.64 \\,. \\end{align*}\\] (c) Let \\(X\\) be the number of winning tickets. Then \\[\\begin{align*} P(X = 1 \\vert X \\geq 1) &amp;= \\frac{P(X = 1 \\cap X \\geq 1)}{P(X \\geq 1)} = \\frac{P(X=1)}{1-P(X=0)} \\\\ &amp;= \\frac{\\binom{2}{1}(0.4)^1(0.6)^1}{1 - \\binom{2}{0}(0.4)^0(0.6)^2} = \\frac{2 \\cdot 0.24}{1 - 0.36} = 0.48/0.64 = 0.75 \\,. \\end{align*}\\] Problem 3 The first step is to write down the probability mass function for the number of insured drivers, given that the number of insured drivers is odd: \\[\\begin{align*} P(X=1 \\vert X=1 \\cup X=3) &amp;= \\frac{p_X(1)}{p_X(1)+p_X(3)} = \\frac{\\binom{3}{1}(1/2)^1(1/2)^2}{\\binom{3}{1}(1/2)^1(1/2)^2 + \\binom{3}{3}(1/2)^3(1/2)^0} \\\\ &amp;= \\frac{3}{3 + 1} = \\frac34 \\\\ P(X=3 \\vert X=1 \\cup X=3) &amp;= 1 - P(X=1 \\vert X=1 \\cup X=3) = \\frac14 \\,. \\end{align*}\\] So the expected value is \\[\\begin{align*} E[X \\vert X=1 \\cup X=3] = \\sum_{1,3} x p_X(x \\vert x=1 \\cup x=3) = 1 \\cdot 3/4 + 3 \\cdot 1/4 = 3/2 \\,. \\end{align*}\\] Problem 4 (a) We are conducting a negative binomial experiment with \\(s = 2\\). The random variable is the number of failures…here, \\(X = 2\\). So: \\[\\begin{align*} p_X(2) = \\binom{2+2-1}{2} \\left(\\frac{1}{2}\\right)^2 \\left(\\frac{1}{2}\\right)^2 = \\frac{3!}{1!2!} \\frac{1}{16} = \\frac{3}{16} \\,. \\end{align*}\\] (b) The sum of the data is negatively binomially distributed for \\(s=4\\) successes and probability of success \\(p=1/2\\). The overall number of failures here is \\(X = 1\\). So \\[\\begin{align*} p_X(1) = \\binom{1+4-1}{1} \\left(\\frac{1}{2}\\right)^4 \\left(\\frac{1}{2}\\right)^1 = \\frac{4!}{1!3!} \\frac{1}{32} = \\frac{4}{32} = \\frac18 \\,. \\end{align*}\\] (c) This is a negative binomial experiment, so \\(E[X] = s(1-p)/p\\), which decreases as \\(p\\) increases. Referring to the confidence interval reference table, we see that \\(q = 1-\\alpha = 0.9\\). Problem 5 (a) We have that \\(f_X(x) = 3x^2\\) and thus that \\(F_X(x) = x^3\\). Plugging these into the formula for \\(f_{(j)}(x)\\) (along with \\(j=1\\)) yields \\[\\begin{align*} f_{(1)}(x) = \\frac{4!}{3!0!}(1-x^3)^3 3 x^2 = 12(1-x^3)^3x^2 \\,, \\end{align*}\\] for \\(x \\in [0,1]\\). (b) We have that \\[\\begin{align*} E[X_{(4)}] = \\int_0^1 x 12x^{11} = \\int_0^1 12x^{12} = \\left.\\frac{12}{13}x^{13}\\right|_0^1 = \\frac{12}{13} \\,. \\end{align*}\\] Problem 6 (a) The cdf is \\(F_X(x) = \\int_0^x y dy = \\frac{x^2}{2}\\). Thus \\[\\begin{align*} f_{(3)} = 3x\\left[F_X(x)\\right]^{2} = 3x\\frac{x^4}{4} = \\frac{3}{4} x^5 \\end{align*}\\] for \\(x \\in [0,\\sqrt{2}]\\). (b) The variance is \\(V[X_{(3)}] = E[X_{(3)}^2] - (E[X_{(3)}])^2\\), where \\[\\begin{align*} E[X_{(3)}] &amp;= \\int_0^{\\sqrt{2}} x \\left(\\frac{3}{4} x^5\\right)dx =\\frac{3}{4} \\frac{x^7}{7}\\bigg|_0^{\\sqrt{2}} = \\frac{3\\cdot 2^3}{28} \\sqrt{2} = \\frac{6}{7}\\sqrt{2} = 1.212 \\,, \\end{align*}\\] and \\[\\begin{align*} E[X_{(3)}^2] &amp;= \\int_0^{\\sqrt{2}} x^2 \\left(\\frac{3}{4} x^5\\right)dx =\\frac{3}{4} \\frac{x^8}{8}\\bigg|_0^{\\sqrt{2}} = \\frac{3\\cdot2^4}{32} = \\frac{3}{2} \\,. \\end{align*}\\] Thus \\[\\begin{align*} V[X_{(3)}] = \\frac{3}{2} - \\frac{36\\cdot 2}{49} = \\frac{147 - 144}{98} = \\frac{3}{98} = 0.031 \\,. \\end{align*}\\] Problem 7 We have that \\(f_X(x) = 1\\), \\(F_X(x) = x\\), and \\(j = \\frac{n+1}{2} = 2\\), so \\[\\begin{align*} f_{(2)}(x) = \\frac{3!}{1!1!}x^1(1-x)^1(1) = 6x(1-x) \\end{align*}\\] for \\(x \\in [0,1]\\). Therefore \\[\\begin{align*} P\\left(\\frac{1}{3} \\leq X_{(2)} \\leq \\frac{2}{3}\\right) &amp;= \\int_{1/3}^{2/3} 6x(1-x) dx = 6\\left[ \\frac{x^2}{2}\\bigg|_{1/3}^{2/3} - \\frac{x^3}{3}\\bigg|_{1/3}^{2/3}\\right]\\\\ &amp;= 6\\left[ \\frac{1}{2} \\left( \\frac{4}{9} - \\frac{1}{9}\\right) - \\frac{1}{3} \\left( \\frac{8}{27} - \\frac{1}{27}\\right) \\right]\\\\ &amp;= 6\\left[ \\frac{3}{18} - \\frac{7}{81} \\right] = 6\\left[ \\frac{27}{162} - \\frac{14}{162} \\right] = \\frac{13}{27} = 0.481 \\,. \\end{align*}\\] Problem 8 We have that \\(f_X(x) = e^{-x}\\) for \\(x \\geq 0\\) and thus that \\(F_X(x) = 1-e^{-x}\\) over the same domain. Thus \\[\\begin{align*} f_{(2)}(x) &amp;= \\frac{3!}{1!1!}(1 - e^{-x})^1\\left[1 - (1- e^{-x}) \\right]^1 e^{-x} = 6(1 - e^{-x})e^{-x}e^{-x} \\\\ &amp;= 6(1 - e^{-x})e^{-2x} = 6e^{-2x} - 6e^{-3x} \\,, \\end{align*}\\] and \\[\\begin{align*} E[X_{(2)}] &amp;= \\underbrace{\\int_0^{\\infty} 6xe^{-2x} dx}_{\\text{by } y=2x, \\, dy/2 = dx} - \\underbrace{\\int_0^{\\infty} 6xe^{-3x} dx}_{\\text{by } y=3x, \\, dy/3 = dx} \\\\ &amp;= \\int_0^{\\infty} \\frac{3}{2}y e^{-y} dy - \\int_0^{\\infty} \\frac{2}{3}y e^{-y} dy\\\\ &amp;= \\frac{3}{2} \\Gamma(2) - \\frac{2}{3}\\Gamma(2) = \\frac{3}{2} - \\frac{2}{3} =\\frac{5}{6} \\,. \\end{align*}\\] Problem 9 (a) A probability density function is the derivative of its associated cumulative distribution function, so \\[\\begin{align*} f_X(x) = \\frac{d}{dx} x^3 = 3x^2 \\,. \\end{align*}\\] (b) The maximum order statistic has pdf \\[\\begin{align*} f_{(n)}(x) &amp;= n f_X(x) [F_X(x)]^{n-1} \\\\ &amp;= n (3x^2) [x^3]^{n-1} = 3n x^2 x^{3n-3} = 3n x^{3n-1} \\,. \\end{align*}\\] (c) We have that \\[\\begin{align*} F_{(n)}(x) = [F_X(x)]^n ~~ \\Rightarrow ~~ F_{(n)}(x) = x^{3n} \\,. \\end{align*}\\] We can also show this via integration: \\[\\begin{align*} F_{(n)}(x) = \\int_0^x f_{(n)}(y) dy = \\int_0^x 3n y^{3n-1} dy = \\left. y^{3n}\\right|_0^x = x^{3n} \\,. \\end{align*}\\] (d) The expected value is \\[\\begin{align*} E[X_{(n)}] = \\int_0^1 x f_{(n)}(x) dx = \\int_0^1 3n x^{3n} dx = \\left. \\frac{3n}{3n+1} x^{3n+1} \\right|_0^1 = \\frac{3n}{3n+1} \\,. \\end{align*}\\] Problem 10 (a) The cdf within the domain is \\[\\begin{align*} F_X(x) = \\int_0^x \\frac12 y dy = \\left. \\frac14 y^2 \\right|_0^x = \\frac{x^2}{4} \\,. \\end{align*}\\] (b) We plug \\(n\\), \\(f_X(x)\\), and \\(F_X(x)\\) into the order statistic pdf equation (where \\(j = n = 2\\)): \\[\\begin{align*} f_{(2)}(x) = 2 f_X(x) \\left[ F_X(x) \\right]^{2-1} \\left[ 1 - F_X(x) \\right]^{2-2} = 2 \\left( \\frac12 \\right) x \\left( \\frac14 \\right) x^2 = \\frac14 x^3 \\,. \\end{align*}\\] (c) The expected value is \\[\\begin{align*} E[X_{(2)}] = \\int_0^2 x f_{(2)}(x) dx = \\int_0^2 \\frac14 x^4 dx = \\frac{1}{20} \\left. x^5 \\right|_0^2 = \\frac{32}{20} = \\frac85 = 1.6 \\,. \\end{align*}\\] (d) They cannot be independent: given the value of one, the other has to be either smaller (\\(X_{(1)}\\)) or larger (\\(X_{(2)}\\)). Problem 11 Let \\(X_1, \\ldots, X_n\\) denote the samples from the Bernoulli distribution. The log-likelihood is \\[\\begin{align*} \\ell(X_1, \\ldots, X_n | p) = \\sum_{i = 1}^n [X_i \\log(p) + (1 - X_i) \\log (1-p)] \\,. \\end{align*}\\] The first two derivatives are \\[\\begin{align*} \\frac{d}{dp} \\ell(X_1, \\ldots, X_n | p) &amp;= \\sum_{i = 1}^n\\bigg[ \\frac{X_i}{p} - \\frac{(1 - X_i)}{1-p}\\bigg] \\\\ \\frac{d^2}{dp^2} \\ell(X_1,\\ldots, X_n | p ) &amp;= \\sum_{i = 1}^n \\bigg[-\\frac{X_i}{p^2}- \\frac{(1- X_i)}{(1-p)^2}\\bigg] \\,. \\end{align*}\\] The Fisher information is thus \\[\\begin{align*} I_n(p) = E\\bigg[-\\frac{d^2}{dp^2} \\ell(X_1,\\ldots, X_n | p )\\bigg] &amp;= \\sum_{i=1}^n \\bigg[\\frac{E[X_i]}{p^2} + \\frac{E[(1- X_i)]}{(1-p)^2}\\bigg] \\\\ &amp;= \\sum_{i=1}^n \\bigg[\\frac{p}{p^2} + \\frac{1-p}{(1-p)^2}\\bigg] \\\\ &amp;= \\sum_{i=1}^n \\bigg[\\frac{1}{p} + \\frac{1}{1-p}\\bigg] \\\\ &amp;= \\sum_{i=1}^n \\bigg[\\frac{1-p}{p(1-p)} + \\frac{p}{p(1-p)}\\bigg] \\\\ &amp;= \\frac{n}{p(1-p)} \\,, \\end{align*}\\] and the asymptotic distribution of the MLE is \\(N(p,\\frac{p(1-p)}{n})\\). Problem 12 (a) The log-likelihood and its derivative are \\[\\begin{align*} \\ell(p \\vert \\mathbf{x}) &amp;= \\log (1-p) \\sum_{i=1}^n (x_i - 1) + n \\log p\\\\ \\ell&#39;(p \\vert \\mathbf{x}) &amp;= -\\frac{\\sum_{i=1}^n x_i - n}{1 - p} + \\frac{n}{p} \\,. \\end{align*}\\] Setting the derivative to zero, we find that \\[\\begin{align*} \\frac{\\sum_{i=1}^n x_i - n}{1 - p} &amp;= \\frac{n}{p} \\\\ \\Rightarrow ~~~ \\left(\\sum_{i=1}^n x_i - n\\right) p &amp;= n (1 - p) \\\\ \\Rightarrow ~~~ p \\,\\sum_{i=1}^n x_i &amp;= n \\\\ \\Rightarrow ~~~ \\hat{p} &amp;= \\frac{1}{\\bar{X}} \\,. \\end{align*}\\] Using the invariance property of the MLE, we find that \\(\\widehat{1/p}_{MLE} = \\bar{X}\\). (b) The variance of this estimator is \\[\\begin{align*} V\\left[\\widehat{1/p}_{MLE}\\right] = V \\left[ \\frac{\\sum_{i=1}^n X_i}{n} \\right] = \\frac{V[X]}{n} = \\frac{1 - p}{np^2} \\,. \\end{align*}\\] Problem 13 The likelihood for \\(p\\) is \\[\\begin{align*} \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n p_X(x_i \\vert p) = \\prod_{i=1}^n -\\frac{1}{\\log(1-p)} \\frac{p^{x_i}}{x_i} = \\underbrace{- \\prod_{i=1}^n \\frac{1}{x_i}}_{h(\\mathbf{x})} \\cdot \\underbrace{\\frac{1}{[\\log(1-p)]^n} p^{\\sum_{i=1}^n x_i}}_{g(p,\\mathbf{x})} \\,. \\end{align*}\\] Given the expression for \\(g(\\cdot)\\), we can see that a sufficient statistic for \\(p\\) is \\(Y = \\sum_{i=1}^n X_i\\). Problem 14 (a) The likelihood is \\[\\begin{align*} \\mathcal{L}(a,b \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n a b x_i^{a-1} (1-x_i^a)^{b-1}\\\\ &amp;= a^n b^n \\left(\\prod_{i=1}^n x_i\\right)^{a-1} \\left(\\prod_{i=1}^n (1-x_i^a)\\right)^{b-1} \\end{align*}\\] At first glance, it seems that we can take \\[\\begin{align*} \\mathbf{Y} = \\left\\{ \\prod_{i=1}^n x_i, \\prod_{i=1}^n (1-x_i^a) \\right\\} \\end{align*}\\] as the joint sufficient statistics. However, note that the parameter \\(a\\) occurs in the second statistic. Because this second statistic includes a parameter value, it cannot be a sufficient statistic…and thus we conclude that we cannot identify joint sufficient statistics for \\(a\\) and \\(b\\). (b) With \\(a=1\\) the density function becomes \\(f_X(x) = b \\cdot (1-x)^{b-1}\\), with resulting likelihood \\[\\begin{align*} \\mathcal{L}(b \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n b \\cdot (1-x_i)^{b-1}\\\\ &amp;= b^n \\left(\\prod_{i=1}^n (1-x_i)\\right)^{b-1}\\\\ &amp;= h(\\mathbf{x}) g(b,\\mathbf{x}) \\,. \\end{align*}\\] Hence, a sufficient statistic for \\(b\\) is \\(Y = \\prod_{i=1}^n (1-X_i)\\). Problem 15 (a) The likelihood is \\[\\begin{align*} \\mathcal{L}(\\beta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\beta) &amp;= \\prod_{i=1}^n \\frac{x_i}{\\beta^2} \\exp\\left(-\\frac{x}{\\beta}\\right) \\\\ &amp;= \\underbrace{\\prod_{i=1}^n x_i}_{h(\\mathbf{x})} \\cdot \\underbrace{\\frac{1}{\\beta^{2n}} \\exp\\left(-\\frac{1}{\\beta}\\sum_{i=1}^n x_i\\right)}_{g(\\beta,\\mathbf{x})} \\,. \\end{align*}\\] We can examine \\(g(\\cdot)\\) and immediately identify that a sufficient statistic for \\(\\beta\\) is \\(Y = \\sum_{i=1}^n X_i\\). (b) We have that \\[\\begin{align*} E[Y] = E[\\sum_{i=1}^n X_i] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n 2\\beta = 2n\\beta \\,. \\end{align*}\\] Hence \\[\\begin{align*} E\\left[\\frac{Y}{2n}\\right] = \\beta \\end{align*}\\] and \\(\\hat{\\beta}_{MVUE} = Y/2n = \\bar{X}/2\\). (c) Utilizing the general rule from 235: \\[\\begin{align*} V[\\hat{\\beta}_{MVUE}] = V\\left[\\frac{\\bar{X}}{2}\\right] = \\frac{V[\\bar{X}]}{4} = \\frac{V[X]}{4n} = \\frac{2\\beta^2}{4n} = \\frac{\\beta^2}{2n} \\,. \\end{align*}\\] (d) The first step is to write down the log-likelihood for one datum: \\[\\begin{align*} \\ell(\\beta \\vert x) = \\log f_X(x \\vert \\beta) = \\log x - \\frac{x}{\\beta} - 2\\log\\beta \\,. \\end{align*}\\] We take the first two derivatives: \\[\\begin{align*} \\frac{d\\ell}{d\\beta} &amp;= \\frac{x}{\\beta^2} - \\frac{2}{\\beta} \\\\ \\frac{d^2\\ell}{d\\beta^2} &amp;= -\\frac{2x}{\\beta^3} + \\frac{2}{\\beta^2} \\,, \\end{align*}\\] and then compute the expected value: \\[\\begin{align*} I(\\beta) = E\\left[ \\frac{2X}{\\beta^3} - \\frac{2}{\\beta^2} \\right] = \\frac{2}{\\beta^3}E[X] - \\frac{2}{\\beta^2} = \\frac{2}{\\beta^3}(2\\beta) - \\frac{2}{\\beta^2} = \\frac{2}{\\beta^2} \\,. \\end{align*}\\] Thus \\(I_n(\\beta) = (2n)/\\beta^2\\) and the CRLB is \\(1/I_n(\\beta) = \\beta^2/(2n)\\). The MVUE achieves the CRLB. Problem 16 (a) We can factorize the likelihood as follows: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\theta} e^{x_i} e^{-e^{x_i}/\\theta} = e^{\\sum_{i=1}^n x_i} \\theta^{-n} e^{-(\\sum_{i=1}^n e^{x_i})/\\theta} \\,. \\end{align*}\\] The first term does not contain \\(\\theta\\) and thus can be ignored. Thus we identify \\(Y = \\sum_{i=1}^n e^{X_i}\\) as a sufficient statistic. (b) We can determine \\(E[Y]\\) by noticing that \\(Y \\sim\\) Gamma\\((n,\\theta)\\), as stated in the question…so \\(E[Y] = n\\theta\\) and \\(E[Y/n] = \\theta\\). Thus the MVUE for \\(\\theta\\) is \\((\\sum_{i=1}^n e^{X_i})/n\\). (c) The MVUE will be a function of the sufficient statistic for \\(\\theta\\), so let’s try \\((\\sum_{i=1}^n e^{X_i})^2\\): \\[\\begin{align*} E\\left[\\left(\\sum_{i=1}^n e^{X_i}\\right)^2\\right] = V\\left[\\left(\\sum_{i=1}^n e^{X_i}\\right)\\right] + E\\left[\\sum_{i=1}^n e^{X_i}\\right]^2 = n \\theta^2 + (n\\theta)^2 = n(n+1)\\theta^2 \\,. \\end{align*}\\] Therefore \\((\\sum_{i=1}^n e^{X_i})^2/(n(n+1))\\) is the MVUE for \\(\\theta^2\\). Problem 17 (a) We factorize the likelihood: \\[\\begin{align*} \\mathcal{L}(a \\vert \\mathbf{x}) = \\prod_{i=1}^n \\sqrt{\\frac{2}{\\pi}} \\frac{x_i^2}{a^3} e^{-x_i^2/(2a^2)} = \\left[ \\left(\\frac{2}{\\pi}\\right)^{n/2} \\left( \\prod_{i=1}^n x_i^2 \\right) \\right] \\cdot \\left[ \\frac{1}{a^{3n}} e^{-(\\sum_{i=1}^n x_i^2)/(2a^2)} \\right] = h(\\mathbf{x}) \\cdot g(a,\\mathbf{x}) \\,. \\end{align*}\\] We can read off from the \\(g(\\cdot)\\) function term that \\(Y = \\sum_{i=1}^n X_i^2\\). (Including the minus sign, for instance, is fine because a function of a sufficient statistic is itself sufficent and we will get to the same MVUE in the end.) (b) We utilize the shortcut formula: \\[\\begin{align*} E[X^2] = V[X] + (E[X])^2 = a^2 \\frac{(3 \\pi - 8)}{\\pi} + (2a)^2 \\frac{2}{\\pi} = 3 a^2 + \\frac{8 a^2}{\\pi} - \\frac{8 a^2}{\\pi} = 3 a^2 \\,. \\end{align*}\\] (c) We compute the expected value for \\(Y\\): \\[\\begin{align*} E[Y] = E\\left[\\sum_{i=1}^n X_i^2\\right] = \\sum_{i=1}^n E[X_i^2] = n E[X^2] = 3 n a^2 \\,. \\end{align*}\\] Thus the expected value for \\(Y/(3n)\\) is \\(a^2\\): \\[\\begin{align*} \\widehat{a^2}_{MVUE} = \\frac{1}{3n} \\sum_{i=1}^n X_i^2 \\,. \\end{align*}\\] (d) There is no invariance principle for the MVUE. Maybe the desired result holds and maybe it doesn’t, but we cannot simply state that it does. Problem 18 We are constructing an upper-tail test where the test statistic is trivially \\(Y = X\\). (So the NP Lemma does not really come into play here, given the lack of choices for the test statistic.) The expected value of \\(Y\\) is \\[\\begin{align*} E[Y] = \\int_0^2 y f_Y(y) dy = \\int_0^2 \\frac{\\theta}{2^\\theta} y^{\\theta} dy = \\left. \\frac{\\theta}{2^\\theta} \\frac{y^{\\theta+1}}{\\theta+1} \\right|_0^2 = \\frac{2\\theta}{\\theta+1} \\,. \\end{align*}\\] \\(E[Y]\\) increases as \\(\\theta\\) increases, so we will be on the “yes” line of the hypothesis test reference table. Hence the rejection region will be of the form \\[\\begin{align*} y_{\\rm obs} &gt; F_Y^{-1}(1-\\alpha \\vert \\theta_o) \\,. \\end{align*}\\] The cdf \\(F_Y(y)\\) is \\[\\begin{align*} F_Y(y) = \\int_0^y \\frac{\\theta}{2^\\theta} u^{\\theta-1} du = \\left. \\frac{u^\\theta}{2^\\theta}\\right|_0^y = \\left(\\frac{y}{2}\\right)^\\theta \\,, \\end{align*}\\] and the inverse cdf \\(F_Y^{-1}(q)\\) is \\(y = 2q^{1/\\theta}\\). Hence the test we seek rejects the null hypothesis if \\[\\begin{align*} y_{\\rm obs} &gt; 2(1-\\alpha)^{1/\\theta_o} \\,. \\end{align*}\\] The rejection-region boundary does not depend on \\(\\theta_a\\), so we know that the test is the most powerful one for all alternative values \\(\\theta_a &gt; \\theta_o\\)…thus it is a uniformly most powerful test. Problem 19 (a) The likelihood is \\[\\begin{align*} \\mathcal{L}(\\beta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\theta}{\\beta}x_i^{\\theta-1}\\exp\\left(-\\frac{x_i^\\theta}{\\beta}\\right) = \\left[ \\theta x_i^{\\theta-1} \\right] \\cdot \\left[ \\frac{1}{\\beta} \\exp\\left(-\\frac{1}{\\beta} x_i^\\theta \\right) \\right] = h(\\mathbf{x}) \\cdot g(\\beta,\\mathbf{x}) \\,, \\end{align*}\\] thus a sufficient statistic for \\(\\beta\\) is \\(Y = \\sum_{i=1}^n X_i^\\theta\\). (b) We are given that \\(X^\\theta \\sim\\) Exp(\\(\\beta\\)). The mgf for an exponential distribution is \\[\\begin{align*} m_X(t) = (1 - \\theta t)^{-1} \\,, \\end{align*}\\] and hence the mgf for \\(Y = \\sum_{i=1}^n X_i\\) will be \\[\\begin{align*} m_Y(t) = \\prod_{i=1}^n (1 - \\theta t)^{-1} = \\left[ (1 - \\theta t)^{-1} \\right]^n = (1 - \\theta t)^{-n} \\,. \\end{align*}\\] Following the hint given in the question, we find that \\(Y\\) is a gamma-distributed random variable with “shape” parameter \\(n\\) and “scale” parameter \\(\\theta\\). (There are two common parameterizations of the gamma distribution\\(-\\)shape/scale and shape/rate\\(-\\)and it is imperative to determine the correct one! This will impact the answer to part (c).) (c) The statistic \\(Y\\) has expected value \\(E[Y] = n\\theta\\), which increases with \\(\\theta\\). Hence we utilize the upper-tail/yes line of the hypothesis test reference table: \\(y_{\\rm RR} = F_Y^{-1}(1 - \\alpha \\vert \\theta_o)\\), or, in code, y.rr &lt;- qgamma(1-alpha,shape=n,scale=theta) Problem 20 (a) The moment-generating function for the random variable \\(X\\) is \\[\\begin{align*} m_X(t) = E\\left[e^{tX}\\right] &amp;= \\int_b^\\infty e^{tx} \\frac{1}{\\theta} e^{-(x-b)/\\theta} dx \\\\ &amp;= e^{b/\\theta} \\frac{1}{\\theta} \\int_b^\\infty e^{-x(1/\\theta - t)} dx \\\\ &amp;= e^{b/\\theta} \\frac{1}{\\theta} \\frac{e^{-b(1/\\theta - t)}}{(1/\\theta-t)} \\\\ &amp;= e^{bt} (1-t\\theta)^{-1} \\,. \\end{align*}\\] (Here we make the implicit assumption that \\(t &lt; 1/\\theta\\), so that the integral evaluated at \\(\\infty\\) is zero.) This is the final answer, but recall that when \\(X = U+b\\), \\(m_X(t) = e^{bt} m_U(t)\\). Since we recognize that \\((1-t\\theta)^{-1}\\) is the mgf for an exponential distribution, we can state that \\(U = X-b\\) is an exponentially distributed random variable. (b) The mgf for \\(Y = \\sum_{i=1}^n X_i\\) is \\[\\begin{align*} m_Y(t) = \\prod_{i=1}^n e^{bt} (1-t\\theta)^{-1} = \\left[ e^{bt} (1-t\\theta)^{-1} \\right]^n = e^{nbt} (1-t\\theta)^{-n} \\,. \\end{align*}\\] (c) Going back to our answer for (a) (and our answer for the previous problem), we recognize that the mgf for \\(\\sum_{i=1}^n U_i\\) is \\((1-t\\theta)^{-n}\\), which is the mgf for a gamma distribution with shape parameter \\(n\\) and scale parameter \\(\\theta\\). Hence \\(Y&#39; = Y - nb \\sim \\text{Gamma}(n,\\theta)\\). (d) We are on the lower-tail/yes line of the hypothesis test reference table: \\(y_{\\rm RR}&#39; = F_Y^{-1}(\\alpha \\vert \\theta_o)\\), or, in code, y.rr.prime &lt;- qgamma(alpha,shape=n,scale=theta) We would reject the null hypothesis if \\(y_{\\rm obs} - nb &lt; y_{\\rm RR}&#39;\\). Because this test is constructed using a sufficient statistic and because no value of the alternative hypothesis appears in the definition of the rejection region, we indeed have defined a uniformly most powerful test of \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta &lt; \\theta_o\\). Problem 21 (a) Let’s first find a sufficient statistic: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\theta e^{-\\theta x_i} = \\theta^n e^{-\\theta \\sum_{i=1}^n x_i} \\,. \\end{align*}\\] A sufficient statistic is \\(Y = \\sum_{i=1}^n X_i\\). We are conducting a lower-tail test, and since \\(E[X] = 1/\\theta\\) decreases as \\(\\theta\\) increases, we are on the “no” line of the reference table. We reject the null if \\(y_{\\rm obs} = \\sum_{i=1}^n x_i &gt; y_{\\rm RR}\\). (b) \\(\\theta_o\\) is plugged in to compute the rejection-region boundary, but \\(\\theta_a\\) does not appear at all. Hence the defined test is uniformly most powerful, since it is most powerful for any value of \\(\\theta_a &lt; \\theta_o\\). Problem 22 (a) The sampling distribution is Binom(\\(nk,p\\)). We can determine this using the method of moment-generating functions, if necessary. (b) \\(E[Y] = nkp\\) increases with \\(p\\), so we are on the upper-tail/“yes” line of the hypothesis test reference tables. The rejection-region boundary is given by \\(F_Y^{-1}(1-\\alpha \\vert \\theta_o)\\), or, in code, with \\(p_o\\) in place of \\(\\theta_o\\), qbinom(1-alpha,n*k,p.o) (c) For an upper-tail/yes test, the \\(p\\)-value is \\(1 - F_Y(y_{\\rm obs} \\vert \\theta_o)\\). In code, with \\(p_o\\) in place of \\(\\theta_o\\), the \\(p\\)-value is 1 - pbinom(y.obs,n*k,p.o) However, we have to apply a discreteness correction, because otherwise we will not be summing over the correct range of \\(y\\) values, i.e., our \\(p\\)-value will be wrong. Here, that factor is \\(-1\\), applied to the input. So… 1 - pbinom(y.obs-1,n*k,p.o) is the final answer. Problem 23 This is straightforward if we remember to set the link function to the equation for the line: \\[\\begin{align*} -(Y \\vert x)^{-1} = \\beta_0 + \\beta_1 x ~~~ \\Rightarrow ~~~ (Y \\vert x)^{-1} = -\\beta_0 - \\beta_1 x ~~~ \\Rightarrow ~~~ Y \\vert x = (-\\beta_0 - \\beta_1 x)^{-1} \\,. \\end{align*}\\] Problem 24 (a) The degrees of freedom for the residual deviance is \\(n-p\\), where \\(p\\) is the number of parameters (here, two: \\(\\beta_0\\) and \\(\\beta_1\\)). Hence \\(n = 32\\). (b) \\(\\beta_1\\) is set to zero to compute the null deviance. So \\(-2\\log\\mathcal{L}_{\\rm max} = 43.230\\). (c) The odds are \\(O(x) = \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 x)\\) or just \\(\\exp(\\hat{\\beta}_0)\\) for \\(x = 0\\), meaning thet \\(O(x=0) = \\exp(12.040)\\). (d) The estimated slope \\(\\hat{\\beta}_1\\) is negative, and we know that \\(O(x+1) = O(x) \\exp(\\hat{\\beta}_1)\\), so we know that \\(O(x+1) &lt; O(x)\\)…the odds decrease as \\(x\\) increases. Problem 25 (a) We have that \\[\\begin{align*} O(x) = \\frac{p \\vert x}{1 - p \\vert x} = \\frac{0.1}{1-0.1} = \\frac19 = 0.111 \\,. \\end{align*}\\] (b) The new odds are \\[\\begin{align*} O(589+100) = \\exp(\\hat{\\beta}_0 + \\hat{\\beta_1}(589+100)) = O(589) \\exp(100\\hat{\\beta}_1) = \\frac19 \\exp(0.14684) = 0.129 \\,. \\end{align*}\\] (c) We have that \\(Y_1 = 0\\) and \\(\\hat{Y}_i = 0.07\\), so \\[\\begin{align*} d_1 &amp;= \\mbox{sign}(Y_1-\\hat{Y}_1)\\sqrt{-2[Y_1\\log\\hat{Y}_1+(1-Y_1)\\log(1-\\hat{Y}_1)]} = \\mbox{sign}(-0.07) \\sqrt{-2\\log(0.93)} \\\\ &amp;= -\\sqrt{-2\\log(0.93)} = 0.381 \\,. \\end{align*}\\] (d) The null deviance is computed assuming \\(\\beta_1 = 0\\). This model lies “farther” from the observed data than the model with \\(\\hat{\\beta}_1 = 0.00147\\), meaning it deviates more from the data, meaning that the deviance would be higher. Problem 26 Let’s start by collecting the basic pieces of information that we would combine in a Naive Bayes regression model: \\[\\begin{align*} p(0) = 3/5 ~~\\mbox{and}~~ p(1) = 2/5 \\,, \\end{align*}\\] where \\(0\\) and \\(1\\) are the two response (i.e., \\(Y\\)) values. Next up, the conditionals: \\[\\begin{align*} p(x1 = N \\vert 0) = 2/3 ~~ &amp;\\mbox{and}&amp; ~~ p(x1 = Y \\vert 0) = 1/3 \\\\ p(N \\vert 1) = 1/2 ~~ &amp;\\mbox{and}&amp; ~~ P(Y \\vert 1) = 1/2 \\\\ \\\\ p(x2 = T \\vert 0) = 2/3 ~~ &amp;\\mbox{and}&amp; ~~ p(x2 = F \\vert 0) = 1/3 \\\\ p(T \\vert 1) = 1/2 ~~ &amp;\\mbox{and}&amp; ~~ P(F \\vert 1) = 1/2 \\,. \\end{align*}\\] The estimated probability of observing a datum of Class 0 given Y and F is thus \\[\\begin{align*} p(0 \\vert Y,F) &amp;= \\frac{p(Y \\vert 0) p(F \\vert 0) p(0)}{p(Y \\vert 0) p(F \\vert 0) p(0) + p(Y \\vert 1) p(F \\vert 1) p(1)} \\\\ &amp;= \\frac{1/3 \\cdot 1/3 \\cdot 3/5}{1/3 \\cdot 1/3 \\cdot 3/5 + 1/2 \\cdot 1/2 \\cdot 2/5} \\\\ &amp;= \\frac{1/15}{1/15 + 1/10} = \\frac{2/30}{2/30+3/30} = \\frac{2}{5} \\,. \\end{align*}\\] Problem 27 (a) The pdf is of the form \\[\\begin{align*} k x^{\\alpha-1} (1-x)^{\\beta-1} \\,, \\end{align*}\\] with \\(0 \\leq x \\leq 1\\), so what we have is a beta distribution: \\(X \\sim\\) Beta\\((1,2)\\). (b) We have that \\[\\begin{align*} E[X] = \\frac{\\alpha}{\\alpha+\\beta} = \\frac{1}{3} \\end{align*}\\] and \\[\\begin{align*} E[X^2] = V[X] + (E[X])^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} + \\left(\\frac{1}{3}\\right)^2 = \\frac{2}{36} + \\frac{4}{36} = \\frac{1}{6} \\,. \\end{align*}\\] (c) These expressions are straightforward to evaluate: \\[\\begin{align*} E[C] = E[10X] = 10E[X] = \\frac{10}{3} = 3.333 \\end{align*}\\] and \\[\\begin{align*} V[C] = E[C^2] - (E[C])^2 = E[100X^2] - \\frac{100}{9} = 100E[X^2] - \\frac{100}{9} = \\frac{150}{9} - \\frac{100}{9} = \\frac{50}{9} = 5.556 \\,. \\end{align*}\\] Problem 28 (a) \\(f_X(x) = 12x - 24x^2 + 12x^3 = 12x(1-x)^2\\) for \\(x \\in [0,1]\\)…so this is a Beta(2,3) distribution. (b) \\(X \\sim {\\rm Beta}(2,3) \\Rightarrow E[X] = \\alpha/(\\alpha+\\beta) = 2/(2+3) = 2/5 = 0.4\\) ,. Problem 29 One way to solve this problem is to utilize the shortcut formula: \\(E[X^2] = V[X] + E[X]^2\\). With this in hand: \\[\\begin{align*} V[X] = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} = \\frac{6}{25 \\cdot 6} = \\frac{1}{25} \\end{align*}\\] and \\[\\begin{align*} E[X] = \\left(\\frac{\\alpha}{\\alpha + \\beta}\\right)^2 = \\left(\\frac{2}{5}\\right)^2 = \\frac{4}{25} \\,. \\end{align*}\\] Therefore, \\[\\begin{align*} E[X^2] = \\frac{1}{25} + \\frac{4}{25} = \\frac{1}{5} \\,. \\end{align*}\\] A second way to solve this problem is by brute-force integration: \\[\\begin{align*} E[X^2] &amp;= \\int_0^1 x^2 \\frac{x(1-x)^2}{B(2,3)}dx = \\int_0^1 \\frac{x^3(1-x)^2}{B(2,3)}dx = \\int_0^1 \\frac{x^3(1-x)^2}{B(2,3)} \\frac{B(4,3)}{B(4,3)}dx \\\\ &amp;= \\frac{B(4,3)}{B(2,3)}\\underbrace{\\int_0^1 \\frac{x^3(1-x)^2}{B(4,3)}dx}_{=1} \\\\ &amp;= \\frac{B(4,3)}{B(2,3)} = \\frac{\\Gamma(4) \\Gamma(3)}{\\Gamma(7)}\\frac{\\Gamma(5)}{\\Gamma(2)\\Gamma(3)}\\ = \\frac{\\Gamma(4) \\Gamma(5)}{\\Gamma(2)\\Gamma(7)} = \\frac{3! 4!}{1!6!} = \\frac{1}{5} \\,. \\end{align*}\\] Problem 30 (a) The median is the second sampled datum. The pdf for \\(X\\) is \\(f_X(x) = 3x^2\\) and \\(F_X(x) = x^3\\), both for \\(x \\in [0,1]\\). Thus \\[\\begin{align*} f_{(2)}(x) = \\frac{3!}{1!1!} [x^3]^1 [1 - x^3]^1 3x^2 = 18 x^5 (1 - x^3) \\,, \\end{align*}\\] for \\(x \\in [0,1]\\), and \\[\\begin{align*} E[X_{(2)}] &amp;= \\int_0^1 x 18 x^5 (1-x^3) dx = 18 \\int_0^1 (x^6 - x^9) dx \\\\ &amp;= 18 \\left( \\left.\\frac{x^7}{7}\\right|_0^1 - \\left.\\frac{x^{10}}{10}\\right|_0^1 \\right) = 18 \\left( \\frac{1}{7}-\\frac{1}{10} \\right) = \\frac{18 \\cdot 3}{70} = \\frac{27}{35} \\,. \\end{align*}\\] Problem 31 (a) This is a Beta(2,2) distribution. (b) We can determine \\(c\\) via brute-force integration: \\[\\begin{align*} c \\int_0^1 x(1-x) dx = c\\left[ \\int_0^1 x dx - \\int_0^1 x^2 dx \\right] &amp;= c \\left[ \\left.\\frac{x^2}{2}\\right|_0^1 - \\left.\\frac{x^3}{3}\\right|_0^1 \\right] \\\\ &amp;= c \\left[ \\frac{1}{2} - \\frac{1}{3} \\right] \\\\ &amp;= c \\frac{1}{6} = 1 ~~\\Rightarrow~~ c = 6 \\,. \\end{align*}\\] Alternatively, we can recognize that \\[\\begin{align*} c &amp;= \\frac{1}{B(2,2)} = \\frac{\\Gamma(4)}{\\Gamma(2) \\Gamma(2)} = \\frac{3!}{1! 1!} = 6 \\,. \\end{align*}\\] (c) Since \\(\\alpha = \\beta\\), the distribution is symmetric around \\(x = 1/2\\), which is its mean value. (d) We have that \\[\\begin{align*} P(X \\leq 1/4 \\vert X \\leq 1/2) &amp;= \\frac{P(X \\leq 1/4 \\cap X \\leq 1/2)}{P(X \\leq 1/2)} = \\frac{P(X \\leq 1/4)}{P(X \\leq 1/2)} = \\frac{P(X \\leq 1/4)}{1/2} \\\\ &amp;= 2P(X \\leq 1/4) = 2 \\int_0^{1/4} 6 x (1-x) dx = 12 \\int_0^{1/4} x (1-x) dx \\\\ &amp;= 12 \\left[ \\left.\\frac{x^2}{2}\\right|_0^{1/4} - \\left.\\frac{x^3}{3}\\right|_0^{1/4} \\right] = 12 \\left[ \\frac{1}{32} - \\frac{1}{192} \\right] = 12 \\frac{5}{192} = \\frac{60}{192} = \\frac{30}{96} = \\frac{5}{16} \\,. \\end{align*}\\] Problem 32 (a) We carry out a chi-square goodness-of-fit test: \\[\\begin{align*} W = \\sum_{i=1}^n \\frac{(X_i - kp_i)^2}{kp_i} = \\frac{1}{7}[(10-7)^2+(5-7)^2+(6-7)^2] = 2 \\,. \\end{align*}\\] (b) There are \\(m=3\\) outcomes, but we lose one degree of freedom because of the constraint that \\(\\sum_{i=1}^m X_i = 21\\), so the number of degrees of freedom is 2. (c) We are given that \\(\\alpha = 0.1\\), so we want \\(F_W^{-1}(0.9) = 4.61\\). Problem 33 (a) The correct answer is homogeneity, since the researcher is splitting the respondents into groups by giving them the different types of leaflets to read, and the goal is to determine the willingness to spend government funding is homogenous across the type of pamphlets. (b) Since chi-square statistics involving summing squared differences over all combinations of leaflets and spending opinions, there are \\(12 = 3 \\cdot 4\\) terms. (c) Since the all the information but one for each factor is sufficent, the test statistics will follow a chi-square distribution with \\(6 = (3-1) \\cdot (4-1)\\) degrees of freedom. (d) The correct answer is (i) since \\(p\\)-value is defined as a probability of events at least as extreme as the what actually observed under the null hypothesis, and larger values of the test statistic here correspond to larger differences between what we would expect under the null and what we observe. Problem 34 (a) The exponential distribution with mean 1 is \\(e^{-x}\\), for \\(x \\geq 0\\). Therefore, the probabilities for arriving between 1 and 2 minutes after the previous person is given by \\[\\begin{align*} \\int_1^2 e^{-x} dx = -\\left.e^{-x}\\right|_1^2 = 0.233 \\,. \\end{align*}\\] (b) The probabilities for the other two “bins” are \\[\\begin{align*} \\int_0^1 e^{-x} dx &amp;= -\\left.e^{-x}\\right|_0^1 = 0.632 \\\\ \\int_2^\\infty e^{-x} dx &amp;= 1 - 0.632 - 0.233 = 0.135 \\,. \\end{align*}\\] We can now carry out a chi-square goodness-of-fit test: \\[\\begin{align*} W = \\frac{(52-63.2)^2}{63.2} + \\frac{(23-23.3)^2}{23.3} + \\frac{(25-13.5)^2}{13.5} = 1.985 + 0.004 + 9.796 = 11.785 \\,, \\end{align*}\\] and \\(W \\sim \\chi_2^2\\). The rejection region is \\(W &gt; w_{\\rm RR} = 5.991\\) (i.e., qchisq(0.95,2)), and the \\(p\\)-value is 0.0028 (i.e., 1-pchisq(11.785,2)). We have sufficient evidence to reject the null hypothesis and conclude that the time elapsed between people walking through a particular door is not exponentially distributed with mean 1. (c) If \\(n = 10\\) and there are 3 bins, then there is no way that \\(np_i \\geq 5\\) for all bins. Thus the chi-square goodness-of-fit test should not be applied. Problem 35 (a) We have that \\(p_{\\rm out} = 8/9\\) and \\(p_{\\rm in} = 1/9\\), so \\(kp_{\\rm out} = 180 (8/9) = 160\\) and \\(kp_{\\rm in} = 180 (1/9) = 20\\). (b) We perform a chi-square goodness-of-fit test: \\[\\begin{align*} W = \\frac{(150-160)^2}{160} + \\frac{(30-20)^2}{20} = \\frac{100}{160} + \\frac{100}{20} = \\frac58 + 5 = 5.625~(\\mbox{or}~5~5/8) \\,. \\end{align*}\\] (c) \\(W\\) is sampled from a chi-square distribution for \\(2-1 = 1\\) degree of freedom. (d) The rejection region for a chi-square GoF test is \\(W &gt; w_{\\rm RR}\\), so, since 5.625 is greater than 3.841, we would reject the null hypothesis. Chapter 4 Problem 1 (a) This is a Poisson problem: \\(X \\sim\\) Poisson(\\(\\lambda = 2 \\cdot 1/4 = 1/2\\)). So \\[\\begin{align*} \\mu = E[X] = \\lambda = 1/2 ~~\\text{and}~~ \\sigma = \\sqrt{V[X]} = \\sqrt{\\lambda} = \\sqrt{1/2} \\approx 0.707 \\,. \\end{align*}\\] Thus \\[\\begin{align*} P(1/2 \\leq X \\leq 1/2+1.414) = p(1) = \\frac{\\lambda^1}{1!} e^{-\\lambda} = \\frac12 e^{-1/2} = 0.303 \\,. \\end{align*}\\] (b) We have that \\(X \\sim\\) Exp(\\(\\beta = 1/2\\)) (since there is a half-hour on average between calls). By the memorylessness property, \\(P(X &gt; 1/2 \\vert X &gt; 1/4) = P(X &gt; 1/4)\\). Thus \\[\\begin{align*} P(X &gt; 1/4) = \\int_{1/4}^\\infty \\frac{1}{\\beta} e^{-x/\\beta} dx = \\int_{1/4}^\\infty 2 e^{-2x} dx = \\left.-e^{-2x}\\right|_{1/4}^\\infty = e^{-1/2} = 0.607 \\,. \\end{align*}\\] (c) The overall time \\(T\\) is \\(10X + (10-X)\\), where \\(X\\), the number of calls from the friend, is sampled from a binomial distribution with \\(k = 10\\) and \\(p = 0.2\\). Thus the average total number of minutes is \\[\\begin{align*} E[T] = E[10X + (10-X)] = E[9X+10] = 9E[X] + 10 = 9kp + 10 = 28 \\,. \\end{align*}\\] Problem 2 (a) The number of (successful) shots can be infinite; only on average is the number of shots in eight minutes going to be four. So we are working with a Poisson distribution whose parameter \\(\\lambda\\) (the expected number of successful shots) is \\[\\begin{align*} \\lambda = (1 \\quad \\frac{\\text{shot}}{2\\text{min}})(\\frac{1}{2} \\frac{\\text{success}}{\\text{shot}})(4 \\quad 2\\text{min}) = 2 \\,. \\end{align*}\\] Let \\(X\\) be the number of successful shots. Then \\[\\begin{align*} P(X \\leq 1) = \\frac{\\lambda^0}{0!}e^{-\\lambda} + \\frac{\\lambda^1}{1!}e^{-\\lambda} = e^{-2}(1+2) = 3e^{-2} \\,. \\end{align*}\\] (b) We know that \\(E[X] = \\lambda = 2\\), \\(V[X] = \\lambda = 2\\), and \\(\\sigma = \\sqrt{\\lambda} = \\sqrt{2}\\). Thus \\[\\begin{align*} P(2 - \\sqrt{2} &lt; X &lt; 2 + \\sqrt{2}) &amp;= p_X(1) + p_X(2) + p_X(3) = e^{\\lambda}\\left(\\lambda + \\frac{\\lambda^2}{2} + \\frac{\\lambda^3}{6} \\right) \\\\ &amp;= e^{2}\\left(2 + 2+ \\frac{8}{6} \\right) = \\frac{16}{3} e^{-2} = 0.722 \\,. \\end{align*}\\] Problem 3 The particular event in question happens 8 times per year in the three states, and thus the expected number of events in a two-year window is 16. The appropriate distribution in this case is the Poisson distribution. If the total number of observed events is denoted \\(X\\), then \\(X \\sim\\) Poisson(\\(\\lambda\\) = 16), and \\(E[X] = V[X] = 16\\). Problem 4 (a) We utilize the shortcut formula: \\[\\begin{align*} E[X^2] = V[X] + (E[X])^2 = 2\\sigma^2 - \\frac{\\pi}{2}\\sigma^2 + \\frac{\\pi}{2}\\sigma^2 = 2\\sigma^2 \\,. \\end{align*}\\] (b) The first population moment is \\(\\mu_1&#39; = E[X] = \\sigma\\sqrt{\\pi/2}\\) and the first sample moment is \\(m_1&#39; = (1/n)\\sum_{i=1}^n X_i = \\bar{X}\\). We set these equal and determine that \\[\\begin{align*} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{2}{\\pi}} \\bar{X} \\,. \\end{align*}\\] (c) The bias is \\(E[\\hat{\\theta}-\\theta] = E[\\hat{\\theta}] - \\theta\\), or \\[\\begin{align*} B[\\hat{\\theta}_{MoM}] &amp;= E\\left[\\sqrt{\\frac{2}{\\pi}} \\bar{X}\\right] - \\sigma = \\sqrt{\\frac{2}{\\pi}} E\\left[\\bar{X}\\right] - \\sigma = \\sqrt{\\frac{2}{\\pi}} E\\left[X\\right] - \\sigma = \\sqrt{\\frac{2}{\\pi}} \\sqrt{\\frac{\\pi}{2}} \\sigma - \\sigma = 0 \\,. \\end{align*}\\] (d) The variance is \\[\\begin{align*} V[\\hat{\\theta}_{MoM}] &amp;= V\\left[\\sqrt{\\frac{2}{\\pi}} \\bar{X}\\right] = \\frac{2}{\\pi} V\\left[\\bar{X}\\right] = \\frac{2}{\\pi} \\frac{V\\left[X\\right]}{n} = \\frac{2}{n\\pi} \\frac{(4-\\pi)\\sigma^2}{2} = \\frac{(4-\\pi)\\sigma^2}{n\\pi} \\,. \\end{align*}\\] (e) The second population moment is \\(\\mu_2&#39; = E[X^2] = 2\\sigma^2\\) (from part a) and the second sample moment is \\(m_2&#39; = (1/n)\\sum_{i=1}^n X_i^2 = \\overline{X^2}\\). We set these equal and determine that \\[\\begin{align*} \\widehat{\\sigma^2}_{MoM} = \\frac{\\overline{X^2}}{2} \\,. \\end{align*}\\] Problem 5 (a) We have that \\(\\mu_1&#39; = \\frac{1}{p}\\) and \\(m_1&#39; = X\\). It follows from moment equation \\(\\mu_1&#39; = m_1&#39;\\) that \\(\\frac{1}{p} = X\\), so \\(\\hat{p}_{MoM} = \\frac{1}{X}\\). (b) We have that \\(\\mu_2&#39; = \\frac{1 - p}{p^2} + \\frac{1}{p^2} = \\frac{2 - p}{p^2}\\) and \\(m_2&#39; = X^2\\). It follow from the second moment equation \\(\\mu_2&#39; = m_2&#39;\\) that \\[\\begin{align*} \\frac{2-p}{p^2} &amp; = X^2 \\\\ \\Rightarrow ~~~ 2-p &amp; = p^2 X^2 \\\\ \\Rightarrow ~~~ p^2 X^2 +p -2 &amp;= 0 \\\\ \\Rightarrow ~~~ \\hat{p}_{MoM} &amp; = \\frac{-1 + \\sqrt{1 + 8 X^2}}{2X^2} \\,. \\end{align*}\\] Problem 6 (a) The expected value for a beta distribution is \\(E[X] = \\alpha/(\\alpha+\\beta)\\), so, using the first sample moment, we get that \\[\\begin{align*} E[X] &amp;= \\bar{X} \\\\ \\Rightarrow ~~~ \\frac{\\alpha}{\\alpha+\\beta} &amp;= \\bar{X} \\\\ \\Rightarrow ~~~ \\frac{\\alpha+\\beta}{\\alpha} = 1 + \\frac{\\beta}{\\alpha} &amp;= \\frac{1}{\\bar{X}} \\\\ \\Rightarrow ~~~ \\frac{\\beta}{\\alpha} &amp;= \\frac{1}{\\bar{X}}-1 \\\\ \\Rightarrow ~~~ \\hat{\\beta}_{MoM} &amp;= \\alpha\\left(\\frac{1}{\\bar{X}}-1\\right) \\,. \\end{align*}\\] (b) There is no invariance property for the method-of-moments estimator, so the answer is no. Problem 7 (a) The argument list is fine. (b) We need to change mean(X) to sum(X) and lambda to n*lambda. (c) The reference table tells us that a one-sided lower bound where \\(E[U]\\) increases with \\(\\lambda\\) will have a value of \\(q\\) equal to \\(1-\\alpha\\). So we plug in 0.95. (d) Confidence intervals derived from discrete sampling distributions will have coverages \\(\\geq 1-\\alpha\\), so, here, we would say “greater than or equal to 95%.” Problem 8 (a) The likelihood ratio test statistic is \\[\\begin{align*} \\lambda_{LR} = \\frac{\\mbox{sup}_{\\theta \\in \\Theta_o} \\mathcal{L}(\\theta \\vert \\mathbf{x})}{\\mbox{sup}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta \\vert \\mathbf{x})} \\end{align*}\\] Here, that becomes \\[\\begin{align*} \\lambda_{LR} &amp;= \\frac{\\mathcal{L}(p_o \\vert \\mathbf{x})}{\\mathcal{L}(\\hat{p}_{MLE} \\vert \\mathbf{x})} = \\frac{p_o^{\\sum_{i=1}^n X_i}(1-p_o)^{n-\\sum_{i=1}^n X_i}}{\\bar{X}^{\\sum_{i=1}^n X_i}(1-\\bar{X})^{n-\\sum_{i=1}^n X_i}} = \\frac{p_o^U(1-p_o)^{n-U}}{\\bar{X}^U(1-\\bar{X})^{n-U}} \\,. \\end{align*}\\] (b) The test is a two-sided test, so we cannot proclaim it to be uniformly most powerful. However, it very well may be…we just cannot say with the information we have at hand. So: “maybe.” Problem 9 (a) The maximum-likelihood estimate is \\[\\begin{align*} \\ell(\\lambda \\vert x) &amp;= x \\log \\lambda - \\log x! - \\lambda \\\\ \\Rightarrow ~~~ \\ell&#39;(\\lambda \\vert x) &amp;= \\frac{x}{\\lambda} - 1 = 0\\\\ \\Rightarrow ~~~ \\hat{\\lambda}_{MLE} &amp;= X \\,. \\end{align*}\\] which here takes on the value \\(x_{\\rm obs}\\). (b) The likelihood-ratio test statistic is \\[\\begin{align*} \\frac{\\mbox{sup}_{\\theta \\in \\Theta_o}\\mathcal{L}(\\theta \\vert \\mathbf{x})}{\\mbox{sup}_{\\theta \\in \\Theta}\\mathcal{L}(\\theta \\vert \\mathbf{x})} \\,. \\end{align*}\\] Here, that means that for the numerator, we insert the Poisson pmf (remember: one datum) with \\(\\lambda_o\\) plugged in, i.e., \\[\\begin{align*} \\frac{\\lambda_o^{x_{\\rm obs}}}{x_{\\rm obs}!} e^{-\\lambda_o} \\,, \\end{align*}\\] while for the denominator, we plug in the MLE for \\(\\lambda\\), i.e., \\[\\begin{align*} \\frac{x_{\\rm obs}^{x_{\\rm obs}}}{x_{\\rm obs}!} e^{-x_{\\rm obs}} \\,. \\end{align*}\\] So the ratio is \\[\\begin{align*} \\left( \\frac{\\lambda_o}{x_{\\rm obs}} \\right)^{x_{\\rm obs}} e^{-(\\lambda_o-x_{\\rm obs})} \\,. \\end{align*}\\] (c) The expression is \\[\\begin{align*} W = -2 \\log \\lambda_{LR} = -2 x_{\\rm obs} \\log \\left( \\frac{\\lambda_o}{x_{\\rm obs}} \\right) + 2 (\\lambda_o-x_{\\rm obs}) \\,. \\end{align*}\\] (d) \\(W\\) is sampled from a chi-square distribution for 1 degree of freedom. Problem 10 (a) The factorized likelihood is \\[\\begin{align*} \\mathcal{L}(\\lambda \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!}e^{-\\lambda} = \\left(\\frac{1}{\\prod_{i=1}^n x_i!}\\right) \\cdot \\lambda^{\\sum_{i=1}^n x_i} e^{-n\\lambda} = h(\\mathbf{x}) \\cdot g(\\lambda,\\mathbf{x}) \\,. \\end{align*}\\] The sufficient statistic is \\(Y = \\sum_{i=1}^n X_i\\). (b) The sum of \\(n\\) iid Poisson random variables is a Poisson random variable with parameter \\(n\\lambda\\). The moment-generating function for a Poisson random variable is \\(m_X(t) = \\exp(\\lambda(e^t-1))\\), so the mgf for \\(Y\\) is \\[\\begin{align*} m_Y(t) = \\prod_{i=1}^n m_{X_i}(t) = \\left[ m_{X_i}(t) \\right]^n = \\exp(n\\lambda(e^t-1)) \\,. \\end{align*}\\] This is the mgf for a Poisson(\\(n\\lambda\\)) distribution. (c) Recall that in an LRT context, \\(\\Theta = \\Theta_o \\cup \\Theta_a\\); in other words, the null must contain all possible values of \\(\\theta\\) that are not in the alternative. Hence: \\(H_o : \\theta \\geq \\theta_o\\). This inequality does not actually change how the test is constructed, but does change how we interpret it: the true \\(\\alpha\\) for this test will be less than or equal to the stated \\(\\alpha\\). (d) We are performing a lower-tail test, and we are on the “yes” line (since \\(E[Y] = n\\lambda\\) increases with \\(\\lambda\\)). From the reference table, that means \\(y_{\\rm RR}\\) is equal to \\(F_Y^{-1}(\\alpha \\vert n\\lambda_o)\\), which in code is qpois(0.05,n*lambda.o). (Recall that there are no discreteness corrections in rejection-region boundary computations.) Problem 11 (a) The likelihood function for the sample is: \\[\\begin{align*} \\mathcal{L}(\\beta \\vert \\mathbf{x}) = \\prod_{i=1}^{n} \\frac{1}{\\beta}e^{-x/\\beta} = \\frac{1}{\\beta^n} e^{-\\frac{\\sum_{i=1}^{n}x_i}{\\beta}} \\,. \\end{align*}\\] Under the null \\(H_0 : \\beta = 1\\), the likelihood function becomes \\[\\begin{align*} \\mathcal{L}(\\beta_0 \\vert \\mathbf{x}) = e^{-\\sum_{i=1}^{n}x_i} = e^{-n\\bar x} \\,, \\end{align*}\\] while under the alternative, \\[\\begin{align*} \\sup_{\\beta &gt; 0} \\mathcal{L}(\\beta \\vert \\mathbf{x}) &amp;= \\mathcal{L}(\\hat{\\beta}_{MLE} \\vert \\mathbf{x})\\\\ &amp; = \\frac{1}{\\bar x^n} e^{-\\frac{\\sum_{i=1}^{n}x_i}{\\bar x}} = \\frac{1}{\\bar x^n} e^{-n} \\,. \\end{align*}\\] So the likelihood ratio test statistic is \\[\\begin{align*} \\lambda_{LR} &amp;= \\frac{\\mathcal{L}(\\beta_o\\vert \\mathbf{x})}{\\sup_{\\beta &gt; 0} \\mathcal{L}(\\beta \\vert \\mathbf{x})}\\\\ &amp;= \\frac{e^{-n\\bar x}}{\\frac{1}{\\bar x^n} e^{-n}} \\\\ &amp;= \\bar x^ne^{-n(\\bar x-1)} \\,. \\end{align*}\\] (b) Under the null hypothesis, the number of degrees of freedom is \\(r_o = 0\\), because \\(\\beta = 1\\) is set to a constant. Under the alternative hypothesis, the number of degrees of freedom is \\(r = 1\\), because we have one free parameter: \\(\\beta\\). Therefore, according to Wilks’ theorem, the degree of freedom of the \\(\\chi^2\\) distribution is \\(r - r_o = 1-0=1\\). Under the large-\\(n\\) approximation, \\(-2\\log(\\lambda_{LR}) \\sim \\chi^2(1)\\). Therefore, the rejection region corresponds to: \\[\\begin{align*} -2\\log(\\lambda) &amp;&gt; \\chi^2_{0.95, 1}\\\\ \\Rightarrow ~~~ -2\\log\\left(\\bar x^ne^{-n(\\bar x-1)}\\right) &amp;&gt; \\chi^2_{0.95,1} = 3.84 \\\\ \\Rightarrow ~~~ n\\left(\\log(\\bar x) - \\bar x+1\\right) &amp;&lt; \\frac{3.84}{2} \\,. \\end{align*}\\] Problem 12 (a) \\(H_0: p = p_0 = 0.5\\) and \\(H_a: p \\neq 0.5\\) (b) \\(\\Theta_0 = \\{p_0\\}\\) and \\(\\Theta_a = \\{p\\, \\vert\\, p \\in [0,1] ~~\\text{and}~~ p \\neq p_0\\}\\) (c) \\(r_0 = 0\\) (\\(p\\) is fixed) and \\(r = 1\\) (d) The likelihood ratio test statistic is \\[\\begin{align*} \\lambda = \\frac{\\mathcal{L}(p_0 \\vert x)}{\\mathcal{L}(\\hat{p}_{MLE} \\vert x)} = \\frac{\\frac{1000!}{550!450!} 0.5^{550} (1-0.5)^{450} }{ \\frac{1000!}{550!450!} 0.55^{550} (1-0.55)^{450}} = \\frac{0.5^{1000}}{0.55^{550} \\cdot 0.45^{450}} = 0.00668 \\,, \\end{align*}\\] where we make use of the fact that \\(\\hat{p}_{MLE} = x/n = 0.55\\). (e) We have that \\(W_{\\rm obs} = -2 \\log(\\lambda_{LR}) = 10.017\\). According to Wilk’s theorem, the \\(p\\)-value is \\[\\begin{align*} \\int_{W_{\\rm obs}}^\\infty f_W(w) dw \\,, \\end{align*}\\] for 1 degree of freedom, or 1 - pchisq(10.017,1) (= 0.00155). (f) We have sufficient evidence to reject the null hypothesis and thus to conclude that the coin is not a fair one. Problem 13 By inspection, \\(X \\sim\\) Gamma(3,2/3). Thus \\(E[X] = \\alpha \\beta = 2\\) and \\(V[X] = \\alpha \\beta^2 = 3 (2/3)^2 = 4/3\\). Problem 14 (a) \\(E[X] = \\alpha \\beta\\) and \\(V[X] = \\alpha \\beta^2\\), so \\(V[X]/E[X] = \\beta = 10/5 = 2\\), and \\(\\alpha = 5/2 = 2.5\\). (b) \\(\\beta = 2\\) and \\(\\alpha = 2.5\\) \\(\\Rightarrow\\) chi-square distribution (for 5 degrees of freedom). Problem 15 We have that \\[\\begin{align*} E[X^{-1}] &amp;= \\int_0^\\infty \\frac1x f_X(x) dx = \\int_0^\\infty \\frac1x \\frac{x^{\\nu/2-1}}{2^{\\nu/2}} \\frac{e^{-x/2}}{\\Gamma(\\nu/2)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\nu/2-2}}{2^{\\nu/2}} \\frac{e^{-x/2}}{\\Gamma(\\nu/2)} dx = \\int_0^\\infty \\frac{x^{\\nu/2-2}}{2^{\\nu/2}} \\frac{2^{-1}}{2^{-1}} \\frac{e^{-x/2}}{\\Gamma(\\nu/2)} \\frac{\\Gamma(\\nu/2-1)}{\\Gamma(\\nu/2-1)} dx \\\\ &amp;= 2^{-1} \\frac{\\Gamma(\\nu/2-1)}{\\Gamma(\\nu/2)} \\int_0^\\infty \\frac{x^{\\nu/2-2}}{2^{\\nu/2-1}} \\frac{e^{-x/2}}{\\Gamma(\\nu/2-1)} dx = 2^{-1} \\frac{\\Gamma(\\nu/2-1)}{\\Gamma(\\nu/2)} \\\\ &amp;= \\frac12 \\frac{\\Gamma(\\nu/2-1)}{(\\nu/2-1)\\Gamma(\\nu/2-1)} = \\frac{1}{2(\\nu/2-1)} = \\frac{1}{\\nu-2} \\,. \\end{align*}\\] Problem 16 We have that \\[\\begin{align*} E[X] = \\int_0^\\infty x f_X(x) dx &amp;= \\int_0^\\infty x \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{1}{x^{\\alpha+1}} e^{-\\beta/x} dx \\\\ &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{1}{x^{\\alpha}} e^{-\\beta/x} dx \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\beta{\\alpha-1}} \\frac{\\Gamma(\\alpha-1)}{\\Gamma(\\alpha)} \\int_0^\\infty \\frac{\\beta^{\\alpha-1}}{\\Gamma(\\alpha-1)} \\frac{1}{x^{\\alpha}} e^{-\\beta/x} dx \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\beta{\\alpha-1}} \\frac{\\Gamma(\\alpha-1)}{\\Gamma(\\alpha)} \\cdot 1 \\\\ &amp;= \\beta \\frac{\\Gamma(\\alpha-1)}{(\\alpha-1)\\Gamma(\\alpha-1)} \\\\ &amp;= \\frac{\\beta}{\\alpha-1} \\,. \\end{align*}\\] Problem 17 (a) If \\(\\beta = 2\\) and \\(\\alpha\\) is a half-integer or an integer, then \\(X\\) is sampled from a “chi-square” distribution. (Note that because \\(\\alpha\\) is a half-integer here, we cannot answer “Erlang” or “exponential.”) (b) The gamma pdf with \\(\\alpha = 3/2\\) is \\[\\begin{align*} f_X(x) = \\frac{x^{1/2}}{\\beta^{3/2}} \\frac{e^{-x/\\beta}}{\\Gamma(3/2)} \\,, \\end{align*}\\] so the likelihood function is \\[\\begin{align*} \\mathcal{L}(\\beta \\vert \\mathbf{x} = \\prod_{i=1}^n \\frac{x_i^{1/2}}{\\beta^{3/2}} \\frac{e^{-x_i/\\beta}}{\\Gamma(3/2)} = \\frac{\\sqrt{\\prod_{i=1}^n x_i}}{[\\Gamma(3/2)]^n} \\cdot \\frac{e^{-(\\sum_{i=1}^n x_i)/\\beta}}{\\beta^{3/2}} = h(\\mathbf{x}) \\cdot g(\\beta,\\mathbf{x}) \\,. \\end{align*}\\] We can read off of the \\(g(\\cdot)\\) function that \\(Y = \\sum_{i=1}^n X_i\\) is a sufficient statistic for \\(\\beta\\). (c) We start by computing \\[\\begin{align*} E[Y] = E\\left[ \\sum_{i=1}^n X_i \\right] = \\sum_{i=1}^n E[X_i] = nE[X] = n \\alpha \\beta = \\frac{3}{2}n\\beta \\,. \\end{align*}\\] Thus \\[\\begin{align*} E\\left[\\frac{2Y}{3n}\\right] = \\beta \\end{align*}\\] and \\[\\begin{align*} \\hat{\\beta}_{MVUE} = \\frac{2Y}{3n} = \\frac{2}{3}\\bar{X} \\,. \\end{align*}\\] (d) The first population moment is \\(\\mu_1&#39; = E[X] = (3/2)\\beta\\) and the first sample moment is \\(m_1&#39; = (1/n)\\sum_{i=1}^n X_i = \\bar{X}\\). We set these equal and find that \\[\\begin{align*} \\hat{\\beta}_{MoM} = \\frac{2}{3}\\bar{X} \\,. \\end{align*}\\] (e) Because the MoM is equivalent to the MVUE, we know immediately that the bias of the MoM is 0. Problem 18 We have that \\[\\begin{align*} E[X^{1/2}] = \\int_0^\\infty x^{1/2} f_X(x) dx &amp;= \\int_0^\\infty x^{1/2} \\frac{x^{\\alpha-1}}{\\beta^\\alpha}\\frac{e^{-x/\\beta}}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha-1/2}}{\\beta^\\alpha}\\frac{e^{-x/\\beta}}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha-1/2}}{\\beta^\\alpha}\\frac{e^{-x/\\beta}}{\\Gamma(\\alpha)} \\frac{\\beta^{\\alpha+1/2}}{\\beta^{\\alpha+1/2}} \\frac{\\Gamma(\\alpha+1/2)}{\\Gamma(\\alpha+1/2)} dx \\\\ &amp;= \\frac{\\beta^{\\alpha+1/2}}{\\beta^{\\alpha}} \\frac{\\Gamma(\\alpha+1/2)}{\\Gamma(\\alpha)} \\int_0^\\infty \\frac{x^{\\alpha-1/2}}{\\beta^{\\alpha+1/2}}\\frac{e^{-x/\\beta}}{\\Gamma(\\alpha+1/2)} dx \\\\ &amp;= \\frac{\\beta^{\\alpha+1/2}}{\\beta^{\\alpha}} \\frac{\\Gamma(\\alpha+1/2)}{\\Gamma(\\alpha)} \\\\ &amp;= \\sqrt{\\beta} \\frac{\\Gamma(\\alpha+1/2)}{\\Gamma(\\alpha)} \\,. \\end{align*}\\] We note that in general, \\[\\begin{align*} E[X^k] = \\beta^k \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)} \\,. \\end{align*}\\] Problem 19 (a) The overdispersion parameter in a negative binomial regression is dubbed “Theta” and is thus 214,488. (b) If the overdispersion parameter is \\(\\infty\\), then Poisson regression is recovered. The value here is sufficiently large that we can say with confidence that there is no overdispersion. (Backing up this conclusion are the nearly identical results since when learning both regression models.) (c) The answer is the Likelihood Ratio test. The statistic is the difference in the deviance values, which we assume under the null is chi-square distributed for the difference in the numbers of degree of freedom. (d) The null hypothesis in the LRT is \\(\\beta_1 = 0\\) and the alternative is \\(\\beta_1 \\neq 0\\). The test statistic is so large (\\(\\approx\\) 165), especially considering that the expected value for a chi-square distribution is 1 for 1 degree of freedom, that we can safely conclude that \\(\\beta_1 \\neq 0\\). Chapter 5 Problem 1 Here, the first population moment is \\(\\mu_1&#39; = E[X] = \\frac{3}{2} \\theta\\) and the first sample moment is \\(m_1&#39; = \\frac{1}{n} \\sum X_i = \\bar{X}\\). So the MoM estimator for \\(\\theta\\), following from setting \\(\\mu_1&#39; = m_1&#39;\\), is \\(\\hat{\\theta}_{MoM} = \\frac{2}{3} \\bar{X}\\). Problem 2 We have that \\[\\begin{align*} P(X &gt; a+b \\vert X &gt; b) = \\frac{P(X &gt; a+b \\cap X &gt; b)}{P(X &gt; b)} = \\frac{P(X &gt; a+b)}{P(X &gt; b)} \\,, \\end{align*}\\] and \\[\\begin{align*} P(X &gt; a+b) = \\int_{a+b}^1 dx = 1-(a+b) ~~~ P(X &gt; b) = \\int_b^1 dx = 1-b \\,. \\end{align*}\\] So \\[\\begin{align*} \\frac{P(X &gt; a+b)}{P(X &gt; b)} = \\frac{1 - (a+b)}{1-b} \\,. \\end{align*}\\] The uniform distribution does not exhibit the memoryless property, as the ratio above does not depend just on \\(a\\). (If \\(b\\) cancelled out top and bottom, then the distribution would exhibit memorylessness.) Problem 3 (a) It doesn’t matter when she arrives: \\[\\begin{align*} P(x_0 \\leq X \\leq x_0 + 10) = \\int_{x_0}^{x_0 + 10} \\frac{1}{70} dx = \\frac{x_0 + 10}{70} - \\frac{x_0}{70} = \\frac{1}{7} \\,. \\end{align*}\\] (b) We have that \\(Y =\\) Binomial\\((n=5,p=1/7)\\), so \\[\\begin{align*} P(Y\\geq 1) = 1 - P(Y = 0) = 1 - {5 \\choose 0}\\frac{1}{7}^0\\left(1 - \\frac{1}{7}\\right) = 1 - \\left( \\frac{6}{7}\\right)^5. \\end{align*}\\] Problem 4 We have that \\[\\begin{align*} P(X \\leq 2u | X \\geq u) = \\frac{P(X \\leq 2u \\cap X \\geq u)}{P(X \\geq u)} = \\frac{\\int_u^{2u}dx}{\\int_u^{1}dx} = \\frac{2u - u}{1 - u} = \\frac{u}{1-u} \\,. \\end{align*}\\] Problem 5 We have that \\[\\begin{align*} P(X_1 &lt; 2X_2 \\vert X_2 &lt; 1/2) = \\frac{P(X_2 &gt; X_1/2 \\cap X_2 &lt; 1/2)}{P(X_2 &lt; 1/2)} \\,. \\end{align*}\\] We can approach this geometrically. The denominator is, by inspection, 1/2, so \\[\\begin{align*} P(X_1 &lt; 2X_2 \\vert X_2 &lt; 1/2) = 2P(X_2 &gt; X_1/2 \\cap X_2 &lt; 1/2) \\,. \\end{align*}\\] The remaining expression evaluates as the area of the triangle with vertices (0,0), (1,1/2), and (0,1/2), which is (1/2)(1)(1/2) = 1/4. Thus \\[\\begin{align*} P(X_1 &lt; 2X_2 \\vert X_2 &lt; 1/2) = 2 \\cdot 1/4 = 1/2 \\,. \\end{align*}\\] Problem 6 (a) Since \\(\\theta\\) is a lower bound, the sufficient statistic is \\(X_{(1)}\\), the minimum observed datum, by inspection. (b) The cdf \\(F_{(1)}(x)\\) is \\[\\begin{align*} F_{(1)}(x) = 1 - [1 - F_X(x)]^n \\,. \\end{align*}\\] The cdf \\(F_X(x)\\) is \\[\\begin{align*} F_X(x) = - \\int_{\\theta}^x \\frac{1}{\\theta} dy = - \\frac{1}{\\theta} \\int_{\\theta}^x dy = - \\frac{1}{\\theta} (x - \\theta) = 1 - x/\\theta \\,. \\end{align*}\\] Thus \\[\\begin{align*} F_{(1)}(x) = 1 - [1 - (1 - x/\\theta)]^n = 1 - \\left(\\frac{x}{\\theta}\\right)^n \\,, \\end{align*}\\] (c) If the null is true, we cannot observe a value of \\(X_{(1)}\\) that is smaller than \\(\\theta_o\\). So the “trivial rejection region” is \\(X_{(1)} &lt; \\theta_o\\). This is “trivial” because we can write it down via inspection (and it does not depend on \\(\\alpha\\)). (d) We can only reject the null if \\(X_{(1)} &gt; \\theta_o\\), so we have to “all the \\(\\alpha\\)” on that side of \\(\\theta_o\\): \\(1 - \\alpha\\). (e) We have that \\[\\begin{align*} 1 - \\left(\\frac{x_{RR}}{\\theta_o}\\right)^n &amp;= 1 - \\alpha ~~~ \\Rightarrow ~~~ \\left(\\frac{x_{RR}}{\\theta_o}\\right)^n = \\alpha ~~~ \\Rightarrow ~~~ x_{RR} = \\theta_o \\alpha^{1/n} \\,. \\end{align*}\\] Problem 7 (a) By inspection, \\(\\hat{\\theta}_{MLE} = X_{(n)}\\). (b) The cdf for \\(X_{(n)}\\) is \\([F_X(x)]^n = (x/\\theta)^{2n}\\), and the pdf is thus \\[\\begin{align*} f_{(n)}(x) = \\frac{d}{dx} F_{(n)}(x) = \\frac{2n}{\\theta^{2n}} x^{2n-1} \\,. \\end{align*}\\] Thus \\[\\begin{align*} E[X_{(n)}] &amp;= \\int_0^\\theta x f_{(n)}(x) dx = \\frac{2n}{\\theta^{2n}} \\int_0^\\theta x^{2n} dx = \\frac{2n}{\\theta^{2n}} \\left. \\frac{x^{2n+1}}{2n+1}\\right|_0^{\\theta} = \\frac{2n}{2n+1}\\theta \\,. \\end{align*}\\] (c) Since \\[\\begin{align*} E[X_{(n)}] &amp;= \\frac{2n}{2n+1}\\theta \\,, \\end{align*}\\] we have that \\[\\begin{align*} E\\left[\\frac{2n+1}{2n}X_{(n)}\\right] &amp;= \\theta \\end{align*}\\] and thus \\[\\begin{align*} \\hat{\\theta}_{MVUE} = \\frac{2n+1}{2n}X_{(n)} \\,. \\end{align*}\\] Problem 8 (a) As \\(\\theta\\) is a lower bound, \\(X_{(1)}\\) is a sufficient statistic. (b) The MLE is the sufficient statistic in (a): \\(\\hat{\\theta}_{MLE} = X_{(1)}\\). (c) The pdf for the minimum datum is \\[\\begin{align*} f_{(1)}(x) = n f_X(x) [1-F_X(x)]^{n-1} = n e^{-(x-\\theta)} \\left(e^{-(x-\\theta)}\\right)^{n-1} = n e^{-n(x-\\theta)} \\,. \\end{align*}\\] (d) The expected value of \\(X_{(1)}\\) is \\[\\begin{align*} E[X_{(1)}] = \\int_{\\theta}^\\infty x n e^{-n(x-\\theta)} dx = n \\int_{\\theta}^\\infty x n e^{-n(x-\\theta)} dx \\,. \\end{align*}\\] Let \\(u = n(x-\\theta)\\). Then \\(du = n dx\\), and if \\(x = \\theta\\), \\(u = 0\\), and if \\(x = \\infty\\), \\(u = \\infty\\). Thus \\[\\begin{align*} E[X_{(1)}] &amp;= n \\int_0^\\infty \\left(\\frac{u}{n}+\\theta\\right) e^{-u} \\frac{du}{n} = \\int_0^\\infty \\frac{u}{n} e^{-u} du + \\int_0^\\infty \\theta e^{-u} du = \\frac{1}{n} \\int_0^\\infty u e^{-u} du + \\theta \\int_0^\\infty e^{-u} du \\\\ &amp;= \\frac{1}{n} \\Gamma(2) + \\theta \\Gamma(1) = \\frac{1}{n} \\cdot 1! + \\theta \\cdot 0! = \\theta + \\frac{1}{n} \\,. \\end{align*}\\] Hence \\(\\hat{\\theta}_{MVUE} = X_{(1)} - 1/n\\). Problem 9 (a) Since \\(X_1\\) and \\(X_2\\) are independent, \\(P(X_1 &gt; 1/2 \\vert X_2 &lt; 1/2) = P(X_1 &gt; 1/2) = 1/2\\). (b) We have that \\[\\begin{align*} P\\left(X_1 &gt; \\frac12 \\vert X_1 &lt; \\frac34\\right) = \\frac{P(X_1 &gt; 1/2 \\cap X_1 &lt; 3/4)}{P(X_1 &lt; 3/4)} = \\frac{P(1/2 &lt; X_1 &lt; 3/4)}{P(X_1 &lt; 3/4)} = \\frac{0.25}{0.75} = \\frac13 \\,. \\end{align*}\\] (c) \\(X_1 &lt; 3X_2\\) is equivalent to \\(X_2 &gt; \\frac13 X_1\\), i.e., \\(X_2\\) lies above the line with intercept 0 and slope 1/3. The area of this region is 1 minus the area of the triangle with vertices (0,0), (1,0), and (1,1/3) or \\(1 - 1/6\\) = 5/6. (d) We have that \\[\\begin{align*} P\\left(X_2 &lt; X_1 \\vert X_2 &lt; \\frac12\\right) = \\frac{P(X_2 &lt; X_1 \\cap X_2 &lt; 1/2)}{P(X_1 &lt; 1/2)} = 2P(X_2 &lt; X_1 \\cap X_2 &lt; 1/2) \\,. \\end{align*}\\] The probability is the area of the polygon with vertices (0,0), (1,0), (1,1/2), and (1/2,1/2), or 3/8. So \\(P\\left(X_2 &lt; X_1 \\vert X_2 &lt; \\frac12\\right) = 6/8\\) or 3/4. Problem 10 (a) The expected value indicates that we are on the “yes” line of the confidence interval reference table, hence we want to solve \\[\\begin{align*} 1 - e^{-n(y_{\\rm obs}-\\theta)} - (1-\\alpha) = 0 \\end{align*}\\] for \\(\\theta\\): \\[\\begin{align*} e^{-n(y_{\\rm obs}-\\theta)} &amp;= \\alpha \\\\ \\Rightarrow ~~~ -n(y_{\\rm obs}-\\theta) &amp;= \\log(\\alpha) \\\\ \\Rightarrow ~~~ \\theta - y_{\\rm obs} &amp;= \\frac{1}{n}\\log(\\alpha) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L &amp;= y_{\\rm obs} + \\frac{1}{n}\\log(\\alpha) \\,. \\end{align*}\\] (b) Note that although this is a two-tailed test, it is impossible to sample a statistic value less than \\(\\theta\\), so we derive the rejection region boundary as if we are performing an upper-tail test. We are on the “yes” line of the hypothesis test reference table, hence we want to solve \\[\\begin{align*} 1 - e^{-n(y_{\\rm RR}-\\theta_o)} - (1-\\alpha) = 0 \\end{align*}\\] for \\(y_{\\rm RR}\\): \\[\\begin{align*} e^{-n(y_{\\rm RR}-\\theta_o)} &amp;= \\alpha ~~~ \\Rightarrow ~~~ -n(y_{\\rm RR}-\\theta_o) &amp;= \\log(\\alpha) ~~~ \\Rightarrow ~~~ y_{\\rm RR} - \\theta_o = -\\frac{1}{n}\\log(\\alpha) ~~~ \\Rightarrow ~~~ y_{\\rm RR} = \\theta_o - \\frac{1}{n}\\log(\\alpha) \\,. \\end{align*}\\] Chapter 6 Problem 1 (a) We have that \\[\\begin{align*} \\int_0^1 \\left( \\int_0^1 dx_2 k (x_1 + x_2^2) \\right) dx_1 &amp;= 1 = k \\left[ \\int_0^1 x_1 \\left( \\int_0^1 dx_2 \\right) dx_1 + \\int_0^1 \\left( \\int_0^1 x_2^2 dx_2 \\right) dx_1 \\right] \\\\ &amp;= k \\left[ \\int_0^1 x_1 dx_1 + \\int_0^1 \\frac13 dx_1 \\right] = k \\left[ \\left. \\frac{x_1^2}{2} \\right|_0^1 + \\left. \\frac{x_1}{3} \\right|_0^1 \\right] \\\\ &amp;= k \\left( \\frac{1}{2} + \\frac{1}{3} \\right) = k \\frac{5}{6} \\,. \\end{align*}\\] Thus \\(k = 6/5\\). (b) We have that \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \\frac{k (x_1 + x_2^2)}{f_{X_2}(x_2)} \\,. \\end{align*}\\] So we need to compute the marginal density: \\[\\begin{align*} f_{X_2}(x_2) = k \\int_0^1 dx_1 (x_1+x_2^2) = k \\left[ \\int_0^1 x_1 dx_1 + \\int_0^1 x_2^2 dx_1 \\right] = k \\left[ \\left. \\frac{x_1^2}{2} \\right|_0^1 + x_2^2 (\\left. x_1\\right|_0^1) \\right] = k \\left( \\frac{1}{2} + x_2^2 \\right) \\,. \\end{align*}\\] Thus \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{k (x_1 + x_2^2)}{k ( \\frac{1}{2} + x_2^2 )} = \\frac{2x_1 + 2x_2^2}{1 + 2x_2^2} \\,. \\end{align*}\\] This may initially appear strange (in that if \\(x_1 &gt; 1/2\\), \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) &gt; 1\\)), but we simply need to remind ourselves that \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\) is a conditional probability density function, not a probability itself. Problem 2 (a) Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2]\\) = \\(1 \\cdot 1 \\cdot 0.1 - (1 \\cdot 0.4 + 1 \\cdot 0.1)^2 = 0.1 - 0.25 = -0.15\\). (b) \\(\\rho\\) = Cov(\\(X_1,X_2\\))/(\\(\\sigma_1\\sigma_2\\)), where \\[\\begin{align*} \\sigma_1 = \\sqrt{E[X_1^2] - (E[X_1])^2} = \\sqrt{0.5 - (0.5)^2} = \\sqrt{0.25} = 0.5 = \\sigma_2 \\,. \\end{align*}\\] So \\(\\rho = -0.15/0.5/0.5 = -0.15/0.25 = -0.6\\). (c) \\(E[X_1 \\vert X_2 &lt; 1]\\) is equivalent to \\(E[X_1 \\vert X_2 = 0]\\), i.e., the expected value for data drawn from the first row of the given table. \\[\\begin{align*} E[X_1 \\vert X_2 = 0] &amp;= \\sum_{x_1=0}^1 x_1 p(x_1 \\vert x_2=0) = \\sum_{x_1=0}^1 x_1 \\frac{p(x_1,x_2=0)}{p(p_2{x_2}=0)} \\\\ &amp;= \\frac{0 \\cdot p(x_1=0,x_2=0) + 1 \\cdot p(x_1=1,x_2=0)}{p(x_1=0,x_2=0)+p(x_1=1,x_2=0)} = \\frac{0.4}{0.5} = 0.8 \\,. \\end{align*}\\] This answer could be reasoned out by inspecting the table. (d) We have that \\[\\begin{align*} V[X_2 \\vert X_1=1] &amp;= E[X_2^2 \\vert X_1=1] - (E[X_2\\vert X_1=1])^2 \\\\ &amp;= \\sum_{x_2=0}^1 x_2^2 p(x_2 \\vert x_1=1) - \\left[\\sum_{x_2=0}^1 x_2 p(x_2 \\vert x_1=1)\\right]^2 \\\\ &amp;= \\sum_{x_2=0}^1 x_2^2 \\frac{p(x_1=1,x_2)}{p(p_1{x_1=1})} - \\left[\\sum_{x_2=0}^1 x_2 \\frac{P(x_1=1,x_2)}{p(p_1{x_1=1})}\\right]^2 \\\\ &amp;= 1 \\cdot \\frac{p(x_1=1,x_2=1)}{p(x_1=1,x_2=0)+p(x_1=1,x_2=1)} - &amp;~~~~~\\left[1 \\cdot \\frac{p(x_1=1,x_2=1)}{p(x_1=1,x_2=0)+p(x_1=1,x_2=1)}\\right]^2 \\\\ &amp;= \\frac{0.1}{0.5} - \\left(\\frac{0.1}{0.5}\\right)^2 = 0.2 - 0.04 = 0.16 \\,. \\end{align*}\\] This answer could also be reasoned out by inspecting the table. Problem 3 (a) We have that \\(X \\vert p\\) \\(\\sim\\) Bin(\\(n,p\\)) and that \\(p \\sim\\) Uniform(0,0.1). The expected value of \\(X \\vert p\\) is \\(np\\) and the expected value of \\(p\\) is \\((0+0.1)/2 = 0.05\\). Thus \\[\\begin{align*} E[X] = E[E[X \\vert p]] = E[np] = nE[p] = 0.05n \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} V[X] &amp;= E[V[X \\vert p]] + V[E[X \\vert p]] = E[np(1-p)] + V[np] = n(E[p] - E[p^2]) + n^2V[p] \\\\ &amp;= n[E[p] - (V[p]+E[p]^2)] + n^2V[p] = n(n-1)V[p] + nE[p] - n(E[p])^2 \\,. \\end{align*}\\] Problem 4 (a) The area of integration lies between the \\(x_1\\) axis, the \\(x_2\\) axis, and the line \\(x_2 = 1-x_1\\), in the first quadrant. \\[\\begin{align*} 1 &amp;= \\int_0^1 \\int_0^{1-x_1} k x_1^2 x_2 dx_2 dx_1 = k \\int_0^1 x_1^2 \\int_0^{1-x_1} x_2 dx_2 dx_1 = k \\int_0^1 x_1^2 \\left( \\left. \\frac{x_2^2}{2}\\right|_0^{1-x_1}\\right) dx_1 \\\\ &amp;= \\frac{k}{2} \\int_0^1 x_1^2 (1-x_1)^2 dx_1 = \\frac{k}{2} B(3,3) = \\frac{k \\Gamma(3) \\Gamma(3)}{2 \\Gamma(6)} = \\frac{4k}{240} \\,. \\end{align*}\\] So \\(k = 60\\). (b) \\(P(X_1 &gt; 0.25 \\vert X_2 = 0.5) = \\int_{0.25}^{0.5} f(x_1 \\vert x_2=0.5)dx_1\\). \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) = f_{X_1,X_2}(x_1,x_2)/f_{X_2}(x_2)\\), and \\[\\begin{align*} f_{X_2}(x_2) = \\int_0^{1-x_2} 60 x_1^2 x_2 dx_1 = 60 x_2 \\left( \\left.\\frac{x_1^3}{3}\\right|_0^{1-x_2}\\right) = 20x_2(1-x_2)^3 \\,. \\end{align*}\\] So: \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{60x_1^2y_2}{20x_2(1-y_2)^3} = \\frac{3x_1^2}{(1-x_2)^3} \\,. \\end{align*}\\] Plugging in \\(x_2 = 0.5\\), we get \\(24x_1^2\\). Finally: \\[\\begin{align*} \\int_{1/4}^{1/2} 24 x_1^2 dx_1 = \\left.8 x_1^3\\right|_{1/4}^{1/2} = 7/8 \\,. \\end{align*}\\] (c) The region over which \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\) is non-zero is not rectangular: \\(X_1\\) and \\(X_2\\) are dependent random variables. Problem 5 (a) We sum over rows: \\(x_2\\) 0 1 2 \\(p_{x_2}(x_2)\\) 0.4 0.4 0.2 (b) We have that \\(x_2\\) \\((-\\infty,0)\\) [0,1) [1,2) \\(F_{X_2}(x_2)\\) 0 0.4 0.8 (c) We have that \\[\\begin{align*} E[X] &amp;= \\sum_x x p_X(x) = 0 \\cdot 0.4 + 1 \\cdot 0.4 + 2 \\cdot 0.2 = 0.8 \\\\ E[Y^2] &amp;= \\sum_x x^2 p_X(x) = 0^2 \\cdot 0.4 + 1^2 \\cdot 0.4 + 2^2 \\cdot 0.2 = 1.2 \\\\ \\end{align*}\\] so \\(V[X] = E[X^2] - (E[X])^2 = 0.56\\) and \\(\\sigma = \\sqrt{0.56} = 0.748\\). Problem 6 The region of integration lies between the \\(x\\) axis, the \\(y\\) axis, and the line \\(x_2 = 1-x_1\\), in the first quadrant. (a) \\({\\rm Cov}(X_1,X_2) = E[X_1X_2] - E[X_1]E[X_2]\\)…so we need to compute \\(E[X_1X_2]\\): \\[\\begin{align*} E[X_1X_2] &amp;= \\int_0^1 \\int_0^{1-x_1} x_1 x_2 60 x_1^2 x_2 dx_2 dx_1 = 60 \\int_0^1 x_1^3 \\int_0^{1-x_1} x_2^2 dx_2 dx_1 \\\\ &amp;= 60 \\int_0^1 x_1^3 \\frac{(1-x_1)^3}{3} dx_1 = 20 B(4,4) = 20 \\frac{3! 3!}{7!} = 1/7 \\,. \\end{align*}\\] Hence Cov(\\(X_1,X_2\\)) = 1/7 - 1/2(1/3) = \\(-1/42\\). (b) We have that \\[\\begin{align*} {\\rm Corr}(X_1,X_2) = \\frac{{\\rm Cov}(X_1,X_2)}{\\sigma_{X_1}\\sigma_{X_2}} = \\frac{-1/42}{\\sqrt{2/7-1/4} \\sqrt{1/7-1/9}} = \\cdots = -1/\\sqrt{2} \\,. \\end{align*}\\] (c) \\(V[X_1-2X_2] = V[X_1] + 4V[X_2] - 2 \\cdot 2 \\cdot {\\rm Cov}(X_1,X_2) = 1/28 + 8/63 + 4/42 = \\cdots = 65/252\\). Problem 7 Let \\(N\\) be the number of laid egges: \\(N \\sim\\) Poi(\\(\\lambda\\)), and \\(E[N] = V[N] = \\lambda\\). Let \\(X\\) be the number of hatched eggs: \\(X \\vert N \\sim\\) Bin(\\(N,p\\)), and \\(E[X \\vert N] = Np\\) and \\(V[X \\vert N] = Np(1-p)\\). (a) \\(E[X] = E[E[X\\vert N]] = E[Np] = pE[N] = \\lambda p\\). (b) We have that \\[\\begin{align*} V[X] &amp;= V[E[X\\vert N]] + E[V[X \\vert N]] = V[Np] + E[Np(1-p)] \\\\ &amp;= p^2V[N] + p(1-p)E[N] = \\lambda p^2 + \\lambda p - \\lambda p^2 = \\lambda p \\,. \\end{align*}\\] Problem 8 (a) \\(X_1\\), \\(X_2\\) are independent \\(\\Rightarrow f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2) = \\left[ k_1 x_1 e^{-x_1/2}\\right]\\left[k_2x_2(1-x_2) \\right]\\). \\(X_1 \\sim \\text{Gamma}(2,2) \\left[ \\text{or } \\chi^2_4\\right]\\), and \\(X_2 \\sim \\text{Beta}(2,2)\\), thus \\[\\begin{align*} k = k_1 k_2 = \\frac{1}{\\beta_1^{\\alpha_1}\\Gamma(\\alpha_1)}\\underbrace{\\frac{\\Gamma(\\alpha_2 + \\beta_2)}{\\Gamma(\\alpha_2)\\Gamma(\\beta_2)}}_{1/B(\\alpha_2,\\beta_2)} = \\frac{1}{2^21!}\\frac{3!}{1!1!} = \\frac{3}{2} \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} V[X_1 - X_2] &amp;= V[X_1] + V[X_2] = \\underbrace{\\alpha_1 \\beta_1^2}_{\\text{Gamma}} + \\underbrace{\\alpha\\beta/\\left[(\\alpha_2 + \\beta_2)^2(\\alpha_2 + \\beta_2 + 1)\\right]}_{\\text{Beta}}\\\\ &amp;= 8 + 4/(16 \\cdot 5) = 8 + \\frac{1}{20} = \\frac{161}{20} \\,. \\end{align*}\\] Problem 9 (a) \\(X_1\\) and \\(X_2\\) are uniformly distributed. In a plane described by \\(X_1\\), \\(X_2\\), the region where \\(f_{X_1,X_2}(x_1, x_2)&gt;0\\) corresponds to a rectangular trapezoid, with long base going from the point with coordinate \\((0,0)\\) to the point with coordinate \\((2,0)\\), and short base going from the point with coordinate \\((0,1)\\) to the point with coordinate \\((1,1)\\). Therefore the segment connecting the two points \\((1,1)-(2,0)\\) corresponds to the line \\(X_2 = -X_1 + 2\\), or \\(X_1 = 2 - X_2\\). Thus \\[\\begin{align*} f_{X_1,X_2}(x_1, x_2) = \\frac{1}{\\text{area of the region for which }f_{X_1,X_2}(x_1, x_2)&gt;0 }= \\frac{1}{3/2} = \\frac{2}{3} \\,. \\end{align*}\\] (b) \\(f_{X_2}(x_2) = \\int_{x_1 = 0}^{x_1 = 2-x_2} k dx_1 = k \\left[ x_1|_0^{2-x_2}\\right] = k(2-x_2)\\),i if \\(0\\leq x_2 \\leq 1\\). Therefore, \\(f_{X_2}(x_2) = k(2-x_2)\\) for \\(x_2 \\in [0,1]\\). (c) \\(f_{X_1 | X_2}(x_1 | x_2) = f_{X_1,X_2}(x_1, x_2)/f_{X_2}(x_2) = 1/(2-x_2)\\), for \\(x_1 \\in [0,2-x_2]\\) and \\(x_2 \\in [0,1]\\). (d) We have that \\[\\begin{align*} E[X_1] =&amp; \\int_0^1 \\int_{x_1 = 0}^{x_1 = 2-x_2} x_1 k dx_1 dx_2 = k \\int_0^1 \\left[\\frac{x_1^2}{2}\\bigg|_{0}^{2-x_2} \\right] dx_2 \\\\ =&amp; k \\int_0^1 \\frac{1}{2} (2-x_2)^2 dx_2 = \\frac{k}{2} \\int_0^1 (4 - 4x_2 + x_2^2) dx_2\\\\ =&amp; \\frac{k}{2} \\left[ 4x_2\\bigg|_0^1 - 2x_2^2\\bigg|_0^1 + \\frac{x_2^3}{3}\\bigg|_0^1\\right] = \\frac{k}{2}\\left(4 -2 +\\frac{1}{3}\\right) = \\frac{7}{6}k \\,. \\end{align*}\\] (e) They are dependent since the region over which \\(f_{X_1,X_2}(x_1, x_2) &gt;0\\) is non-rectangular. Problem 10 (a) \\(p \\sim {\\rm Beta}(2,2)\\), and \\(X|p \\sim {\\rm Bin}(5,p)\\), thus \\[\\begin{align*} E\\left[ E\\left[ X|p\\right] \\right] = E[5p] = 5E[p] = 5\\frac{\\alpha}{\\alpha + \\beta} = 2.5 \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} V[X] &amp;= V\\left[ E\\left[ X|p\\right] \\right] + E\\left[ V\\left[ X|p\\right] \\right] = V[5p] + E[5p(1-p)] = 25V[p] + 5\\left[E[p] - E[p^2] \\right]\\\\ &amp;= 25V[p] + 5E[p] -5\\left[ V[p] + (E[p])^2 \\right] = 20V[p] + 5E[p] -5(E[p])^2\\\\ &amp;= 20 \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} + 5\\frac{\\alpha}{\\alpha + \\beta} - 5\\left(\\frac{\\alpha}{\\alpha + \\beta}\\right)^2\\\\ &amp;= 20 \\frac{4}{16 \\cdot 5} + 2.5 -1.25 = 1 + 2.5 - 1.25 = 2.25 \\,. \\end{align*}\\] Problem 11 The region where \\(f_{X_1,X_2}(x_1, x_2)&gt;0\\) can be represented by a triangle with vertices (0,0), (1,0), and (1,1). Thus: \\[\\begin{align*} E[X_1 X_2] &amp;= \\int_0^1 \\left( \\int_0^{x_1} x_1 x_2 (3 x_1) dx_2 \\right) dx_1 = \\int_0^1 3x_1^2 \\left( \\int_0^{x_1} x_2 dx_2\\right) dx_1 \\\\ &amp;= \\int_0^1 3x_1^2 \\frac{x_1^2}{2} dx_1 = \\frac{3}{2} \\int_0^1x_1^4 dx_1 = \\frac{3}{10} x_1^5\\bigg|_0^1 = \\frac{3}{10} \\,. \\end{align*}\\] Therefore \\[\\begin{align*} \\rho = \\frac{{\\rm Cov}(X_1,X_2)}{\\sqrt{V[X_1] V[X_2]}} = \\frac{E[X_1X_2] - E[X_1] E[X_2]}{\\sqrt{V[X_1] V[X_2]}} = \\frac{\\frac{3}{10} - \\frac{9}{32}}{\\sqrt{\\frac{3}{80}\\frac{19}{320}}} = 0.397 \\,. \\end{align*}\\] Problem 12 The region of integration lies in the first quadrant, below the line \\(x_2 = x_1\\). (a) We have that \\[\\begin{align*} \\int_0^\\infty \\left[ \\int_0^{x_1} k e^{-x_1} dx_2 \\right] dx_1 = k \\int_0^\\infty e^{-x_1} \\int_0^{x_1} dx_2 dx_1 = k \\int_0^\\infty x_1 e^{-x_1} dx_1 = k \\Gamma(2) = 1 \\Rightarrow k = 1 \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} P(X_2 &lt; 1) &amp;= \\int_0^1 \\left[ \\int_{x_2}^\\infty e^{-x_1} dx_1 \\right] dx_2 = \\int_0^1 \\left(-\\left.e^{-x_1}\\right|_{x_2}^\\infty \\right) dx_2 \\\\ &amp;= \\int_0^1 e^{-x_2} dx_2 = -\\left.e^{-x_2}\\right|_0^1 = -(e^{-1}-1) = 1-e^{-1} \\,. \\end{align*}\\] (c) The marginal distribution is \\[\\begin{align*} f_{X_2}(x_2) = \\int_{x_2}^\\infty e^{-x_1} dx_1 = -\\left.e^{-x_1}\\right|_{x_2}^\\infty = -(0-e^{-x_2}) = e^{-x_2} \\,, \\end{align*}\\] or, in full, \\[\\begin{align*} f_{X_2}(x_2) = \\left\\{ \\begin{array}{cl} e^{-x_2} &amp; x_2 \\geq 0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (d) The conditional pdf is \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \\frac{e^{-x_1}}{e^{-x_2}} = e^{x_2-x_1} \\,, \\end{align*}\\] or, in full, \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\left\\{ \\begin{array}{cl} e^{x_2-x_1} &amp; x_1,x_2 \\geq 0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\end{align*}\\] (e) We have that \\[\\begin{align*} P(X_1 &gt; 2 \\vert X_1 = 1) &amp;= \\int_2^\\infty f_{X_1 \\vert X_2=1}(x_1 \\vert x_2=1) dx_1 = \\int_2^\\infty e^{1-x_1} dx_1 \\\\ &amp;= e^1 \\left(-\\left.e^{-x_1}\\right|_2^\\infty\\right) = e^1(-(0-e^{-2})) = e^{-1} \\,. \\end{align*}\\] Problem 13 We are given that \\(X|\\lambda \\sim\\) Poisson\\((\\lambda)\\), and \\(\\lambda \\sim\\) NegBinom\\((4,1/2)\\). Thus \\(E[X|\\lambda] = \\lambda\\) and \\(E[\\lambda] = \\frac{r}{p} = 8\\), and \\(V[X|\\lambda] = \\lambda\\) and \\(V[\\lambda] = \\frac{r(1-p)}{p^2} = \\frac{2}{1/4} = 8\\). (a) \\(E[X] = E\\left[ E\\left[ X|\\lambda\\right] \\right] = E[\\lambda] = 8\\). (b) \\(V[X] = E\\left[ V\\left[ X|\\lambda\\right] \\right] + V\\left[ E\\left[ X|\\lambda\\right] \\right] = E[\\lambda] + V[\\lambda] = 8 + 8 = 16\\). Problem 14 The region where \\(f_{X_1,X_2}(x_1, x_2)\\) is positive can be described as the union of two rectangular triangle. The first one with vertexes \\((-2,0) - (-2, 2) - (0,0)\\). The second one with vertexes \\((2,0) - (2, 2) - (0,0)\\). (a) The region is not rectangular: \\(X_1, X_2\\) are not independent. (b) Uniformity \\(\\Rightarrow k = \\frac{1}{\\text{geometric area}} = \\frac{1}{4}\\). (c) We have that \\[\\begin{align*} E[X_2] &amp;= \\int_0^2 \\left[\\int_{-2}^{-x_2} \\frac{x_2}{4} dx_1 + \\int_{x_2}^{2} \\frac{x_2}{4} dx_1 \\right] dx_2 = 2\\int_0^2 \\left[\\int_{x_2}^{2} \\frac{x_2}{4} dx_1 \\right] dx_2\\\\ &amp;=\\frac{1}{2}\\int_0^2 x_2 \\left[\\int_{x_2}^{2} dx_1 \\right] dx_2 =\\frac{1}{2}\\int_0^2 x_2(2-x_2) dx_2 = \\frac{1}{2} \\left[ x_2^2\\bigg|_0^2 - \\frac{x_2^3}{3}\\bigg|_0^2\\right] \\\\ &amp;= \\frac{1}{2} \\left[ 4 - \\frac{8}{3}\\right]= \\frac{2}{3} \\,. \\end{align*}\\] Problem 15 (a) We have that \\[\\begin{align*} \\int_0^1 \\int_0^1 f_{X_1,X_2}(x_1, x_2) dx_1 dx_2 = 1 = k \\left[ \\int_0^1 x_1 dx_1 \\int_0^1 x_2 dx_2 \\right] = k \\left[ \\frac{x_1^2}{2}\\bigg|_0^1 \\frac{x_2^2}{2}\\bigg|_0^1 \\right] = \\frac{k}{4} \\,. \\end{align*}\\] Therefore \\(k = 4\\). (b) \\(X_1, X_2\\) are independent, so \\[\\begin{align*} P(X_2 &gt; 1/2|X_1 = 1/2) &amp;= P(X_2 &gt; 1/2) = \\int_{1/2}^1 \\left[ \\int_0^1 4 x_1 x_2 dx_1\\right] dx_2\\\\ &amp;= \\int_{1/2}^1 x_2 \\left[ \\int_0^1 4 x_1 dx_1\\right] dx_2 = \\int_{1/2}^1 x_2 \\frac{1}{2} dx_2 = 2 \\left( \\frac{x_2^2}{2}\\bigg|_{1/2}^1\\right) \\\\ &amp;= 1^2 - (1/2)^2 = \\frac{3}{4} \\,. \\end{align*}\\] (c) \\(X_1, X_2\\) are independent, so Cov\\((X_1, X_2) = 0\\). Problem 16 We have that \\[\\begin{align*} V[U] = a^T\\Sigma a = \\begin{bmatrix} 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 2\\\\ 2 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = 8 - 1 =7 \\,. \\end{align*}\\] Alternatively, \\[\\begin{align*} V[U] = a_1^2V[X_1] + a_2^2 V[X_2] + 2a_1a_2 \\text{Cov}(X_1, X_2) = 4 \\cdot 3 + 1 \\cdot 3 + 2 (2)(-1)(2) = 15-8 = 7 \\,. \\end{align*}\\] Problem 17 Before we begin, we can write down that \\(X \\sim \\text{Bernoulli}(1/2)\\), so \\(E[X] = \\frac{1}{2}\\) and \\(V[X] = \\frac{1}{2}\\left( 1 - \\frac{1}{2}\\right) = \\frac{1}{4}\\), and that \\(U \\vert X \\sim \\text{Uniform}(X,2)\\), so \\(E[U \\vert X] = \\frac{X+2}{2}\\) and \\(V[U \\vert X] = \\frac{1}{12}\\left( 2 - X^2 \\right)\\). We have that \\[\\begin{align*} V[U] &amp;= E\\left[V \\left[ U|X \\right] \\right] + V\\left[E \\left[ U|X \\right] \\right] \\\\ &amp;= E\\left[\\frac{1}{12}\\left( 2 - X^2 \\right) \\right] + V\\left[\\frac{X+2}{2} \\right] \\,, \\end{align*}\\] where \\[\\begin{align*} V\\left[\\frac{X+2}{2}\\right] = V\\left[\\frac{X}{2}\\right] = \\frac{1}{4}V[X] = \\frac{1}{16} \\end{align*}\\] and \\[\\begin{align*} E\\left[\\frac{1}{12}\\left( 2 - X^2 \\right) \\right] &amp;= \\frac{1}{12} E[4 - 4X + X^2] = \\frac{1}{3} - \\frac{1}{6} + \\frac{1}{12}\\left[V[X] + (E[X])^2 \\right]\\\\ &amp;= \\frac{1}{6} + \\frac{1}{12} \\left[ \\frac{1}{4} \\frac{1}{4}\\right] = \\frac{4}{24} + \\frac{1}{24} = \\frac{5}{24} \\,. \\end{align*}\\] Thus \\(V[U] = 5/24 + 1/16 = 13/48\\). Problem 18 (a) Is the region rectangular? Yes. And \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) f_{X_2}(x_2) = f_{X_1}(x_1)\\), with \\(f_{X_2}(x_2) = 1\\), so \\(X_1, X_2\\) are independent random variables. (b) \\(f_{X_1,X_2}(x_1, x_2) =f_{X_1}(x_1) f_{X_2}(x_2) = 12 x_1^2(1-x_1) \\cdot 1\\), therefore \\(f_{X_2}(x_2) = 1\\) for \\(x_2 \\in [0,1]\\), or \\(X_2 \\sim \\text{Uniform}(0,1)\\). Other ways to derive this result include \\[\\begin{align*} f_{X_2}(x_2) &amp;= \\underbrace{\\int_0^1 12 x_1^2(1-x_1) dx_1}_{\\text{Beta}(3,2)} = 1 \\\\ &amp;= \\int_0^1 12 x_1^2(1-x_1) dx_1 = 12 \\left[ \\frac{x_1^3}{3}\\bigg|_0^1 - \\frac{x_1^4}{4}\\bigg|_0^1 \\right] = \\frac{12}{12} = 1 \\,. \\end{align*}\\] (c) \\(X_1 \\sim \\text{Beta}(3,2)\\), hence \\(E[X_1] = \\alpha/(\\alpha + \\beta) = \\frac{3}{5}\\). Another way to derive this result is \\[\\begin{align*} f_{X_1}(x_1) &amp;= \\int_0^1 12 x_1^2(1-x_1) dx_1 = 12 x_1^2(1-x_1) \\\\ E[X_1] &amp;= \\int_0^1 x_1 f_{X_1}(x_1) dx_1 = \\int_0^1 12 x_1^3(1-x_1) dx_1 = 12 \\left[ \\frac{x_1^4}{4}\\bigg|_0^1 - \\frac{x_1^5}{5}\\bigg|_0^1 \\right] = \\frac{12}{20} = \\frac{3}{5} \\,. \\end{align*}\\] Problem 19 (a) The area where \\(f_{X_1,X_2}(x_1,x_2)&gt;0\\) is a triangle with vertices at (0,0), (1,1), and (2,0). For a bivariate uniform, \\(c = 1/\\)(the area of the region), so \\(c = 1\\). Another way to derive this is via brute force: \\[\\begin{align*} \\int_0^1 \\left[\\int_{x_2}^{2 - x_2} c dx_1\\right] dx_2 &amp;= c \\int_0^1 (2- x_2) - x_2 dx_2\\\\ &amp;= c \\int_0^1 2(1- x_2) dx_2 = 2c \\left[ x_2\\bigg|_0^1 - \\frac{x_2^2}{2}\\bigg|_0^1 \\right] = c = 1 \\,. \\end{align*}\\] (b) \\(f_{X_2}(x_2) = \\int_{x_2}^{2 - x_2} dx_1 = 2(1- x_2)\\) for \\(x_2 \\in [0,1]\\). (c) \\(f_{X_1|X_2}(x_1|x_2) =\\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = 1/[2(1-x_2)]\\) for \\(x_1 \\in [0,2]\\), and \\(x_2 \\in [0,x_1]\\) and \\(x_2 \\in [0,2-x_1]\\). Problem 20 (a) We can recognize immediately that \\(X_1,X_2\\) are independent, thus \\(f_{X_1}(x_1) = 2x_1\\) for \\(x_1 \\in [0,1]\\). We can also derive this result via brute force: \\[\\begin{align*} f_{X_1}(x_1) = \\int_0^1 4 x_1 x_2 dx_2 = 4x_1 \\frac{x_2^2}{2}\\bigg|_0^1 = 2x_1 \\,. \\end{align*}\\] (b) We have that \\(f_{X_2|X_1}(x_2|x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \\frac{4 x_1 x_2}{2x_1} = 2x_2\\) for \\(x_1 \\in [0,1]\\) and \\(x_2 \\in [0,1]\\). (c) We have that \\[\\begin{align*} P(X_2 &lt; 1/2 |X_1 = x_1) = \\int_0^{1/2} f_{X_2|X_1}(x_2|x_1) dx_2 = \\int_0^{1/2}2x_2dx_2 = x_2^2\\bigg|_0^{1/2} = \\frac{1}{4} \\,. \\end{align*}\\] Problem 21 We begin by noting that \\(V[X_1] = E[X_1^2] - E[X_1]^2\\), and that \\[\\begin{align*} E[X_1] &amp;= \\int_0^1 \\int_0^{1-x_1} 2x_1 dx_2 dx_1 = 2 \\int_0^1 x_1(1-x_1) dx_1 = 2B(2,2) = 2 \\frac{\\Gamma(2)\\Gamma(2)}{\\Gamma(4)} = \\frac{1!1!}{3!} = \\frac{1}{3} \\,. \\\\ E[X_1^2] &amp;= \\int_0^1 \\int_0^{1-x_1} 2x_1 x_1 dx_2 dx_1 = 2 \\int_0^1 x_1^2(1-x_1) dx_1 = 2B(3,2) = 2 \\frac{2!1!}{4!} = \\frac{1}{6} \\,. \\end{align*}\\] Thus \\(V[X_1] = 1/6 - \\left( 1/3 \\right)^2 = 1/18\\). Problem 22 We have that \\(V[X_1 - X_2] = V[X_1] + V[X_2] - 2\\)Cov\\((X_1,X_2)\\). So we need to compute every part of the formula above: \\[\\begin{align*} E[X_1] &amp;= \\sum \\sum x_1 p_{X_1,X_2}(x_1,x_2) = 1 \\cdot \\frac{2}{9} + 1 \\cdot \\frac{2}{9} + 2 \\cdot \\frac{1}{9} = \\frac{6}{9}\\\\ V[X_1] &amp;= E[X_1^2] - E[X_1]^2 = \\frac{8}{9} - \\frac{36}{81} = \\frac{4}{9}\\\\ E[X_2] &amp;= \\sum \\sum x_2 p_{X_1,X_2}(x_1,x_2) = 1 \\cdot \\frac{3}{9} + 1 \\cdot \\frac{2}{9} + 1 \\cdot \\frac{1}{9} = \\frac{6}{9}\\\\ V[X_1] &amp;= E[X_1^2] - E[X_1]^2 = \\frac{6}{9} - \\frac{36}{81} = \\frac{2}{9}\\\\ E[X_1X_2] &amp;= \\sum \\sum x_1 x_2 p_{X_1,X_2}(x_1,x_2) = 1 \\cdot 1 \\cdot \\frac{2}{9} + 2 \\cdot 1 \\cdot \\frac{1}{9} = \\frac{4}{9}\\\\ \\mbox{Cov}(X_1, X_2) &amp;= \\frac{4}{9} - \\left( \\frac{6}{9}\\right)^2 = 0 \\,. \\end{align*}\\] Thus \\(V[X_1 - X_2] = 6/9 = 2/3\\). Problem 23 We recognize that \\(X|\\beta \\sim \\text{Gamma}(\\alpha, \\beta)\\) and that \\(\\beta \\sim \\text{Exp}(\\gamma)\\), with \\(E[\\beta] = \\gamma, V[\\beta] = \\gamma^2\\), and \\(E[X|\\beta] = \\alpha \\beta\\), \\(V[X|\\beta] = \\alpha \\beta^2\\). (a) \\(E[X] = E\\left[ E\\left[ X|p\\right] \\right] = E[\\alpha \\beta] = \\alpha E[\\beta] = \\alpha \\gamma\\). (b) We have that \\[\\begin{align*} V[X] &amp;= V\\left[ E\\left[ X|p\\right] \\right] + E\\left[ V\\left[ X|p\\right] \\right] = E[\\alpha \\beta^2] + V[\\alpha \\beta]\\\\ &amp;= \\alpha E[\\beta^2] + \\alpha^2V[\\beta]= \\alpha \\left[ V[\\beta] + E[\\beta]^2\\right] + \\alpha^2 \\gamma^2\\\\ &amp;= \\alpha \\left[ \\gamma^2+\\gamma^2\\right] + \\alpha^2 \\gamma^2 = (2\\alpha + \\alpha^2) \\gamma^2 \\,. \\end{align*}\\] Problem 24 The region of integration is a triangle with vertices at (0,0), (0,1), and (1,1). (a) The area of the triangle is 1/2, so \\(f_{X_1,X_2}(x_1,x_2) = 2\\). (b) Geometry will not directly help us here; we still have to integrate. The expected value \\(E[X_1]\\) is \\[\\begin{align*} E[X_1] &amp;= \\int_0^1 \\left[ \\int_0^{x_1} 2 x_1 dx_2 \\right] dx_1 = 2 \\int_0^1 x_1 \\left[ \\int_0^{x_1} dx_2 \\right] dx_1 \\\\ &amp;= 2 \\int_0^1 x_1 x_1 dx_1 = 2 \\left.\\frac{x_1^3}{3}\\right|_0^1 = \\frac23 \\,. \\end{align*}\\] (c) We have that Cov[\\(X_1,X_2\\)] = \\(E[X_1X_2] - E[X_1]E[X_2]\\). So we need to compute \\(E[X_1X_2]\\): \\[\\begin{align*} E[X_1X_2] &amp;= \\int_0^1 \\left[ \\int_0^{x_1} 2 x_1 x_2 dx_2 \\right] dx_1 = 2 \\int_0^1 x_1 \\left[ \\int_0^{x_1} x_2 dx_2 \\right] dx_1 \\\\ &amp;= 2 \\int_0^1 x_1 \\left[ \\left.\\frac{x_2^2}{2}\\right|_0^{x_1} \\right] dx_1 = \\int_0^1 x_1 x_1^2 dx_1 = \\left.\\frac{x_1^4}{4}\\right|_0^1 = \\frac14 \\,. \\end{align*}\\] So the covariance is \\(1/4 - (2/3)(1/3) = 9/36 - 8/36 = 1/36\\). Problem 25 (a) The long way to do this involves integration. The short way is to see by inspection that \\(X_1\\) and \\(X_2\\) are independent (the region of integration is “rectangular,” and \\(x_2\\) doesn’t directly appear in \\(f_{X_1,X_2}(x_1,x_2)\\), so \\(f_{X_1,X_2}(x_1,x_2)\\) can be trivially split into two functions \\(g(x_1)h(x_2)\\)), and to see that \\[\\begin{align*} X_1 \\sim {\\rm Exp}(\\beta = 2) ~{\\rm and}~ X_2 \\sim {\\rm Unif}(0,2) \\,. \\end{align*}\\] We know \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1)f_{X_2}(x_2) = (1/\\beta){\\rm exp}(-x_1/2) \\cdot (1/2)\\), so \\(k = 1/(2\\beta) = 1/4\\). (b) The key here is to realize that since \\(X_1\\) and \\(X_2\\) are independent, \\(E[X_1]\\) is just the expected value of \\(f_{X_1}(x_1)\\), i.e., it is the expected value of an exponential distribution with mean 2. So: \\(E[X_1] = \\beta = 2\\). Problem 26 The region of integration is a triangle with vertices (0,0), (2,0), and (1,1). The area of the region of integration is 1, so \\(f_{X_1,X_2}(x_1,x_2) = 1\\). (a) We have that \\[\\begin{align*} f_{X_2}(x_2) &amp;= \\int_{x_2}^{2-x_2} dx_1 = 2(1-x_2) ~~~ x_2 \\in [0,1] \\,. \\end{align*}\\] (b) We have that \\[\\begin{align*} f_{X_1 \\vert X_2}(x_1 \\vert x_2) &amp;= \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \\frac{1}{2(1-x_2)} ~~~ x_1 \\in [x_2,2-x_2] ~~~ x_2 \\in [0,1] \\,. \\end{align*}\\] (c) The new region of integration is a triangle with vertices at (0,0), (1,0), and (1/2,1/2). Since we are dealing with a bivariate uniform, we know that the probability of sampling data from this triangle is the ratio of its area to the area of the total region of integration for the pdf. Thus \\(P(X_1 + X_2 \\leq 1) = (1/2 \\cdot 1 \\cdot 1/2)/1 = 1/4\\). Problem 27 We are given that \\(X \\vert \\mu \\sim N(\\mu,1)\\) and that \\(\\mu \\sim N(0,1)\\). We thus know that \\(E[X \\vert \\mu] = \\mu\\) and \\(V[X \\vert \\mu] = 1\\), while \\(E[\\mu] = 0\\) and \\(V[\\mu] = 1\\). Thus \\[\\begin{align*} V[X] &amp;= V\\left[E[X \\vert \\mu]\\right] + E\\left[V[X \\vert \\mu]\\right] = V[\\mu] + E[1] = 1 + 1 = 2 \\,. \\end{align*}\\] Problem 28 (a) The conditional expected value is \\[\\begin{align*} E[X_1 \\vert X_2=1] &amp;= (x_1 = 0) \\cdot p(x_1=0 \\vert x_2=1) + (x_1 = 1) \\cdot p(x_1=1 \\vert x_2=1) \\\\ &amp;= 1 \\cdot \\frac{p(1,1)}{p_2(1)} = 1 \\cdot \\frac{0.1}{0.3+0.1} = 0.25 \\,. \\end{align*}\\] (b) The conditional variance is \\[\\begin{align*} V[X_1 \\vert X_2=1] &amp;= E[X_1^2 \\vert X_2=1] - (E[X_1 \\vert X_2=1])^2 \\,. \\end{align*}\\] We know the second term. The first term can be derived in a manner similar to above: \\[\\begin{align*} E[X_1^2 \\vert X_2=1] &amp;= \\ldots = 1^2 \\cdot \\frac{p(1,1)}{p_2(1)} = 1 \\cdot \\frac{0.1}{0.3+0.1} = 0.25 \\,. \\end{align*}\\] Thus the conditional variance is \\(V[X_1 \\vert X_2=1] = 0.25 - 0.25^2 = 1/4 - 1/16 = 3/16\\). Problem 29 (a) Because the boundary of the domain is not rectangular (see \\(x_1 + x_2 \\leq 2\\)), \\(X_1\\) and \\(X_2\\) are not independent. (b) We need the distribution to integrate to 1: \\[\\begin{align*} 1 &amp;= \\int_{x_1} \\int_{x_2} c x_1^2 dx_2 dx_1 \\,. \\end{align*}\\] The domain is a triangle with vertices (0,0), (2,0), and (0,2), and thus there is no real advantage gained by utilizing either order of integration, so we’ll keep the integral over \\(x_2\\) as our “inner” integral: \\[\\begin{align*} \\int_0^2 \\int_0^{2-x_1} c x_1^2 dx_2 dx_1 &amp;= c \\int_0^2 x_1^2 \\left( \\int_0^{2-x_1} dx_2 \\right) dx_1 \\\\ &amp;= c \\int_0^2 x_1^2 (2 - x_1) dx_1 \\\\ &amp;= c \\left( 2 \\int_0^2 x_1^2 dx_1 - \\int_0^2 x_1^3 dx_1 \\right) \\\\ &amp;= c \\left( 2 \\left. \\frac{x_1^3}{3}\\right|_0^2 - \\left. \\frac{x_1^4}{4}\\right|_0^2 \\right) \\\\ &amp;= c \\left( \\frac{16}{3} - 4 \\right) = c\\frac{4}{3} \\,. \\end{align*}\\] Hence \\(c = 3/4\\). (c) We actually already derived the marginal distribution above: \\[\\begin{align*} f_{X_1}(x_1) &amp;= c x_1^2 \\int_0^{2-x_1} dx_2 \\\\ &amp;= c x_1^2 (2 - x_1) ~~~ x_1 \\in [0,2] \\,. \\end{align*}\\] (d) The conditional distribution is \\[\\begin{align*} f_{X_2 \\vert X_1}(x_2 \\vert x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \\frac{cx_1^2}{cx_1^2(2-x_1)} = \\frac{1}{2-x_1} ~~~ x_2 \\in [0,2-x_1] \\,. \\end{align*}\\] (e) Given the domain and the flat pdf, we know that \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1)\\) is a “uniform” distribution. Problem 30 (a) We need to determine \\(E[X_1]\\), \\(E[X_2]\\), and \\(E[X_1X_2]\\): \\[\\begin{align*} E[X_1] &amp;= 1 \\cdot (0.1 + 0.5) = 0.6 \\\\ E[X_2] &amp;= 1 \\cdot (0.1 + 0.5) = 0.6 \\\\ E[X_1X_2] &amp;= 1 \\cdot 1 \\cdot 0.5 = 0.5 \\,. \\end{align*}\\] Thus Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2] = 0.5 - 0.6^2 = 0.14\\). (b) Now we need \\(E[X_1^2]\\) and \\(E[X_2^2]\\): \\[\\begin{align*} E[X_1^2] &amp;= 1^2 \\cdot (0.1 + 0.5) = 0.6 \\\\ E[X_2^2] &amp;= 1^2 \\cdot (0.1 + 0.5) = 0.6 \\,. \\end{align*}\\] Thus \\[\\begin{align*} V[X_1] &amp;= E[X_1^2] - (E[X_1])^2 = 0.6 - 0.6^2 = 0.24 \\\\ V[X_2] &amp;= E[X_2^2] - (E[X_2])^2 = 0.6 - 0.6^2 = 0.24 \\,, \\end{align*}\\] and \\(\\sigma_1\\sigma_2 = \\sqrt{V[X_1]V[X_2]} = 0.24\\), and \\(\\rho_{X_1,X_2} = 0.14/0.24 = 7/12\\). (c) We have that \\[\\begin{align*} V[Y] &amp;= a_1^2 V[X_1] + a_2^2 V[X_2] + 2 a_1 a_2 \\mbox{Cov}(X_1,X_2) \\\\ &amp;= 4 \\cdot 0.24 + 1 \\cdot 0.24 - 2 \\cdot 2 \\cdot 1 \\cdot 0.14 \\\\ &amp;= 1.2 - 4 \\cdot 0.14 = 0.64 \\,. \\end{align*}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["preface.html", "Modern Probability and Statistical Inference Illustrated with R Preface", " Modern Probability and Statistical Inference Illustrated with R Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University) June 2023 Preface I have developed this textbook for a new mathematical statistics course sequence for statistics students at Carnegie Mellon University (numbered 36-235 and 36-236), first piloted in Fall 2022 and Spring 2023. The primary difference between it and others that are commonly used in math-stat classes is that I explicitly implement a spiral-learning framework, with concepts that are usually covered in relative isolation elsewhere being repeatedly covered here (such as, e.g., point estimation). My use of a spiral-learning framework is motivated by the observation that many students fixate on mathematics and coding and pay less attention to statistical concepts, with the details of concepts that are seen once often quickly forgotten. The spiral approach is meant to result in enhanced conceptual retention. To build the spiral structure, I have rather radically revised the order in which I cover topics, relative to the order one usually sees in a classic math-stat textbook. That textbook might have one chapter that illustrates the properties of discrete probability distributions using, e.g., the binomial, geometric, and Poisson distributions, then a second chapter that illustrates the properties of continuous distributions such as, e.g., the normal and gamma distributions, with neither chapter showing how these distributions are applied in statistical inference. Here, major distributions are broken out into their own chapters, and within each I revisit fundamental concepts: probability mass and density functions, cumulative distribution functions, statistics, sampling distibutions, point estimation, interval estimation, and hypothesis testing, etc. Then, as I move from chapter to chapter, I cover concepts at greater depth. As a concrete example: when discussing point estimation in Chapter 1, I introduce the concepts of bias, variance, and using the likelihood function to define estimators; when I return to point estimation in Chapter 2, I review these concepts (and derive the MLEs for normal distribution parameters), then add the concepts of consistency, Fisher information, the Cramer-Rao lower bound, and the asymptotic distribution of maximum likelihood estimates. Then in Chapter 3, I add in sufficient statistics and likelihood factorization, along with the minimum variance unbiased estimator. Etc. Another important difference between this textbook and older, more established math-stat textbooks is that I utilize R for coding visualizations, analyses, and simulations. It is expected that this will help students understand concepts more readily; it also allows me to broaden the “problem space” beyond typically used, analytically tractable distributions. (But I note that the newest generation of textbooks often employ enhanced computation…so it is really the spiral structure that makes this textbook fundamentally different.) If you are an instructor, feel free to utilize aspects of this book for your own class(es), but please do not share this document without my expressed consent. If you have comments or questions (or you want an updated version), please send email to the address given below. Peter Freeman pfreeman@cmu.edu August 2023 "],["the-basics-of-probability-and-statistical-inference.html", "1 The Basics of Probability and Statistical Inference 1.1 Data and Statistical Populations 1.2 Sample Spaces and the Axioms of Probability 1.3 Conditional Probability and the Independence of Events 1.4 Further Laws of Probability 1.5 Random Variables 1.6 Probability Distributions 1.7 Characterizing Probability Distributions 1.8 Working With R: Probability Distributions 1.9 Cumulative Distribution Functions 1.10 The Law of Total Probability 1.11 Working With R: Data Sampling 1.12 Statistics and Sampling Distributions 1.13 The Likelihood Function 1.14 Point Estimation 1.15 Statistical Inference with Sampling Distributions 1.16 Confidence Intervals 1.17 Hypothesis Testing 1.18 Working With R: Simulating Statistical Inference", " 1 The Basics of Probability and Statistical Inference 1.1 Data and Statistical Populations Data surround us, in the form of numbers, texts, images, and more that are collected and analyzed across disciplines. Tweets contain data about user sentiments. Receipts contain data about people’s buying habits. Pictures help us differentiate between, e.g., goldfish and dogs. These data\\(-\\)tweets, receipts, pictures\\(-\\)are unstructured data, so-called because we generally cannot visualize or analyze them directly. So what can we do? We can provide structure: to determine if a tweet indicates that a film was liked or disliked, we can extract counts of words indicating sentiments (e.g., “good” and “bad”). To determine the whether an image is that of a goldfish or a dog, it can be passed through appropriate filters that break down the images to a series of analyzable numbers. Etc. The result of all this “pre-processing” is generation of structured data, data in the form of a table in which the columns represent particular measurements (e.g., the number of instances of the word “good”) and the rows representing the objects of study (e.g., individual films). Let’s focus on a single table column. Perhaps its data look like this: 34.1 28.6 37.7 52.1 26.6 28.9 ... Such data are dubbed quantitative data. Quantitative data are numbers that might be discretely valued (e.g., 1, 2, and 3) or continuously valued and measured to arbitrary precision (e.g., 15.4959735). Data may also look like this: Heads Heads Tails Heads ... These data are categorical data; each outcome is one element from a set of categories. Here that set is {Heads,Tails}. An experiment is the act of measuring and recording a datum (like when after each flip of a coin we record \\(H\\) for heads and \\(T\\) for tails). The data we generate from experiments are drawn from populations, the sets of all possible experimental outcomes. A population can be an existing group of objects (e.g., 52 cards in a deck, eight socks of different colors in a drawer), but it can also be hypothetical (e.g., a mathematical function, like a bell curve, which indicates the relative rates at which we would draw samples with particular values). To boil down the discipline of statistics to its essence, our goal is to use the data we have drawn from a population to say something (i.e., to infer something) about the population itself. If we record the heights of 100 people, we would like to say something about the average height of humans. If we record the ice-cream flavor preferences of 500 people, we would like to infer the proportion of humans that prefer chocolate to vanilla. Etc. Data surround us and the possibilities for inference are plentiful. We pictorially summarize what we write above in Figure 1.1. One might immediately notice the word “statistic,” which we have yet to define. As we see later in this chapter, a statistic is simply a function of data (such as their average value) that helps reduce data volume while (hopefully!) retaining sufficient information to allow useful inferences to happen. Defining and understanding useful statistics is a major part of this course! Figure 1.1: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, and use statistics, or functions of the data, to infer population properties. But, the reader says: the course is called Modern Probability and Statistical Inference. Where is probability in all of this? Probability is the so-called “language of statistics,” and it provides the mathematical framework upon which we can build statistical inference. Remember how above we say that a population might be a mathematical function indicating the relative rates of observing experimental outcomes? Those relative rates are probabilities (or at least probability densities). Thus the structure of this chapter (and “mathemtical statistics” courses as a whole): we discuss probability first, and then use our newfound knowledge to show how the enterprise of statistical inference works, both algorithmically and mathematically. 1.2 Sample Spaces and the Axioms of Probability Probability is the long-term frequency of the occurrence of an event. For instance, what is the probability of flipping a coin and observing heads? (Intuitively, this probability is 1/2, if the coin is fair.) Or: what is the probability that a student finishes a particular test in between 30 and 40 minutes? Etc. To build up an understanding of probability, it is conventional to start with the concept of a sample space. A sample space is the set of all possible outcomes of an “experiment” (or “trial”), which is simply some process that can, in theory, be repeated an infinite number of times. (For instance, the flipping of a coin.) For instance, if our experiment is to flip a single coin twice, the sample space would be \\[ \\Omega = \\{HH,HT,TH,TT\\} \\,, \\] where \\(H\\) and \\(T\\) represent observing heads and tails, respectively. (The Greek letter \\(\\Omega\\) is a capital “omega,” or “oh-MAY-gah.”) See Figure 1.2. Figure 1.2: This is an example of a sample space \\(\\Omega\\), representing the experimental outcomes of flipping a single coin twice and recording the observed side of the coin. For purposes of intuition, it is common to associate the area shown for each outcome with that outcome’s probability of occurrence, so here we may view the coin as an unfair one that favors tails. The members of the set \\(\\Omega\\) are dubbed events and they come in two varieties: simple events: specific experimental outcomes (e.g., \\(HH\\)); any two simple events in \\(\\Omega\\) are mutually exclusive, or disjoint, as they cannot be observed simultaneously in a single experiment. compound events: sets of two or more simple events (e.g., \\(\\{HH,HT,TH\\}\\), which represents the set of outcomes where heads was observed at least once). As stated above, a sample space is a set of possible experimental outcomes; thus we can apply set notation to, e.g., define specific events as functions of others: term notation intuitive terminology superset \\(A \\supset B\\) “encompasses” subset \\(A \\subset B\\) “within” union \\(A \\cup B\\) “or” intersection \\(A \\cap B\\) “and” complement \\(\\bar{A}\\) “not” We show examples of how we use set notation in the context of samples spaces below. Here are a few more things to keep in mind regarding sample spaces: The number of simple events in \\(\\Omega\\) (i.e., the set’s cardinality) may be finite (as in the example above) or either countably or uncountably infinite (e.g., the set of all non-negative integers versus the set of real numbers). (For instance, the simple events in the experiment of repeating a task until one fails are \\(\\{F,SF,SSF,SSSF,\\ldots\\}\\), where \\(S\\) denotes success and \\(F\\) denotes failure.) The definition of a sample space can depend upon whether the order of outcomes matters. For instance, if the order of outcomes does not matter, we could rewrite our two-coin-flip sample space as \\(\\Omega = \\{HH,HT,TT\\}\\) (or \\(\\{HH,TH,TT\\}\\)). At no point thus far have we indicated the probability of observing any simple event. It is not the case in general that each experimental outcome is equally likely! Regarding the last point above: while we may not know the probability of observing any simple event, there are some things we can say about its long-term relative frequency of occurrence: it must be \\(&gt; 0\\) and \\(\\leq 1\\); the relative frequencies of all simple events in \\(\\Omega\\) must sum to 1; and the relative frequency of a compound event must equal the sum of the relative frequencies of its component simple events. These statements appear to be self-evident, and as thus may be dubbed axiomatic. (A mathematical axiom is a statement accepted without proof.) In probability theory, these statements were recast as the so-called Kolmogorov axioms, introduced by Andrey Kolmogorov in 1933. Let \\(A\\) denote an event within \\(\\Omega\\) (i.e., \\(A \\subset S\\)), either simple or compound. A probability measure on \\(\\Omega\\) is a function \\(P\\) from subsets of \\(\\Omega\\) to \\(\\mathbb{R}^n\\) that satisfies the following: \\(P(A) \\in [0,1]\\); \\(P(\\Omega) = 1\\); and if \\(\\{B_1,\\ldots,B_k\\}\\) is a set of mutually exclusive simple or compound events, then \\(P(\\bigcup_{i=1}^k B_i) = \\sum_{i=1}^k P(B_i)\\), where the symbol \\(\\bigcup\\) refers to the union of the set of events, i.e., the combination of all the events in the set into a single compound event. 1.2.1 Utilizing Set Notation Let’s suppose that we have in our hand one six-sided die, with faces numbered 1 through 6. We roll it once, and observe the value on the uppermost face. The sample space is \\[ \\Omega = \\{1,2,3,4,5,6\\} \\,. \\] Let the event \\(A\\) be all odd-numbered outcomes, and let the event \\(B\\) be all outcomes less than 4. Thus \\(A = \\{1,3,5\\}\\) and \\(B = \\{1,2,3\\}\\). What is \\(\\bar{A}\\), the complement of \\(A\\)? It is the set of all outcomes not in \\(A\\), i.e., the set of all even-numbered faces: \\(\\bar{A} = \\{2,4,6\\}\\). What is \\(A \\cup B\\), the union of \\(A\\) and \\(B\\)? It is the set of all outcomes observed in either \\(A\\) or \\(B\\), without double counting: \\(A \\cup B = \\{1,2,3,5\\}\\) (and not \\(A \\cup B = \\{1,1,2,3,3,5\\}\\)…it is meaningless to write out the same experimental outcome twice). What is \\(A \\cap B\\), the intersection of \\(A\\) and \\(B\\)? It is the set of all outcomes observed in both \\(A\\) and \\(B\\), again without double counting: \\(A \\cap B = \\{1,3\\}\\). We note that we can combine unions, intersections, and complements in, e.g., the distributive law, \\[ A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C) \\,, \\] the associative law, \\[ A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) \\,, \\] and De Morgan’s laws, \\[ \\overline{A \\cup B} = \\bar{A} \\cap \\bar{B} ~~\\mbox{and}~~ \\overline{A \\cap B} = \\bar{A} \\cup \\bar{B} \\,. \\] 1.2.2 Working With Contingency Tables Let’s assume that we are given the following information about two events \\(A\\) and \\(B\\): \\(A\\) \\(\\bar{A}\\) \\(B\\) 0.45 0.12 \\(\\bar{B}\\) 0.21 0.22 This is dubbed a contingency table (or, more specifically here, a two-by-two contigency table). The numbers in each cell represent probabilities; for instance, \\(P(A \\cap B) = 0.45\\). A contingency table is appropriate to work with if the probabilities associated with each event do not change from experiment to experiment. What is \\(P(A)\\)? We can determine this by summing down the \\(A\\) column: \\(P(A) = P[A \\cap \\Omega] = P[A \\cap (B \\cup \\bar{B})] = P[(A \\cap B) \\cup (A \\cap \\bar{B})]\\); since \\(A \\cap B\\) and \\(A \\cap \\bar{B}\\) are disjoint, we can view the \\(\\cup\\) as addition, and so \\(P(A) = P(A \\cap B) + P(A \\cap \\bar{B}) = 0.45 + 0.21 = 0.66\\). What is \\(P(A \\cup B)\\)? Utilizing De Morgan’s laws, this would be \\(1 - P(\\overline{A \\cup B}) = 1 - P(\\bar{A} \\cap \\bar{B}) = 1 - 0.22 = 0.78\\). What is the probability of observing \\(A\\) or \\(B\\), but not both? This would be \\(P(A \\cup B) - P(A \\cap B)\\), which is \\(0.78 - 0.45 = 0.33\\). It is well worth taking the time to see how one could derive each of these answers through visual inspection of the table. For instance, \\(P(A \\cup B)\\) is 1 minus the value in the cell at lower right, which does not lie in the row for \\(B\\) or the column for \\(A\\). 1.3 Conditional Probability and the Independence of Events Intuitively, we can picture a sample space \\(\\Omega\\) and two of its constituent events as looking something like what we show in Figure 1.3. Figure 1.3: A sample space with non-disjoint events \\(A\\) and \\(B\\). We can imagine that the geometric areas of each region represent probability, with \\(P(\\Omega) = 1\\) (given the second Kolmogorov axiom) and \\(P(A \\cap B) &gt; 0\\) being the probability that both \\(A\\) and \\(B\\) occur during an experiment. (Perhaps \\(A\\) is the event of speaking French and \\(B\\) is the event of living is Brussels. The symbol \\(\\cap\\) denotes the intersection or overlap between two sets, whereas the analogous symbol \\(\\cup\\) represents the union of two sets.) We can use this intuitive picture to illustrate the concept of conditional probability. A stated event probability, such as \\(P(A)\\), is an unconditional probability: its occurrence does not depend on whether or not other events occur. To denote a conditional probability, we add a vertical bar and place the conditions to the right of it. For instance, \\(P(A \\vert B)\\) denotes the probability that the event \\(A\\) is observed, given that the event \\(B\\) is observed. (Note that there is no implied causality: it is not necessarily the case that \\(B\\) occurring is “causing” changes to the probability that \\(A\\) will occur.) To illustrate why \\(P(A)\\) may not equal \\(P(A \\vert B)\\), we first point out that \\(P(A) = P(A \\vert \\Omega)\\), which we may think of as “the probability of observing the event \\(A\\) if we observe the event \\(\\Omega\\),” which is the ratio of geometric areas of \\(A \\cap \\Omega\\) and \\(\\Omega\\): \\[ P(A) = P(A \\vert \\Omega) = \\frac{P(A \\cap \\Omega)}{P(\\Omega)} \\,. \\] When we condition on the event \\(B\\), we are reducing the set of possible outcomes from the full sample space \\(\\Omega\\) to \\(B\\), i.e., we are replacing \\(\\Omega\\) in the expression above with \\(B\\): \\[ P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)} \\,, \\] In the context of our intuitive picture, we are changing the one shown above to the one we show in Figure 1.4. Figure 1.4: The new sample space that arises when we condition on the event \\(B\\). \\(B\\) thus defines a new “sample space.” Two events \\(A\\) and \\(B\\) are independent if the probability of observing one does not depend on the probability of observing the other. The intuitive picture many have of independence is the one shown in Figure 1.5. Figure 1.5: To many, \\(A\\) and \\(B\\) appear to be independent events…but they are simply disjoint. The events \\(A\\) and \\(B\\) do not overlap…hence they are independent events, right? No: they are simply disjoint events. Also, with reflection, we realize that if, e.g., the event \\(A\\) is observed in a given experiment, then we know that \\(B\\) cannot be observed. So these events are very much dependent! Figure 1.6 shows how we can actually represent \\(A\\) and \\(B\\) as independent events. In this figure, the ratio of the geometric area associated with the event \\(A\\) to the geometric area of \\(\\Omega\\) is equal to the ratio of the areas of \\(A \\cap B\\) and \\(B\\). Thus we can write that \\[ P(A) = P(A \\vert \\Omega) = \\frac{P(A \\cap \\Omega)}{P(\\Omega)} = \\frac{P(A \\cap B)}{P(B)} = P(A \\vert B) \\,. \\] The probability of observing the event \\(A\\) is unchanged if the event \\(B\\) occurs: \\(A\\) and \\(B\\) are independent events. Figure 1.6: \\(A\\) and \\(B\\) are independent events. 1.3.1 Visualizing Conditional Probabilities: Contingency Tables Let’s recall the two-by-two contingency table we defined in the previous section, but with some additional information added: \\(A\\) \\(\\bar{A}\\) \\(B\\) 0.45 0.12 0.57 \\(\\bar{B}\\) 0.21 0.22 0.43 0.67 0.33 The numbers in the so-called “margins” are the row and column sums, so, for instance, \\(P(A) = 0.67\\). This information is useful to have when computing conditional probabilities. In analogy with what was stated above about imposing conditions and what that does to the sample space, here we can say that imposing a condition will restrict us to a given row or a given column. For instance, what is \\(P(\\bar{A} \\vert B)\\)? The condition restricts us to the top row, and within that row, the probability of observing the event \\(\\bar{A}\\) is 0.12/0.57 = 0.21. So \\(P(\\bar{A} \\vert B) = P(\\bar{A} \\cap B) / P(B) = 0.21\\). Now, are \\(A\\) and \\(B\\) independent events? The easy way to visually infer this given a two-by-two table is to see if the rows (or columns) are multiples of each other…meaning, here, is there a number \\(a\\) such that \\(0.45 = 0.21 a\\) and \\(0.12 = 0.22 a\\)? The answer here is no…so the events \\(A\\) and \\(B\\) are dependent events. (The conventional, yet longer way to determine independence is to see if, e.g., \\(P(A \\vert B) = P(A)\\); if so, \\(A\\) and \\(B\\) are independent.) 1.3.2 Conditional Independence If \\(A\\) and \\(B\\) are independent events, is it automatically the case that the events \\(A \\vert C\\) and \\(B \\vert C\\) are also independent events? Recall the figure above that shows how independent events appear in a Venn diagram. Recall also that if we impose a condition \\(C\\), we effectively change the sample space from \\(\\Omega\\) to \\(C\\). Imagine \\(C\\) as an arbitrarily shaped region superimposed on the last figure, so that now we have a situation like the one in Figure 1.7. Figure 1.7: \\(A\\) and \\(B\\) are not necessarily independent events, given \\(C\\). The events \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if \\(P(C) &gt; 0\\) and \\(P(A \\cap B \\vert C) = P(A \\vert C) P(B \\vert C)\\). As we can see in Figure 1.7, \\(C\\) can be made to overlap \\(B\\), \\(A\\), and \\(A \\cap B\\) in any number of ways such that \\(P(A \\cap B \\vert C) \\neq P(A \\vert C) P(B \\vert C)\\)…so it is not the case that if \\(A\\) and \\(B\\) are independent, \\(A \\vert C\\) and \\(B \\vert C\\) are always independent. (If \\(C\\) had a rectangular shape with a horizontal base and top and vertical sides, conditional independence would hold. Think through why this would be true…) 1.4 Further Laws of Probability Now that we have learned about the concepts of conditional probabilities and independence, we can write down some useful laws that one can use to solve an array of probability-based problems. Multiplicative Law. This follows simply from rearranging the definition of conditional probability: \\[ P(A \\cap B) = P(A) P(B \\vert A) = P(B) P(A \\vert B) \\] We can generalize this law given an arbitrary number of events \\(k\\): \\[ P(A_1 \\cap A_2 \\cap \\cdots \\cap A_k) = P(A_1 \\vert A_2 \\cap \\cdots \\cap A_k) P(A_2 \\cap \\cdots \\cap A_k) = \\cdots = P(A_k) \\prod_{i=1}^{k-1} P(A_i \\vert A_{i+1} \\cap \\cdots \\cap A_k) \\,, \\] where \\(\\prod\\) is the product symbol, the multiplicative analogue to the summation symbol \\(\\sum\\). Additive Law. The probability of the union of two events \\(A\\) and \\(B\\) is \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\,. \\] If the events \\(A\\) and \\(B\\) are not disjoint, then if we add their probabilities, we count the probability of \\(A \\cap B\\) twice…hence the subtracted term. Law of Total Probability (LoTP). Assume that we partition the sample space \\(\\Omega\\) into \\(k\\) disjoint (simple or compound) events \\(\\{B_1,\\ldots,B_k\\}\\), all of which have non-zero probability of occurring. Then, given any event \\(A\\), we can write \\[ P(A) = \\sum_{i=1}^k P(A \\vert B_i) P(B_i) \\,. \\] Bayes’ Rule. Continue to assume that the sample space is partitioned into the events \\(\\{B_1,\\ldots,B_k\\}\\). The conditional probability of each of these events, given that \\(A\\) occurs, is \\[ P(B_i \\vert A) = \\frac{P(A \\vert B_i) P(B_i)}{\\sum_{j=1}^k P(A \\vert B_j)P(B_j)} = \\frac{P(A \\vert B_i)P(B_i)}{P(A)} \\,. \\] 1.4.1 The Additive Law for Independent Events Let’s assume that for a given experiment, we can define the independent events \\(A\\) and \\(B\\), with \\(P(A) = 0.6\\) and \\(P(B) = 0.4\\). What is \\(P(A \\cup B)\\)? In general, when solving probability problems, we look at all the rules and relationships at our disposal and see which one (or more!) contains the probabilities we know and the one we don’t know, and we use that rule or relationship to derive the solution. Here, there is nothing that directly relates \\(A\\) and \\(B\\) to \\(A \\cup B\\)…the events may overlap when represented on a Venn diagram, and we don’t know by how much. Except…we are given the word “independent.” That allows us to say that \\(P(A \\cap B) = P(A)P(B)\\)…and now we know that the additive law is in play: \\[\\begin{align*} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;= P(A) + P(B) - P(A \\vert B)P(B) \\\\ &amp;= P(A) + P(B) - P(A)P(B) = 0.6 + 0.4 - 0.6 \\cdot 0.4 = 0.76 \\,. \\end{align*}\\] Is this the only way to solve the problem? No…we know from De Morgan’s laws that \\(\\overline{A \\cup B} = \\bar{A} \\cap \\bar{B}\\), and thus \\[\\begin{align*} P(A \\cup B) &amp;= 1 - P(\\overline{A \\cup B}) = 1 - P(\\bar{A} \\cap \\bar{B}) = 1 - P(\\bar{A} \\vert \\bar{B})P(\\bar{B}) \\\\ &amp;= 1 - P(\\bar{A})P(\\bar{B}) = 1 - (1-0.6)(1-0.4) = 0.76 \\,. \\end{align*}\\] There is no “right” way to solve a probability problem…just correct ones. 1.4.2 The Monty Hall Problem Let’s Make a Deal is a game show that has appeared on television at various times since 1963. During one part of the show, contestants are brought on stage and presented with three closed doors; behind one is an expensize prize (say, a car or an around-the-world cruise), and behind the other two are inexpensive prizes (like a year’s supply of Turtle Wax). The contestant is asked to pick a door (say, Door #1), at which point the show’s host will open another door (say, Door #3) and show the inexpensive prize behind that door (thereby taking that door out of play). The contestant is then asked if they want to stick with the door they’ve chosen (here, Door #1), or switch their choice to the other unopened door (here, Door #2). What should we advise the constestant to do? The original, and most famous, host of Let’s Make a Deal was Monty Hall. Hence: the Monty Hall Problem. (Note that the problem is often stated such that there is a car being behind one door and goats behind the other two. The author is old enough to have seen the show in its heyday and he recalls seeing no goats. Or maybe they made no impression at the time…) Assume, without loss of generality, that Door #1 is chosen. Then, let \\(O_i\\) = “Monty Hall opens Door #\\(i\\)” \\(C_i\\) = “The car is behind Door #\\(i\\)” and assume that \\(P(C_i) = 1/3\\) for all \\(i\\). (The car could have been placed behind any door before the show was filmed.) The sample space of experimental outcomes is \\[ \\Omega = \\{ O_2 \\cap C_1 , O_2 \\cap C_3 , O_3 \\cap C_1 , O_3 \\cap C_2\\} \\,. \\] Why not \\(O_2 \\cap C_2\\) and \\(O_3 \\cap C_3\\)? Monty is not stupid: he won’t open the door the car is behind. (He knows where it is!) Let’s assume, again without any loss of generality, that Monty opens Door #3. The probability we want to compute is \\(P(C_2 \\vert O_3)\\): what is the probability that the car is actually behind Door #2? (Note that this is \\(1 - P(C_1 \\vert O_3)\\)…again, \\(P(C_3 \\vert O_3) = 0\\), as Monty is not stupid.) Is this probability 1/2? We utilize Bayes’ rule and the LoTP to write \\[ P(C_2 \\vert O_3) = \\frac{P(O_3 \\vert C_2) P(C_2)}{P(O_3)} = \\frac{P(O_3 \\vert C_2) P(C_2)}{P(O_3 \\vert C_2) P(C_2) + P(O_3 \\vert C_1) P(C_1)} = \\frac{P(O_3 \\vert C_2)}{P(O_3 \\vert C_2) + P(O_3 \\vert C_1)}\\,. \\] What do we know? \\(P(O_3 \\vert C_2) = 1\\): if the car is behind Door #2, Monty has to open Door #3 \\(P(O_3 \\vert C_1) = 1/2\\): Monty can open either Door #2 or #3 if the car is behind Door #1 Hence \\[ P(C_2 \\vert O_3) = \\frac{1}{1 + 1/2} = \\frac{2}{3} \\,. \\] We should advise the contestant to open Door #2! Confused? Think about the solution this way: the contestant has a one-third chance of correctly picking the door the car is behind, and a two-thirds chance of being wrong. Opening one of the other doors (while knowing there is no car behind it) doesn’t change these conditions at all: the contestant still has a one-third chance of having initially picked the correct door. Thus the contestant should change their pick to the other unopened door. 1.4.3 Visualizing Conditional Probabilities: Tree Diagrams In the previous sections, we show how one can use contingency tables to aid the visualization of probabilities (and to solve for probabilities of simple and/or compound events). Here we show another, somewhat more general probability visualizer: the tree diagram. Why “somewhat more general”? First, a tree in a tree diagram can have arbitrary depth: if we have events \\(A\\), \\(B\\), and \\(C\\), the table would be three-dimensional, with the axes representing the experimental outcome in terms of \\(A\\) and \\(\\bar{A}\\), \\(B\\) and \\(\\bar{B}\\), and \\(C\\) and \\(\\bar{C}\\). A table is not an optimal means to represent probabilities. And second, a tree is arguably a more natural means to represent probabilities when an experiment represents sequential outcomes, particularly when we sample without replacement. Let’s elaborate on that second point. Let’s say we have a drawer with five socks, three of which are red and two of which are blue. We plan to draw three socks in succession from the drawer without placing the socks back into the drawer, but we will stop early if we draw two socks of the same color on the first two draws. What is the probability that our final sample contains two blue socks? We can write out the following: if \\(B_i\\) and \\(R_i\\) are the probabilities of drawing a blue and red sock from the drawer when taking out the \\(i^{\\rm th}\\) sock, then \\(P(B_1) = 2/5\\) and \\(P(R_1) = 3/5\\)…and \\(P(B_2 \\vert B_1) = 1/4\\) because there is one less blue sock in the drawer, and… Actually, this gets tiring quickly. Let’s use a tree diagram instead. Figure 1.8: An example of visualizing probabilities using a decision tree. In Figure 1.8, we show the tree diagram for this problem. We note some aspects of this diagram: the tree can be truncated along some branches (here, that’s because we stop removing socks from the drawer if we remove two of the same color in the first two draws); at any branching point, the (conditional) probabilities of going down each branch sum to one; and the probability of ending up at a particular leaf (where the leaves collectively represent the simple events of the experiment) is the product of all the branch probabilities leading to that leaf. So, now, what is the probability of drawing two blue socks in this experiment? To find that, we determine which leaves are associated with drawing two blue socks; from the top, that would be leaves 1, 2, and 4, with probabilities 1/10, 1/10, and 1/10. Because simple events are disjoint by definition, the probability of the compound event is simply the sum of the probabilities of the simple events, which here is 3/10. In any given replication of this experiment, we have a 30% chance of ending up with two blue socks. 1.5 Random Variables Let’s say that we perform an experiment in which we flip a fair coin three times. Let \\(H\\) denote observing heads, and \\(T\\) tails. The sample space of outcomes \\(\\Omega\\) is \\[ \\{ HHH,HHT,HTH,THH,HTT,THT,TTH,TTT\\} \\] and each outcome is observed with probability 1/8. What is the probability of observing exactly one tail? We can determine this by laboriously generating a table of probabilities, like so: \\[\\begin{align*} P(\\mbox{``no tails&#39;&#39;}) &amp;= P(HHH) = 1/8 \\\\ P(\\mbox{``one tail&#39;&#39;}) &amp;= P(HHT \\cup HTH \\cup THH) = 3/8 \\\\ P(\\mbox{``two tails&#39;&#39;}) &amp;= P(HTT \\cup THT \\cup TTH) = 3/8 \\\\ P(\\mbox{``three tails&#39;&#39;}) &amp;= P(TTT) = 1/8 \\,. \\end{align*}\\] One can easily imagine how, if we were to flip a coin 50 times, or 500 times, the generation of tables would be onerous. A better way to portray the information in a sample space is to use a random variable. In probability theory, a random variable \\(X\\) is a measurable function mapping from a set of outcomes (here, \\(\\Omega\\)) to a measurable space (here, \\(\\mathbb{R}^n\\), where \\(\\mathbb{R}^1 = \\mathbb{R}\\) is the real-number line). (See Figure 1.9.) While \\(X\\) is a function, it is natural in an undergraduate context to think of it as a variable whose value is an experimental outcome. For instance, if we define \\(X\\) as being “the number of tails observed in three flips of a fair coin,” then \\(P(X=1) = 3/8\\). (Below, we will complete our transition away from laboriously built probability tables by introducing mathematical functions\\(-\\)probability mass functions or probability density functions associated with distributions\\(-\\)that allow us to compute probabilities more generally, as a function of an arbitrary observed value \\(X=x\\) or a range of observed values \\(X \\in [a,b]\\).) Figure 1.9: A random variable is a function that maps events in \\(\\Omega\\) to the real-number line \\(\\mathbb{R}\\). There are a few initial things to note about random variables. First, they are conventionally denoted with capital Latin letters (e.g., \\(X\\), \\(Y\\), \\(Z\\)). Second, note the words “if we define” above. There is no unique random variable associated with a sample space. In addition to \\(X\\), we could just as easily have defined \\(Y\\) as the number of heads observed, or \\(Z\\) as having value 0 if at least one head and at least one tail are observed, and 1 otherwise, etc. Third, and most important, is that random variables come in two types, discrete and continuous: A discrete random variable \\(X\\) maps the sample space \\(\\Omega\\) to countably finite (e.g., \\(\\{0,1\\}\\)) or infinite (e.g., \\(\\{0,1,\\ldots,\\}\\)) outcomes. A continuous random variable \\(X\\) maps the sample space \\(\\Omega\\) to an outcome that is uncountably infinite (e.g., \\([0,1]\\) or \\([0,\\infty)\\)). 1.6 Probability Distributions A probability distribution is a mapping \\(P: \\Omega \\rightarrow \\mathbb{R}^n\\) that describes how probabilities are distributed across the values of a random variable. (A random variable simply maps events in a sample space to a measurable space like the real-number line, without regard to the probability of the event. A distribution adds this additional layer of information.) There are different ways to mathematically define a distribution; here, we concentrate upon the probability mass function (or pmf): if \\(X\\) is a discrete random variable, this represents the probability that \\(X\\) takes on a particular value \\(x\\), i.e., \\(p_X(x) = P(X = x)\\); or the probability density function (or pdf): if \\(X\\) is a continuous random variable, this represents the probability density (think of this as the “probability per unit interval”) at the value \\(x\\), i.e., \\(f_X(x)\\). To be clear, we can represent a given distribution with a pmf or a pdf, but not both simultaneously; the choice is dictated by whether \\(X\\) is discretely or continuously valued. (It is possible to mix probability masses and densities into a single distribution, however. See the example below.) Later, we introduce two alternatives to pmfs/pdfs: the cumulative distribution function (cdf), and the moment-generating function (mgf). Probability mass and density functions have two fundamental constraints: (a) they are non-negative; and (b) they sum or integrate to 1: pmf pdf \\(p_X(x) \\in [0,1]\\) \\(f_X(x) \\in [0,\\infty)\\) \\(\\sum_x p_X(x) = 1\\) \\(\\int_x f_X(x) dx = 1\\) Before continuing on to discussing properties of distributions, we reiterate the point that one cannot interpret a pdf \\(f_X(x)\\) as the probability of sampling the value \\(x\\)! It is, again, a probability density function and not a probability itself; to determine a probability, we utilize integration: \\[ P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx \\,. \\] To drive home the point that a pdf does not itself represent probability, we note that for any value \\(a\\), \\[ P(X = a) = \\int_a^a f_X(x) dx = 0 \\,. \\] 1.6.1 A Simple Probability Density Function Let’s assume that we have defined the following pdf: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} 2x &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We visualize this pdf in Figure 1.10. Figure 1.10: The probability density function \\(f_X(x) = 2x\\), for \\(0 \\leq x \\leq 1\\). This pdf helps to illustrate many of the points made above. Note that it is (a) non-negative and although its maximum value is \\(&gt; 1\\), (b) it integrates &gt; to 1. (We need not actually integrate here, as geometry is sufficient: the area under the curve is 1/2 \\(\\times\\) 1 \\(\\times\\) 2 = 1.) How would one interpret this pdf? Where its value is larger, we are more likely to sample data. Full stop. What is the probability of sampling a datum between 0 and 1/2? Again, we can use geometry and see that the area under the curve is 1/2 \\(\\times\\) 1/2 \\(\\times\\) 1 = 1/4. (Which means the probability of sampling a datum between 1/2 and 1 must be \\(1 - 1/4 = 3/4\\).) Let’s extend this example a bit by adding a condition. For instance, what is the probability of sampling a datum between 1/4 and 1/2, given that we sample a datum between 0 and 3/4? In analogy with how we worked with conditional probabilities above, we can write that \\[ P(1/4 \\leq X \\leq 1/2 \\, \\vert \\, 0 \\leq X \\leq 3/4) = \\frac{P(1/4 \\leq X \\leq 1/2 \\cap 0 \\leq X \\leq 3/4)}{P(0 \\leq X \\leq 3/4)} = \\frac{P(1/4 \\leq X \\leq 1/2)}{P(0 \\leq X \\leq 3/4)} \\,. \\] (How does this differ from computing the unconditional probability \\(P(1/4 \\leq X \\leq 1/2)\\)? Technically, it does not…we could write out a similar expression to the one above. But we note that the denominator would be \\(P(0 \\leq X \\leq 1) = 1\\) and thus it would “go away.”) Using geometrical arguments, we should be able to convince ourselves that the answer we seek is 1/3. One last point we will make here is that for a continuous distribution, it is meaningless to compute \\(P(X = a)\\). For instance: \\[ P\\left(X = \\frac{1}{2}\\right) = \\int_{1/2}^{1/2} 2 x dx = \\left. x^2 \\right|_{1/2}^{1/2} = \\frac{1}{4} - \\frac{1}{4} = 0 \\,. \\] What are we to make of this? Recall that a pdf is a probability density function, and that one can think of it as having units of probability per unit interval…so one has to integrate the pdf over an interval of length greater than zero to derive a non-zero probability value. 1.6.2 Shape Parameters and Families of Distributions In the previous example, the stated pdf was the stated pdf: there was no means by which to change its shape. We can generalize it by utilizing a shape parameter: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] where in the previous example, \\(\\theta = 2\\). It is conventional to denote a population parameter or a set of such parameters with the Greek letter \\(\\theta\\) (theta, pronounced “thay-tah”). Here, \\(\\theta\\) represents a single, constant parameter whose value is \\(&gt; 0\\). (If \\(\\theta\\) were negative, for instance, \\(f_X(x \\vert \\theta)\\) would be \\(&lt; 0\\), which is not allowed!) \\(f_X(x \\vert \\theta)\\), with \\(\\theta \\in \\Theta = (0,\\infty)\\), is perhaps confusingly dubbed a family of distributions. (One might think that a family would refer to a set of different mathematical forms for pdfs, like \\(\\theta x^{\\theta-1}\\) and \\(e^{-x/\\theta}/\\theta\\), etc., but it actually refers to the fact that \\(\\theta\\) can take on more than one value, yielding a family of shapes as illustrated in Figure 1.11.) Figure 1.11: Examples of the family of pdfs \\(f_X(x \\vert \\theta) = \\theta x^{\\theta-1}\\) for \\(0 \\leq x \\leq 1\\), with parameters \\(\\theta =\\) 1/2 (red), 1 (blue), and 2 (green). 1.6.3 A Simple Probability Mass Function Let’s play a game: we throw a dart at a board that has ten numbers on it, 1 through 10. Assume that we are guaranteed to hit the board, and that the regions associated with each number have the exact same size. If we hit an even number, we get 0 points, while if we hit an odd number, we get 2 points. Furthermore, if we hit a prime number, we get a bonus of 1 point. What is the probability mass function for the number of points we will score given a single throw of the dart? If we hit the 4, 6, 8, or 10, we get 0 points. (2 is prime, so we’d get a bonus of 1 point by hitting that.) If we hit the 9, we get 2 points, and if we hit the 1, 3, 5, or 7, we get 3 points. Hence the probability mass function is \\(x\\) \\(p_X(x)\\) 0 4/10 1 1/10 2 1/10 3 4/10 We see that this pmf is (a) non-negative and (b) has values \\(p_X(x)\\) that lie between 0 and 1. Because we are dealing with masses and not densities, probability calculations involve summations (while taking care to note whether one or both limits of summation lie at a mass, and if so, whether or not the inequality is, e.g., \\(&gt;\\) or \\(\\geq\\)). For instance, what is the probability of achieving a score greater than 1 point? \\(P(X &gt; 1) = p_X(2) + p_X(3) = 1/2\\). What about a score of 3 points, given a score greater than 0 points? \\[ P(X = 3 \\vert X &gt; 0) = \\frac{P(X = 3 \\cap X &gt; 0)}{P(X &gt; 0)} = \\frac{P(X = 3)}{P(X &gt; 0)} = \\frac{p_X(3)}{p_X(1)+p_X(2)+p_X(3)} = \\frac{4}{1+1+4} = \\frac{2}{3} \\,. \\] 1.6.4 A More Complex Example Involving Both Masses and Densities There is no reason why masses and densities cannot be combined into a single probability distribution. For instance, perhaps we have the following: \\[ h_X(x) = \\left\\{ \\begin{array}{cc} 1/2 &amp; x \\in [0,1] \\\\ 1/2 &amp; x = 2 \\end{array} \\right. \\,. \\] There is nothing special about this function; the mathematics of probability calculations is just a tad more complicated than before. For instance, what is the probability of sampling a value greater than 3/4? \\[ P(X &gt; 3/4) = \\int_{3/4}^1 h_X(x) dx + h_X(2) = \\frac{1}{2} \\left. x \\right|_{3/4}^1 + \\frac{1}{2} = \\frac{1}{8} + \\frac{1}{2} = \\frac{5}{8} \\,. \\] Integrate over the domain(s) where densities are defined and sum over the domain(s) where masses are defined. Done! 1.7 Characterizing Probability Distributions A probability distribution represents the rates of occurrence of different experimental outcomes. Can we determine an “average” outcome? In other words, can we determine what value to expect when we next run the experiment? The answer is yes: this is the expected value of a random variable (or expectation) and it is the weighted average of all possible experimental outcomes: \\[\\begin{align*} E[X] &amp;= \\frac{\\sum_x x p_X(x)}{\\sum_x p_X(x)} = \\sum_x x p_X(x) ~~ \\mbox{(discrete r.v.)} \\\\ &amp;= \\frac{\\int_x x f_X(x) dx}{\\int_x f_X(x) dx} = \\int_x x f_X(x) dx ~~ \\mbox{(continuous r.v.)} \\,. \\end{align*}\\] In each case, the denominator disappears because it equals 1, by definition. Note that Greek letter \\(\\mu\\) (mu, pronounced “myoo”), which conventionally denotes the mean value of a pdf or pmf, is also sometimes used interchangeably with \\(E[X]\\). See Figure 1.12. It is important here to note the following: The input to the expected value operator is (usually!) a random variable, so that input is capitalized. In other words, we always write \\(E[X]\\) and not \\(E[x]\\). (\\(x\\) is just a coordinate on the real-number line…its expected value is simply \\(x\\) itself. See “Expected Value Tricks” in the examples below.) The expected value is a constant; it is not random! For any given pmf or pdf, the average value of a sampled datum does not change from experiment to experiment…there is no randomness. Now, because the expected value is simply a weighted average, we can write down a more general expression for it: \\[\\begin{align*} E[g(X)] &amp;= \\frac{\\sum_x g(x)p_X(x)}{\\sum_x p_X(x)} = \\sum_x g(x) p_X(x) ~~ \\mbox{(discrete r.v.)} \\\\ &amp;= \\frac{\\int_x g(x)f_X(x) dx}{\\int_x f_X(x) dx} = \\int_x g(x) f_X(x) dx ~~ \\mbox{(continuous r.v.)} \\,. \\end{align*}\\] This has been dubbed the “Law of the Unconscious Statistician” (e.g., Ross 1988, as noted by Casella &amp; Berger 2002) due to the fact that we all think of it a definition…and not the result of a theorem. A probability distribution may have an extended domain (e.g., \\([0,\\infty)\\)) but often the probability mass or density is concentrated in a relatively small interval. A metric that represents the square of the “width” of that interval is the variance, which is defined as \\[ V[X] = \\sigma^2 = E[(X-\\mu)^2] = E[X^2] - (E[X])^2 \\,. \\] The “width” itself\\(-\\)the square root of the variance\\(-\\)is dubbed the standard deviation and is denoted with the Greek letter \\(\\sigma\\) (“sigma,” pronounced “SIG-muh”). Note that because the variance is the expected value of a squared quantity, it is always non-negative. (And like the expected value, it is a constant.) See Figure 1.12. Figure 1.12: Examples of a probability mass function (left) and a probability density function (right), with the expected values \\(E[X]\\) indicated by the vertical lines and the distribution “widths” (here, \\(E[X]-\\sqrt{V[X]}\\) to \\(E[X]+\\sqrt{V[X]}\\)) indicated by the horizontal lines. Both the expected value and variance are examples of moments of probability distributions. Moments represent elements of a distribution’s location and shape, and they come in two “flavors”: those that are defined around the coordinate origin (i.e., around \\(x=0\\)) and central moments which are defined around the distribution’s mean, \\(\\mu\\): \\[\\begin{align*} \\mu_k&#39; &amp;= E[X^k] \\\\ \\mu_k &amp;= E[(X-\\mu)^k] \\,. \\end{align*}\\] (The expected value \\(E[X]\\) is the first moment of a distribution, \\(\\mu_1&#39;\\), while the variance \\(V[X]\\) is the second central moment, \\(\\mu_2\\).) Other metrics used to describe a probability distribution, such as its skewness, are also related to moments. (One definition of skewness is Fisher’s moment coefficient: \\(\\mu_3/\\sigma^3\\).) 1.7.1 Expected Value Tricks The expected value operator \\(E[X]\\) has the following properties. If we multiply \\(X\\) by a constant \\(a\\), that constant can be moved out of the operator, i.e., \\[ E[aX] = aE[X] \\,. \\] The expected value of a constant is simply that constant, i.e., \\[ E[b] = b \\,. \\] The expected value operator is a linear operator, which means that we can split it at \\(+\\)’s and \\(-\\)’s, with the sign being preserved: \\[ E[aX - b] = E[aX] - E[b] = aE[X] - b \\,. \\] If, for example, we define a random variable \\(Y = 10X - 5\\) and we know that \\(E[X] = 4\\), then we can write that \\(E[Y] = 10E[X] - 5 = 35\\). Note that we have said nothing about \\(E[XY]\\) here. In general, we cannot simplify this expression at all, unless \\(X\\) and \\(Y\\) are independent random variables (a concept we haven’t discussed yet), in which case \\(E[XY] = E[X]E[Y]\\). 1.7.2 Variance Tricks The variance operator \\(V[X]\\) has the following properties. If we multiply \\(X\\) by a constant \\(a\\), that constant can be moved out of the operator, but it is then squared, i.e., \\[ V[aX] = a^2V[X] \\,. \\] The variance of a constant is zero: \\[ V[b] = 0 \\,. \\] The variance operator is a linear operator, which means that we can split it at \\(+\\)’s and \\(-\\)’s, with all signs becoming positive: \\[ V[aX - b] = V[aX] + V[b] = a^2V[X] + 0 = a^2V[X] \\,. \\] If, again, \\(Y = 10X - 5\\), and if \\(V[X] = 2\\), then \\(V[Y] = 100V[X] = 200\\). 1.7.3 The Shortcut Formula for Variance Above, we indicate that \\[ V[X] = E[(X-\\mu)^2] = E[X^2] - (E[X])^2 \\,. \\] This is the so-called shortcut formula for determining the variance of a distribution. We can derive it as follows, making use of the “tricks” we show above: \\[\\begin{align*} V[X] = E[(X-\\mu)^2] &amp;= E[X^2 - 2X\\mu + \\mu^2] ~~\\mbox{(expand)} \\\\ &amp;= E[X^2] - E[2X\\mu] + E[\\mu^2] ~~\\mbox{(split on + and -)} \\\\ &amp;= E[X^2] - 2\\mu E[X] + \\mu^2 ~~\\mbox{(slide constants out)}\\\\ &amp;= E[X^2] - 2(E[X])^2 + (E[X])^2 = E[X^2] - (E[X])^2 \\,, \\end{align*}\\] where in the last line we make use of the fact that \\(E[X] = \\mu\\). Note what this shortcut formula means: it means that to compute a variance, it is sufficient to compute both \\(E[X]\\) and \\(E[X^2]\\) and combine the results. It also means that if we are given any two of the quantities \\(E[X]\\), \\(E[X^2]\\), and \\(V[X]\\), we can immediately derive the third one. 1.7.4 The Expected Value and Variance of a Probability Density Function In the last section, we define the pdf \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] For this pdf, the expected value is \\[ E[X] = \\int_0^1 x f_X(x) dx = \\int_0^1 \\theta x^\\theta dx = \\frac{\\theta}{\\theta+1} \\left. x^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,. \\] As for the variance, we utilize the shortcut formula, which means that we compute \\(E[X^2]\\) first: \\[ E[X^2] = \\int_0^1 x^2 f_X(x) dx = \\int_0^1 \\theta x^{\\theta+1} dx = \\frac{\\theta}{\\theta+2} \\left. x^{\\theta+2} \\right|_0^1 = \\frac{\\theta}{\\theta+2} \\,. \\] Hence the variance is \\[ V[X] = E[X^2] - (E[X])^2 = \\frac{\\theta}{\\theta+2} - \\frac{\\theta^2}{(\\theta+1)^2} = \\frac{\\theta}{(\\theta+2)(\\theta+1)^2} \\,. \\] We see that the value for our new pdf is similar: 0.643. 1.8 Working With R: Probability Distributions In this section, we introduce R as a tool with which to, e.g., visualize and numerically manipulate probability distributions. We start with the concept of the vector: x &lt;- c(&quot;Hello, world!&quot;) (The reader should feel free to open R and type in these lines at the Console prompt.) In this example, we define a vector of character strings which we name x; here, x has length 1: length(x) ## [1] 1 c() is an R function whose arguments (e.g., \"Hello, world!\") are what are to be the constituents of the vector. The arrow is an assignment operator; = is equally valid. We can create a numeric vector as follows: x &lt;- c(1,2,4,8) print(x) ## [1] 1 2 4 8 length(x) ## [1] 4 but when the numbers follow a (long) sequence, it can be easier to utilize seq(): x &lt;- seq(0,pi,by=0.01) # 0, 0.01, 0.02, ..., 3.14 (but not 3.15) # pi and Inf are built-in constants length(x) ## [1] 315 When it comes to probability distributions, what might we want to do first? Let’s suppose that our data are sampled from this pdf: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} c x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] \\(c\\) is a normalization constant, meaning it has some value (to be determined) such that the integral of \\(f_X(x)\\) from 0 to \\(\\pi\\) is 1. Below, we will show how we can determine the value of \\(c\\) using R code. But first, we will assume \\(c = 1\\) and determine if \\(f_X(x)\\) is non-negative (as it should be!): x &lt;- seq(0,pi,by=pi/100) f.x &lt;- x*sin(x) min(f.x) ## [1] 0 Note how we do not have to use a for-loop here, as one of the hallmarks of R is vectorization: if R sees that x is a vector, it will work with the vector directly and thus f.x will itself be a vector with the same length as x (and with the first element of x corresponding to the first element of f.x, etc.). We see that the minimum value is 0. If we want to go further, we can make a simple plot (see Figure 1.13): x &lt;- seq(0,pi,by=pi/100) f.x &lt;- x*sin(x) df &lt;- data.frame(x=x,f.x=f.x) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_hline(yintercept=0,col=&quot;red&quot;,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.13: The function \\(x \\sin x\\). #TBD: explain ggplot() instead of plot() The plot() function puts x on the \\(x\\)-axis, f.x on the \\(y\\)-axis, connects each point with a line (typ=\"l\"), makes the line twice as wide as the default width (lwd=2), makes the line blue (col=\"blue\"), and changes the default \\(y\\)-axis label to one that includes the subscript “X”. We then overlay a horizontal red line at \\(y = 0\\) (abline, with h=0). The next step is to determine the normalization constant. Let’s suppose that we have forgotten integration by parts and thus we are not sure how to integrate \\(f_X(x)\\). We can code numerical integration in R using a combination of a function that evaluates \\(f_X(x)\\) and a call to the built-in function integrate(), which performs numerical integration: f &lt;- function(x) { return(x*sin(x)) } integrate(f,0,pi) # integrate the function f between 0 and pi ## 3.141593 with absolute error &lt; 3.5e-14 We see that the integral is \\(\\pi\\), so to make the pdf valid, we have to set \\(c\\) to \\(1/\\pi\\): \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] Let’s suppose we sample data from this distribution. What is the probability that the next observed datum will have a value between 1 and 2? We can use integrate to figure that out: f &lt;- function(x) { return(x*sin(x)/pi) # we now include the normalization constant } integrate(f,1,2) # integrate the function f between 1 and 2 ## 0.4585007 with absolute error &lt; 5.1e-15 The answer is 0.4585…there is a 45.85% chance that the next datum will have a value between 1 and 2. What is the expected value, \\(E[X]\\), of \\(f_X(x)\\)? f &lt;- function(x) { return(x^2*sin(x)/pi) # add an additional x } integrate(f,0,pi) ## 1.868353 with absolute error &lt; 2.1e-14 The expected value is 1.868. Given the appearance of the pdf, this number makes sense. 1.8.1 Numerical Integration and Conditional Probability Let’s suppose that we would like to numerically evaluate \\[ P(1 \\leq X \\leq 2 \\vert X &gt; 0.5) = \\frac{P(1 \\leq X \\leq 2 \\cap X &gt; 0.5)}{P(X &gt; 0.5} = \\frac{P(1 \\leq X \\leq 2)}{P(X &gt; 0.5} \\,. \\] As we have already defined At first, it would appear that all we have to do is to call integrate() twice f &lt;- function(x) { return(x*sin(x)/pi) } integrate(f,1,2) / integrate(f,0.5,pi) However, this will not work, since integrate() returns a list, not a single numerical value. So we have to figure out where the value of the integral value is stored: names(integrate(f,1,2)) # return the names of each list element ## [1] &quot;value&quot; &quot;abs.error&quot; &quot;subdivisions&quot; &quot;message&quot; &quot;call&quot; What we want is value. To reference the value directly, we use a dollar sign, as shown here: integrate(f,1,2)$value / integrate(f,0.5,pi)$value ## [1] 0.3836833 Done. Our conditional probability is 0.4645. 1.8.2 Numerical Integration and Variance Above, we compute the expected value of \\(f_X(x)\\). For the variance, we adapt the same code to compute \\(E[X^2]\\), then utilize the shortcut formula: f &lt;- function(x) { return(x^2*sin(x)/pi) # same code as above } E.X &lt;- integrate(f,0,pi)$value f &lt;- function(x) { return(x^3*sin(x)/pi) # add one more power of x } V.X &lt;- integrate(f,0,pi)$value - E.X^2 V.X ## [1] 0.3788611 sqrt(V.X) ## [1] 0.6155169 The variance is 0.379 and the standard deviation is 0.616. We interpret these numbers as saying that the majority of the observed data will lie between \\(1.868 - 0.616 = 1.252\\) and \\(1.868 + 0.616 = 2.484\\). If we recall introductory statistics, the proportion of values within one standard deviation of the mean for a normal distribution (i.e., a bell curve) is 0.683…but that value changes from distribution to distribution. What is the value here? f &lt;- function(x) { return(x*sin(x)/pi) # back to the original pdf } integrate(f,E.X-sqrt(V.X),E.X+sqrt(V.X))$value ## [1] 0.642609 We see that the value for our new pdf is similar: 0.643. 1.9 Cumulative Distribution Functions A cumulative distribution function (a cdf) is another means by which to mathematically express a probability distribution, which is to say, if we have a cdf, we can derive the associated pmf/pdf and vice-versa. A cdf is, in the discrete case, a sum of probability masses that lie to the left of a chosen coordinate \\(x\\) on the real-number line\\(-\\) \\[ F_X(x) = \\sum_{y \\leq x} p_Y(y) \\] \\(-\\)while in the continuous case it is an integral of the probability density that lies to the left of \\(x\\)\\(-\\) \\[ F_X(x) = \\int_{y \\leq x} f_Y(y) dy \\,. \\] In both cases, we utilize a dummy variable for the pmf/pdf itself because \\(x\\) is the upper limit of summation/integration. See Figure 1.14, which illustrates how a cdf “collects” all the probability masses or density “to the left” of a given value of \\(x\\). Given this figure, it should be clear that \\(F_X(-\\infty) = 0\\) (there is nothing to collect “to the left” of \\(-\\infty\\)) and \\(F_X(\\infty) = 1\\) (since, by the time we reach \\(x = \\infty\\), all masses or density have been collected). Another thing to keep in mind is that even if a random variable is discrete, its associated cdf \\(F_X(x)\\) is continuously valued, because it is defined at all values of \\(x\\) (although it is technically not “mathematically continuous” due to the steps that \\(F_X(x)\\) takes at each value of \\(x\\) where there is a probability mass). Figure 1.14: Illustration of the relationship between a probability mass function (left) and a probability density function (right) and its associated cdf (evaluated here at \\(x = 2.5\\)). For the pmf, the cdf is the sum of the probability masses to the left of \\(x = 2.5\\) (the masses marked in green), while for the pdf, the cdf is the integral over \\(x \\in [0,2.5]\\) (the area under curve shown in green). Figure 1.15: Examples of the cdfs \\(F_X(x)\\) for the probability mass function (left) and the probability density function (right) shown in Figure 1.14. A cdf is useful to have when our goal is to compute the probability of that the value of a sampled random variable lies between \\(x = a\\) and \\(x = b\\). For the case of a continuous random variable, \\[ P(a &lt; X &lt; b) = F_X(b) - F_X(a) \\,. \\] As we can see, if we have the cdf, we do not need to perform integration to compute the probability…we just plug in coordinate values. (Note that the form of the inequality, i.e., whether we have \\(&lt;\\) or \\(\\leq\\), does not matter.) However, when we are dealing with a discrete random variable, we need to tread far more carefully, because the form of the inequality can matter. Let’s suppose we have a pmf with masses given at \\(x = \\{0,1\\}\\). Then, e.g., \\[\\begin{align*} P(0 \\leq X \\leq 1) &amp;= \\sum_{x \\in [0,1]} p_X(x) = p_X(0) + p_X(1) = F_X(1) \\\\ P(0 &lt; X \\leq 1) &amp;= \\sum_{x \\in (0,1]} p_X(x) = p_X(1) = F_X(1) - F_X(0) \\\\ P(0 &lt; X &lt; 1) &amp;= \\sum_{x \\in (0,1)} p_X(x) = 0 \\,. \\end{align*}\\] Figure 1.16: An illustration of the relationship between a cdf and probability. The probability \\(P(1 &lt; X &lt; 3)\\) is given by the distance between the two red lines (i.e., \\(F_X(3)-F_X(1)\\)). We will make two final points here about cdfs. First, as indicated above, given a cdf, we can find the associated pmf/pdf. If a pmf has non-zero masses at values \\(x - \\Delta x\\) and \\(x\\), and none in between, then \\[ p_X(x) = F_X(x) - F_X(x-\\Delta x) \\,, \\] while in the continuous case, \\[ f_X(x) = \\frac{d}{dx}F_X(x) \\,, \\] assuming \\(F_X(x)\\) is differentiable at \\(x\\). Second, we can define an inverse cumulative distribution function, or inverse cdf. The inverse cdf takes as input the total probability collected to the left of \\(x\\) (e.g., the green region shown in the right panel of Figure 1.14) and returns the associated value of \\(x\\). In other words, if \\(q = F_X(x)\\), then \\(x = F_X^{-1}(q)\\). One issue that arises with the inverse cdf is that if \\(F_X(x)\\) is not strictly monotonically increasing (i.e., if for some range of values, \\(\\frac{d}{dx}F_X(x) = 0\\)) then there is no unique inverse. For instance, see the left panel of Figure 1.15: if we input \\(F_X(x) = 0.35\\), then \\(x \\in [2,3)\\). We can circumvent this issue by utilizing the generalized inverse cdf instead, for which \\[ x = F_X^{-1}(q) = \\mbox{inf}\\{ x : F_X(x) \\geq q \\} \\,. \\] The symbol “inf” indicates that we are finding the infimum, or smallest value, of the indicated set of values. Here, the output \\(x\\) is the smallest value for which \\(F_X(x) \\geq q\\) holds. For our given example, \\(x = 2\\). On the other hand, if we pick a value of \\(F_X(x)\\) that lies between the steps, we would choose the smallest \\(x\\) value associated with the next higher step. For instance, if for our example we want the inverse cdf for \\(F_X(x) = 0.5\\), which lies between the steps at 0.35 and 0.6, we would take the smallest value of \\(x\\) associated with \\(F_X(x) = 0.6\\), which is \\(x = 3\\). (Note that R utilizes the generalized form of the inverse cdf.) 1.9.1 The Cumulative Distribution Function for a Probability Density Function We work again with our simple parameterized pdf: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] The cdf for this function is simply the integral of the pdf “to the left” of the coordinate \\(x\\): \\[ F_X(x \\vert \\theta) = \\int_0^x f_Y(y \\vert \\theta) dy \\,. \\] Because the upper bound of the integral is \\(x\\), we replace \\(x\\) in the integrand with a dummy variable. (Here, \\(y\\) was chosen arbitrarily.) Thus \\[ F_X(x \\vert \\theta) = \\int_0^x \\theta y^{\\theta-1} dy = \\left. y^\\theta \\right|_0^x = x^\\theta \\,. \\] We can answer a variety of questions given this cdf. For example… What is the median of this distribution? The median \\(\\tilde{x}\\) is the point on the real-number line where \\[ P(X \\leq \\tilde{x}) = \\frac{1}{2} \\,. \\] For our distribution, \\[ \\tilde{x}^\\theta = \\frac{1}{2} ~\\Rightarrow~ \\tilde{x} = \\left( \\frac{1}{2} \\right)^{1/\\theta} \\,. \\] Now let \\(\\theta = 3\\). What is the probability of sampling a datum between \\(x = 1/4\\) and \\(x = 3/4\\)? \\[ P\\left(\\frac{1}{4} \\leq X \\leq \\frac{3}{4}\\right) = F_X\\left(\\frac{3}{4} \\vert \\theta=3\\right) - F_X\\left(\\frac{1}{4} \\vert \\theta=3\\right) = \\left(\\frac{3}{4}\\right)^3 - \\left(\\frac{1}{4}\\right)^3 = \\frac{26}{64} = \\frac{13}{32} \\,. \\] 1.9.2 Visualizing the Cumulative Distribution Function in R We continue with the pdf we use above, with \\(\\theta = 3\\). To show the region being integrated over to compute a cdf value, for say \\(x = 0.6\\), we utilize R’s polygon() function. (See Figure 1.17.) TBD - explain geom_area() instead of polygon() x &lt;- seq(0,1,by=0.01) f.x &lt;- 3*x^2 x.o &lt;- 0.6 df &lt;- data.frame(x=x,f.x=f.x) df.shade &lt;- subset(df,x&lt;= x.o) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_area(data = df.shade,aes(x,y=f.x),fill=&quot;green&quot;,col=&quot;blue&quot;,outline.type=&quot;full&quot;) + geom_vline(xintercept=x.o,col=&quot;red&quot;,lwd=1) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.17: The cdf for \\(f_X(x) = 3x^2\\) at \\(x = 0.6\\) is the area represented in green. What is happening in this code chunk? We first define a sequence of values for x (via seq()), then compute the pdf for each x value (f.x). We then determine which indices of the x vector correspond to values of x that are less than or equal to 0.6 (via which()). which() returns a list of indices (stored as w) such that, e.g., x[w] contains only those value of x that are less than or equal to 0.6. (We note that x[-w] would contain only those values of x that are greater than 0.6.) To create the polygon, we define a series of points along its border, in order around the border (meaning that x[w], which counts upwards, is followed by rev(x[w]), which takes the same coordinates but counts downwards). As we count upwards in x, the border value has a \\(y\\)-coordinate of 0 (representing the lower edge of the polygon), and rather than writing 0 many times, we use the rep() function to repeat the value. If we wish to visualize the full cdf, we can do the following. (See Figure 1.18.) x &lt;- seq(0,1,by=0.01) F.x &lt;- x^3 df &lt;- data.frame(x=x,F.x=F.x) ggplot(data=df,aes(x=x,y=F.x)) + geom_hline(yintercept=0,lty=2,col=&quot;red&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;red&quot;) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_segment(x=-1,xend=0,y=0,yend=0,col=&quot;blue&quot;,lwd=1) + geom_segment(x=1,xend=2,y=1,yend=1,col=&quot;blue&quot;,lwd=1) + labs(y = expression(F[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.18: The cdf for \\(f_X(x) = 3x^2\\). 1.9.3 The CDF for a Mathematically Discontinuous Distribution Assume that we are handed the following pdf: \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} 1/2 &amp; 0 \\leq x \\leq 1 \\\\ 2-x &amp; 1 \\leq x \\leq 2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] which we display in Figure 1.19. df &lt;- data.frame(x=c(-0.25,0,0,1,1,2,2.25),f.x=c(0,0,0.5,0.5,1,0,0)) ggplot(data=df,aes(x=x,y=f.x)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_hline(yintercept=0,lty=2,col=&quot;blue&quot;) + labs(y = expression(f[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.19: A continuous probability density function that is mathematically discontinuous at \\(x=1\\). This is a completely valid, “continuous” pdf that has a mathematical discontinuity at \\(x = 1\\). What is the cdf for this function? The key insight is that we should not try to evaluate the integral of \\(f_X(x)\\) from 0 to \\(x\\) when \\(x &gt; 1\\) with a single integral…this will not work! We simply have to break the problem up so as to define the cdf over the domain [0,1), and then over the domain [1,2]. \\[\\begin{align*} F_X(x \\vert x &lt; 1) &amp;= \\int_0^x f_Y(y) dy = \\frac{1}{2} \\int_0^y dy = \\frac{x}{2} \\\\ F_X(x \\vert x \\geq 1) &amp;= \\int_0^1 f_Y(y) dy + \\int_1^x f_Y(y) dy = \\left. \\frac{y}{2} \\right|_0^1 + \\int_1^x (2-y) dy = \\frac{1}{2} - \\left. \\frac{(2-y)^2}{2} \\right|_1^x \\\\ &amp;= \\frac{1}{2} - \\left( \\frac{(2-x)^2}{2} - \\frac{1}{2} \\right) = 1 - \\frac{(2-x)^2}{2} \\,. \\end{align*}\\] (Not sure if this is right? We can at the very least do sanity checking, as we know \\(F_X(1) = 1/2\\) and \\(F_X(2) = 1\\)…and our formula produces these results! Alternatively, we can take the derivative of \\(F_X(x)\\) and see if it matches \\(f_X(x)\\).) We display the cdf in Figure 1.20. x.seq &lt;- seq(1.01,2,by=0.01) x &lt;- c(0,1,x.seq) F.x &lt;- c(0,0.5,1-(2-x.seq)^2/2) df &lt;- data.frame(x=x,F.x=F.x) ggplot(data=df,aes(x=x,y=F.x)) + geom_hline(yintercept=0,lty=2,col=&quot;red&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;red&quot;) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_segment(x=-1,xend=0,y=0,yend=0,col=&quot;blue&quot;,lwd=1) + geom_segment(x=2,xend=3,y=1,yend=1,col=&quot;blue&quot;,lwd=1) + labs(y = expression(F[X]*&quot;(x)&quot;)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.20: The cdf for our mathematically discontinuous pdf. 1.10 The Law of Total Probability One of the laws of probability that we introduce earlier in this chapter is the Law of Total Probability, or LoTP: if we partition a sample space \\(\\Omega\\) into \\(k\\) disjoint events \\(\\{B_1,\\ldots,B_k\\}\\), then for any event \\(A\\) we can write \\[ P(A) = \\sum_{i=1}^k P(A \\vert B_i) P(B_i) \\,. \\] We are in a position now, having introduced random variables and probability distributions, to update how we think of this law: it can express the probability of a random variable \\(X\\) when it is sampled from a discrete distribution with parameter \\(\\theta\\)…and when \\(\\theta\\) itself is not a fixed constant (as it has been up until now), but is itself a discrete random variable. To see this, let’s rewrite the LoTP given this description: \\[ p_X(x) = \\sum_\\theta p_{X \\vert \\theta}(x \\vert \\theta) p_{\\Theta}(\\theta) \\,. \\] This equation is saying that the probability mass associated with the coordinate \\(x\\) is the value of the mass for \\(x\\), given the value \\(\\theta\\), weighted by the probability that we would even observe the value \\(\\theta\\) in the first place. Or, that \\(p_X(x)\\) is a weighted average of the values of the conditional distribution \\(p_{X \\vert \\theta}(x \\vert \\theta)\\), where the weights are given by \\(p_{\\Theta}(\\theta)\\). What if \\(\\theta\\) is actually a continuous random variable? We can extend the LoTP to handle that possibility by replacing the summation over a discrete random variable with an integral over a continuous one: \\[ p_X(x) = \\int_\\theta p_{X \\vert \\theta}(x \\vert \\theta) f_{\\Theta}(\\theta) d\\theta \\,. \\] And what if the distribution of \\(X \\vert \\theta\\) is continuous? We would just replace the \\(p_X\\) and the \\(p_{X \\vert \\theta}\\) in the equations above with \\(f_X\\) and \\(f_{X \\vert \\theta}\\), i.e., we would use the LoTP to define a probability density instead of a probability mass. 1.10.1 The LoTP With Two Simple Discrete Distributions Let’s suppose we have two random variables \\(X\\) and \\(Y\\), where the probability mass function for \\(Y\\) is \\(y\\) \\(p_Y(y)\\) 0 2/3 1 1/3 and where, if \\(Y = 0\\), the pmf for \\(X\\) is \\(x \\vert y=0\\) \\(p_{X \\vert Y}(x \\vert y=0)\\) 0 4/5 1 1/5 and if \\(Y = 1\\) the pmf for \\(X\\) is \\(x \\vert y=1\\) \\(p_{X \\vert Y}(x \\vert y=1)\\) 0 3/5 1 2/5 What is the pmf \\(p_X(x)\\)? The Law of Total Probability tells us that \\[ p_X(x) = \\sum_y p_{X \\vert Y}(x \\vert y) p_{Y}(y) \\,, \\] so \\[\\begin{align*} p_X(0) &amp;= p_{X \\vert Y}(0 \\vert 0) p_{Y}(0) + p_{X \\vert Y}(0 \\vert 1) p_{Y}(1) = \\frac{4}{5} \\cdot \\frac{2}{3} + \\frac{3}{5} \\cdot \\frac{1}{3} = \\frac{11}{15} \\\\ p_X(1) &amp;= p_{X \\vert Y}(1 \\vert 0) p_{Y}(0) + p_{X \\vert Y}(1 \\vert 1) p_{Y}(1) = \\frac{1}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{3} = \\frac{4}{15} \\,. \\end{align*}\\] The pmf is thus \\(x\\) \\(p_X(x)\\) 0 11/15 1 4/15 The masses sum to 1, so indeed this is a proper pmf. 1.10.2 The Law of Total Expectation If we inspect the tables above, we see that, e.g., \\[\\begin{align*} E[X \\vert Y=0] &amp;= 0 \\cdot \\frac{4}{5} + 1 \\cdot \\frac{1}{5} = \\frac{1}{5} \\,. \\end{align*}\\] A similar calculation yields \\(E[X \\vert Y=1] = 2/5\\). What then is the expected value of \\(X\\) itself? A result related to the Law of Total Probability is the Law of Total Expectation (LoTE), which states that when \\(Y\\) is finite and countable, \\[ E[X] = E[E[X \\vert Y]] = \\sum_y E[X \\vert Y=y] ~ p_Y(y) \\,, \\] i.e., the overall expected value is a weighted average of the individual values \\(E[X \\vert Y=y]\\). Here, the LoTE yields \\[ E[X] = \\frac{1}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{3} = \\frac{4}{15} \\,. \\] 1.10.3 The LoTP With Two Continuous Distributions Let’s suppose that we have two random variables, \\(X\\) and \\(\\theta\\), such that \\[\\begin{align*} f_{X \\vert \\Theta}(x \\vert \\theta) &amp;= \\theta \\exp(-\\theta x) \\\\ f_{Theta}(\\theta) &amp;= \\exp(-\\theta) \\,, \\end{align*}\\] for \\(x \\in [0,\\infty)\\) and \\(\\theta &gt; 0\\). What is \\(f_X(x)\\)? As mentioned above, the primary change to the LoTP would be that we use integrate over all possible values of \\(\\theta\\), rather than sum, so the LoTP looks like this: \\[ f_X(x) = \\int_0^\\infty f_{X \\vert \\Theta}(x \\vert \\theta) f_{Theta}(\\theta) d\\theta \\,. \\] Now that we’ve established this equation, the rest is math…except as we’ll see, we need to use integration by parts. \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\theta \\exp(-\\theta x) \\exp(-\\theta) d\\theta \\\\ &amp;= \\int_0^\\infty \\theta \\exp(-\\theta (x+1)) d\\theta \\,. \\end{align*}\\] We set up the integration as follows: \\[\\begin{align*} u = \\theta ~~~ &amp; ~~~ dv = \\exp(-\\theta (x+1)) d\\theta \\\\ du = d\\theta ~~~ &amp; ~~~ v = -\\frac{1}{x+1}\\exp(-\\theta (x+1)) \\,. \\end{align*}\\] Then \\[\\begin{align*} f_X(x) &amp;= \\left.(u v)\\right|_0^\\infty - \\int_0^\\infty v du \\\\ &amp;= -\\left.\\frac{\\theta}{x+1}\\exp(-\\theta (x+1))\\right|_0^\\infty + \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\\\ &amp;= 0 + \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\,. \\end{align*}\\] (We will stop here momentarily to remind the reader that when we evaluate an expression of the form \\(x e^{-x}\\), the result as \\(x \\rightarrow \\infty\\) is zero because \\(e^{-x} \\rightarrow 0\\) faster than \\(x \\rightarrow \\infty\\). We now carry on…) \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\\\ &amp;= \\left. -\\frac{1}{(x+1)^2} \\exp(-\\theta (x+1)) \\right|_0^\\infty \\\\ &amp;= \\frac{1}{(x+1)^2} \\,, \\end{align*}\\] for \\(x \\in [0,\\infty)\\). Done. We will leave it as an exercise to the reader to confirm that \\(f_X(x)\\) is a valid pdf that integrates to one. Above, we say that “we need to use integration by parts.” This is not quite true. A handy result that we will utilize as the book goes on is that \\[ \\Gamma(x) = \\int_0^\\infty u^{x-1} \\exp(-u) du \\,. \\] This is the gamma function. (The symbol \\(\\Gamma\\) represents a capital gamma.) One of the properties that makes this function useful is that when \\(x\\) is a non-negative integer, the gamma function is related to the factorial function: \\(\\Gamma(x) = (x-1)! = (x-1) (x-2) \\cdots 1\\). But the reason why the gamma function is useful here is that we can use it to avoid integration by parts. Our integral is \\[ f_X(x) = \\int_0^\\infty \\frac{1}{x+1}\\exp(-\\theta (x+1)) d\\theta \\,. \\] To solve this, we implement variable substitution. The three steps of variable substitution are to write down a viable substitution \\(u = g(\\theta)\\); to then derive \\(du = h(u,\\theta) d\\theta\\); and finally to use \\(u = g(\\theta)\\) to transform the bounds of the integral. For our integral \\[ (1) ~~ u = (x+1)\\theta ~~ \\implies ~~ (2) ~~ du = (x+1)d\\theta \\] and \\[ (3) ~~ \\theta = 0 ~\\implies~ u = 0 ~~~ \\mbox{and} ~~~ \\theta = \\infty ~\\implies~ u = \\infty \\,, \\] We see from point (3) that making the variable substitution will not affect the bounds of the integral. Thus we have that \\[\\begin{align*} f_X(x) &amp;= \\int_0^\\infty \\frac{1}{x+1}\\exp(-u) \\frac{du}{x+1} \\\\ f_X(x) &amp;= \\frac{1}{(x+1)^2} \\int_0^\\infty u^0 \\exp(-u) du \\\\ f_X(x) &amp;= \\frac{1}{(x+1)^2} \\Gamma(1) = \\frac{1}{(x+1)^2} 0! = \\frac{1}{(x+1)^2} \\,. \\end{align*}\\] (Here, we utilize the fact that zero factorial is one.) 1.11 Working With R: Data Sampling One of the primary uses of R is to perform simulations in which we repeatedly create mock datasets and analyze them. But: how do we create such datasets? Below, we will describe two methods for randomly sampling data given a probability distribution. The first, rejection sampling, is appropriate to use when we cannot work with the cumulative distribution function of the assumed distribution analytically. As we will see, rejection sampling is (relatively) computationally inefficient, but it does have the benefit that we can apply it in just about any sampling situation. The second method, inverse transform sampling, is efficient and should always be our first choice when the cdf is tractable. To head off a question the reader may have: no, we do not always have to hand-code samplers when working in R…for commonly used distributions, R supplies “wrapper functions” that effectively abstract away the details of inverse transform sampling. However, knowing how to code a sampler is a good skill to have! Let’s suppose we are working with one of the pdfs that we define above: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] The cdf for this distribution is \\[ F_X(x) = \\frac{1}{\\pi}\\left( \\sin x - x \\cos x \\right) \\,, \\] which is not easily inverted. Thus to sample data from this distribution, we utilize the following algorithmic steps. Determine the range of values over which we will sample data values: \\([x_{lo},x_{hi}]\\). Nominally this will be the domain of the distribution, but sometimes that’s not viable, such as when the domain is semi- or fully infinite. (Here, the range is easily specified: \\([0,\\pi]\\).) Within \\([x_{lo},x_{hi}]\\), determine the maximum value of \\(f_X(x)\\). (For our assumed distributione, this is not necessarily a simple calculation, as the derivative of \\(f_X(x)\\) is \\((\\sin x + x \\cos x)/\\pi\\). We can solve for the root using, e.g., R’s uniroot() function: f &lt;- function(x) { (sin(x) + x*cos(x))/pi } uniroot(f,interval=c(0.01,pi))$root ## [1] 2.028758 This looks for the root of the given function within the stated interval; since there is a root at 0, corresponding to a functional minimum, we exclude that point by setting the interval lower bound to 0.01. uniroot() is an extremely useful function and we will see it again throughout the rest of this book. The root is \\(x_{max} = 2.0288\\) and \\(f_X(x_{max}) = 0.5792\\).) 3. We repeat the following steps until we reach our target sample size \\(n\\): (a) sample a random number \\(u\\) assuming uniform weighting between \\(x_{lo}\\) and \\(x_{hi}\\); (b) sample another random number \\(v\\) assuming uniform weighting between 0 and \\(f_X(x_{max})\\); and (c) keep \\(u\\) as part of our sample if \\(v \\leq f_X(u)\\). (a) and (b) are summed up by the statement “draw a rectangle whose vertices are \\((x_{lo},0)\\), \\((x_{hi},0)\\), \\(x_{hi},f_X(x_{max}))\\), and \\(x_{lo},f_X(x_{max}))\\) and pick a random point inside the rectangle,” while (c) is summed up by saying “keep the random point if it lies below \\(f_X(x)\\).” Note that we will assume that at the very least, we can use an R wrapper function to sample a numbers with uniform weighting; without this assumption, we would have to wade into the quagmire that is random number generation, which is well beyond the scope of this book! In a code chunk and in Figure 1.21 we show how we sample \\(n = 1000\\) data sampled from our distribution, and the final result. (We dub the observed distribution the empirical distribution of the data, where “empirical” simply means “what we actually observe.”) Rejection sampling seems quick and easy…should we always use it when we are not already provided a sampling function for our pmf or pdf? No, not necessarily, because as noted above it is computationally inefficient: we might have to sample \\(m \\gg n\\) points in order to populate a sample of size \\(n\\). (We will also note here that this is the first time that we are running across the R function set.seed(). This initializes the underlying random number generator such that we generate the same numerical results every time we run the subsequent code…which is useful when doing analyses that we want to be reproducible. If we leave out set.seed(), then every time we run the subsequent code, we get a different data sample. The number that we pass to set.seed() can be anything…we adopt 101 here, but it can any real number.) set.seed(101) n &lt;- 1000 x.lo &lt;- 0 x.hi &lt;- pi f.x.hi &lt;- 0.58 # rounding up is OK, it just decreases algorithm efficiency X.sample &lt;- rep(NA,n) # rejection sampling ii &lt;- 0 while ( ii &lt; n ) { u &lt;- runif(1,min=x.lo,max=x.hi) v &lt;- runif(1,min=0,max=f.x.hi) if ( v &lt; u*sin(u)/pi ) { ii &lt;- ii+1 X.sample[ii] &lt;- u } } empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,pi,by=0.01) f.x &lt;- x*sin(x)/pi true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,3.2,by=0.2)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.21: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = (x \\sin x)/\\pi\\) via the rejection sampling algorithm. We observe that our empirical distribution follows the true distribution well. A primary alternative to rejection sampling is inverse transform sampling, in which we utilize the inverse cdf function to generate appropriately distributed data. Inverse transform sampling is efficient in that every proposal point is kept. Let’s suppose we are working with the pdf: \\[ f_X(x) = \\theta x^{\\theta-1} \\,, \\] where \\(\\theta &gt; 0\\) and \\(x \\in [0,1]\\). The inverse cdf, as derived in an example above, is \\(F_X^{-1}(q) = q^{1/\\theta}\\). Inverse transform sampling utilizes the following algorithmic steps. Pick the target sample size \\(n\\). Sample \\(n\\) data with uniform weighting between 0 and 1. These are the cdf bounds. Call these data \\(q\\). Transform the data \\(q\\) to be \\(x = F_X^{-1}(q)\\). In a code chunk and in Figure 1.22 we display our inverse-transform sampling code as well as the empirical distribution of \\(n = 1000\\) data sampled from our distribution (assuming \\(\\theta = 3\\)). We note that the code to generate our sample is much simpler than the code needed to perform rejection sampling! set.seed(101) theta &lt;- 3 n &lt;- 1000 q &lt;- runif(n,min=0,max=1) X.sample &lt;- q^(1/theta) #TBD - NOTE DIFFERENCE IN DENSITY LINE empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,1,by=0.01) f.x &lt;- theta*x^(theta-1) true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.1)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.22: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = 3x^2\\) via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well. 1.11.1 More Inverse-Transform Sampling Let’s suppose that we are to sample \\(n\\) data from the following distribution: \\[ f_X(x) = 2(1-x) ~~~ x \\in [0,1] \\,. \\] Can we do this via inverse-transform sampling? The answer is yes, if (a) we can derive the cdf \\(F_X(x)\\), and (b) we can invert it. Here, \\[ F_X(x) = \\int_0^x f_V(v) dv = \\int_0^x 2(1-v) dv = \\left. -(1-v)^2 \\right|_0^x = -(1-x)^2 - (-1) = 1 - (1-x)^2 \\,. \\] To invert the cdf, we set it equal to \\(q\\) and solve for \\(x\\): \\[\\begin{align*} q &amp;= 1 - (1-x)^2 \\\\ \\Rightarrow ~~~ 1 - q &amp;= (1-x)^2 \\\\ \\Rightarrow ~~~ \\sqrt{1 - q} &amp;= 1-x \\\\ \\Rightarrow ~~~ x &amp;= 1 - \\sqrt{1 - q} \\., \\end{align*}\\] To check for the correctness of our inversion, we utilize a code like the one in the main body of the section above and compare our sampled data against the pdf. See Figure 1.23. set.seed(101) n &lt;- 1000 q &lt;- runif(n,min=0,max=1) X.sample &lt;- 1 - sqrt(1-q) empirical.dist &lt;- data.frame(X.sample=X.sample) x &lt;- seq(0,1,by=0.01) f.x &lt;- 2*(1-x) true.dist &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.dist,aes(x=X.sample)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.1)) + geom_line(data=true.dist,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.23: \\(n = 1000\\) data sampled from the distribution \\(f_X(x) = 2(1-x)\\) via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well. 1.12 Statistics and Sampling Distributions Let’s say that we run an experiment in which we randomly sample many data from some distribution \\(P\\): \\[ \\mathbf{X} = \\{X_1,X_2,\\ldots,X_n\\} \\overset{iid}{\\sim} P \\,. \\] The expected value for this distribution is \\(E[X] = \\mu\\), while the variance is \\(V[X] = \\sigma^2\\) (assumed to be finite). So we have data…now what? Figure 1.24: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, assume that the population can be represented by some family of distributions parameterized by \\(\\theta\\), and compute and use statistics to infer the value(s) of \\(\\theta\\). The answer, typically, is that we would use these data to infer the (unknown) properties of the population from which they are drawn. A simple picture of the experiment-and-infer cycle is given in Figure 1.24. Notice how in this figure we use the term “statistical inference.” (Fitting, as it is part of the name of this course!) This is an appropriate term to use because we utilize statistics when trying to infer the properties of the unknown underlying population, like its true mean \\(\\mu\\). But this motivates a next question… What is a statistic? A statistic is simply a function of the data we observe. It can be any function of the data\\(-\\)\\(X_1\\), sin\\((X_1) + \\pi X_2\\), etc.\\(-\\)and it provides a useful means by which to summarize data (i.e., reduce \\(n\\) numbers to a single number). But it should be intuitively obvious (we would hope!) that some statistics are going to be more informative than others: for instance, if we are trying to infer what \\(\\mu\\) might be, \\(X_1\\) is probably going to be more useful to us than sin\\((X_1) + \\pi X_2\\). But it may not (nay, will not) be the most useful quantity when the sample size \\(n &gt; 1\\). We could say that much of what we do as statisticians is to pick appropriate (and optimal!) statistics to perform inference. What are some common statistics? The sample mean: \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i \\,. \\] This is always useful for inferring a population mean. Why it is better than, e.g., \\(X_1\\) when \\(n &gt; 1\\) will become more clear below. (Foreshadowing: there are metrics we can compute that provide numerical assessments of the usefulness of a statistic for performing inference. We introduce some of these metrics in the next section.) The sample variance: \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_1 - \\bar{X})^2 \\,. \\] As we might guess, this one helps us infer population variances, and thus helps us make sense of the “width” of the pmf or pdf from which the data are sampled. The square root of \\(S^2\\) is the sample standard deviation. (One might ask: why \\(n-1\\)? Metrics, again…we will return to this point below when we discuss point estimation.) The sample range: \\[ R = X_{(n)} - X_{(1)} \\,. \\] Here, we introduce a notational wrinkle: \\(X_{(i)}\\) represents the \\(i^{\\rm th}\\) smallest datum. So \\(X_{(1)}\\) is the observed datum with the smallest value (but not necessarily the first one to be recorded in our experiment), and \\(X_{(n)}\\) is the one with the largest value. \\(X_{(\\cdot)}\\) is dubbed an order statistic and we will illustrate its use as we go on, beginning in Chapter 3. There are a myriad of others: the interquartile range, the median, etc. We have stated what a statistic is: it is a function of the observed data. But what does this imply? It implies that statistics, which are functions of random variables, are themselves random variables, and thus are sampled from a pmf or pdf. It is convention to call the pmf or pdf associated with a given statistic the sampling distribution, but we are not necessarily fans of the term: it makes it sound like something new and different, when in reality a sampling distribution is just another pmf or pdf, with properties equivalent to those discussed earlier in the chapter (e.g., a sampling distribution has an expected value, a variance, etc.). As we will see in the first example below, if the statistic is a linear function of random variables (like the sample mean), we can derive its expected value, variance, and standard error now given the tools we already have at our disposal. The term standard error simply refers to the standard deviation of a sampling distribution, i.e., \\[ se[Y] = \\sqrt{V[Y]} \\,, \\] where \\(Y\\) is our statistic. Now, can we go beyond this and derive the mathematical form of a statistic’s pmf or pdf now? The short answer is no…we have not yet introduced methods for deriving the functional forms of sampling distributions. In Chapter 2, we will introduce moment-generating functions, which can help us derive sampling distributions for linear functions of random variables (an example of which is the sample mean). In Chapter 3, we will show how one can write down the pmf or pdf for order statistics (like the sample median); once the sampling distribution is known, then we can derive its expected value and variance. (As an aside, we should mention here the empirical rule. While nominally about the normal distribution, we will think of it as stating that nearly all statistics should be observed as laying within three standard errors of their population means. For instance, as we show below in an example, \\(E[\\bar{X}] = \\mu\\), so virtually all values of \\(\\bar{X}\\), as observed over repetitions of an experiment, should lie in the range \\([\\mu - 3 \\cdot se(\\bar{X}),\\mu + 3 \\cdot se(\\bar{X})]\\), a range that gets smaller and smaller as the sample size \\(n\\) goes to infinity.) There are, however, two paths that one could follow that allow us to build up an empirical sampling distribution for a statistic. The first assumes that we know (or are willing to assume) the pmf or pdf for the individual data: we would repeatedly simulate data from the distribution and record the values for the statistic. This builds off of the material in the last section above. (See the example below, as well as the last section of this chapter.) The other is useful for situations where we do not know nor are willing to assume the form of the pmf or pdf for the individual data…this is the bootstrap. We discuss the bootstrap technique in Chapter 4. Many important results in statistical inference assume that we have collected a sample of \\(n\\) iid random variables, so over the course of the rest of this chapter and for the next several, we will assume that when we have sampled two or more random variables, they will be iid random variables. (We will discuss concepts related to simultaneously sampling values for two or more dependent random variables in Chapter 6.) 1.12.1 Expected Value and Variance of the Sample Mean (For All Distributions) Given \\(n\\) iid data from some distribution, we can use the results from earlier in this chapter to immediately show that \\[ E[\\bar{X}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}E\\left[\\sum_{i=1}^n X_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{1}{n} n \\mu = \\mu \\,, \\] and \\[ V[\\bar{X}] = V\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right] = \\frac{1}{n^2} V\\left[\\sum_{i=1}^n X_i\\right] = \\frac{1}{n^2} \\sum_{i=1}^n V[X_i] = \\frac{1}{n^2} n \\sigma^2 = \\frac{\\sigma^2}{n} \\,. \\] The standard error for the sample mean is thus \\(\\sqrt{\\bar{X}} = \\sigma/\\sqrt{n}\\). There are two important conclusions to take away from this simple example. First, we never state the distribution from which we sample the initial data, so this is a general result that holds for all distributions. Second, we see that the width of the sampling distribution for the sample mean decreases as we collect more and more data, as \\(1/\\sqrt{n}\\), meaning that any inferences that we make about the population mean will become more and more accurate as the sample size increases. 1.12.2 Visualizing the Distribution of the Sample Mean In an example in the last section above, we use inverse-transform sampling to sample data from the pdf \\(f_X(x) = 2(1-x)\\) for \\(x \\in [0,1]\\). Here, we extend our R code so as to visualize the distribution of the sample mean of \\(n = 10\\) data drawn from this distribution. We note that some of the material below foreshadows that which we cover in the last section of this chapter, when we discuss numerical simulation. set.seed(101) n &lt;- 10 num.sim &lt;- 1000 X.bar &lt;- rep(NA,num.sim) # set aside storage for X.bar # NA == Not Available - this is overwritten for ( ii in 1:num.sim ) { q &lt;- runif(n,min=0,max=1) X.sample &lt;- 1 - sqrt(1-q) X.bar[ii] &lt;- mean(X.sample) } empirical.mean &lt;- data.frame(X.bar=X.bar) x &lt;- seq(0,1,by=0.01) f.x &lt;- 2*(1-x) pdf &lt;- data.frame(x=x,f.x=f.x) ggplot(data=empirical.mean,aes(x=X.bar)) + geom_histogram(aes(y=after_stat(density)),col=&quot;black&quot;,fill=&quot;blue&quot;, breaks=seq(0,1,by=0.05)) + geom_line(data=pdf,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + geom_vline(xintercept=1/3,col=&quot;green&quot;,lwd=1) + geom_segment(x=1/3-0.0745,xend=1/3+0.0745,y=2.5,yend=2.5,col=&quot;green&quot;,lwd=1) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.25: The empirical distribution of the sample mean of \\(n = 10\\) data sampled from the distribution \\(f_X(x) = 2(1-x)\\) for \\(x \\in [0,1]\\). The red line indicates \\(f_X(x)\\), while the vertical and horizontal green lines indicate \\(E[\\bar{X}] = \\mu = 1/3\\) and the range \\([\\mu-se(\\bar{X}),\\mu+se(\\bar{X})] = [0.2588,0.4078]\\). The shape of the empirical distribution is approaching that of a normal distribution, a result that we will discuss in Chapter 2 when introducing the Central Limit Theorem. See Figure 1.25. The mean of our pdf is \\[ E[X] = \\int_0^1 2x(1-x) dx = \\int_0^1 2xdx - \\int_0^1 2x^2dx = \\left. x^2 \\right|_0^1 - \\left. \\frac{2}{3}x^3 \\right|_0^1 = 1 - \\frac{2}{3} = \\frac{1}{3} \\,. \\] This is indicated via the green vertical line in the figure. The variance is \\[\\begin{align*} V[X] &amp;= E[X^2] - (E[X])^2 \\\\ &amp;= \\left[ \\int_0^1 2x^2dx - \\int_0^1 2x^3dx \\right] - \\left(\\frac{1}{3}\\right)^2 \\\\ &amp;= \\left(\\frac{2}{3} - \\frac{1}{2}\\right) - \\frac{1}{9} \\\\ &amp;= \\frac{1}{6} - \\frac{1}{9} = \\frac{1}{18}\\\\ \\end{align*}\\] The standard error for \\(\\bar{X}\\) is thus \\[ se(\\bar{X}) = \\sqrt{\\frac{V[X]}{n}} = \\sqrt{\\frac{1}{180}} = 0.0745 \\,. \\] The range from the mean minus one standard error to the mean plus one standard error is indicated via the green horizontal line segment in the figure. We observe that the distribution of the sample mean values matches the expected mean and standard error well, and is definitely different from the distribution of the individual data values (which is shown as the red line in the figure). The sample mean distribution almost looks like a normal distribution, but it isn’t one exactly…and for now we will have to content ourselves with only knowing the mean and the standard error of the distribution, and not its mathematical details. However, as we’ll see in Chapter 2, the sample mean distribution will look more and more like a normal distribution as the sample size \\(n\\) goes to infinity, in a result dubbed the Central Limit Theorem. 1.13 The Likelihood Function Assume we are given iid data \\(\\mathbf{X} = \\{X_1,\\ldots,X_n\\}\\), with each datum sampled from a continuous distribution \\(f_X(x \\vert \\theta)\\). (Recall that \\(\\theta\\) is the conventionally used symbol for a population parameter or set of parameters. Here, without loss of generality, we will assume that \\(\\theta\\) represents one parameter.) The likelihood function for the entire sample is defined as \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = f_X(\\mathbf{x} \\vert \\theta) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\,. \\] The second equality holds because the data are assumed to be iid. Note that the same definition holds for discrete distributions, with the notational change \\(f \\rightarrow p\\). Additionally, recall that \\(\\prod\\) is the product symbol, the multiplicative analogue of the summation symbol \\(\\sum\\): \\(\\prod_{i=1}^n f_X(x_i \\vert \\theta) = f_X(x_1 \\vert \\theta) \\cdot f_X(x_2 \\vert \\theta) \\cdot \\cdots \\cdot f_X(x_n \\vert \\theta)\\). As a last comment, we will find that we often work not with the likelihood function itself, but with the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\); given iid data, the log-likelihood is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\left[ \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\right] = \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta) \\,. \\] Let’s step back for an instant here, and assume our sample size is \\(n = 1\\). At first blush, it would appear that a likelihood is the same as a probability density function, because, after all, \\(\\mathcal{L}(\\theta \\vert x) = f_X(x \\vert \\theta)\\). But note what is being conditioned upon in both functions: for the likelihood, we consider that the datum is fixed to its observed value (i.e., \\(X = x\\)) and that we are free to vary the value of \\(\\theta\\). To show the difference between a probability distribution and the likelihood function, let’s take a look at a simple example where we flip a potentially unfair coin twice, with \\(p\\) being the probability of observing heads in any single flip. The probability mass function for \\(X\\), the random variable denoting the number of heads observed, is \\[ p_X(x \\vert p) = \\left\\{ \\begin{array}{cl} 2 &amp; p^2 \\\\ 1 &amp; 2p(1-p) \\\\ 0 &amp; (1-p)^2 \\end{array} \\right. \\,. \\] The probability mass functions that arise when we set \\(p\\) to, e.g., 0.3, 0.5, and 0.8 are shown in Figure 1.26, while the likelihood functions for each observable value of \\(x\\) are shown in Figure 1.27. None of the plots in Figure 1.27 shows the pmf \\(p_X(x \\vert p)\\); rather, they each show the relative plausibility of a particular probability \\(p\\) given the number of observed heads. If we observe zero (or two) heads, then a probability of \\(p=0\\) (or \\(p=1\\)) is the most plausible value…but only \\(p=1\\) (or \\(p=0\\)) is impossible. On the other hand, when we observe one head and one tail, the most plausible value for \\(p\\) is 0.5, although all other values (save \\(p=0\\) and \\(p=1\\)) are possible as well. Figure 1.26: From left to right, the probability mass functions \\(p_X(x \\vert p)\\) for probabilities \\(p =\\) 0.3, 0.5, and 0.8. Figure 1.27: From left to right, the likelihood function \\(\\mathcal{L}(p \\vert x)\\) for the probability parameter \\(p\\) given that we observe \\(x=0\\) heads, \\(x=1\\) head, and \\(x=2\\) heads, respectively. One might question at this point why the likelihood function is important, as it is indeed not a pmf or pdf. We will see below that it is often used when trying to uncover the truth about a population: e.g., given a set of data, randomly sampled from some family of distributions parameterized by \\(\\theta\\), we can utilize the likelihood to infer, or estimate, \\(\\theta\\). Before we talk about estimation, however, we need to discuss how one might summarize a set of data so as to make it mathematically more easy to work with…in other words, we need to talk about statistics. 1.13.1 Examples of Likelihood Functions for IID Data (For more information on the product-symbol manipulations utilized below, see the material on useful product symbol tricks in Chapter 8.) We are given \\(n\\) iid samples from \\[ f_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\theta x^{\\theta-1} &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\theta x_i^{\\theta-1} = \\theta^n \\left(\\prod_{i=1}^n x_i\\right)^{\\theta-1} \\,, \\] while the log-likelihood function is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x}) = n \\log \\theta + (\\theta-1) \\log \\prod_{i=1}^n x_i = n \\log \\theta + (\\theta-1) \\sum_{i=1}^n \\log x_i \\,. \\] We are given \\(n\\) iid samples from \\[ p_X(x \\vert p) = \\left\\{ \\begin{array}{cl} p &amp; x=1 \\\\ 1-p &amp; x=0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We observe \\(k\\) values of 1, and \\(n-k\\) values of 0. Thus the likelihood function is \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n p_X(x_i \\vert p) = \\prod_{i=1}^k p \\times \\prod_{i=1}^{n-k} (1-p) = p^k(1-p)^{n-k} \\,, \\] and the log-likelihood function is \\[ \\ell(p \\vert \\mathbf{x}) = \\log \\mathcal{L}(p \\vert \\mathbf{x}) = k \\log p + (n-k) \\log (1-p) \\,. \\] We are given \\(n\\) iid samples from the mixed distribution \\[ h_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} \\frac{x}{\\theta} &amp; x \\in [0,1] \\\\ 1-\\frac{1}{2\\theta} &amp; x=2 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] We observe \\(l\\) values between 0 and 1, along with \\(n-l\\) values of 2. Let \\(j\\) denote the indices of those data with values between 0 and 1, and \\(k\\) denote the indices of those data with value 2. The likelihood function is then \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n h_X(x_i \\vert p) = \\prod_{j=1}^l \\frac{x_j}{\\theta} \\times \\prod_{k=1}^{n-l} \\left(1-\\frac{1}{2\\theta}\\right) = \\left( \\frac{1}{\\theta^l} \\prod_{j=1}^l x_j \\right) \\left(1-\\frac{1}{2\\theta}\\right)^{n-l} \\] 1.13.2 Coding the Likelihood Function in R Let’s code and display the likelihood function for the mixed distribution we introduce immediately above. The first thing we will do, however, is generate data from this distribution using inverse transform sampling. The cdf for this distribution is \\[ H_X(x \\vert \\theta) = \\left\\{ \\begin{array}{cl} 0 &amp; x &lt; 0 \\\\ x^2/2\\theta &amp; 0 \\leq x \\leq 1 \\\\ 1/2\\theta &amp; 1 &lt; x &lt; 2 \\\\ 1 &amp; x \\geq 2 \\end{array} \\right. \\,. \\] For cdf values \\(q &lt; 1/(2\\theta)\\), the inverse function is \\(x = \\sqrt{2 \\theta q}\\). We thus code the inverse transform sampler as follows… set.seed(101) n &lt;- 100 theta &lt;- 1.5 q &lt;- runif(n,min=0,max=1) w &lt;- which(q &lt; 1/(2*theta)) X.sample &lt;- rep(2,n) X.sample[w] &lt;- sqrt(2*theta*q[w]) Given our dataset, we can compute and visualize the likelihood function \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\)…but…this will be problematic. If we examine the likelihood function, we see that its values will be (a) tiny, and (b) spread over a large dynamic range. (In fact, for a sufficiently large dataset, the likelihood function will have a value too small to be recordable as a floating-point number on a computer!) Thus in practice it is often best to visualize the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x})\\) as a function of \\(\\theta\\). We do this below in Figure 1.28. (We will leave the derivation of the log-likelihood as an exercise to the reader.) loglike &lt;- function(theta,x) { w &lt;- which(x &lt; 1) n &lt;- length(x) l &lt;- length(w) return(log(prod(x[w])) - l*log(theta) + (n-l)*log(1-1/2/theta)) } theta &lt;- seq(0.51,10.0,by=0.01) llike &lt;- rep(NA,length(theta)) for ( ii in 1:length(theta) ) { llike[ii] &lt;- loglike(theta[ii],X.sample) } w &lt;- which.max(llike) df &lt;- data.frame(theta=theta,llike=llike) ggplot(data=df,aes(x=theta,y=llike)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_vline(xintercept=theta[w],col=&quot;red&quot;,lwd=1) + labs(x=expression(theta),y=&quot;Log-Likelihood&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.28: The log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\) for the pdf \\(h_X(x \\vert \\theta)\\) defined in the text. The red line indicates the value of \\(\\theta\\) (1.56) for which \\(\\ell(\\theta \\vert \\mathbf{x})\\) is maximized. 1.14 Point Estimation Let’s suppose we are given a sample of iid data \\(\\{X_1,\\ldots,X_n\\}\\), sampled from some distribution with mean \\(E[X]\\) and finite variance \\(V[X]\\). As we started to discuss above, we can use functions of these data, or statistics, to make inferences about population properties: what are plausible values for the population mean \\(\\mu\\)? or the population variance \\(\\sigma^2\\)? or, more generally, any population parameter \\(\\theta\\)? In point estimation, the statistic that we choose is an estimator of \\(\\theta\\). We dub the estimate \\(\\hat{\\theta}\\): this is a number that is our best guess for what the true value of \\(\\theta\\) is, given the data we have observed thus far. Now…how do we define an estimator? Well, to start, we can guess what might be good estimators, and compare their properties. For instance, here, let’s propose two estimators for \\(\\mu\\): \\(\\hat{\\mu} = X_1\\) and \\(\\hat{\\mu} = \\bar{X}\\). Which is better? It may seem intuitively obvious that \\(\\bar{X}\\) is better, because it incorporates more data…but how do we quantify “better”? In the last section, we indicated that we can assess the utility of a statistic used as an estimator by computing metrics, quantities that allow us to directly compare estimators. Here, we will highlight two of them, the bias and the variance; later, we will highlight others. (Recall that an estimator is a statistic, and thus it is a random variable that is drawn from a sampling distribution with some mean and some variance.) Bias: does the estimator yield the true value, on average? In other words, is \\(B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta] = E[\\hat{\\theta}] - \\theta = 0\\)? If so, we say that our estimator is unbiased. Here, both estimators are unbiased: \\(E[X_1-\\mu] = E[X_1]-\\mu = \\mu-\\mu = 0\\), and \\(E[\\bar{X}-\\mu] = E[\\bar{X}] - \\mu = \\mu-\\mu = 0\\). Variance: is the spread of values that the estimator generates from experiment to experiment relatively small, or relatively large? (Recall that it cannot be zero, due to the randomness inherent in the data-generating process!) Here, the first estimator has variance \\(V[X_1] = \\sigma^2\\), while the second has variance \\(V[\\bar{X}] = \\sigma^2/n\\). (Recall that we derive the latter result in the previous section!) If \\(n &gt; 1\\), the second estimator, with its smaller variance, is the better one to use. (If \\(n = 1\\), then \\(\\bar{X} = X_1\\), so the two estimators are identical anyway.) See Figure 1.29 for a graphical representation of bias and variance. Figure 1.29: A graphical representation of the concepts of bias (how far on average an estimate is from the truth…represented here as an offset from the bullseye) and variance (the spread of estimate values…represented here as the spatial spread of the plotted points). Given a choice, we generally prefer unbiased estimators with smaller variances; however, it is theoretically possible that we might be better off with an estimator with a small bias if it has an even lower variance than the best unbiased one. We can start making sense of this statement now by stating that we can combine the information about bias and variance together into a single metric, the mean-squared error (or MSE). The MSE is defined as \\[ MSE[\\hat{\\theta}] = B[\\hat{\\theta}]^2 + V[\\hat{\\theta}] \\,, \\] and smaller values are better. For \\(\\hat{\\theta} = X_1\\), the MSE is \\(0^2 + \\sigma^2 = \\sigma^2\\), while for \\(\\hat{\\theta} = \\bar{X}\\), the MSE is \\(0^2 + \\sigma^2/n = \\sigma^2/n\\); \\(\\bar{X}\\) is still the better estimator. At this point, one might be thinking that guessing estimators would be a sub-optimal approach. And one would be correct. Over the remainder of the book, we introduce different algorithmic approaches for defining estimators with (presumably) good properties. Here we examine a first one: maximum likelihood estimation (or MLE). Above, in the section introducing the likelihood, we discussed how the likelihood function \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) encapsulates the relative plausibilities of different values of \\(\\theta\\) given the data that are observed. Maximum likelihood estimation takes this idea to its natural conclusion: the most plausible value of \\(\\theta\\), the one that maximizes the likelihood function, is indeed a good way to estimate the true value of \\(\\theta\\). Assuming that the likelihood function achieves a maximum away from \\(\\theta_{\\rm lo}\\) and \\(\\theta_{\\rm hi}\\), the parameter bounds, then the steps to find \\(\\hat{\\theta}_{MLE}\\) involve straightforward calculus, albeit with a simplifying twist: Write down the likelihood function: \\(\\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta)\\). Take the natural logarithm of \\(\\mathcal{L}\\): \\(\\ell(\\theta \\vert \\mathbf{x}) = \\log \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\sum_{i=1}^n \\log f_X(x_i \\vert \\theta)\\). Compute the first derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) with respect to \\(\\theta\\). If \\(\\theta\\) represents more than one parameter, take the first partial derivative with respect to the parameter of interest. Set \\(\\ell&#39;(\\theta \\vert \\mathbf{x}) = 0\\). Solve for \\(\\theta\\). The solution is \\(\\hat{\\theta}_{MLE}\\), assuming that the second derivative of \\(\\ell(\\theta \\vert \\mathbf{x})\\) is negative (i.e., concave down); otherwise, we have actually found a local minimum of the likelihood function. Note that the “simplifying twist” is transforming the likelihood \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) to the log-likelihood \\(\\ell(\\theta \\vert \\mathbf{x})\\); differentiating the latter is often, if not always, easier than differentiating the former. But even if we skip step 2 and compute the first derivative of \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) directly, we will eventually get the same expression for \\(\\hat{\\theta}_{MLE}\\). 1.14.1 Comparing Two Estimators Let’s assume that we are given \\(n\\) iid data sampled from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta} \\] for \\(0 \\leq x \\leq \\theta\\), with \\(\\theta\\) unknown. The expected value and variance of this distribution are \\(\\mu = E[X] = \\theta/2\\) and \\(\\sigma^2 = V[X] = \\theta^2/12\\), respectively. We propose two estimators for \\(\\theta\\): \\(2\\bar{X}\\), and \\(X_1+X_2\\). Which is better? Intuitively, we know the answer, but can we quantify it, i.e., can we determine which of the two estimators has a smaller mean-squared error? For \\(\\hat{\\theta} = 2\\bar{X}\\), the expected value is \\[ E[2\\bar{X}] = 2E[\\bar{X}] = 2\\mu = \\theta \\,, \\] and thus we can see that \\(2\\bar{X}\\) is unbiased. (Here, we utilize the general result that \\(E[\\bar{X}] = \\mu\\).) Thus the MSE will simply be the variance of this estimator: \\[ MSE[\\hat{\\theta}] = V[2\\bar{X}] = 4V[\\bar{X}] = 4\\frac{\\sigma^2}{n} = \\frac{\\theta^2}{3n} \\,. \\] (Here, we utilize the general result that \\(V[\\bar{X}] = \\frac{\\sigma^2}{n}\\).) For \\(\\hat{\\theta} = X_1 + X_2\\), the expected value is \\[ E[X_1+X_2] = E[X_1] + E[X_2] = \\frac{\\theta}{2} + \\frac{\\theta}{2} = \\theta \\,. \\] The estimator is unbiased. The MSE is thus \\[ MSE[\\hat{\\theta}] = V[X_1+X_2] = V[X_1] + V[X_2] = \\frac{\\theta^2}{12} + \\frac{\\theta^2}{12} = \\frac{\\theta^2}{6} \\,. \\] Compare the two MSE expressions, keeping in mind that the second one is meaningless if \\(n=1\\). For \\(n=2\\) the MSEs are equivalent, which makes sense since the estimators themselves are equivalent. If \\(n &gt; 2\\), then the MSE for \\(\\hat{\\theta} = 2\\bar{X}\\), is smaller, and it will continue getting smaller as \\(n\\) increases, unlike the MSE for \\(\\hat{\\theta} = X_1 + X_2\\). 1.14.2 Maximum Likelihood Estimate of Population Parameter Let’s assume that we are given \\(n\\) iid data sampled from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta}\\exp({-x/\\theta}) \\,, \\] with \\(x \\geq 0\\) and \\(\\theta &gt; 0\\). (To be clear: in real-life situations, we do not know the form of the pmf or pdf from which the data are sampled! We assume a family of distributions, then estimate the value of the population parameter of interest.) For this distribution, \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2\\). The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\theta}\\exp\\left(-\\frac{x_i}{\\theta}\\right) = \\frac{1}{\\theta^n} \\prod_{i=1}^n \\exp\\left(-\\frac{x_i}{\\theta}\\right) = \\frac{1}{\\theta^n} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i\\right) \\,, \\] and the log-likelihood is \\[ \\ell(\\theta \\vert \\mathbf{x}) = \\log \\left[ \\frac{1}{\\theta^n} \\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i\\right) \\right] = -n \\log \\theta - \\frac{1}{\\theta} \\sum_{i=1}^n x_i \\,. \\] The next step in determining \\(\\hat{\\theta}_{MLE}\\) is to take the first derivative with respect to \\(\\theta\\), \\[ \\ell&#39;(\\theta \\vert \\mathbf{x}) = \\frac{d}{d\\theta} \\ell(\\theta \\vert \\mathbf{x}) = -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^n x_i \\,, \\] and set the result equal to zero: \\[ -\\frac{n}{\\theta} + \\frac{1}{\\theta^2} \\sum_{i=1}^n x_i = 0 = -n + \\frac{1}{\\theta} \\sum_{i=1}^n x_i \\,. \\] Solving for \\(\\theta\\), we get \\[ \\hat{\\theta}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\,. \\] When we switched from solving for the generic quantity \\(\\theta\\) to solving for an estimator, which is a function of random variables, we switched from using the lower-case generic variable \\(x\\) to using an upper-case-denoted random variable \\(X\\). (Note that we should check to see whether the second derivative is negative at the extremum, indicating that \\(\\hat{\\theta}_{MLE}\\) is located at a local maximum of the likelihood function rather than a minimum. So: \\[ \\ell&#39;&#39;(\\theta \\vert \\mathbf{x}) = \\frac{d}{d\\theta} \\ell&#39;(\\theta \\vert \\mathbf{x}) = \\frac{n}{\\theta^2} - \\frac{2}{\\theta^3} \\sum_{i=1}^n x_i = \\frac{n}{\\theta^2} \\left( 1 - \\frac{2n}{\\theta}\\bar{x} \\right)\\,. \\] Let’s plug in \\(\\theta = \\hat{\\theta}_{MLE} = \\bar{x}\\): \\[ \\ell&#39;&#39;(\\hat{\\theta}_{MLE} \\vert \\mathbf{x}) = \\frac{n}{\\bar{x}^2} \\left( 1 - 2n \\right) \\,. \\] We know that \\(n\\) is positive and \\(\\geq 1\\) and that \\(\\bar{x} &gt; 0\\), so indeed \\(\\ell&#39;&#39;(\\hat{\\theta}_{MLE} \\vert \\mathbf{x}) &lt; 0\\) and thus we have detected a maximum of the likelihood function.) Now, is this estimate biased? We know from results shown above that \\(E[\\bar{X}] = \\theta\\), thus \\(B[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}] - \\theta = \\theta - \\theta = 0\\) and thus the estimator is unbiased. We also know that \\(V[\\bar{X}] = \\sigma^2/n = \\theta^2/n\\). The question, to be answered in a future chapter, is whether we can possibly find an estimator with a lower variance via some other estimation approach…or if this indeed the best that we can do. Now, when we solve for, e.g., \\(\\hat{\\theta}_{MLE}\\), we can solve for other quantities as well…it is just algebra. Meaning, for instance, that if we want to estimate \\(\\hat{\\theta}_{MLE}^2\\) for whatever reason, we can just square both sides in the solution above: \\[ (\\hat{\\theta}_{MLE})^2 = (\\bar{X})^2 ~~\\Rightarrow~~ \\hat{\\theta}_{MLE}^2 = \\bar{X}^2 \\,. \\] (This is a manifestation of the so-called invariance property of the MLE.) Now, is this a biased estimator for \\(\\theta^2\\)? We can utilize the shortcut formula for computing variance to find that \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{\\sigma^2}{n} + \\mu^2 = \\frac{\\theta^2}{n} + \\theta^2 = \\theta^2 \\left( 1 + \\frac{1}{n} \\right) \\neq \\theta^2 \\,. \\] This estimator is biased…but the bias goes away as the sample size \\(n\\) increases. That means that we would call this estimator asymptotically unbiased, i.e., it is unbiased when the sample size is infinite. We note here that maximum likelihood estimates are always either unbiased or asymptotically unbiased. 1.15 Statistical Inference with Sampling Distributions A fundamental issue with point estimation is that, e.g., it does not provide a notion of how uncertain an estimate is. By this, we do not mean the standard error of the sampling distribution for the statistic we use when making the estimate (which is a quantity that we can derive), but rather how large (or small) is the range of plausible values of \\(\\theta\\) given the observed value of the statistic? Point estimation also does not allow us to answer the question of “is a particular hypothesized value of \\(\\theta\\) plausible given what we observe?” We can resolve the first issue by computing a confidence interval for \\(\\theta\\), and the second via hypothesis testing. We discuss each of these concepts in turn in the next two sections. Before doing so, however, we will show how the construction of confidence intervals and the performance of hypothesis tests both boil down to performing a root-finding exercise in which we work directly with the sampling distribution. Let \\(Y = g(X_1,\\ldots,X_n)\\) be a statistic, with the \\(X_i\\)’s being independent and identically distributed (iid) random variables. (To be clear, \\(Y\\) does not have to be a function of all the observed data, but rather at least some of them. Think of the sample median in contrast to the sample mean.) As we describe above, \\(Y\\) has a sampling distribution, whose probability density function (pdf) is \\(f_Y(y \\vert \\theta)\\) and whose cumulative distribution function (cdf) is \\(F_Y(y \\vert \\theta)\\). (For simplicity, and without loss of generality, we assume \\(Y\\) is a continuous random variable.) We can alter the shape and/or location of the sampling distribution by change the value(s) of \\(\\theta\\). To illustrate this, let’s make up a sampling distribution pdf: \\[ f_Y(y \\vert \\theta) = \\frac{1}{2} ~~~ y \\in [\\theta-1,\\theta+1] \\] In Figure 1.30, we show how the pdf moves with \\(\\theta\\); as \\(\\theta\\) changes from 2.8 to 5.2, the pdf shifts (smoothly) from the location indicated by the red lines to that indicated by the blue lines. Now, let’s assume that the green line in the figure, at value \\(y_{\\rm obs} = 3.4\\), is the statistic value that we actually observe. What can we conclude right away, on the basis of this figure? We can conclude that this observed value is plausible if \\(\\theta = 2.8\\), but implausible if \\(\\theta = 5.2\\). In other words, 2.8 is an “acceptable” value for \\(\\theta\\), but 5.2 is not. Figure 1.30: An illustration of how a sampling distribution pdf can move as the distribution parameter is changed. Here, \\(f_Y(y) = 1/2\\) for \\(y \\in [\\theta-1,\\theta+1]\\); the red lines represent the pdf for \\(\\theta = 2.8\\) and the blue lines represents the pdf for \\(\\theta=5.2\\). The green vertical line represents the observed value of the statistic \\(Y\\): \\(y_{\\rm obs} = 3.4\\). To tie this illustration back to confidence intervals and hypothesis testing, a confidence interval for \\(\\theta\\) is a range of “acceptable values” for \\(\\theta\\) given what we observe (which initially we can think of here as \\([\\hat{\\theta}_L,\\hat{\\theta}_H] = [2.4,4.4]\\), since these values generate pdfs that overlap \\(y_{\\rm obs}\\)), and a hypothesis test is the determination of whether a stated value of \\(\\theta\\) is “acceptable” given what we observe (here, a hypothesis that \\(\\theta = 3.1\\) would be acceptable, while a hypothesis that \\(\\theta = 10\\) definitely would not be). Let’s generalize this a bit: we cannot, for instance, construct a confidence interval simply by asking if the sampling distribution pdf for our statistic overlaps the observed value, because given its domain, it may always overlap the observed value! (Albeit perhaps with \\(f_Y(y)\\) having a very small value.) So, by convention, we adopt a value \\(\\alpha\\) and ask the following questions for a two-sided interval: for what value of \\(\\theta\\) is \\(\\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta) = \\alpha/2\\) (in other words, for what value of \\(\\theta\\) is \\(F_Y(y_{\\rm obs} \\vert \\theta) = \\alpha/2\\)); and for what value of \\(\\theta\\) is \\(\\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta) = \\alpha/2\\) (or is \\(F_Y(y_{\\rm obs} \\vert \\theta) = 1 - \\alpha/2\\))? Figure 1.31: Given the setting defined in Figure 1.30, we can ask (a) for what value of \\(\\theta\\) is the area under \\(f_Y(y)\\) to the left of \\(y_{\\rm obs}\\) equal to \\(\\alpha/2\\), and (b) for what other value of \\(\\theta\\) is the area under \\(f_Y(y)\\) to the right of \\(y_{\\rm obs}\\) equal to \\(\\alpha/2\\)? These two values comprise a two-sided confidence interval. Here, \\(\\alpha = 0.1\\). (See Figure 1.31.) And here we come back to the notion that this is a “root-finding exercise.” To answer the first bullet-pointed question above, we would find the root \\(\\theta_{\\alpha/2}\\) for the equation \\[ F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} = 0 \\,. \\] As we can see in the left panel of the figure, \\(\\theta_{\\alpha/2}\\) represents the upper bound on \\(\\theta\\), but in general that is not always going to be true; in the next section we’ll dig more into the details of how we determine whether or not \\(\\theta_{\\alpha/2}\\) is a lower bound or an upper bound. To answer the second question, we would find the root \\(\\theta_{1-\\alpha/2}\\) for the equation \\[ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) = 0 \\,. \\] Referring back to the right panel of the figure, we see that \\(\\theta_{1-\\alpha/2}\\) represents the lower bound on \\(\\theta\\). We will note here that in relatively rare circumstances, we can solve for the roots by hand, but as we will see in examples sprinkled throughout the rest of the book, we can always solve for them numerically using functions like R’s uniroot(). Now, what about hypothesis testing? As we will see below, in a hypothesis test, we adopt a null hypothesis (\\(\\theta = \\theta_o\\)) and an alternative hypothesis (e.g., \\(\\theta \\neq \\theta_o\\)), and we combine this information with \\(y_{\\rm obs}\\) to determine whether the null hypothesis is viable (in which case we “fail to reject the null”) or not (in which case we “reject the null”). For a two-tail test, we would determine the boundaries of the so-called rejection region by finding the roots \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) for each of the following equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 ~~\\Rightarrow~~ y_{\\alpha/2} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o) \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 ~~\\Rightarrow~~ y_{1-\\alpha/2} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\,, \\end{align*}\\] where \\(F_Y^{-1}(\\cdot)\\) is the inverse cdf function associated with the sampling distribution. Here, if \\(y_{\\rm obs} \\leq y_{\\alpha/2}\\) or \\(y_{\\rm obs} \\geq y_{1-\\alpha/2}\\), we would reject the null, i.e., we would deem \\(\\theta_o\\) implausible. Note how the equations above are the same ones we use when constructing confidence intervals; here we fix \\(\\theta = \\theta_o\\) (as opposed to solving for \\(\\theta\\)) and we solve for \\(y\\) (as opposed to fixing \\(y = y_{\\rm obs}\\)). As is the case for confidence intervals, sometimes we can find the roots by hand, but if not, we can often find them by using off-the-shelf inverse cdf codes like R’s qnorm() (if the sampling distribution is a normal distribution), etc. An important message to take away from this section is that confidences interval estimation and hypothesis testing both involve the same fundamental root-finding exercise: they only differ in terms of what values are fixed and what values we are solving for! 1.16 Confidence Intervals Interval estimation is a mechanism to determine a range of possible values for a distribution parameter \\(\\theta\\) that overlaps the true but unknown value with some stated probability. A two-sided interval estimate, or confidence interval, has the form \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the user-set confidence coefficient, which typically has the value 0.95 (or \\(\\alpha = 0.05\\)). One can also define one-sided intervals: \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\right) = 1 - \\alpha ~~\\mbox{and}~~ P\\left( \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,. \\] It is important for us to interpret an interval estimate correctly: it is a random interval, meaning that if we re-do the data-generating experiment, the interval will change; and we state that the probability that, e.g., the two-sided interval estimate \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) will overlap \\(\\theta\\) is \\(1 - \\alpha\\). In particular, one does not say “the probability that \\(\\theta\\) lies in the stated interval is \\(1-\\alpha\\).” \\(\\theta\\) is a population quantity, and thus we cannot make probabilistic statements about it. It has the (unknown) value it has. Stated another way, in reference to point 2: the probability of \\(1 - \\alpha\\) refers to the reliability of the estimation procedure and not to any one specific interval. For instance, if we compute an interval of [1,2] and then re-do the experiment and compute an interval of [2.5,3.5], and if we say there is a 95% chance that \\(\\theta\\) is in [1,2] and a 95% chance it is in [2.5,3.5], then there must be a 190% chance it is in either. A computed interval either overlaps the true value, or it does not; it is no longer a matter of probability. See Figure 1.32. Figure 1.32: Schematic illustration of ten confidence intervals constructed given ten separate independent datasets sampled from the same underlying population. We indicate the true parameter value \\(\\theta\\) with the vertical dashed line. Counting from the bottom, we observe that the fourth and seventh intervals do not overlap the true value; if we were to claim that the probability that \\(\\theta\\) lies within each interval is, e.g., 0.95, then the probability that \\(\\theta\\) lies in either the fourth or seventh interval is 1.9…which is clearly wrong. Thus the proper interpretation would be that 95 percent of evaluated intervals overlap the true value. As laid out in the last section above, to determine a two-sided confidence interval, one would solve for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] There is a very important nuance here, however, that we ignored in the last section, which is that it is not guaranteed that the value of the statistic \\(Y\\) that we adopt to construct the interval increases as \\(\\theta\\) increases. What that means here is that, e.g., \\(\\theta_{\\alpha/2}\\) might represent an upper bound on \\(\\theta\\), and it will if \\(Y\\) increases as \\(\\theta\\) increases…but it might represent a lower bound, if it turns out that \\(Y\\) actually decreases as \\(\\theta\\) increases. This is not particularly worrisome if we are constructing a two-sided interval; if we find that \\(\\theta_{\\alpha/2} &gt; \\theta_{1-\\alpha/2}\\), we can simply switch the two numbers (which themselves are correct). Where we must exercise care is when we are trying to construct a one-sided interval. In that case: if \\(E[Y]\\) increases as \\(\\theta\\) increases, then the one-sided intervals are \\((-\\infty,\\theta_{\\alpha}]\\) (upper bound) and \\([\\theta_{1-\\alpha},\\infty)\\) (lower bound; note that we use \\(\\alpha\\) instead of \\(\\alpha/2\\)); and if \\(E[Y]\\) decreases as \\(\\theta\\) increases, then the intervals are \\((-\\infty,\\theta_{1-\\alpha}]\\) (upper bound) and \\([\\theta_{\\alpha},\\infty)\\) (lower bound; note how the \\(\\alpha\\) and \\(1-\\alpha\\) have been reversed). We summarize these results below: type \\(E[Y]\\) increases with \\(\\theta\\)? \\(\\hat{\\theta}_L\\) \\(\\hat{\\theta}_H\\) two-sided yes \\(\\hat{\\theta}_{1-\\alpha/2}\\) \\(\\hat{\\theta}_{\\alpha/2}\\) no \\(\\hat{\\theta}_{\\alpha/2}\\) \\(\\hat{\\theta}_{1-\\alpha/2}\\) one-sided yes \\(\\hat{\\theta}_{1-\\alpha}\\) \\(\\infty\\) lower bound no \\(\\hat{\\theta}_{\\alpha}\\) \\(\\infty\\) one-sided yes \\(-\\infty\\) \\(\\hat{\\theta}_{\\alpha}\\) upper bound no \\(-\\infty\\) \\(\\hat{\\theta}_{1-\\alpha}\\) 1.16.1 Confidence Interval Where \\(E[Y]\\) Increases With \\(\\theta\\) We have conducted an experiment in which we sample \\(n=1\\) datum from the following pdf: \\[ f_X(x) = \\theta x^{\\theta-1} ~~~ x \\in [0,1] \\,. \\] The value we observe is \\(x_{\\rm obs}\\). Below, we define a two-sided confidence interval on \\(\\theta\\) assuming \\(\\alpha = 0.05\\), as well as show how we would define either a lower or an upper bound on \\(\\theta\\). Before proceeding, we will make a notation change by saying our statistic \\(Y\\) equals \\(X\\) (because, with a single datum, what else could our statistic realistically be?), and thus the observed value is \\(y_{\\rm obs} = x_{\\rm obs}\\). (This makes our random variable notation consistent with that of the last section.) The sampling distribution pdf is \\(f_Y(y) = \\theta y^{\\theta-1}\\) and the cdf is \\[ F_Y(y) = \\int_0^y \\theta z^{\\theta-1} dz = \\left. z^\\theta \\right|_0^y = y^\\theta \\,. \\] Now, does \\(E[Y]\\) increase with \\(\\theta\\)? \\[\\begin{align*} E[Y] = \\int_0^1 y f_Y(y) dy &amp;= \\int_0^1 y \\theta y^{\\theta-1} dy \\\\ &amp;= \\theta \\int_0^1 y^{\\theta} dy \\\\ &amp;= \\frac{\\theta}{\\theta+1} \\left. y^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,. \\end{align*}\\] As \\(\\theta \\rightarrow \\infty\\), \\(E[Y]\\) increases towards 1. This tells us that given the two equations \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,, \\end{align*}\\] \\(\\theta_{1-\\alpha/2}\\) represents the lower bound \\(\\hat{\\theta}_L\\) and \\(\\theta_{\\alpha/2}\\) represents the upper bound \\(\\hat{\\theta}_U\\). Now let’s solve. For the lower bound, we find that \\[\\begin{align*} y_{\\rm obs}^{\\hat{\\theta}_L} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L \\log y_{\\rm obs} &amp;= \\log \\left( 1 - \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L &amp;= \\frac{\\log \\left( 1 - \\frac{\\alpha}{2} \\right)}{\\log y_{\\rm obs}} \\,, \\end{align*}\\] while for the upper bound, we simply switch \\(1-\\alpha/2\\) to \\(\\alpha/2\\): \\[ \\hat{\\theta}_H = \\frac{\\log \\left( \\frac{\\alpha}{2} \\right)}{\\log y_{\\rm obs}} \\,, \\] For instance, for \\(\\alpha = 0.05\\) and \\(y_{\\rm obs}\\) = 0.6, the confidence interval would be \\(\\hat{\\theta}_L = 0.05\\) to \\(\\hat{\\theta}_H = 7.22\\). Not surprisingly, if we have but a single datum, we cannot say that much about \\(\\theta\\)! We can also solve for a “95% upper bound,” wherein we use the same upper bound equation as above but substitute \\(\\alpha\\) for \\(\\alpha/2\\): \\[ \\hat{\\theta}_H = \\frac{\\log \\alpha}{\\log y_{\\rm obs}} \\,, \\] The one-sided interval would be \\((-\\infty,5.86]\\) (although because \\(\\theta\\) is limited to be \\(&gt; 0\\), we would actually write \\((0,5.86]\\)). The one-sided lower bound is then \\[ \\hat{\\theta}_L = \\frac{\\log (1 - \\alpha)}{\\log y_{\\rm obs}} \\,, \\] which in our example evaluates to \\([0.10,\\infty)\\). 1.16.2 Confidence Interval Where \\(E[Y]\\) Decreases With \\(\\theta\\) Let’s switch things up a bit from the previous example, and assume that instead of sampling a single datum from the pdf \\(\\theta x^{\\theta-1}\\), we instead sample one from the pdf \\[ f_X(x) = \\theta (1-x)^{\\theta-1} \\,, \\] where \\(x \\in [0,1]\\) and the cdf is \\[\\begin{align*} F_X(x) &amp;= \\int_0^x \\theta (1-y)^{\\theta-1} dy \\\\ &amp;= \\left. -(1-y)^\\theta \\right|_0^x = 1 - (1-x)^\\theta \\,. \\end{align*}\\] How does this change our confidence interval computations? We first compute the expected value (after changing the notation from \\(x\\) to \\(y\\) to be consistent about how we denote statistics). This computation is a bit more complicated to do, as it involves integration by parts, but it is still relatively straightforward: \\[\\begin{align*} E[Y] = \\int_0^1 y f_Y(y) dy &amp;= \\int_0^1 y \\theta (1-y)^{\\theta-1} dy \\\\ &amp;= \\left. -y(1-y)^\\theta \\right|_0^1 + \\int_0^1 (1-y)^\\theta dy \\\\ &amp;= 0 - \\frac{1}{\\theta+1} \\left. (1-y)^{\\theta+1} \\right|_0^1 = \\frac{1}{\\theta+1} \\,. \\end{align*}\\] As \\(\\theta\\) increases, \\(E[Y]\\) descreases, and thus the statistic \\(Y\\) decreases on average. Thus, unlike in the last example, \\(\\theta_{\\alpha/2}\\) will represent the lower bound on the interval, while \\(\\theta_{1-\\alpha/2}\\) will represent the upper bound. Given this, the calculation proceeds in an analogous manner as in the last example: \\[\\begin{align*} 1-(1-y_{\\rm obs})^{\\hat{\\theta}_L} &amp;= \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ (1-y_{\\rm obs})^{\\hat{\\theta}_L} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L \\log (1-y_{\\rm obs}) &amp;= \\log \\left( 1 - \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_L &amp;= \\frac{\\log \\left( 1 - \\frac{\\alpha}{2} \\right)}{\\log (1-y_{\\rm obs})} \\,, \\end{align*}\\] and \\[\\begin{align*} 1-(1-y_{\\rm obs})^{\\hat{\\theta}_H} &amp;= 1 - \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ (1-y_{\\rm obs})^{\\hat{\\theta}_H} &amp;= \\frac{\\alpha}{2} \\\\ \\Rightarrow ~~~ \\hat{\\theta}_H \\log (1-y_{\\rm obs}) &amp;= \\log \\left( \\frac{\\alpha}{2} \\right) \\\\ \\Rightarrow ~~~ \\hat{\\theta}_H &amp;= \\frac{\\log \\left( \\frac{\\alpha}{2} \\right)}{\\log (1-y_{\\rm obs})} \\,. \\end{align*}\\] For instance, if \\(\\alpha = 0.05\\) and \\(y_{\\rm obs} = 0.6\\), the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_H] = [0.028,4.03]\\). We can also construct one-sided intervals, as we do above. We leave the computation of these intervals to the reader, noting that the 95% upper bound is 3.27 and the 95% lower bound is 0.056. 1.17 Hypothesis Testing Hypothesis testing is, in short, an inference mechanism that takes a pre-conceived notion about \\(\\theta\\) into account. This is in contrast to both point estimation and interval estimation, where we derive \\(\\hat{\\theta}\\) or \\((\\hat{\\theta}_L,\\hat{\\theta}_U)\\) without regard to what we might think the true value of \\(\\theta\\) might be. Perhaps we have randomly sampled a set of students and recorded their weights. Point estimation provides an estimate of the average student weight; interval estimation provides an interval that has probability \\(1-\\alpha\\) of overlapping the true student weight; but hypothesis testing allows one to ask questions like “do my data support my hypothesis that the true average weight of students on campus is 140 pounds?” The steps of basic hypothesis testing are as follows. We formulate a null hypothesis, denoted \\(H_o\\) (“H naught”), in which we specify the value we are testing. Here, that hypothesis might be \\(H_o: \\mu = 140\\). We formulate an alternate or alternative hypothesis, denoted \\(H_a\\) (“H a”). This also takes our pre-conceived notions into account: do we wish to test whether the average student weight is higher, lower, or simply substantially different from \\(H_o\\)? (So here, the possibilities are \\(H_a: \\mu &lt; 140\\), \\(H_a: \\mu &gt; 140\\), or \\(H_a: \\mu \\neq 140\\).) We choose a test statistic (e.g., \\(\\bar{X}\\)) whose sampling distribution we can specify assuming \\(H_o\\) is true. We choose the level of the test \\(\\alpha\\). Combined with the sampling distribution for the test statistic, this allows us to determine a rejection region: if the test statistic falls into this region, we reject the null hypothesis…otherwise, we “fail to reject the null.” There is much to unpack here. We will start by stating that it is imperative that one specify \\(H_o\\), \\(H_a\\), and \\(\\alpha\\) before any data are collected (or at the very least before any data are examined). To look at the data first and then formulate hypotheses and set levels acts to bias the process: it is a manifestation of human nature that we might be tempted to define the test so as to maximize our chances observing the result we desire to achieve. In the end, testing is about assessing pre-conceived notions; actively working to achieve a particular result should never be our aim. Another thing to keep in mind is that hypothesis testing generates decisions, not proofs. When we formulate a test, we are defining a decision boundary: we decide whether we have sufficient evidence to reject the null. When we do not, we fail to reject the null, i.e., we do not say that we have proven that the null is correct, but rather we have concluded that we simply do not have enough data to convince ourselves that the null hypothesis is wrong. And the decisions we make can be wrong! This leads to some more important hypothesis-testing terminology: Type I error: this is the probability that we will decide to reject the null when it is actually true. We set this…this is \\(\\alpha\\) (the level of the test). A conventional choice for \\(\\alpha\\) is 0.05: if the null is true, we have a 1 in 20 chance of erroneously rejecting it. Type II error: this is the probability that we would fail to reject the null given an arbitrary value of \\(\\theta\\); it is denoted \\(\\beta\\) (or perhaps more clearly \\(\\beta(\\theta,\\alpha)\\), since the value is a function of both \\(\\theta\\) and \\(\\alpha\\)). Now that we have laid out the basic framework and terminology, we need to illustrate how hypothesis testing works in practice. But we have the same issue here that we have when constructing confidence intervals in the last section: we do not yet have the tools to derive the sampling distribution for the test statistic. (We introduce these later, beginning in Chapter 2.) So, for now, we illustrate hypothesis testing by assuming that we sample a single datum \\(X\\) from a pdf (which is, by definition, the sampling distribution for our test statistic \\(Y = X\\)). Let’s assume that our statistic \\(Y\\) is drawn from the following pdf: \\[ f_Y(y) = \\frac{1}{\\theta} \\exp\\left(-\\frac{y}{\\theta}\\right) \\,, \\] where \\(\\theta &gt; 0\\) and \\(x \\geq 0\\). (See Figure 1.33.) Let’s further assume that our null hypothesis is \\(\\theta_o = 2\\). To reiterate: if the null hypothesis holds, we expect to observe a statistic \\(y_{\\rm obs}\\) that is consistent with this distribution. The question now is: how close or far from zero does \\(y_{\\rm obs}\\) have to be for us to decide to reject the null? Figure 1.33: Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for \\(\\alpha = 0.1\\). Each curve represents the sampling distribution for the hypothesis test statistic \\(Y\\), given that the null hypothesis is true. If \\(y_{\\rm obs}\\) falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null. In the section before last, we discuss how we would answer this question for a two-tail hypothesis test: we determine the roots \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) that are solutions to the following equations: \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 ~~\\Rightarrow~~ y_{\\alpha/2} = F_Y^{-1}(\\alpha/2 \\vert \\theta_o) \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 ~~\\Rightarrow~~ y_{1-\\alpha/2} = F_Y^{-1}(1-\\alpha/2 \\vert \\theta_o)\\,, \\end{align*}\\] where \\(F_Y^{-1}(\\cdot)\\) is the inverse cdf function associated with the sampling distribution. Both \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) are rejection region boundaries: if \\(y_{\\rm obs} \\leq y_{\\alpha/2}\\) or \\(y_{\\rm obs} \\geq y_{1-\\alpha/2}\\), we make the decision to reject the null. For the example we are working with here: \\[ F_Y(y) = \\int_0^y \\frac{1}{\\theta} \\exp\\left(-\\frac{z}{\\theta}\\right) dz = -\\exp\\left.\\left(-\\frac{z}{\\theta}\\right)\\right|_0^y = 1 - \\exp\\left(-\\frac{y}{\\theta}\\right) \\,, \\] and so \\[ 1 - \\exp\\left(-\\frac{y_{\\alpha/2}}{\\theta_o}\\right) = \\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ y_{\\alpha/2} = -\\theta_o \\log\\left(1-\\frac{\\alpha}{2}\\right) \\,, \\] and \\[ 1 - \\exp\\left(-\\frac{y_{1-\\alpha/2}}{\\theta_o}\\right) = 1-\\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ y_{1-\\alpha/2} = -\\theta_o \\log\\left(\\frac{\\alpha}{2}\\right) \\,. \\] For \\(\\theta_o = 2\\) and \\(\\alpha = 0.05\\), the rejection region bounds are \\(y_{\\alpha/2} = -2\\log(0.975) = 0.051\\) and \\(y_{1-\\alpha/2} = -2\\log(0.025) = 7.378\\). In other words, if the value we sample is \\(\\leq 0.051\\) or \\(\\geq 7.378\\), we decide to reject the null hypothesis that \\(\\theta = 2\\). If we wish to perform a lower- or upper-tail test instead, we need to tread a bit more carefully, in that we need determine how \\(E[Y]\\) varies with \\(\\theta\\): does it increase as \\(\\theta\\) increases, or does it decrease? This dictates whether \\(y_{\\alpha}\\), for instance, is the correct rejection region bound (as opposed to \\(y_{1-\\alpha}\\)). For our current example, \\(E[Y] = \\theta\\), which obviously increases with \\(\\theta\\)…and so \\(y_{\\alpha}\\) is appropriate for lower-tail tests and \\(y_{1-\\alpha}\\) is appropriate for upper-tail tests. Our rejection regions are \\[\\begin{align*} \\mbox{lower-tail:} &amp;~~~ y_{\\alpha} = -\\theta_o \\log(1-\\alpha) = -2 \\log(0.95) = 0.103 \\\\ \\mbox{upper-tail:} &amp;~~~ y_{1-\\alpha} = -\\theta_o \\log(\\alpha) = -2 \\log(0.05) = 5.991 \\,. \\end{align*}\\] We summarize the evaluation of rejection region boundaries in the table below. Type \\(E[Y]\\) increases with \\(\\theta\\)? Rejection Region(s) two-tail yes \\(y_{\\rm obs} \\leq y_{\\alpha/2}\\) and \\(y_{\\rm obs} \\geq y_{1-\\alpha/2}\\) no \\(y_{\\rm obs} \\leq y_{1-\\alpha/2}\\) and \\(y_{\\rm obs} \\geq y_{\\alpha/2}\\) lower-tail yes \\(y_{\\rm obs} \\leq y_{\\alpha}\\) no \\(y_{\\rm obs} \\geq y_{1-\\alpha}\\) upper-tail yes \\(y_{\\rm obs} \\geq y_{1-\\alpha}\\) no \\(y_{\\rm obs} \\leq y_{\\alpha}\\) Note that “lower” and “upper” refer to being less than \\(\\theta_o\\) or greater than \\(\\theta_o\\), and not necessarily to the tail of the sampling distribution itself that contains the rejection region. Also, we note the following. If we reject the null using a one-tail test, for a given dataset, it is not guaranteed that we would have rejected the null using a two-tail test. For instance, for our example, a value \\(y_{\\rm obs} = 0.075\\) would lead us to reject the null for a lower-tail test, but fail to reject the null for a two-tail test. If we reject the null using a two-tail test, it is also not guaranteed that we would have rejected the null using a one-tail test. For instance, a value of \\(y_{\\rm obs} = 8\\) would lead us to reject the null if we perform a two-tail test, but fail to reject the null if we perform a lower-tail test. 1.17.1 Hypothesis Test Where \\(E[Y]\\) Increases With \\(\\theta\\) We repeat the first example in the confidence interval section above, in which \\(f_X(x) = \\theta x^{\\theta-1}\\) for \\(x \\in [0,1]\\), and in which we sample a single datum (and thus assume that our test statistic is \\(Y = X\\), with sampling distribution given by \\(f_X(x)\\)). Let’s assume our null hypothesis is \\(H_o : \\theta = \\theta_o = 1\\), and that we are testing this against the alternative \\(H_a : \\theta &lt; \\theta_o\\). What is the rejection region for this test, assuming \\(\\alpha = 0.05\\)? We start by reminding ourselves that \\(E[Y] = \\theta/(\\theta+1)\\) and that it increases with \\(\\theta\\). Given this information, we examine the table above and see that the rejection region is \\[ y_{\\rm obs} \\leq y_{\\alpha} \\,, \\] where \\(y_{\\alpha}\\) is the solution to the equation \\[ F_Y(y_{\\alpha} \\vert \\theta_o) - \\alpha = 0 \\,. \\] It is important to point out here, before continuing, that the numerical evaluation of rejection region boundaries only depends on \\(\\theta_o\\), and specifically not on the observed statistic value. (Where the regions are, and whether there are one or two, is a function of the alternative hypothesis, but not the numerical evaluation of the boundaries themselves.) The cdf for \\(Y\\) is \\(F_Y(y) = y^\\theta\\). Hence \\[\\begin{align*} F_Y(y_{\\alpha} \\vert \\theta_o) - \\alpha &amp;= 0 \\\\ \\Rightarrow ~~~ y_{\\alpha}^{\\theta_o} &amp;= \\alpha \\\\ \\Rightarrow ~~~ y_{\\alpha} &amp;= \\alpha^{1/\\theta_o} = \\alpha \\,. \\end{align*}\\] Thus if we observe \\(y_{\\rm obs} = x \\leq \\alpha = 0.05\\), we reject the null hypothesis. 1.17.2 Hypothesis Test Where \\(E[Y]\\) Decreases With \\(\\theta\\) Here we repeat the second example in the confidence interval section above, in which \\(f_X(x) = \\theta (1-x)^{\\theta - 1}\\) for \\(x \\in [0,1]\\), and in which we again sample a single datum. Here, we assume the null hypothesis \\(H_o : \\theta = \\theta_o = 2\\), and will we test this against the alternative \\(H_a : \\theta &lt; \\theta_o\\). We know that \\(E[Y] = 1/(\\theta+1)\\), which decreases with \\(\\theta\\). Thus we define the rejection region as \\[ y_{\\rm obs} \\geq y_{1-\\alpha} \\,, \\] where \\(y_{1-\\alpha}\\) is the solution to the equation \\[ F_Y(y_{1-\\alpha} \\vert \\theta_o) - (1-\\alpha) = 0 \\,. \\] Further, we know that \\(F_Y(y) = 1 - (1-y)^\\theta\\). So \\[\\begin{align*} F_Y(y_{1-\\alpha} \\vert \\theta_o) - (1-\\alpha) &amp;= 0 \\\\ \\Rightarrow ~~~ 1 - (1-y_{1-\\alpha})^{\\theta_o} &amp;= 1-\\alpha \\\\ \\Rightarrow ~~~ (1-y_{1-\\alpha})^{\\theta_o} &amp;= \\alpha \\\\ \\Rightarrow ~~~ y_{1-\\alpha} &amp;= 1 - \\alpha^{1/\\theta_o} = 1 - \\alpha^{1/2} \\,. \\end{align*}\\] Thus if we observe \\(y_{\\rm obs} = x \\geq 1 - \\alpha^{1/2} = 1 - 0.05^{1/2} = 0.776\\), we reject the null hypothesis. 1.18 Working With R: Simulating Statistical Inference We wrap up this chapter by discussing how one might simulate aspects of statistical inference. For instance, we might want to know, e.g., the empirical distribution of a maximum likelihood estimator \\(\\hat{\\theta}_{MLE}\\), from which we can estimate the standard error or the mean-squared error; whether a proposed confidence interval has its advertised coverage; and whether a proposed hypothesis test has its advertised Type I error, etc. All of these examples utilize the same basic algorithm, which we render in pseudocode as set random number seed initialize vector(s) for storing result(s) begin for-loop sample dataset compute statistic compute and store result end for-loop optional: process result vector display result This builds upon the last “Working With R” section, which describes dataset sampling. Note that we don’t always need to follow this pseudocoded algorithm to the letter, as R’s vectorization facility plus its apply() function can help us both eliminate the for loop. We illustrate these shortcuts in the examples below. 1.18.1 Estimating the Sampling Distribution for the Sample Median The median of a data sample is defined as the middle value of the sorted data: if we have \\(n=5\\) data, for instance, the median is \\(x_{(3)}\\). (Recall that the parentheses indicating that we are examining sorted data, with \\(x_{(1)}\\) and \\(x_{(n)}\\) being the smallest and largest observed values.) Here, we show how to simulate the distribution of a sample median. In Chapter 3, we show how we can attempt to derive distributions related to sorted data mathematically. Let’s assume that we draw data according to the following distribution: \\[ f_X(x) = \\left\\{ \\begin{array}{cl} \\frac{1}{\\pi} x \\sin x &amp; 0 \\leq x \\leq \\pi \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,, \\] To simulate the distribution of the sample median, for a given sample size \\(n\\), we repeatedly sample datasets (here, we use our rejection sampling code defined earlier in this chapter) and we record the median value for each. In the code chunk below, we show the estimated mean and standard error for our distribution, while in Figure 1.34 we show the empirical distribution of the median values. set.seed(101) # Let&#39;s put the rejection sampling code inside a function. sample_data &lt;- function(n,x.lo,x.hi,f.x.hi) { X &lt;- rep(NA,n) ii &lt;- 0 while ( ii &lt; n ) { u &lt;- runif(1,min=x.lo,max=x.hi) v &lt;- runif(1,min=0,max=f.x.hi) if ( v &lt; u*sin(u)/pi ) { ii &lt;- ii+1 X[ii] &lt;- u } } return(X) } num.sim &lt;- 10000 # number of medians to record n &lt;- 11 x.lo &lt;- 0 x.hi &lt;- pi f.x.hi &lt;- 0.58 X.median &lt;- rep(NA,num.sim) # initialize vector to store result for ( ii in 1:num.sim ) { X &lt;- sample_data(n,x.lo,x.hi,f.x.hi) X.median[ii] &lt;- median(X) } # What do we want to do with the result? Here... # - we visualize the empirical distribution of medians with a histogram # - we process the median vector to estimate the mean and standard error ggplot(data=data.frame(X.median=X.median),aes(x=X.median,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(1,2.7,by=0.1)) + labs(x=&quot;x&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.34: The empirical distribution of the sample median for the pdf \\(f_X(x) = (x \\sin x)/\\pi\\), assuming a sample size of \\(n = 11\\). cat(&quot;The empirical mean is &quot;,round(mean(X.median),3),&quot;\\n&quot;) ## The empirical mean is 1.907 cat(&quot;The empirical standard error is &quot;,round(sd(X.median),3),&quot;\\n&quot;) ## The empirical standard error is 0.255 1.18.2 The Empirical Distribution of Maximum Likelihood Estimates To drive home the point that maximum likelihood estimates are random variables, we simulate the process of estimating \\(\\theta\\) for the pdf \\(f_X(x) = \\theta x^{\\theta-1}\\), where \\(x \\in [0,1]\\). Let’s write down the (log-)likelihood: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n \\theta x_i^{\\theta-1} \\\\ &amp;= \\theta^n \\left( \\prod_{i=1}^n x_i \\right)^{\\theta - 1} \\\\ \\Rightarrow ~ \\ell(\\theta \\vert \\mathbf{x}) &amp;= n \\log \\theta + (\\theta-1)\\log \\left( \\prod_{i=1}^n x_i \\right) \\\\ &amp;= n \\log \\theta + (\\theta-1) \\sum_{i=1}^n \\log x_i \\,, \\end{align*}\\] and solve for the MLE: \\[ \\frac{d}{d\\theta} \\ell(\\theta \\vert \\mathbf{x}) = \\frac{n}{\\theta} + \\sum_{i=1}^n \\log x_i ~ \\Rightarrow ~ \\hat{\\theta}_{MLE} = -\\frac{n}{\\sum_{i=1}^n \\log x_i} \\,. \\] Given this expression, we can now determine the empirical distribution. See Figure 1.35. We observe immediately that the distribution is right-skewed. (As we will see in Chapter 2, as \\(n \\rightarrow \\infty\\), the empirical distribution will tend more and more to a bell-curve-like shape.) set.seed(101) theta &lt;- 2.25 n &lt;- 40 # Here, we show an alternative means by which to sample data. # (This assumes that there is a data-sampling code that we can easily call.) sample_data &lt;- function(n,theta) { q &lt;- runif(n,min=0,max=1) X &lt;- q^(1/theta) } num.sim &lt;- 1000 # first: create one long vector of data, of length num.sim * n X.all &lt;- sample_data(n*num.sim,theta) # second: &quot;fold&quot; the vector into a matrix with num.sim rows and n columns # every row of X is thus a separate independent dataset X &lt;- matrix(X.all,nrow=num.sim) # third: use R&#39;s apply() function to compute MLE for each row/dataset # the argument 1 means &quot;apply the following function to each row&quot; # x in the third argument is the data row; the function returns the mle # mle is thus a vector of maximum-likelihood estimates mle &lt;- apply(X,1,function(x){-n/sum(log(x))}) # What do we want to do with the result? Here... # - we visualize the empirical distribution of medians with a histogram # - we process the median vector to estimate the mean and standard error ggplot(data=data.frame(mle=mle),aes(x=mle,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(1.2,4.2,by=0.2)) + geom_vline(xintercept=2.25,col=&quot;red&quot;,lwd=1) + labs(x=expression(theta)) + theme(axis.title=element_text(size = rel(1.25))) Figure 1.35: The empirical distribution of maximum likelihood estimates for \\(\\theta\\) for the distribution \\(f_X(x) = \\theta x^{\\theta-1}\\) (\\(x \\in [0,1]\\)) with \\(\\theta = 2.25\\) (red line) and \\(n = 40\\). cat(&quot;The empirical mean is &quot;,round(mean(mle),3),&quot;\\n&quot;) ## The empirical mean is 2.317 cat(&quot;The empirical standard deviation is &quot;,round(sd(mle),3),&quot;\\n&quot;) ## The empirical standard deviation is 0.377 1.18.3 Empirically Verifying the Confidence Coefficient Value In the example above, where \\[ f_X(x) = \\theta x^{\\theta-1} \\,, \\] with \\(\\theta &gt; 0\\) and \\(x \\in [0,1]\\), we find the two-sided confidence interval \\[ \\left[ -\\frac{0.0253}{\\log y_{\\rm obs}} , -\\frac{3.689}{\\log y_{\\rm obs}} \\right] \\,, \\] where we assume a confidence coefficient of \\(\\alpha = 0.05\\). A question that naturally arises is: does this confidence interval actually overlap, or cover, the true value \\(\\theta\\) 95% of the time? We can check this with a relatively simple simulation. Let’s assume that the true value of \\(\\theta\\) is 1. Let’s also specify here that \\(f_X(x)\\) is part of the beta family of distributions, which we officially introduce in Chapter 3. (This allows us to utilize the R random variable sampling function rbeta() rather than having to create our own inverse-transform data sampler.) set.seed(101) num.sim &lt;- 10000 theta &lt;- 1 y.obs &lt;- rbeta(num.sim,theta,1) # R can do the sampling for us here hat.theta.L &lt;- -0.0253/log(y.obs) hat.theta.U &lt;- -3.689/log(y.obs) (coverage &lt;- sum(theta&gt;=hat.theta.L &amp; theta&lt;=hat.theta.U)/num.sim) ## [1] 0.9523 Before discussing the result, let’s talk about what is happening in the last line of code above. First of all, by surrounding this line with parentheses, we are telling R to not only assign a value to the variable coverage, but to print the value of coverage as well. As far as the argument passed to sum(): theta&gt;=hat.theta.L is a logical comparison, an implicit function call that returns TRUE if \\(\\theta \\geq \\hat{\\theta}_L\\) and FALSE otherwise. The ampersand &amp; is the logical AND operator that combines the results of the comparison on the left and the one on the right: TRUE &amp; TRUE returns TRUE, otherwise FALSE is returned. (This stands in contrast to the logical OR operator, |, which only returns FALSE for FALSE | FALSE.) So the long argument passed to sum() turns into a logical vector of TRUEs (if the true value is inside the bounds) and FALSEs (if the true value is outside the bounds). What happens when we pass a logical vector to sum()? It treats TRUE as 1 and FALSE as 0…in other words, sum() returns the number of TRUE values. We find that 9,523 of the 10,000 intervals overlap \\(\\theta = 1\\). Because we do not perform an infinite number of simulations, the coverage that we compute is a random variable: we do not expect the value to match 9500 exactly. Thus what we observe is very strong evidence that our interval construction algorithm has the coverage that we defined it to have. (Once we introduce the binomial distribution in Chapter 3, we can turn this “strong evidence” into something more firmly quantitative!) 1.18.4 Empirically Verifying the Type I Error \\(\\alpha\\) In the second example in the last section above, we determine if we sample a statistic \\(Y\\) from the distribution \\(f_Y(y) = \\theta (1-y)^{\\theta-1}\\), with \\(y \\in [0,1]\\), then if \\(y_{\\rm obs} \\geq 0.776\\), we reject the null hypothesis that \\(\\theta = 1\\). This threshold is based on our adoption of the Type I error value \\(\\alpha = 0.05\\): if the null is correct and we repeatedly randomly sample data from the sampling distribution \\(f_Y(y) = 2y\\), we should observe values of \\(y\\) larger than 0.776 only five percent of the time. Is this what we actually observe? As a reminder, we have identified \\(f_Y(y)\\) as being part of the beta family of distributions, which we officially introduce in Chapter 3; thus we can utilize rbeta() like we do in the last example above. set.seed(101) theta &lt;- 2 num.sim &lt;- 10000 y.obs &lt;- rbeta(num.sim,1,theta) (typeIerror &lt;- sum(y.obs&gt;=(1-sqrt(0.05)))/num.sim) ## [1] 0.0508 We find that we make Type I errors 508 times, or that the empirical Type I error rate is 508/10,000 = 0.0508. Because we do not perform an infinite number of simulations, the coverage that we compute is a random variable: we do not expect the value to match 500 exactly. Thus what we observe is very strong evidence that our Type I error rate is what we expect it to be; in Chapter 3, we learn how to turn this evidence into something more quantitative!) "],["the-normal-and-related-distributions.html", "2 The Normal (and Related) Distributions 2.1 Motivation 2.2 Probability Density Function 2.3 Cumulative Distribution Function 2.4 Moment-Generating Functions 2.5 Linear Functions of Normal Random Variables 2.6 Standardizing a Normal Random Variable with Known Variance 2.7 General Transformations of a Single Random Variable 2.8 Squaring Standard Normal Random Variables 2.9 Sample Variance of Normal Random Variables 2.10 Standardizing a Normal Random Variable with Unknown Variance 2.11 Point Estimation 2.12 The Central Limit Theorem 2.13 Confidence Intervals 2.14 Hypothesis Testing: Testing for Normality 2.15 Hypothesis Testing: Population Mean 2.16 Hypothesis Testing: Population Variance 2.17 Simple Linear Regression 2.18 One-Way Analysis of Variance", " 2 The Normal (and Related) Distributions 2.1 Motivation In this chapter, we will illustrate probability and statistical inference concepts utilizing the normal distribution. The normal, also known in the physical sciences as the Gaussian distribution and colloquially as the “bell curve,” is the most often utilized probability distribution in data analyses, for a number of reasons. Empirically, we observe that the data that we sample in many experiments are at least approximately normally distributed. And while other, more general families of distributions (such as the gamma distribution) might explain data just as well as the normal, the normal has the advantage of having intuitively easy-to-understand parameters: \\(\\mu\\) for the mean, and \\(\\sigma^2\\) for the variance (meaning that \\(\\sigma\\) directly indicates the “width” of the region on the real-number line over which \\(f_X(x)\\) is effectively non-zero). The normal distribution is the limiting distibution for many other distributions (i.e., there are many families of distributions that, with the right choice(s) for parameter value(s), can mimic the shape of the normal. The normal distribution figures prominently in the Central Limit Theorem, which states that the sample mean of a sufficiently large sample of data, from any distribution with finite variance, is approximately normally distributed. 2.2 Probability Density Function Recall: a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) \\(f_X(x) \\geq 0\\) and (b) \\(\\int_x f_X(x) dx = 1\\), where the integral is over the full domain of \\(f_X(x)\\). Let \\(X\\) be a continuous random variable sampled from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\): \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\). The pdf for \\(X\\) is \\[ f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) ~~~~~~ x \\in (-\\infty,\\infty) \\,. \\] The first thing we notice about this pdf is that it is symmetric around \\(\\mu\\). The second thing that we notice is that the integral under this curve approaches 1 over the range \\(\\mu - 3\\sigma \\leq x \\leq \\mu + 3\\sigma\\). (The value of the integral of \\(f(x)\\) over this range is 0.9973. This gives rise to one aspect of the so-called empirical rule: if data are approximately normally distributed, we expect all of the data to lie with three standard deviations of the sample mean, with rare exceptions.) See Figure 2.1. Figure 2.1: A normal pdf with mean 0 and standard deviation 1. The expected value of \\(X\\) is \\[ E[X] = \\int_{-\\infty}^\\infty x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx \\] Recall: the expected value of a continuously distributed random variable is \\[ E[X] = \\int_x x f_X(x) dx\\,, \\] where the integral is over the full domain of \\(f_X(x)\\). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(f_X(x)\\). We implement a variable substitution to evaluate this integral. Recall that the three steps of variable substitution are to write down a viable substitution \\(u = g(x)\\); to then derive \\(du = h(u,x) dx\\); and finally to use \\(u = g(x)\\) to transform the bounds of the integral. For this particular integral: \\[ (1) ~~ u = \\frac{x-\\mu}{\\sigma} ~~ \\implies ~~ (2) ~~ du = \\frac{1}{\\sigma}dx \\] and \\[ (3) ~~ x = -\\infty ~\\implies~ u = -\\infty ~~~ \\mbox{and} ~~~ x = \\infty ~\\implies~ u = \\infty \\,, \\] Thus \\[ E[X] = \\int_{-\\infty}^\\infty (\\sigma u + \\mu) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{u^2}{2}\\right) \\sigma du = \\int_{-\\infty}^\\infty \\frac{\\sigma u}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du + \\int_{-\\infty}^\\infty \\frac{\\mu}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du \\] The first integrand is the product of an odd function (\\(u\\)) and an even function (\\(\\exp(-u^2/2)\\)), and thus the first integral is zero. The second integral is \\[ E[X] = \\mu \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) du = \\mu \\,, \\] since the integrand is a normal pdf (\\(\\mu = 0\\) and \\(\\sigma^2\\) = 1) and the bounds of the integral match that of the domain of the normal pdf, making the value of the integral 1. Normal Distribution - R Functions quantity R function call PDF dnorm(x,mean,sd) CDF pnorm(x,mean,sd) Inverse CDF qnorm(q,mean,sd) \\(n\\) iid random samples rnorm(n,mean,sd) 2.2.1 Variance of a Normal Probability Density Function Recall: the variance of a continuously distributed random variable is \\[ V[X] = \\int_x (x-\\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\\,, \\] where the integral is over the full domain of \\(f_X(x)\\). The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of \\(x\\) for which \\(f_X(x)\\) is effectively non-zero. As seen above, we want to compute \\(E[X^2] - (E[X])^2\\). We have already computed \\(E[X]\\): it is equal to \\(\\mu\\). Here, we compute \\(E[X^2]\\). The variable substitution setup is entirely the same, except that now \\[\\begin{align*} E[X^2] &amp;= \\int_{-\\infty}^\\infty (\\sigma t + \\mu)^2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{t^2}{2}\\right) \\sigma dt \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{\\sigma^2 t^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\int_{-\\infty}^\\infty \\frac{2 \\mu \\sigma t}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\int_{-\\infty}^\\infty \\frac{\\mu^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt \\,. \\end{align*}\\] The second integral is that of an odd function and is thus zero, and the third integral is, given results above, \\(\\mu^2\\). This leaves \\[ E[X^2] = \\int_{-\\infty}^\\infty \\frac{\\sigma^2 t^2}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{t^2}{2}\\right) dt + \\mu^2 \\,. \\] We note that if we were to apply the shortcut formula, the \\(\\mu^2\\) immediately above would cancel with \\((E[X])^2 = \\mu^2\\), so now we can say that \\[ V[X] = \\frac{\\sigma^2}{\\sqrt{2 \\pi}} \\int_{-\\infty}^\\infty t^2 \\exp\\left(-\\frac{t^2}{2}\\right) dt \\,. \\] This requires integration by parts. We will not go over how integration by parts works; if you have forgotten, you should review this on your own. Setting aside the constants outside the integral, we have that \\[ \\begin{array}{ll} u = -t &amp; dv = -t \\exp(-t^2/2) dt \\\\ du = -dt &amp; v = \\exp(-t^2/2) \\end{array} \\,, \\] and so \\[\\begin{align*} \\int_{-\\infty}^\\infty t^2 \\exp\\left(-\\frac{t^2}{2}\\right) dt &amp;= \\left. uv \\right|_{-\\infty}^{\\infty} - \\int_{-\\infty}^\\infty v du \\\\ &amp;= - \\left. t \\exp(-t^2/2) \\right|_{-\\infty}^{\\infty} + \\int_{-\\infty}^\\infty \\exp(-t^2/2) dt \\\\ &amp;= 0 + \\sqrt{2\\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2) dt \\\\ &amp;= \\sqrt{2\\pi} \\,. \\end{align*}\\] The first term evaluates to zero between for each bound, \\(e^{-t^2/2}\\) goes to zero much faster than \\(\\vert t \\vert\\) goes to infinity. For the second term, we manipulate the constants such that we have an integral of a normal pdf with mean zero and variance one…which is by definition 1. In the end, after restoring the constants we set aside above, we have that \\[ V[X] = \\frac{\\sigma^2}{\\sqrt{2 \\pi}} \\sqrt{2\\pi} = \\sigma^2 \\,. \\] 2.2.2 Skewness of the Normal Probability Density Function The skewness of a pmf or pdf is a metric that indicates its level of asymmetry. Fisher’s moment coefficient of skewness is \\[ E\\left[\\left(\\frac{X-\\mu}{\\sigma}\\right)^3\\right] \\,. \\] which, expanded out, becomes \\[ \\frac{E[X^3] - 3 \\mu \\sigma^2 - \\mu^3}{\\sigma^3} \\,. \\] What is the skewness of the normal distribution? At first, this appears to require a long and involved series of integrations so as to solve \\(E[X^3]\\). But let’s try to be more clever about this. We know, from above, that the quantity \\((\\sigma t + \\mu)^3\\) will appear in the integral for \\(E[X^3]\\). Let’s expand this out: \\[ \\sigma^3 t^3 + 3 \\sigma^2 \\mu t^2 + 3 \\sigma \\mu^2 t + \\mu^3 \\,. \\] Each of these terms will appear separately in integrals of \\(\\exp(-t^2/2)/\\sqrt{2\\pi}\\) over the domain \\(-\\infty\\) to \\(\\infty\\). From above, what do we already know? First, we know that if \\(t\\) is raised to an odd power, the integral will be zero. This eliminates \\(\\sigma^3 t^3\\) and \\(3 \\sigma \\mu^2 t\\). Second, we know that for the \\(t^0\\) term, the result will be \\(\\mu^3\\), and we know that for the \\(t^2\\) term, the result will be \\(3 \\sigma^2 \\mu\\). (Examine the results of the integrals in the last example again if you do not see this immediately.) Thus \\[ E[X^3] = 3\\sigma^2\\mu + \\mu^3 \\] and the skewness is \\[ \\frac{3\\sigma^2\\mu + \\mu^3 - 3 \\mu \\sigma^2 - \\mu^3}{\\sigma^3} = 0 \\,. \\] The skewness is zero, meaning that a normal pdf is symmetric around its mean \\(\\mu\\). 2.2.3 Computing Probabilities Before diving into this example, we will be clear that this is not the optimal way to compute probabilities associated with normal random variables, as you should utilize R’s pnorm() function, as shown in the next section. However, showing how to utilize integrate() is useful review. (We also show how to pass distribution parameters to integrate(), which we did not do in the last chapter.) If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? To find this probability, we integrate over the normal pdf with mean \\(\\mu = 10\\) and variance \\(\\sigma^2 = 4\\). Visually, we are determining the area of the red-shaded region in Figure 2.2. Figure 2.2: The probability \\(P(8 \\leq X \\leq 13.5)\\) is the area of the red-shaded region, i.e., the integral of the normal probability density function from 8 to 13.5, assuming \\(\\mu = 10\\) and \\(\\sigma^2 = 4\\). integrand = function(x,mu,sigma2) { return(exp(-(x-mu)^2/2/sigma2)/sqrt(2*pi*sigma2)) } integrate(integrand,8,13.5,mu=10,sigma2=4)$value ## [1] 0.8012856 The result is 0.801. If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? Recall that \\(P(a \\leq X \\leq b \\vert X \\leq c) = P(a \\leq X \\leq b)/P(X \\leq c)\\), assuming that \\(a &lt; b &lt; c\\). So this probability is, visually, the ratio of the area of the brown-shaded region in Figure 2.3 to the area of the red-shaded region underlying it. We can reuse the integrand() function from above, calling it twice: integrate(integrand,8,13.5,mu=13,sigma2=5)$value / integrate(integrand,-Inf,14,mu=13,sigma2=5)$value ## [1] 0.8560226 The result is 0.856. Note how we use -Inf in the second call to integrate(): Inf, like pi, is a reserved word in the R programming language. Figure 2.3: The conditional probability \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\) is the ratio of the area of the brown-shaded region to the area of the red-shaded region underlying the brown-shaded region. 2.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as \\(F_X(x) = \\int_{y \\leq x} f_Y(y) dy\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). The cdf for the normal distribution is the “accumulated probability” between \\(-\\infty\\) and the functional input \\(x\\): \\[ F_X(x) = \\int_{-\\infty}^x f_Y(y) dy = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) dy \\,. \\] (See Figure 2.4.) Recall that because \\(x\\) is the upper bound of the integral, we have to replace \\(x\\) with some other variable in the integrand itself. (Here we choose \\(y\\). The choice is arbitrary.) Figure 2.4: The cdf is the area of the red-shaded polygon. Again, we implement a variable substitution strategy: \\[ (1) ~~ t = \\frac{(y-\\mu)}{\\sqrt{2}\\sigma} ~~\\implies~~ (2) ~~ dt = \\frac{1}{\\sqrt{2}\\sigma}dy \\] and \\[ (3) ~~ y = -\\infty ~\\implies~ t = -\\infty ~~~ \\mbox{and} ~~~ y = x ~\\implies~ t = \\frac{(x-\\mu)}{\\sqrt{2}\\sigma} \\,, \\] so \\[ F_X(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right) dy = \\int_{-\\infty}^{\\frac{x-\\mu}{\\sqrt{2}\\sigma}} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-t^2) \\sqrt{2}\\sigma dt = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\frac{x-\\mu}{\\sqrt{2}\\sigma}} \\exp(-t^2) dt \\,. \\] The error function is defined as \\[ \\mbox{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp(-t^2)dt \\,, \\] which is close to, but not quite, our expression for \\(F_X(x)\\). (The integrand is the same, but the bounds of integration differ.) To match the bounds: \\[\\begin{align*} \\frac{2}{\\sqrt{\\pi}} \\int_{-\\infty}^z \\exp(-t^2)dt &amp;= \\frac{2}{\\sqrt{\\pi}} \\left( \\int_{-\\infty}^0 \\exp(-t^2)dt + \\int_0^z \\exp(-t^2)dt \\right) \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\left( -\\int_0^{-\\infty} \\exp(-t^2)dt + \\int_0^z \\exp(-t^2)dt \\right) \\\\ &amp;= -\\mbox{erf}(-\\infty) + \\mbox{erf}(z) = \\mbox{erf}(\\infty) + \\mbox{erf}(z) = 1 + \\mbox{erf}(z) \\,. \\end{align*}\\] Here we make use of two properties of the error function: \\(\\mbox{erf}(-z) = -\\mbox{erf}(z)\\), and \\(\\mbox{erf}(\\infty)=1\\). Thus \\[ \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^z \\exp(-t^2)dt = \\frac{1}{2}[1 + \\mbox{erf}(z)] \\,. \\] By matching this expression with that given for \\(F(x)\\) above, we see that \\[ F_X(x) = \\frac{1}{2}\\left[1 + \\mbox{erf}\\left(\\frac{x-\\mu}{\\sqrt{2}\\sigma}\\right)\\right] \\,. \\] In Figure 2.5, we plot \\(F_X(x)\\). We note that while the error function is available to use directly in some R packages, it is provided only indirectly in base-R, via the pnorm() function. (In general, one computes cdf values for probability distributions in R using functions prefixed with p: pnorm(), pbinom(), punif(), etc.) Examining this figure, we see that this cdf abides by the properties listed in the previous chapter: \\(F(-\\infty) = 0\\) and \\(F(\\infty) = 1\\), and it is (strictly) monotonically increasing. Figure 2.5: The cdf for a normal distribution with mean 0 and standard deviation 1. To compute the probability \\(P(a &lt; X &lt; b)\\), we make use of the cdf. (To remind you: if you have the cdf at your disposal and need to compute a probability…use it!) \\[ P(a &lt; X &lt; b) = P(X &lt; b) - P(X &lt; a) = F(b) - F(a) = \\frac{1}{2}\\left[ \\mbox{erf}\\left(\\frac{b-\\mu}{\\sqrt{2}\\sigma}\\right) - \\mbox{erf}\\left(\\frac{a-\\mu}{\\sqrt{2}\\sigma}\\right)\\right] \\,, \\] which is more simply rendered in R as pnorm(b,mean=mu,sd=sigma) - pnorm(a,mean=mu,sd=sigma). Let’s look again at Figure 2.4. What if, instead of asking the question “what is the shaded area under the curve,” which is answered by integrating the normal pdf from \\(-\\infty\\) to a specified coordinate \\(x\\), we instead ask the question “what value of \\(x\\) is associated with a given area under the curve”? This is the inverse problem, one that we can solve so long as the relationship between \\(x\\) and \\(F(x)\\) is bijective (i.e., one-to-one): \\(x = F_X^{-1}[F_X(x)]\\) for all \\(x\\). Recall: an inverse cdf function \\(F_X^{-1}(q)\\) takes as input the total probability \\(q \\in [0,1]\\) in the range \\((-\\infty,x]\\) and returns the value of \\(x\\). A continuous distribution has a unique inverse cdf over the domain of \\(f_X(x)\\). Let \\(q = F_X(x)\\). Then, for the case of the normal cdf, we can write that \\[ x = \\sqrt{2} \\sigma~ \\mbox{erf}^{-1}\\left(2q-1\\right) + \\mu \\,, \\] where \\(\\mbox{erf}^{-1}(\\cdot)\\) is the inverse error function. Like the error function itself, the inverse error function is available for use in some R packages, but it is most commonly accessed, indirectly via the base-R function qnorm(). (In general, one computes inverse cdf values for probability distributions in R using functions prefixed with q: qnorm(), qpois(), qbeta(), etc.) 2.3.1 Computing Probabilities We utilize the two examples provided in the last section to show how one would compute probabilities associated with the normal pdf by hand. But we note that a computer is still needed to derive the final numbers! If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? We have that \\[ P(8 \\leq X \\leq 13.5) = P(X \\leq 13.5) - P(X \\leq 8) = F_X(13.5 \\vert 10,4) - F_X(8 \\vert 10,4) \\,. \\] Well, this is where hand computation ends, as we cannot evaluate the cdfs using pen and paper. We would have to utilize pnorm(): pnorm(13.5,mean=10,sd=sqrt(4)) - pnorm(8,mean=10,sd=sqrt(4)) ## [1] 0.8012856 We get the same result as we got in the last section: 0.801. If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? Knowing that we cannot solve this by hand, we can skip over the pen and paper and go directly to R: (pnorm(13.5,mean=13,sd=sqrt(5)) - pnorm(8,mean=13,sd=sqrt(5))) / pnorm(14,mean=13,sd=sqrt(5)) ## [1] 0.8560226 The answer, as before, is 0.856. But let’s go ahead and add some complexity here. How would we answer the following question? If \\(\\mu = 20\\) and \\(\\sigma^2 = 3\\), what is the value of \\(a\\) such that \\(P(20 - a \\leq X \\leq 20 + a) = 0.48\\)? (The first thing to thing about here is: does the value of \\(\\mu\\) actually matter? The answer here is no. Think about why that may be.) Mathematically, we have that \\[ P(20-a \\leq X \\leq 20+a) = 0.48 = P(X \\leq 20+a) - P(X \\leq 20-a) = F_X(20+a \\vert 20,3) - F_X(20-a \\vert 20,3) \\,. \\] Hmm…we’re stuck. We cannot invert this equation so as to isolate \\(a\\)…or can we? Remember that a normal pdf is symmetric around the mean. Hence \\[ P(X \\leq 20+a) = 1 - P(X &gt; 20+a) = 1 - P(X \\leq 20-a) \\,. \\] The area under the normal pdf from \\(20+a\\) to \\(\\infty\\) is the same as the area from \\(-\\infty\\) to \\(20-a\\). So… \\[ P(20-a \\leq X \\leq 20+a) = 1 - F_X(20-a \\vert 20,3) - F_X(20-a \\vert 20,3) = 1 - 2F_X(20-a \\vert 20,3) \\,, \\] or \\[ F_X(20-a \\vert 20,3) = \\frac{1}{2}[1 - P(20-a \\leq X \\leq 20+a)] = \\frac{1}{2} 0.52 = 0.26 \\,. \\] Now, we invert the cdf and rearrange terms to get: \\[ a = 20 - F_X{-1}(0.26 \\vert 20,3) \\,, \\] and once again, we have reached the end of the pen and paper road. We finish this off with R: (a = 20 - qnorm(0.26,mean=20,sd=sqrt(3))) ## [1] 1.114307 The result is \\(a = 1.114\\), i.e., \\(P(18.886 \\leq X \\leq 21.114) = 0.48\\). You the reader might be quibbling with our assertion that \\(\\mu\\) doesn’t matter here, because, after all, the value of \\(\\mu\\) appears in the final code. However, if we changed both values of 20 to some other, arbitrary value, we would get the same value for \\(a\\). 2.3.2 Visualizing a Cumulative Distribution Function Let’s say we want to make a figure like that in Figure 2.4, where we are given that our normal distribution as mean \\(\\mu = 10\\) and variance \\(\\sigma^2 = 4\\), and we want to overlay the shaded region associated with \\(F_X(9)\\). How do we do this? The first step is to determine the population standard deviation. Here, that would be \\(\\sigma = \\sqrt{4} = 2\\). The second step is to determine a proper domain for plotting. Here we will assume that \\(\\mu - 4\\sigma\\) to \\(\\mu + 4\\sigma\\) is sufficient; this is \\(x = 2\\) to \\(x = 18\\). The third step is to code. The result of our coding is shown in Figure 2.6. # TBD, edit annotations if necessary x &lt;- seq(2,18,by=0.05) # vector with x = {2,2.05,2.1,...,18} fx &lt;- dnorm(x,mean=10,sd=2) # compute f(x) for all x df &lt;- data.frame(x=x,fx=fx) # define a dataframe with above data df.shaded &lt;- subset(df,x&lt;=9) # get subset of data with x values &lt;= 9 ggplot(data=df,aes(x=x,y=fx)) + geom_line(col=&quot;blue&quot;,lwd=1) + # plot the pdf geom_area(data=df.shaded, # add area underneath subset of data fill=&quot;red&quot;,col=&quot;blue&quot;, outline.type=&quot;full&quot;) + labs(y=&quot;f(x)&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.6: The cumulative distribution function is the area of the red-shaded region. For the polygon, the first \\((x,y)\\) pair is \\((9,0)\\), the second is \\((2,0)\\) (where \\(x = 2\\) is the lower limit of the plot…there is no need to take the polygon further “to the left”), and the next set are \\((x,f_X(x))\\) for all \\(x\\) values \\(\\leq 9\\). The last point is the first point: this closes the polygon. 2.4 Moment-Generating Functions In the previous chapter, we learned that if we linearly transform a random variable, i.e., if we define a new random variable \\(Y = \\sum_{i=1}^n a_i X_i + b\\), where \\(\\{a_1,\\ldots,a_n\\}\\) and \\(b\\) are constants, then \\[ E[Y] = E\\left[\\sum_{i=1}^n a_i X_i + b\\right] = b + \\sum_{i=1}^n a_i E[X_i] \\,. \\] Furthermore, if we assume the \\(X_i\\)’s are independent, then the variance of our new random variable is \\[ V[Y] = V\\left[\\sum_{i=1}^n a_i X_i + b\\right] = \\sum_{i=1}^n a_i^2 V[X_i] \\,. \\] (Because the \\(X_i\\)’s are independent, we do not need to take into account any covariance, or linear dependence, between the \\(X_i\\)’s, simplifying the equation for \\(V[Y]\\). We discuss how covariance is taken into account in Chapter 6.) So we know where \\(Y\\) is centered and we know how “wide” it is. However, we don’t yet know the shape of the distribution for \\(Y\\), i.e., we don’t yet know \\(f_Y(y)\\). To show how we might derive the distribution, we introduce a new concept, that of the moment-generating function, or mgf. In the previous chapter, we introduced the concept of distribution moments, \\[\\begin{align*} \\mu_k&#39; &amp;= E[X^k] \\\\ \\mu_k &amp;= E[(X-\\mu)^k] \\,, \\end{align*}\\] where the first moment, \\(\\mu_1&#39;\\), is the expected value \\(E[X]\\) and the second central moment, \\(\\mu_2\\), is the variance \\(V[X]\\). The moments of a given probability distribution are unique, and can often (but not always) be neatly encapsulated in a single mathematical expression: the moment-generating function (or mgf). To derive an mgf for a given distribution, we invoke the Law of the Unconscious Statistician: \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\int_x e^{tX} f_X(x) \\\\ &amp;= \\int_x \\left[1 + tx + \\frac{t^2}{2!}x^2 + \\cdots \\right] f_X(x) \\\\ &amp;= \\int_x \\left[ f_X(x) + t x f_X(x) + \\frac{t^2}{2!} x^2 f_X(x) + \\cdots \\right] \\\\ &amp;= \\int_x f_X(x) + t \\int_x x f_X(x) + \\frac{t^2}{2!} \\int_x x^2 f_X(x) + \\cdots \\\\ &amp;= 1 + t \\mu_1&#39; + \\frac{t^2}{2!} \\mu_2&#39; + \\cdots \\,. \\end{align*}\\] Note that an mgf only exists for a particular distribution if there is a constant \\(b\\) such that \\(m_X(t)\\) is finite for \\(\\vert t \\vert &lt; b\\). An example of a distribution for which the mgf does not exist is the Cauchy distribution. Moment-generating functions are called such because, as you might guess, they generate moments (via differentiation): \\[ \\left.\\frac{d^k m_X(t)}{dt^k}\\right|_{t=0} = \\frac{d^k}{dt^k} \\left. \\left[1 + t \\mu_1&#39; + \\frac{t^2}{2!} \\mu_2&#39; + \\cdots \\right]\\right|_{t=0} = \\left. [\\mu_k&#39; + t\\mu_{k+1}&#39; + \\cdots] \\right|_{t=0} = \\mu_k&#39; \\,. \\] The \\(k^{\\rm th}\\) derivative of an mgf, with \\(t\\) set to zero, yields the \\(k^{\\rm th}\\) moment. But, you may ask, why are mgfs important here, in the context of normal random variables? We already know all the moments of this distribution that we care to know: they are written down. The answer is that a remarkably useful property of mgfs is the following: if \\(Y = b + \\sum_{i=1}^n a_i X_i\\), where the \\(X_i\\)’s are independent random variables sampled from a distribution whose mgf exists, then \\[ m_Y(t) = e^{bt} m_{X_1}(a_1 t) \\cdot m_{X_2}(a_2 t) \\cdots m_{X_n}(a_n t) = e^{bt} \\prod_{i=1}^n m_{X_i}(a_i t) \\,, \\] where \\(m_{X_i}(\\cdot)\\) is the moment-generating function for the random variable \\(X_i\\). An mgf is typically written as a function of \\(t\\); the notation \\(m_{X_i}(a_it)\\) simply means that when we evaluate the above equation, wherever there is a \\(t\\), we plug in \\(a_it\\). Here’s the key point: if we recognize the form of \\(m_Y(t)\\) as matching that of the mgf for a given distribution, then we know the distribution for \\(Y\\). In the next few sections, we show how the method of moment-generating functions allows us to derive distributions for linearly transformed normal random variables. 2.4.1 Moment-Generating Function for a Probability Mass Function Let’s assume that we sample data from the following pmf: \\(x\\) \\(p_X(x)\\) 0 0.2 1 0.3 2 0.5 What is the mgf for this distribution? What is its expected value? To derive the mgf, we compute \\(E[e^{tX}]\\): \\[ m_X(t) = E[e^{tX}] = \\sum_x e^{tx} p_X(x) = 1 \\cdot 0.2 + e^t \\cdot 0.3 + e^{2t} \\cdot 0.5 = 0.2 + 0.3 e^t + 0.5 e^{2t} \\,. \\] This cannot be simplified, and thus this is the final answer. As far as the expected value is concerned: we could simply compute \\(E[X] = \\sum_x x p_X(x)\\), but since we have the mgf now, we can use it too: \\[ E[X] = \\left.\\frac{d}{dt}m_X(t)\\right|_{t=0} = \\left. (0.3 e^t + e^{2t})\\right|_{t=0} = 0.3 + 1 = 1.3 \\,. \\] 2.4.2 Moment-Generating Function for a Probability Density Function Let’s assume that we now sample data from the following pdf: \\[ f_X(x) = \\frac{1}{\\theta} e^{-x/\\theta} \\,, \\] where \\(x \\geq 0\\) and \\(\\theta &gt; 0\\). What is the mgf of this distribution? \\[\\begin{align*} E[e^{tX}] &amp;= \\int_0^\\infty e^{tx} \\frac{1}{\\theta} e^{-x/\\theta} dx \\\\ &amp;= \\frac{1}{\\theta} \\int_0^\\infty e^{-x\\left(\\frac{1}{\\theta}-t\\right)} dx \\\\ &amp;= \\frac{1}{\\theta} \\int_0^\\infty e^{-x/\\theta&#39;} dx \\\\ &amp;= \\frac{\\theta&#39;}{\\theta} = \\frac{1}{\\theta} \\frac{1}{(1/\\theta - t)} = \\frac{1}{(1-\\theta t)} = (1-\\theta t)^{-1} \\,. \\end{align*}\\] The expected value of this distribution can be computed via the integral \\(\\int_x x f_X(x) dx\\), but again, as we now have the mgf: \\[ E[X] = \\left.\\frac{d}{dt}m_X(t)\\right|_{t=0} = \\left. -(1-\\theta t)^{-2} (-\\theta)\\right|_{t=0} = \\theta \\,. \\] 2.5 Linear Functions of Normal Random Variables Let’s assume we are given \\(n\\) normal random variables \\(\\{X_1,\\ldots,X_n\\}\\), which are independent (but not necessarily identically distributed), and we wish to determine the distribution of the sum \\(Y = b + \\sum_{i=1}^n a_i X_i\\). We recall that the expected value operator \\(E[X]\\) and the variance operator \\(V[X]\\) are linear operators, thus we can note immediately that… the expected value is \\[ E[Y] = E\\left[\\sum_{i=1}^n a_i X_i\\right] = \\sum_{i=1}^n E[a_i X_i] = \\sum_{i=1}^n a_i E[X_i] = \\sum_{i=1}^n a_i \\mu_i \\,; \\] and the variance is \\[ V[Y] = V\\left[\\sum_{i=1}^n a_i X_i\\right] = \\sum_{i=1}^n V[a_i X_i] = \\sum_{i=1}^n a_i^2 V[X_i] = \\sum_{i=1}^n a_i^2 \\sigma_i^2 \\,. \\] As far as deriving the distribution: the mgf for the normal is \\[ m_X(t) = \\exp\\left(\\mu t + \\sigma^2\\frac{t^2}{2} \\right) \\,, \\] and thus if we have \\(n\\) independent normal random variables, we find that \\[\\begin{align*} m_Y(t) &amp;= \\exp\\left(\\mu_1a_1t+a_1^2\\sigma_1^2\\frac{t^2}{2}\\right) \\cdot \\cdots \\cdot \\exp\\left(\\mu_na_nt+a_n^2\\sigma_n^2\\frac{t^2}{2}\\right) \\\\ &amp;= \\exp\\left[ (a_1\\mu_1+\\cdots+a_n\\mu_n)t + \\left(a_1^2\\sigma_1^2 + \\cdots + a_n^2\\sigma_n^2\\right)\\frac{t^2}{2} \\right] \\\\ &amp;= \\exp\\left[ \\left(\\sum_{i=1}^n a_i\\mu_i\\right)t + \\left(\\sum_{i=1}^n a_i^2\\sigma_i^2\\right)\\frac{t^2}{2} \\right] \\,. \\end{align*}\\] When we examine the result, we see immediately that it retains the functional form of a normal mgf, and thus we conclude that \\(Y\\) itself is normally distributed, with mean \\(\\sum_{i=1}^n a_i\\mu_i\\) and variance \\(\\sum_{i=1}^n a_i^2\\sigma_i^2\\), i.e., \\[ Y \\sim \\mathcal{N}\\left(\\sum_{i=1}^n a_i\\mu_i,\\sum_{i=1}^n a_i^2\\sigma_i^2\\right) \\,. \\] 2.5.1 The Distribution of the Sample Mean of iid Normal Random Variables We have previously seen that when have a sample of \\(n\\) iid random variables, \\(E[\\bar{X}] = \\mu\\) and \\(V[\\bar{X}] = \\sigma^2/n\\). Now we want to determine the distribution of \\(\\bar{X}\\), not just its mean and variance. In general, if \\(\\bar{X} = (1/n)\\sum_{i=1}^n X_i\\), then \\[\\begin{align*} m_{\\bar{X}}(t) &amp;= m_{X_1}(a_1 t) \\cdot m_{X_2}(a_2 t) \\cdots m_{X_n}(a_n t) \\\\ &amp;= m_{X_1}\\left(\\frac{t}{n}\\right) \\cdots m_{X_n}\\left(\\frac{t}{n}\\right) = \\prod_{i=1}^n m_{X_i}\\left(\\frac{t}{n}\\right) \\,, \\end{align*}\\] and thus \\[ m_{\\bar{X}}(t) = \\exp\\left[ \\left(\\sum_{i=1}^n \\frac{\\mu}{n}\\right)t + \\left(\\sum_{i=1}^n \\frac{\\sigma^2}{n^2}\\right)\\frac{t^2}{2} \\right] = \\exp\\left( n \\frac{\\mu}{n} t + n \\frac{\\sigma^2}{n^2} \\frac{t^2}{2} \\right) = \\exp\\left(\\mu t + \\frac{\\sigma^2}{n} \\frac{t^2}{2} \\right) \\,. \\] We see that \\(\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n)\\), i.e., that the sample mean observed in any given experiment is sampled from a normal distribution centered on the true mean, with a width that goes to zero as \\(n^{-1/2}\\). 2.5.2 Using Variable Substitution to Determine Distribution of Y = aX + b We set \\(y = ax+b\\) and note that \\(dy = a dx\\) and that if \\(x = \\pm \\infty\\) then \\(y\\) also equals \\(\\pm \\infty\\). Thus \\[\\begin{align*} \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma^2} \\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] dx &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma^2} \\exp\\left[-\\frac{([y-b]/a-\\mu)^2}{2\\sigma^2}\\right] \\frac{dy}{a} \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi a^2\\sigma^2} \\exp\\left[-\\frac{(y-b-a\\mu)^2}{2a^2\\sigma^2}\\right] dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi a^2\\sigma^2} \\exp\\left[-\\frac{(y-[a\\mu+b])^2}{2a^2\\sigma^2}\\right] dy \\end{align*}\\] The key point here is that the integrand has the functional form of a normal pdf and the integral bounds match the domain of a normal pdf…hence \\(Y\\) is normally distributed, with mean \\(a\\mu+b\\) and variance \\(a^2\\sigma^2\\). (This key point holds in general…if you have a pmf or pdf whose functional form and domain match that of a known, “named” family of distributions, then the pmf or pdf belongs to that family.) 2.6 Standardizing a Normal Random Variable with Known Variance To standardize any random variable, we subtract the expected value and divide by the standard deviation, i.e., we set \\[ Z = \\frac{X - E[X]}{\\sqrt{V[X]}} \\,. \\] If \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), what are the mean and variance of \\(Z\\), and can we derive \\(f_Z(z)\\)? Expected Value. If we write \\(Z = aX+b\\), where \\(a\\) is \\(1/\\sigma\\) and \\(b = -\\mu/\\sigma\\), then \\[ E[Z] = E[aX+b] = \\frac{1}{\\sigma}E[X] - b = \\frac{\\mu}{\\sigma} - \\frac{\\mu}{\\sigma} = 0 \\,. \\] Variance. The variance is \\[ V[Z] = V[aX+b] = a^2V[X] = \\frac{1}{\\sigma^2} \\sigma^2 = 1 \\,. \\] OK…so far, so good: the distribution is centered at 0 and has variance 1. To attempt to derive the distribution, we could pursue variable substitution (with \\(z = (x - \\mu)/\\sigma\\) and \\(dz = 1/\\sigma\\), etc.), but we can also pursue the use of mgfs. We will do that here. The mgf for a normal random variance is \\[ m_X(t) = \\exp\\left(\\mu t + \\frac{\\sigma^2 t^2}{2}\\right) \\,. \\] (Note: we need not “prove” anything here, such that there exists a constant \\(b\\) such that \\(m_X(t)\\) is finite for \\(\\vert t \\vert &lt; b\\). The fact that the mgf exists and is written down for us implies that that constant \\(b\\) exists. Never worry about this point unless you are explicitly told to worry about this point!) Harkening back to the last section, the mgf for \\(Z = aX+b\\) is \\[\\begin{align*} m_Z(t) &amp;= e^{bt} m_X(at) \\\\ &amp;= \\exp\\left(-\\frac{\\mu t}{\\sigma}\\right) \\exp\\left(a \\mu t + \\frac{a^2 \\sigma^2 t^2} {2}\\right) \\\\ &amp;= \\exp\\left(-\\frac{\\mu t}{\\sigma}\\right) \\exp\\left(\\frac{\\mu t}{\\sigma} + \\frac{\\sigma^2 t^2}{ 2 \\sigma^2}\\right) \\\\ &amp;= \\exp\\left(0 t + \\frac{1^2 t^2}{2}\\right) \\,. \\end{align*}\\] This mgf retains the functional form of a normal mgf, but with \\(\\mu = 0\\) and \\(\\sigma = 1\\). Thus \\(Z \\sim \\mathcal{N}(0,1)\\), and thus the pdf for \\(Z\\) is \\[ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) ~~~~ z \\in (-\\infty,\\infty) \\,, \\] while the cdf of \\(Z\\) is \\[ F_Z(z) = \\Phi(z) = \\frac{1}{2}\\left[1 + \\mbox{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right] \\,. \\] \\(Z\\) is a so-called standard normal random variable. By historical convention, the cdf of the standard normal distribution is denoted \\(\\Phi(z)\\) (“phi” of z, pronounced “fye”) rather than \\(F_Z(z)\\). Statisticians often standardize normally distributed random variables and perform probability calculations using the standard normal. This is unnecessary in the age of computers, but in the pre-computer era standardization greatly simplified calculations since all one needed was a single table of values of \\(\\Phi(z)\\) to compute probabilities. That said, standardization has its uses: practitioners can rapidly build up and retain probabilistic intuition. For instance, mentally computing an approximate value for \\(P(6 &lt; X &lt; 12)\\) when \\(X \\sim \\mathcal{N}(9,9)\\) can be quite a bit more taxing than if we were to write down the equivalent expression \\(P(-1 &lt; Z &lt; 1)\\) and then evaluate that. (A skilled practitioner would know right away that the latter expression evaluates to \\(\\approx\\) 0.68.) Common \\(Z\\)-Range/Probability Conversions \\((z_L,z_U)\\) \\(\\pm\\) 1 \\(\\pm\\) 2 \\(\\pm\\) 3 \\(\\pm\\) 1.645 \\(\\pm\\) 1.960 \\(\\pm\\) 2.576 prob. 0.6837 0.9544 0.9973 0.90 0.95 0.99 Standard Normal Distribution - R Functions quantity R function call PDF dnorm(x) CDF pnorm(x) Inverse CDF qnorm(q) \\(n\\) iid random samples rnorm(n) 2.6.1 Computing Probabilities Here we will reexamine the three examples that we worked through above in the section in which we introduce the normal cumulative distribution function; here, we will utilize standardization. Note: the final results will all be the same! If \\(X \\sim \\mathcal{N}(10,4)\\), what is \\(P(8 \\leq X \\leq 13.5)\\)? We standardize \\(X\\): \\(Z = (X-\\mu)/\\sigma = (X-10)/2\\). Hence the bounds of integration are \\((8-10)/2 = -1\\) and \\((13.5-10)/2 = 1.75\\), and the probability we seek is \\[ P(-1 \\leq Z \\leq 1.75) = \\Phi(1.75) - \\Phi(-1) \\,. \\] To compute the final number, we utilize pnorm() with its default values of mean=0 and sd=1: pnorm(1.75) - pnorm(-1) ## [1] 0.8012856 The probability is 0.801. If \\(X \\sim \\mathcal{N}(13,5)\\), what is \\(P(8 \\leq X \\leq 13.5 \\vert X &lt; 14)\\)? The integral bounds are \\((8-13)/\\sqrt{5} = -\\sqrt{5}\\) and \\((13.5-13)/\\sqrt{5} = \\sqrt{5}/10\\) in the numerator, and \\(-\\infty\\) and \\((14-13/\\sqrt{5} = \\sqrt{5}/5\\) in the denominator. In R: (pnorm(sqrt(5)/10) - pnorm(-sqrt(5))) / pnorm(sqrt(5)/5) ## [1] 0.8560226 The probability is 0.856. If \\(\\mu = 20\\) and \\(\\sigma^2 = 3\\), what is the value of \\(a\\) such that \\(P(20-a \\leq X \\leq 20+a) = 0.48\\)? Here, \\[ P(20-a \\leq X \\leq 20+a) = P\\left( \\frac{20-a-20}{\\sqrt{3}} \\leq \\frac{X - 20}{\\sqrt{3}} \\leq \\frac{20+a-20}{\\sqrt{3}}\\right) = P\\left(-\\frac{a}{\\sqrt{3}} \\leq Z \\leq \\frac{a}{\\sqrt{3}} \\right) \\,, \\] and we utilize the symmetry of the standard normal around zero to write \\[ P\\left(-\\frac{a}{\\sqrt{3}} \\leq Z \\leq \\frac{a}{\\sqrt{3}} \\right) = 1 - 2P\\left(Z \\leq -\\frac{a}{\\sqrt{3}}\\right) = 1 - 2\\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) \\,. \\] Thus \\[ 1 - 2\\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) = 0.48 ~\\Rightarrow~ \\Phi\\left(-\\frac{a}{\\sqrt{3}}\\right) = 0.26 ~\\Rightarrow~ -\\frac{a}{\\sqrt{3}} = \\Phi^{-1}(0.26) = -0.64 \\,, \\] and thus \\(a = 1.114\\). 2.7 General Transformations of a Single Random Variable Thus far, when discussing transformations of a random variable, we have limited ourselves to linear transformations of independent r.v.’s, i.e., \\(Y = b + \\sum_{i=1}^n a_i X_i\\). What if instead we want to make a more general transformation of a single random variable, e.g., \\(Y = X^2 + 3\\) or \\(Y = \\sin X\\)? We cannot use the method of moment-generating functions here…we need a new algorithm. Let’s assume we have a random variable \\(X\\), and we transform it according to a function \\(g(\\cdot)\\): \\(U = g(X)\\). Then: we identify the inverse function \\(X = g^{-1}(U)\\); we derive the cdf of \\(U\\): \\(F_U(u) = P(U \\leq u) = P(g(X) \\leq u) = P(X \\leq g^{-1}(u))\\); and last we derive the pdf of \\(U\\): \\(f_U(u) = dF_U(u)/du\\). Recall: a continuous distribution’s pdf is the derivative of its cdf. (There is a simplified version of this algorithm that we can use when \\(U = g(X)\\) is strictly increasing or decreasing over the domain of \\(f_X(x)\\): \\[ f_U(u) = f_X(g^{-1}(u)) \\left| \\frac{dg^{-1}(u)}{du} \\right| \\,, \\] We note that the steps we originally laid out above work in all cases, hence we work with them in the examples below and in subsequent sections.) 2.7.1 Distribution of a Transformed Random Variable We are given the following pdf: \\[ f_X(x) = 3x^2 \\,, \\] where \\(x \\in [0,1]\\). What is the distribution of \\(U = X/3\\)? We follow the three steps outlined above. First, we note that \\(U = X/3\\) and thus \\(X = 3U\\). Next, we find \\[ F_U(u) = P(U \\leq u) = P\\left(\\frac{X}{3} \\leq u\\right) = P(X \\leq 3u) = \\int_0^{3u} 3x^2 dx = \\left. x^3 \\right|_0^{3u} = 27u^3 \\,, \\] for \\(u \\in [0,1/3]\\). (The bounds are determined by plugging \\(x=0\\) and \\(x=1\\) into \\(u = x/3\\).) Now we can derive the pdf for \\(U\\): \\[ f_U(u) = \\frac{d}{du} 27u^3 = 54u^2 \\,, \\] for \\(u \\in [0,1/3]\\). What is the distribution of \\(U = -X\\)? We note that \\(U = -X\\) and thus \\(X = -U\\). Next, we find \\[ F_U(u) = P(U \\leq u) = P(-X \\leq u) = P(X \\geq -u) = \\int_{-u}^{1} 3x^2 dx = \\left. x^3 \\right|_{-u}^{1} = 1 - (-u)^3 = 1 + u^3 \\,, \\] for \\(u \\in [-1,0]\\). (Note that the direction of the inequality changed in the probability statement because of the sign change from \\(-X\\) to \\(X\\), and the bounds are reversed to be in ascending order.) Now we derive the pdf: \\[ f_U(u) = \\frac{d}{du} (1+u^3) = 3u^2 \\,, \\] for \\(u \\in [-1,0]\\). What is the distribution of \\(U = 2e^X\\)? We note that \\(U = 2e^X\\) and thus \\(X = \\log(U/2)\\). Next, we find \\[ F_U(u) = P(U \\leq u) = P\\left(2e^X \\leq u\\right) = P\\left(X \\leq \\log\\frac{u}{2}\\right) = \\int_0^{\\log(u/2)} 3x^2 dx = \\left. x^3 \\right|_0^{\\log(u/2)} = \\left(\\log\\frac{u}{2}\\right)^3 \\,, \\] for \\(u \\in [2,2e]\\). The pdf for \\(U\\) is thus \\[ f_U(u) = \\frac{d}{du} \\left(\\log\\frac{u}{2}\\right)^3 = 3\\left(\\log\\frac{u}{2}\\right)^2 \\frac{1}{u} \\,, \\] for \\(u \\in [2,2e]\\). Hint: if you want to see if your derived distribution is correct, one way to check is to code the pdf in R and integrate from the lower to upper bounds: integrand = function(u) { return(3*(log(u/2))^2/u) } integrate(integrand,2,2*exp(1))$value ## [1] 1 Our answer is 1, which it should be for a properly defined pdf. 2.8 Squaring Standard Normal Random Variables The square of a standard normal random variable is an important quantity that arises, e.g., in statistical model assessment (through the “sum of squared errors” when the “error” is normally distributed) and in hypothesis tests like the chi-square goodness-of-fit test. But for this quantity to be useful in statistical inference, we need to know its distribution. In step (1) of the algorithm laid out in the last section, we identify the inverse function: if \\(U = Z^2\\), with \\(Z \\in (-\\infty,\\infty)\\), then \\(Z = \\pm \\sqrt{U}\\). Then, following step (2), we state that \\[ F_U(u) = P(U \\leq u) = P(Z^2 \\leq u) = P(-\\sqrt{u} \\leq Z \\leq \\sqrt{u}) = \\Phi(\\sqrt{u}) - \\Phi(-\\sqrt{u}) \\,, \\] where, as we recall, \\(\\Phi(\\cdot)\\) is the notation for the standard normal cdf. Because of symmetry, we can simplify this expression: \\[ F_U(u) = \\Phi(\\sqrt{u}) - [1 - \\Phi(\\sqrt{u})] = 2\\Phi(\\sqrt{u}) - 1 \\,. \\] To carry out step (3), we utilize the chain rule of differentiation to determine that the pdf is \\[\\begin{align*} f_U(u) = \\frac{d}{du} F_U(u) = \\frac{d}{du} [2\\Phi(\\sqrt{u}) - 1] &amp;= 2 \\frac{d\\Phi}{du}(\\sqrt{u}) \\cdot \\frac{d}{du}\\sqrt{u} \\\\ &amp;= 2 f_Z(\\sqrt{u}) \\cdot \\frac{1}{2\\sqrt{u}} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u}{2}\\right) \\cdot \\frac{1}{\\sqrt{u}} \\\\ &amp;= \\frac{u^{-1/2}}{\\sqrt{2\\pi}} \\exp(-\\frac{u}{2}) \\,, \\end{align*}\\] with \\(u \\in [0,\\infty)\\). This is the pdf for a chi-square distribution with one “degree of freedom”: \\[ U \\sim \\chi_1^2 \\,, \\] with the subscript “1” indicating the number of degrees of freedom. In general, the number of degrees of freedom can be an arbitrary positive integer, and it is conventionally denoted \\(\\nu\\) (nu, pronounced “noo”). Let’s now look at a sample of \\(n\\) independent standard-normal random variables \\(\\{Z_1,\\ldots,Z_n\\}\\). What is the distribution of \\(W = \\sum_{i=1}^n Z_i^2\\)? This is a sum of independent random variables, so we can use mgfs to try to answer this question. The mgf for a chi-square random variable with one degree of freedom is \\[ m_{Z^2}(t) = (1-2t)^{-\\frac{1}{2}} \\,, \\] and thus the mgf for the sum of independent chi-square-distributed random variables will be \\[ m_W(t) = \\prod_{i=1}^n m_{Z_i^2}(t) = \\prod_{i=1}^n (1-2t)^{-\\frac{1}{2}} = (1-2t)^{-\\frac{n}{2}} \\,. \\] We identify \\(m_W(t)\\) as the mgf for a chi-square distribution with \\(\\nu = n\\) degrees of freedom. Thus: if we sum chi-square-distributed random variables, the summed quantity is itself chi-square distributed, with \\(\\nu\\) being the sum of the numbers of degrees of freedom for the original random variables. (This result can be generalized: if \\(X_1 \\sim \\chi_a^2\\) and \\(X_2 \\sim \\chi_b^2\\), then \\(X_1 + X_2 = W \\sim \\chi_{a+b}^2\\).) For completeness, we write down the pdf for \\(\\chi_{\\nu}^2\\): \\[ f_X(x) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2} \\Gamma(\\nu/2)} \\exp\\left(-\\frac{x}{2}\\right) \\,, \\] where \\(x \\geq 0\\) and \\(\\nu\\) is a positive integer; \\(E[X] = \\nu\\) and \\(V[X] = 2\\nu\\). The chi-square distribution is highly skew for small values of \\(\\nu\\), while chi-square random variables converge in distribution to normal random variables as \\(\\nu \\rightarrow \\infty\\). See Figure 2.7. Figure 2.7: Chi-square distributions for \\(\\nu = 3\\) (left) and \\(\\nu = 30\\) (right) degrees of freedom. Chi-square random variables converge in distribution to normal random variables as \\(\\nu \\rightarrow \\infty\\). \\(\\chi^2\\) Distribution - R Functions quantity R function call PDF dchisq(x,df) CDF pchisq(x,df) Inverse CDF qchisq(q,df) \\(n\\) iid random samples rchisq(n,df) 2.8.1 The Expected Value of a Chi-Square Random Variable The expected value of a chi-square distribution for one degree of freedom is \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty x f_X(x) dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{1/2}}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x}{2}\\right) dx \\,. \\end{align*}\\] To find the value of this integral, we are going to utilize the gamma function \\[ \\Gamma(u) = \\int_0^\\infty x^{u-1} \\exp(-x) dx \\] (TBD: this is defined “backwards” in terms of u and x from Chapter 1.) which we first introduced in an example in Chapter 1. (Note that \\(u &gt; 0\\).) Recall that if \\(u\\) is an integer, then \\(\\Gamma(u) = (u-1)! = (u-1) \\times (u-2) \\times \\cdots \\times 1\\), with the exclamation point representing the factorial function. If \\(u\\) is a half-integer and small, one can look up the value of \\(\\Gamma(u)\\) online. The integral we are trying to compute doesn’t quite match the form of the gamma function integral…but as you should recognize by now, we can attempt a variable substitution to change \\(e^{-x/2}\\) to \\(e^{-y}\\): \\[ y = x/2 ~,~ dy = dx/2 ~,~ x = 0 \\implies y = 0 ~,~ x = \\infty \\implies y = \\infty \\] We thus rewrite our expected value as \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\frac{\\sqrt{2}y^{1/2}}{\\sqrt{2\\pi}} \\exp(-y) (2 dy) \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\int_0^\\infty y^{1/2} \\exp(-y) dy \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{3}{2}\\right) \\\\ &amp;= \\frac{2}{\\sqrt{\\pi}} \\frac{\\sqrt{\\pi}}{2} = 1 \\end{align*}\\] As we saw above, the sum of \\(n\\) chi-square random variables, each distributed with 1 degree of freedom, is itself chi-square-distributed for \\(n\\) degrees of freedom. Hence, in general, if \\(W \\sim \\chi_{n}^2\\), then \\(E[W] = n\\). 2.8.2 Computing Probabilities Let’s assume at first that we have a single random variable \\(Z \\sim \\mathcal{N}(0,1)\\). What is \\(P(Z^2 &gt; 1)\\)? We know that \\(Z^2\\) is sampled from a chi-square distribution for 1 degree of freedom. So \\[ P(Z^2 &gt; 1) = 1 - P(Z^2 \\leq 1) = 1 - F_{W(1)}(1) \\,. \\] This is not easily computed by hand, so we utilize R: 1 - pchisq(1,1) ## [1] 0.3173105 The probability is 0.3173. With hindsight, we see why this was going to be the value all along: we know that \\(P(-1 \\leq Z \\leq 1) = 0.6827\\), and thus \\(P(\\vert Z \\vert &gt; 1) = 1-0.6827 = 0.3173\\). If we square both sides in this last probability statement, we see that \\(P(\\vert Z \\vert &gt; 1) = P(Z^2 &gt; 1)\\). Now, what if we want to know the value \\(a\\) such that \\(P(W &gt; a) = 0.9\\), where \\(W = Z_1^2 + \\cdots + Z_4^2\\) and \\(\\{Z_1,\\ldots,Z_4\\}\\) are iid standard normal random variables? First, we recognize that \\(W \\sim \\chi_4^2\\), i.e., \\(W\\) is chi-square distributed for 4 degrees of freedom. Second, we re-express \\(P(W &gt; a)\\) in terms of the cdf of the chi-square distribution, \\[ P(W &gt; a) = 1 - P(W \\leq a) = 1 - F_{W(4)}(a) = 0.9 \\,, \\] and we rearrange terms: \\[ F_{W(4)}(a) = 0.1 \\,. \\] To isolate \\(a\\), we use the inverse CDF function: \\[ F_{W(4)}^{-1} [F_{W(4)}(a)] = a = F_{W(4)}^{-1}(0.1) \\,. \\] To compute \\(a\\), we use the R function qchisq(): (a = qchisq(0.1,4)) ## [1] 1.063623 We find that \\(a = 1.064\\). 2.9 Sample Variance of Normal Random Variables Recall the definition of the sample variance: \\[ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\] The reason why we divide by \\(n-1\\) instead of \\(n\\) is that it makes \\(S^2\\) an unbiased estimator of \\(\\sigma^2\\). We will illustrate this point below when we turn to point estimation. Above, we showed how the mean for a sample of independent and identically distributed normal random variables is itself normal, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). What is the distribution of \\(S^2\\)? (As we’ll see, it turns out that this is not the right question. The right question is what is the distribution of \\((n-1)S^2/\\sigma^2\\)?) It turns out that deriving this distribution is not quite a simple as deriving the distribution of the sample mean, but it can be done! We begin by summing a sequence of standardized and squared random normal variables: \\[ W = \\sum_{i=1}^n \\left( \\frac{X_i-\\mu}{\\sigma} \\right)^2 \\,. \\] In the previous section, we showed that \\(W \\sim \\chi_n^2\\). We can work with the expression to the right of the equals sign now to determine the distribution of \\((n-1)S^2/\\sigma^2\\). \\[\\begin{align*} \\sum_{i=1}^n \\left( \\frac{X_i-\\mu}{\\sigma} \\right)^2 &amp;= \\sum_{i=1}^n \\left( \\frac{(X_i-\\bar{X})+(\\bar{X}-\\mu)}{\\sigma} \\right)^2 \\\\ &amp;= \\sum_{i=1}^n \\left( \\frac{X_i-\\bar{X}}{\\sigma}\\right)^2 + \\sum_{i=1}^n \\left( \\frac{\\bar{X}-\\mu}{\\sigma}\\right)^2 + \\mbox{cross~term~equaling~zero} \\\\ &amp;= \\frac{(n-1)S^2}{\\sigma^2} + n\\left(\\frac{\\bar{X}-\\mu}{\\sigma}\\right)^2 = \\frac{(n-1)S^2}{\\sigma^2} + \\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\right)^2 \\,. \\end{align*}\\] The expression farthest to the right, \\[ \\bar{Z} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\,, \\] is the standardization of \\(\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n)\\), and thus it is standard-normal distributed. And as we now know, \\(\\bar{Z}^2 \\sim \\chi_1^2\\). Thus by utilizing the general result given at the end of the last section, we can immediately identify that \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi_{n-1}^2 \\,. \\] 2.9.1 Computing Probabilities Let’s suppose you sample \\(16\\) iid normal random variables, and you know that \\(\\sigma^2 = 10\\). What is the probability \\(P(S^2 &gt; 12)\\)? (We’ll stop for a moment to answer a question that might occur to you. “Why are we doing a problem where we assume \\(\\sigma^2\\) is known? In real life, it almost certainly won’t be.” This is an entirely fair question. This example is contrived, but it builds towards a particular situation where we can assume a value for \\(\\sigma^2\\): hypothesis testing. The calculation we will do below is equivalent in structure to calculations we will do later when testing hypotheses about normal population variances.) The first question that we should always ask ourselves is whether we know the distribution of the quantity in the probability statement, in this case \\(S^2\\). The answer is no. Thus we cannot compute the probability (by hand, at least…but see below) without manipulating the inequality. What do we know the distribution of? \\((n-1)S^2/\\sigma^2\\). So: \\[\\begin{align*} P(S^2 &gt; 12) &amp;= P\\left(\\frac{(n-1)S^2}{\\sigma^2} &gt; \\frac{(n-1) \\cdot 12}{\\sigma^2}\\right) \\\\ &amp;= P\\left(W &gt; \\frac{15 \\cdot 12}{10}\\right) = P(W &gt; 18) = 1 - P(W \\leq 18) = 1 - F_{W(15)}(18) \\,, \\end{align*}\\] where \\(W \\sim \\chi_{n-1}^2\\) and \\(n-1 = 15\\). To compute the probability, we use pchisq(): 1 - pchisq(18,15) ## [1] 0.2626656 The probability is 0.263: there is only a 26.3% chance that we will sample a value of \\(S^2\\) greater than 12. To compute the probability via simulation, we can repeatedly generate data samples of size \\(n = 16\\) from a normal distribution with some arbitrary mean (the value doesn’t matter) and variance \\(\\sigma^2 = 10\\); compute and record \\(S^2\\); and determine the proportion of our simulated \\(S^2\\) values that are greater than 12. Note that we record \\(S^2\\) and not \\((n-1)S^2/\\sigma^2\\); we don’t need to manipulate quantities when we are running simulations, because we do not need to know the distribution of the statistic here, just the distribution from which the individual data are sampled. Also note that because we are not simulating an infinite number of samples, the proportion that we observe will itself be a random variable with some mean, some variance, and some sampling distribution. The key here is “some variance”: to generate a more accurate accounting of the proportion of \\(S^2\\) values greater than 12, we want to generate as many data samples as we can (a) without having to wait too long for the result, and (b) without causing memory allocation issues by recording too many values of \\(S^2\\), if we choose to record them all. set.seed(101) # set so that the same data are generated every time m &lt;- 100000 # the number of data samples n &lt;- 16 # the size of each data sample sigma2 &lt;- 10 # the true variance s2 &lt;- rep(NA,m) # allocated space for S^2 for ( ii in 1:m ) { x &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) s2[ii] &lt;- var(x) } round(sum(s2&gt;12)/m,3) ## [1] 0.262 Another way to code this to circumvent memory allocation is set.seed(101) # set so that the same data are generated every time m &lt;- 100000 # the number of data samples n &lt;- 16 # the size of each data sample sigma2 &lt;- 10 # the true variance s2true &lt;- 0 # a counter of the number of S^2 values &gt; 12 for ( ii in 1:m ) { x &lt;- rnorm(n,mean=0,sd=sqrt(sigma2)) if ( var(x) &gt; 12) s2true = s2true+1 } round(s2true/m,3) ## [1] 0.262 The tradeoff is that while this second code uses less memory, it takes (slightly) longer to run. 2.9.2 Expected Value of the Sample Variance and Standard Deviation It is extremely straightforward to determine the expected value for the sample variance: \\[ E\\left[ \\frac{(n-1)S^2}{\\sigma^2} \\right] = n-1 ~~~ \\Rightarrow ~~~ E[S^2] = \\frac{(n-1)\\sigma^2}{(n-1)} = \\sigma^2 \\,. \\] Here, we use the fact that \\(W = (n-1)S^2/\\sigma^2\\) is a chi-square-distributed random variable, and that \\(E[W]\\) equals the number of degrees of freedom, which here is \\(n-1\\). What is more difficult to determine is \\(E[(S^2)^a]\\), where \\(a\\) is some constant. For instance, \\(E[S^4]\\) is not \\(\\sigma^4\\) (making the variance of \\(S^2\\) somewhat more difficult to compute than we might initiall expect)…and \\(E[S]\\) is not \\(\\sigma\\). Let’s determine \\(E[S]\\) here. The way we do this is by performing a random variable transformation (\\(S = \\sqrt{S^2}\\)) to determine the pdf \\(f_S(s)\\), and then computing \\(E[S] = \\int s f_S(s) ds\\). We start by writing \\[\\begin{align*} P(S \\leq s) &amp;= P(\\sqrt{S^2} \\leq s) = P(S^2 \\leq s^2) = P\\left( \\frac{(n-1)S^2}{\\sigma^2} \\leq \\frac{(n-1)s^2}{\\sigma^2} \\right) \\\\ &amp;= P\\left( W \\leq \\frac{(n-1)s^2}{\\sigma^2} \\right) = F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) \\,. \\end{align*}\\] We can then use the chain rule of differentiation to write that \\[ f_S(s) = \\frac{d}{ds}F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) = f_{W(n-1)}F_{W(n-1)}\\left( \\frac{(n-1)s^2}{\\sigma^2} \\right) \\frac{2(n-1)s}{\\sigma^2} \\,. \\] We then substitute in the pdf: \\[ f_S(s) = \\frac{2(n-1)s}{\\sigma^2} \\frac{[(n-1)s^2/\\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) \\,. \\] The expected value of \\(S\\) is then \\[ E[S] = \\int_0^\\infty s f_S(s) ds = \\int_0^\\infty \\frac{2(n-1)s^2}{\\sigma^2} \\frac{[(n-1)s^2/\\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) ds \\,. \\] OK…how should we pursue this? Let’s try a variable substitution approach, with \\[ x = \\frac{(n-1)s^2}{2\\sigma^2} ~~~ \\Rightarrow ~~~ dx = \\frac{(n-1)s}{\\sigma^2}ds = \\frac{n-1}{\\sigma^2} \\left(\\frac{2 \\sigma^2 x}{n-1}\\right)^{1/2} ds = \\left(\\frac{ 2 (n-1) x }{\\sigma^2} \\right)^{1/2} ds \\,. \\] We note that if \\(s = 0\\), \\(x = 0\\), and if \\(s = \\infty\\), \\(x = \\infty\\), so the integral bounds are unchanged. So now we have that \\[\\begin{align*} E[S] &amp;= \\int_0^\\infty 4x \\frac{(2x)^{(n-3)/2}}{2^{(n-1)/2}\\Gamma((n-1)/2)} \\exp(-x) \\frac{\\sigma}{\\sqrt{2(n-1)x}} dx \\\\ &amp;= \\sqrt{2} \\int_0^\\infty \\frac{x^{(n-2)/2}}{\\Gamma((n-1)/2)} \\exp(-x) \\frac{\\sigma}{\\sqrt{n-1}} dx \\\\ &amp;= \\sqrt{2} \\frac{\\sigma}{\\sqrt{n-1}} \\frac{1}{\\Gamma((n-1)/2)} \\int_0^\\infty x^{n/2-1} \\exp(-x) dx \\\\ &amp;= \\sigma \\sqrt{\\frac{2}{n-1}} \\frac{\\Gamma(n/2)}{\\Gamma((n-1)/2)} \\,. \\end{align*}\\] The integral in the second-to-last line is that which defines the gamma function \\(\\Gamma(\\cdot)\\). So we see that while \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\), \\(S\\) is a biased estimator of \\(\\sigma\\). We note, without getting into details, that as \\(n \\rightarrow \\infty\\), \\(E[S] \\rightarrow \\sigma\\), so \\(S\\) is asymptotically unbiased. 2.10 Standardizing a Normal Random Variable with Unknown Variance It is generally the case in real-life that when we assume that our data are sampled from a normal distribution, both the mean and the variance are unknown. This means that instead of this \\[ Z = \\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0,1) ~~\\mbox{and}~~ \\bar{Z} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}\\left(0,\\frac{1}{n}\\right) \\,, \\] we have \\[ \\frac{X-\\mu}{S} \\sim \\mbox{?} ~~\\mbox{and}~~ \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}} \\sim \\mbox{?} \\,. \\] Each of the expressions above features two random variables, one in the numerator and one in the denominator, and each is independent of the other. (We will state this without proof, as the proof is beyond the scope of this book.) Let, e.g., \\(U = X-\\mu \\sim \\mathcal{N}(0,\\sigma^2)\\). While we know that \\((n-1)S^2/\\sigma^2 \\sim \\chi_{n-1}^2\\), we have not yet specified the distribution of its square root. It may appeal to your intuition that \\(\\sqrt{n-1}S/\\sigma\\) is chi-distributed (as opposed to chi-square-distributed) for \\(n-1\\) degrees of freedom. The ratio distribution \\(T = U/S\\) is \\[ f_{T(n-1)}(t) = \\int_{-\\infty}^\\infty \\vert s \\vert f_U(ts) f_S(s) ds \\rightarrow \\int_0^\\infty s f_U(ts) f_S(s) ds \\,, \\] where we make use of the fact that \\(s &gt; 0\\). \\(f_U(ts)\\) is a normal pdf, while \\(f_S(s)\\) a transformed chi pdf (where the transformation converts the distribution of \\(\\sqrt{n-1}S/\\sigma\\) to the distribution of \\(S\\) itself). We skip the details of integration (which are also beyond the scope of this book) to get to the end result, which is that \\[ f_{T(n-1)}(t) = \\frac{\\Gamma\\left(\\frac{n}{2}\\right)}{\\sqrt{(n-1)\\pi}\\Gamma\\left(\\frac{n-1}{2}\\right)}\\left(1+\\frac{t^2}{n-1}\\right)^{-\\frac{n}{2}} \\,, \\] i.e., \\(T\\) is sampled from a Student’s t distribution for \\(n-1\\) degrees of freedom. Assuming that \\(n\\) is integer-valued, the expected value of \\(T\\) is \\(E[T] = 0\\) for \\(n \\geq 3\\) (and is undefined otherwise), while the variance is \\((n-1)/(n-3)\\) for \\(n \\geq 4\\), is infinite for \\(n=3\\), and is otherwise undefined. Appealing to intuition, we can form a (symmetric) \\(t\\) distribution by taking a standard normal \\(\\mathcal{N}(0,1)\\) and “pushing down” in the center, the act of which transfers probability density equally to both the lower and upper tails. In Figure 2.8, we see that the smaller the number of degrees of freedom, the more density is transferred to the tails, and that as \\(n\\) increases, the more and more \\(f_{T(n-1)}(t)\\) becomes indistinguishable from a standard normal. (A more technical way of stating this is that the random variable \\(T\\) converges in distribution to a standard normal random variable as \\(n \\rightarrow \\infty\\).) Figure 2.8: The natural logarithm of the pdf for the standard normal (black) and for \\(t\\) distributions with 3 (red), 6 (green), and 12 (blue) degrees of freedom. We observe that as \\(n\\) decreases, there is more probability density in the lower and upper tails of the distributions. \\(t\\) Distribution - R Functions quantity R function call PDF dt(x,df) CDF pt(x,df) Inverse CDF qt(q,df) \\(n\\) iid random samples rt(n,df) 2.10.1 The Sample Mean and Standard Deviation Are Independent Random Variables TBD – didn’t I intend to get rid of this example? We state above that the sample mean, \\(\\bar{X}\\), and the sample standard deviation, \\(S\\), are independent random variables, and that the proof is beyond the scope of this book. Here, we show a means by which one can visually demonstrate independence, using a simulation. The overarching idea in the simulation is to demonstrate that, e.g., \\(f_{\\bar{X}}(\\bar{X} \\vert S) = f_{\\bar{X}}(\\bar{X})\\). We do this by sampling values of \\(\\mu\\) and \\(\\sigma\\), generating a dataset \\(\\{X_1,\\ldots,X_n\\} \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and recording \\(\\bar{X}\\) and \\(S\\), repeating this process \\(m\\) times. We then overlay the observed distribution of all \\(\\bar{X}\\) values with the distribution of, e.g., \\(\\bar{X} \\vert 5 &lt; S &lt; 6\\). (This is an arbitrary choice.) set.seed(101) m &lt;- 100000 sigma &lt;- runif(m,min=0,max=10) mu &lt;- rnorm(m,mean=0,sd=1) Xbar &lt;- rep(NA,m) S &lt;- rep(NA,m) n &lt;- 100 for ( ii in 1:m ) { x &lt;- rnorm(n,mean=mu[ii],sd=sigma[ii]) Xbar[ii] &lt;- mean(x) S[ii] &lt;- sd(x) } df &lt;- data.frame(Xbar=Xbar,S=S) df.subset &lt;- subset(df,S&gt;5 &amp; S&lt;6) ggplot(data=df,aes(x=Xbar,y=after_stat(density))) + geom_histogram(fill=rgb(0,0,1,alpha=0.3),col=&quot;black&quot;,breaks=seq(-4,4,by=0.5)) + geom_histogram(data=df.subset,aes(x=Xbar,y=after_stat(density)), fill=rgb(1,0,0,alpha=0.3),col=&quot;black&quot;,breaks=seq(-4,4,by=0.5)) + labs(x=&quot;Sample Mean&quot;) + coord_cartesian(ylim=c(0,0.4)) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.9: Two histograms, one showing the empirical distribution of all values of the sample mean \\(\\bar{X}\\), and the other showing \\(\\bar{X}\\) only for those data with sample standard deviations between 5 and 6. The striking similarity of the two histograms indicates that \\(\\bar{X}\\) is independent of \\(S\\), with the caveat that visual evidence does not constitute quantitative proof. The result indicates that the distributions are highly similar, indicating independence. Note that because of the arbitrary construction of this simulation (our choices as to how \\(\\mu\\) and \\(\\sigma\\) values are sampled, the range of \\(S\\) values examined, etc.), it is ultimately no substitute for a rigorous mathematical proof of independence. But it is useful for building intuition. We also note that it is less than optimal that we reach our conclusion of apparent independence visually: what we are missing is a hypothesis test that allows us to test the idea that both, e.g., \\(\\bar{X}\\) and \\(\\bar{X} \\vert S \\in (5,6)\\) arise from the same underlying distribution. We return to this point when we discuss hypothesis tests of normality below. 2.10.2 Computing Probabilities In a study, the diameters of 8 widgets are measured. It is known from previous work that the widget diamaters are normally distributed, with sample standard deviation \\(S = 1\\) unit. What is the probability that the the sample mean \\(\\bar{X}\\) observed in the current study will be within one unit of the population mean \\(\\mu\\)? The probability we seek is \\[ P( \\vert \\bar{X} - \\mu \\vert &lt; 1 ) = P(\\mu - 1 \\leq \\bar{X} \\leq \\mu + 1) \\,. \\] The key here is that we don’t know \\(\\sigma\\), so the probability can only be determined if we manipulate this expression such that the random variable inside it is \\(t\\)-distributed…and \\(\\bar{X}\\) itself is not. So the first step is to standardize: \\[ P\\left( \\frac{\\mu - 1 - \\mu}{S/\\sqrt{n}} \\leq \\frac{\\bar{X} - \\mu}{S/\\sqrt{n}} \\leq \\frac{\\mu + 1 - \\mu}{S/\\sqrt{n}}\\right) = P\\left( -\\frac{\\sqrt{n}}{S} \\leq T \\leq \\frac{\\sqrt{n}}{S}\\right) \\,. \\] We know that \\(\\sqrt{n}(\\bar{X} - \\mu)/S\\) is \\(t\\)-distributed (with \\(n-1\\) degrees of freedom), so long as the individual data are normal iid random variables. (If the individual data are not normally distributed, this question might have to be solved via simulations…if we cannot determine the distribution of \\(\\bar{X}\\) using, e.g., moment-generating functions.) The probability is the difference between two cdf values: \\[ P\\left( -\\frac{\\sqrt{n}}{S} \\leq T \\leq \\frac{\\sqrt{n}}{S}\\right) = F_{T(7)}(\\sqrt{8}) - F_{T(7)}(-\\sqrt{8}) \\,, \\] where \\(n-1\\), the number of degrees of freedom, is 7. This is the end of the problem if we are using pen and paper. If we have R at our disposal, we would code the following: pt(sqrt(8),7) - pt(-sqrt(8),7) ## [1] 0.9745364 The probability is 0.975. 2.11 Point Estimation In the previous chapter, we introduced the concept of point estimation, using statistics to make inferences about a population parameter \\(\\theta\\). Recall that a point estimator \\(\\hat{\\theta}\\), being a statistic, is a random variable and has a sampling distribution. One can define point estimators arbitrarily, but in the end, we can choose the best among a set of estimators by assessing properties of their sampling distributions: Bias: \\(B[\\hat{\\theta}] = E[\\hat{\\theta}-\\theta]\\) Variance: \\(V[\\hat{\\theta}]\\) Recall: the bias of an estimator is the difference between the average value of the estimates it generates, minus the true parameter value. If \\(E[\\hat{\\theta}-\\theta] = 0\\), then \\(\\hat{\\theta}\\) is said to be an unbiased estimator. Assuming that our observed data are independent and identically distributed (or iid), then computing each of these quantities is straightforward. Choosing a best estimator based on these two quantities might not lead to a clear answer, but we can overcome that obstacle by combining them into one metric, the mean-squared error (or MSE): MSE: \\(MSE[\\hat{\\theta}] = B[\\hat{\\theta}]^2 + V[\\hat{\\theta}]\\) In the previous chapter, we also discussed how guessing the form of an estimator is sub-optimal, and how there are various approaches to deriving good estimators. The first one that we highlight s maximum likelihood estimation (or MLE). We will apply the MLE here to derive an estimator for the population mean. Then we will introduce methodology by which to assess the estimator in an absolute sense, and extend these results to write down the asymptotic sampling distribution for the MLE, i.e., the sampling distribution as the sample size \\(n \\rightarrow \\infty\\). Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). The setting, again, is that we have randomly sampled \\(n\\) data from a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). This means that the likelihood function is \\[\\begin{align*} \\mathcal{L}(\\mu,\\sigma \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x \\vert \\mu,\\sigma) \\\\ &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right) \\\\ &amp;= \\frac{1}{(2 \\pi)^{n/2}} \\frac{1}{\\sigma^n} \\exp\\left({-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2}\\right) \\,, \\end{align*}\\] and the log-likelihood is \\[ \\ell(\\mu,\\sigma \\vert \\mathbf{x}) = -\\frac{n}{2} \\log (2 \\pi) - n \\log \\sigma - \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\,. \\] Because there is more than one parameter, we take the partial derivative of \\(\\ell\\) with respect to \\(\\mu\\): \\[\\begin{align*} \\ell&#39;(\\mu,\\sigma \\vert \\mathbf{x}) &amp;= \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2} \\log (2 \\pi) - n \\log \\sigma - \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right) \\\\ &amp;= -\\frac{1}{\\sigma^2} \\sum_{i=1}^n 2(x_i - \\mu)(-1) \\\\ &amp;= \\frac{2}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) \\,. \\end{align*}\\] After setting this quantity to zero and dividing out the term \\(2/\\sigma^2\\), we find that \\[ \\sum_{i=1}^n x_i = \\sum_{i=1}^n \\mu ~~\\Rightarrow~~ \\sum_{i=1}^n x_i = n \\mu ~~\\Rightarrow~~ \\frac{1}{n} \\sum_{i=1}^n x_i = n \\mu ~~\\Rightarrow~~ \\mu = \\bar{x} \\,, \\] and thus \\(\\hat{\\mu}_{MLE} = \\bar{X}\\). (Recall that as we go from a purely mathematical derivation to a final definition of an estimator, we need to take into account that the estimator is a random variable and thus we need to change from lower-case to upper-case when writing out the data variable. Also recall that for this MLE to be valid, the second derivative of the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\) at \\(x = \\bar{x}\\) has to be negative. We leave checking this as an exercise to the reader.) Because the estimator is \\(\\bar{X}\\), we know immediately its expected value and variance given previous results: \\(E[\\hat{\\mu}_{MLE}] = E[\\bar{X}] = \\mu\\) (the estimator is unbiased); and \\(V[\\hat{\\mu}_{MLE}] = V[\\bar{X}] = \\sigma^2/n\\) (and thus \\(MSE[\\hat{\\mu}_{MLE}] = \\sigma^2/n\\)). While \\(\\hat{\\mu}_{MLE}\\) is an unbiased estimator, recall that MLEs can be biased…but if they are, they are always asymptotically unbiased, meaning that the bias goes away as \\(n \\rightarrow \\infty\\). Here, we will introduce one more estimator concept, that of consistency: do both the bias and the variance of the estimator go to zero as the sample size \\(n\\) increases? If so, our estimator is said to be consistent. If an estimator is not consistent, it will never converge to the true value \\(\\theta\\), regardless of sample size. Here, we need not worry about the bias (\\(\\hat{\\mu}_{MLE} = \\bar{X}\\) in unbiased for all \\(n\\)), and we note that \\[ \\lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} = 0 \\,. \\] Thus \\(\\hat{\\mu}_{MLE}\\) is a consistent estimator. The question that we are going to pose now refers to a point made obliquely in the previous chapter: how do we assess \\(V[\\hat{\\mu}_{MLE}]\\) in absolute terms? Can we come up with an estimator that is more accurate for any given value of \\(n\\)? The answer lies in the Cramer-Rao theorem, which allows us to derive a lower bound on the variance of an estimator (so long as the domain of \\(p_X(x \\vert \\theta)\\) or \\(f_X(x \\vert \\theta)\\) does not depend on \\(\\theta\\) itself…which is true for the normal). Assuming that we sample \\(n\\) iid data and that \\(\\hat{\\theta}\\) is unbiased, then the Cramer-Rao lower bound (CRLB) on the estimator’s variance is \\[ V[\\hat{\\theta}] = \\frac{1}{I_n(\\theta)} = \\frac{1}{nI(\\theta)} \\,, \\] where \\(I(\\theta)\\) represents the Fisher information for \\(\\theta\\) contained in a single random variable \\(X\\). Assuming we sample our data from a pdf, the Fisher information is \\[ I(\\theta) = E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log f_X(x \\vert \\theta) \\right)^2\\right] ~~~\\mbox{or}~~~ I(\\theta) = -E\\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log f_X(x \\vert \\theta) \\right] \\,. \\] (The equation to the right is valid only under certain regularity conditions, but those conditions are generally met in the context of this book and thus this is the equation that we will use in general.) Let’s step back for a second and think about what the Fisher information represents. It is the average value of the square of the first derivative of a pdf. If a pdf’s slope is relatively small (because the pdf is wide, like a normal with a large \\(\\sigma\\) parameter), then the average value of the slope, squared, is relatively small, i.e., the information that \\(X\\) provides about \\(\\mu\\) or \\(\\sigma\\) is relatively small. This makes intuitive sense: a wide pdf means that, for instance, when we sample data and try to subsequently estimate \\(\\theta\\), we will be more uncertain about its actual value. On the other hand, if the pdf is highly peaked, then the average slope of the pdf is larger and…\\(I(\\theta)\\) is relatively large. A sampled datum contains more information by which to constrain \\(\\theta\\). (See Figure 2.10, in which the pdf to the left has less Fisher information content about \\(\\mu\\) than the pdf to the right.) Figure 2.10: An illustration of Fisher information. For the pdf to the left, the slope changes slowly, and thus the rate of change of the slope is smaller than for the pdf to the right. Thus the pdf to the left contains less information about the population mean, i.e., its Fisher information value \\(I(\\mu)\\) is smaller. For the normal, \\[\\begin{align*} \\log f(X \\vert \\mu) &amp;= -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} \\\\ \\frac{\\partial}{\\partial \\mu} \\log f(X \\vert \\mu) &amp;= 0 - \\frac{2(x-\\mu)(-1)}{2\\sigma^2} = \\frac{(x-\\mu)}{\\sigma^2} \\\\ \\frac{\\partial^2}{\\partial \\mu^2} \\log f(X \\vert \\mu) &amp;= -\\frac{1}{\\sigma^2} \\,, \\end{align*}\\] so \\[ I(\\mu) = -E\\left[-\\frac{1}{\\sigma^2}\\right] = \\frac{1}{\\sigma^2} \\,, \\] and \\[ V[\\hat{\\mu}] = \\frac{1}{n (1/\\sigma^2)} = \\frac{\\sigma^2}{n} \\,. \\] We see that the variance of \\(\\hat{\\mu} = \\bar{X}\\) achieves the CRLB, meaning that indeed we cannot propose a better unbiased estimator for the normal population mean. (Note that the CRLB does not depend on \\(\\mu\\), which totally makes sense: it is the width of the pdf that matters, and thus only parameters that map to a pdf’s width should appear in the CRLB.) We conclude this section by using the Fisher information to state an important result about maximum likelihood estimates. Recall that an estimator is a statistic, and is thus a random variable with a sampling distribution. What do we know about this distribution? For arbitrary values of \\(n\\), nothing, per se; we would have to run simulations to estimate the sampling distribution. However, as \\(n \\rightarrow \\infty\\), the MLE converges in distribution to a normal random variable: \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) ~\\mbox{or}~ (\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y&#39; \\sim \\mathcal{N}\\left(0,\\frac{1}{nI(\\theta)}\\right) \\,. \\] (As usual, some regularity conditions apply that do not concern us at the current time.) We already knew about the mean zero part: we had stated that the MLE is asymptotically unbiased. What’s new is the variance, and the fact that the variance achieves the CRLB, and the identification of the sampling distribution as being normal. (It turns out that the normality of the distribution is related to the Central Limit Theorem, the subject of the next section, and thus this is ultimately not a surprising result.) A sketch of how one would prove the asymptotic normality of the MLE is provided as optional material in Chapter 7. 2.11.1 Maximum Likelihood Estimation for Normal Variance As above, we will suppose that we are given a data sample \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)\\), but here, our desire is to estimate the population variance \\(\\sigma^2\\) instead of the population mean. Borrowing the form of the log-likelihood \\(\\ell(\\mu,\\sigma^2 \\vert \\mathbf{x})\\) from above, we find that \\[ \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2}\\frac{1}{\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n (x_i-\\mu)^2 = 0 ~\\implies~ \\hat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2 = \\hat{\\sigma^2}_{MLE} = \\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2 \\,, \\] where the last equality follows from the fact that \\(\\hat{\\mu}_{MLE} = \\bar{X}\\). We can see immediately that \\[ \\hat{\\sigma^2}_{MLE} = \\frac{n-1}{n}S^2 \\,. \\] Thus \\[ E\\left[\\hat{\\sigma^2}_{MLE}\\right] = E\\left[\\frac{n-1}{n}S^2\\right] = \\frac{n-1}{n}\\sigma^2 \\,. \\] At this point, one might say, “wait a minute…how do we know \\(E[S^2] = \\sigma^2\\)? We know this because \\[ E[S^2] = \\frac{\\sigma^2}{n-1} E\\left[\\frac{(n-1)S^2}{\\sigma^2}\\right] = \\frac{\\sigma^2}{n-1} E[W] = \\frac{\\sigma^2}{n-1} (n-1) = \\sigma^2 \\,, \\] where \\(W\\) is a chi-square-distributed random variable for \\(n-1\\) degrees of freedom; \\(E[W] = n-1\\). Now, to get back to the narrative at hand: the MLE for \\(\\sigma^2\\) is a biased estimator, but it is asymptotically unbiased: \\[ \\lim_{n \\rightarrow \\infty} \\left( E\\left[\\hat{\\sigma^2}_{MLE}\\right] - \\sigma^2 \\right) = \\lim_{n \\rightarrow \\infty} \\left( \\frac{n-1}{n}\\sigma^2 - \\sigma^2 \\right) = 0 \\,. \\] 2.11.2 Asymptotic Normality of the MLE for the Normal Population Variance We will use the alternative formulation of the Fisher information, given immediately above, to write down the Cramer-Rao lower bound for the variances of estimates of the normal population variance. We have that \\[\\begin{align*} \\log f(X \\vert \\sigma^2) &amp;= -\\frac{1}{2} \\log (2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2} \\\\ \\frac{\\partial}{\\partial \\sigma^2} \\log f(X \\vert \\sigma^2) &amp;= -\\frac{1}{2}\\frac{1}{2 \\pi \\sigma^2} 2 \\pi + \\frac{(x-\\mu)^2}{2(\\sigma^2)^2} = -\\frac{1}{2 \\sigma^2} + \\frac{(x-\\mu)^2}{2(\\sigma^2)^2} \\\\ \\frac{\\partial^2}{\\partial (\\sigma^2)^2} \\log f(X \\vert \\sigma^2) &amp;= \\frac{1}{2 (\\sigma^2)^2} - \\frac{2(x-\\mu)^2}{2(\\sigma^2)^3} = \\frac{1}{2(\\sigma^2)^2} - \\frac{(x-\\mu)^2}{(\\sigma^2)^3} \\,, \\end{align*}\\] The expected value is \\[ E\\left[ \\frac{1}{2(\\sigma^2)^2} - \\frac{(x-\\mu)^2}{(\\sigma^2)^3} \\right] = \\frac{1}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} E[(x-\\mu)^2] = \\frac{1}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} \\sigma^2 = -\\frac{1}{2\\sigma^4} \\,, \\] and thus \\(I(\\sigma^2) = -E[-1/(2\\sigma^4)] = 1/(2\\sigma^4)\\), \\(I_n(\\sigma^2) = n/(2\\sigma^4)\\), and \\[ \\hat{\\sigma^2} \\stackrel{d}{\\sim} \\mathcal{N}\\left(\\sigma^2,\\frac{2\\sigma^4}{n}\\right) \\,. \\] 2.11.3 Simulating the Sampling Distribution of the MLE for the Normal Population Variance The result that we derive immediately above is an asymptotic result…but we never actually have an infinite sample size. If our sample size is low, we should expect that the distribution of \\(\\hat{\\sigma^2}\\) will deviate substantially from the asymptotic expectation, but we cannot know by how much unless we run simulations. Below, we show how we might run a simulation if we assume that we have \\(n = 15\\) data drawn from a \\(\\mathcal{N}(0,4)\\) distribution. See Figure 2.11. set.seed(101) n &lt;- 15 sigma2 &lt;- 4 k &lt;- 1000 X &lt;- matrix(rnorm(n*k,sd=sqrt(sigma2)),nrow=k) sigma2.hat &lt;- (n-1)*apply(X,1,var)/n cat(&quot;The sample mean for sigma2.hat = &quot;,mean(sigma2.hat),&quot;\\n&quot;) ## The sample mean for sigma2.hat = 3.679903 cat(&quot;The sample standard deviation for sigma2.hat = &quot;,sd(sigma2.hat),&quot;\\n&quot;) ## The sample standard deviation for sigma2.hat = 1.362853 empirical.dist &lt;- data.frame(sigma2.hat=sigma2.hat) x &lt;- seq(0.1,3*sigma2,by=0.1) y &lt;- dnorm(x,mean=sigma2,sd=sqrt(2)*sigma2/sqrt(n)) asymptotic.pdf &lt;- data.frame(x=x,y=y) ggplot(data=empirical.dist,aes(x=sigma2.hat,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0,10,by=1)) + geom_line(data=asymptotic.pdf,aes(x=x,y=y),col=&quot;red&quot;,lwd=1) + labs(x=&quot;Estimate of sigma^2&quot;) + coord_cartesian(xlim=c(0,10)) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.11: The empirical pdf for \\(\\hat{\\sigma^2}_{MLE}\\) given \\(n = 15\\) and true variance \\(\\sigma^2=4\\). We overlay the asymptotic pdf in red. We see that if \\(n\\) is small, the distribution of the MLE for \\(\\hat{\\sigma^2}\\) is definitely not normal, but right skew with a mean smaller than the true mean (4) and standard deviation slightly smaller than the true standard deviation (1.46). 2.12 The Central Limit Theorem Thus far in this chapter, we have assumed that we have sampled individual data from normal distributions. While we have been able to illustrate many concepts related to probability and statistical inference in this setting, the reader may feel that this is unduly limiting: what if the data we sample are not normally distributed? (We can utilize the bootstrap, but…) While we do examine the world beyond normality in future chapters, there is one major concept we can discuss now: the idea that statistics computed using non-normal data can themselves have sampling distributions that are at least approximately normal. This is big: it means that, e.g., we can utilize (very nearly) the same machinery for deriving normal-based confidence intervals and for conducting normal-based hypothesis tests, machinery that we outline below, to generate inferences for these (approximately) normally distributed statistics. The central limit theorem, or CLT, is one of the most important probability theorems, if not the most important. It states that if we have \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) with mean \\(E[X_i] = \\mu\\) and finite variance \\(V[X_i] = \\sigma^2 &lt; \\infty\\), and if \\(n\\) is sufficiently large, then \\(\\bar{X}\\) is approximately normally distributed: \\[ \\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq z \\right) = \\Phi(z) \\,. \\] In other words, \\[ {\\bar X} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right) ~~\\mbox{or, alternatively,}~~ X_+ = n{\\bar X} = \\sum_{i=1}^n X_i \\stackrel{d}{\\rightarrow} Y_+ \\sim \\mathcal{N}\\left(n\\mu,n\\sigma^2\\right) \\,. \\] Note that above we added the caveat “if \\(n\\) is sufficiently large.” How large is sufficiently large? The historical rule of thumb is that if \\(n \\gtrsim 30\\), then we may utilize the CLT. However, the true answer is that it depends on the distribution from which the data are sampled, and thus that it never hurts to perform simulations to see if, e.g., fewer (or more!) data are needed in a particular setting. A proof of the CLT, utilizing moment-generating functions, is given in Chapter 7. We note an apparent limitation of the CLT: here, we say that \\[ \\frac{\\sqrt{n}(\\bar{X}-\\mu)}{\\sigma} \\stackrel{d}{\\rightarrow} Z \\sim \\mathcal{N}(0,1) \\,, \\] but in reality we rarely, if ever, know \\(\\sigma\\). If we use the sample standard deviation \\(S\\) instead, how does that effect our use of the CLT? The answer is some, but not much…effectively, it does not change the sample size rule of thumb, but it does mean that we will need a few more samples to achieve the same level of accuracy that we would achieve if we know \\(\\sigma\\). Refer to the proof in Chapter 7. We see that initially, we standardize the random variables (i.e., transform \\(X\\) to \\(Z = (X-\\mu)/\\sigma\\)) and declare that \\(E[Z] = 0\\) and \\(V[Z] = 1\\). The latter equality no longer holds if we use \\(Z&#39; = (X-\\mu)/S\\) instead! But \\(V[Z&#39;]\\) does converge to 1 as \\(n \\rightarrow \\infty\\): \\(S^2 \\rightarrow \\sigma^2\\) by the Law of Large Numbers, and \\(S \\rightarrow \\sigma\\) by the continuous mapping theorem. So even if we do not know \\(\\sigma\\), the CLT will “kick in” eventually. Another question one may have is whether or not it is the case that since \\(\\sqrt{n}(\\bar{X}-\\mu)/\\sigma\\) converges in distribution to a standard normal random variable, it would also be the case that \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) will converge in distribution to a \\(t\\)-distributed random variable. The short answer: no. \\(t\\)-distributed data are \\(t\\)-distributed data because when data are drawn from a normal distribution, \\(\\bar{X}\\) is normally distributed and \\((n-1)S^2/\\sigma^2\\) is chi-square distributed, and the ratio of \\(\\bar{X}-\\mu\\) and \\(S/\\sqrt{n}\\) is a \\(t\\)-distributed random variable. If the individual data are drawn from an arbitrary distribution, then \\((n-1)S^2/\\sigma^2\\) will deviate, perhaps markedly, from being chi-square distributed, thus we cannot say anything about the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\)…other than it eventually converges in distribution to a standard normal random variable. A final important note: if we have data drawn from a known, non-normal distribution and we need to derive the distribution of \\(\\bar{X}\\) in order to, e.g., construct confidence intervals or to perform hypothesis tests, we should not just default to utilizing the CLT! We might be able to compute the exact distribution for \\(\\bar{X}\\), for all sample sizes \\(n\\), via, e.g., the method of moment-generating functions. 2.12.1 Computing Probabilities Let’s assume that we have a sample of \\(n = 64\\) iid data drawn from unknown distribution with mean \\(\\mu = 10\\) and finite variance \\(\\sigma^2 = 9\\). What is the probability that the observed sample mean will be larger than 11? This is a good example of a canonical CLT exercise: \\(\\mu\\) and \\(\\sigma\\) are given to us, but the distribution is left unstated…and \\(n \\geq 30\\). This is a very unrealistic: when will we know population parameters exactly? But such an exercise has its place: it allows us to practice the computation of probabilities. \\[\\begin{align*} P\\left( \\bar{X} &gt; 11 \\right) &amp;= 1 - P\\left( \\bar{X} \\leq 11 \\right) \\\\ &amp;= 1 - P\\left( \\frac{\\sqrt{n}(\\bar{X}-\\mu)}{\\sigma} \\leq \\frac{\\sqrt{n}(11-\\mu)}{\\sigma}\\right) &amp;\\approx 1 - P\\left( Z \\leq \\frac{8(11-10)}{3} \\right) = 1 - \\Phi\\left(\\frac{8}{3}\\right) \\,. \\end{align*}\\] As we know by now, we cannot go any further by hand, so we call upon R: 1 - pnorm(8/3) ## [1] 0.003830381 The probability that \\(\\bar{X}\\) is greater than 11 is small: 0.0038. To reiterate a point made above: if we were given \\(\\mu\\) but not \\(\\sigma^2\\), we would plug in \\(S\\) instead and expect that the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) would be not as close to a standard normal as the distribution of \\(\\sqrt{n}(\\bar{X}-\\mu)/\\sigma\\)…it takes \\(\\sqrt{n}(\\bar{X}-\\mu)/S\\) “longer” to converge in distribution to a standard normal as \\(n\\) increases. 2.13 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by, e.g., solving for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The construction of confidence intervals thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) to \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 14 of Chapter 1.) When we construct confidence intervals, we are allowed to utilize any arbitrary statistic, so long as we know the sampling distribution of that statistic (which links the statistic value to the underlying parameter of interest). However, in the context of the normal distribution, there are conventional choices: \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is known: we utilize the sample mean \\(\\bar{X}\\) \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is unknown: we still utilize \\(\\bar{X}\\) \\(\\theta = \\sigma^2\\): we utilize the sample variance \\(S^2\\) These are conventional choices, but we will state here that there are well-established reasons for using these statistics based on the idea of interval length: if we examine the use of two separate statistics for constructing a confidence interval for \\(\\theta\\), we generally want to utilize the one that generates shorter confidence intervals (i.e., the one that indicates the least amount of uncertainty about \\(\\theta\\)). And \\(\\bar{X}\\) and \\(S^2\\) are generally the best statistics to use with normally distributed data. (We keep saying “generally.” In an academic setting, where there are no outlier data and the data-generating process is clean, \\(\\bar{X}\\) and \\(S^2\\) will be the best choices of statistics. In a real-world setting, where data are messy, there may be times where utilizing, e.g., the sample median is warranted, if for instance outliers introduce bias into the sample mean. Luckily for us, here we exist in the world of pristine data…the real world can wait.) Let’s assume that we have collected a sample of \\(n\\) iid data, drawn from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and with sample mean and variance \\(\\bar{X}\\) and \\(S^2\\), and let’s examine each of the three situations outlined above. \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is known. We adopt \\(Y = \\bar{X}\\), with the observed statistic being \\(y_{\\rm obs} = \\bar{x}\\). By using the method of moment-generating functions, we have found that \\[ Y \\sim \\mathcal{N}(\\mu,\\sigma^2/n) \\] and thus that \\[ F_Y(y) = \\frac{1}{2}\\left[ 1 + {\\rm erf}\\left(\\frac{\\sqrt{n}(y - \\mu)}{\\sqrt{2}\\sigma}\\right) \\right] \\,. \\] When we plug this expression into the two equations given in the recall statement above, we can see immediately that we are not solving for \\(\\mu_{1-\\alpha/2}\\) (the lower bound) and \\(\\mu_{\\alpha/2}\\) (the upper bound) by hand! (We know these two quantities are the lower and upper bounds because \\(E[Y] = \\mu\\) increases with \\(\\mu\\).) For instance, \\[ \\hat{\\mu}_H = \\mu_{\\alpha/2} = y_{\\rm obs} - \\frac{\\sigma}{\\sqrt{n}}{\\rm erf}^{-1}(\\alpha-1) \\,, \\] where erf\\(^{-1}(\\cdot)\\) is the inverse error function, which is not analytically tractable. Thus we fall back upon numerical methods, utilizing cdf-evaluation functions. In an example below, we provide R code, utilizing pnorm(), for computing confidence interval bounds. \\(\\theta = \\mu\\), and the variance \\(\\sigma^2\\) is unknown. We adopt \\(Y = \\bar{X}\\), with the observed statistic being \\(y_{\\rm obs} = \\bar{x}\\). Earlier in this chapter, we show that \\[ T = \\frac{Y-\\mu}{S/\\sqrt{n}} \\sim t(n-1) \\,, \\] where \\(t(n-1)\\) is the \\(t\\)-distribution for \\(n-1\\) degrees of freedom. We immediately sense a problem here: we need to know \\(F_Y(y)\\) for our root-finding algorithm to work; \\(F_{T(n-1)}(t)\\) will not help us! (\\(T\\) subsumes the parameter of interest \\(\\mu\\), leaving us unable to solve for \\(\\mu\\) when applying the root-finding algorithm.) However, we can determine \\(F_Y(y)\\) through a random-variable transformation. Let \\(Y = aT+b\\), where \\(a = s_{\\rm obs}/\\sqrt{n}\\) and \\(b = \\mu\\). Then \\[ F_Y(y) = P(Y \\leq y) = P\\left(T \\leq \\frac{y-b}{a}\\right) = F_{T,n-1}\\left(\\frac{y-b}{a}\\right) \\,. \\] Thus, in place of, e.g., \\(F_Y(y_{\\rm obs} \\vert \\mu_{\\alpha/2})\\), we plug into the root-finding equation the quantity \\(F_{T,n-1}((y_{\\rm obs}-\\mu_{\\alpha/2})/(s_{\\rm obs}/\\sqrt{n}))\\); the solution in this case would be \\[ \\hat{\\mu}_H = \\mu_{\\alpha/2} = y_{\\rm obs} - \\frac{s_{\\rm obs}}{\\sqrt{n}}F_{T,n-1}^{-1}\\left(\\frac{\\alpha}{2}\\right) \\,. \\] As we cannot work with \\(F_{T(n-1)}^{-1}\\left(\\frac{\\alpha}{2}\\right)\\) by band, we will again fall back on numerical methods, and we will show in an example below how to compute intervals using R code. \\(\\theta = \\sigma^2\\). We adopt \\(Y = S^2\\), with the observed statistic being \\(y_{\\rm obs} = s_{\\rm obs}^2\\). Earlier in this chapter, we show that \\[ W = \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2(n-1) \\,, \\] where \\(\\chi^2(n-1)\\) is the chi-square distribution for \\(n-1\\) degrees of freedom. Here, we face the same problem that we faced immediately above: we want \\(F_Y(y)\\), but are given \\(F_{W(n-1)}(w)\\). We mitigate this issue in the same manner as above, by employing a random-variable transformation. \\[ F_Y(y) = P(Y \\leq y) = P\\left(W \\leq \\frac{(n-1)y_{\\rm obs}}{\\sigma^2}\\right) = F_{W,n-1}\\left(\\frac{(n-1)y_{\\rm obs}}{\\sigma^2}\\right) \\] As was the case above when we deal with the \\(t\\) distribution, we end up having inverse cdfs that we cannot evaluate by hand, such as in the expression \\[ \\widehat{\\sigma^2}_H = \\sigma_{\\alpha/2}^2 = \\frac{(n-1)y_{\\rm obs}}{F_{W,n-1}^{-1}(\\alpha/2)} \\,. \\] As has been the case thus far, the evaluation of bounds requires coding, and we show an example of such an evaluation below. As this point, the reader may say, “all this looks nothing like the confidence intervals I learned in introductory statistics.” And the reader would be correct. The way in which the construction of confidence intervals is described above and in Chapter 1 is different from the way in which it is described in introductory statistics classes and as well as in traditional calculus-based probability and statistical inference classes. In short: we propose a modern approach that is appropriate in the age of computers that yields the same results as traditional approaches. In introductory statistics classes, equations are generally all that are provided for confidence intervals, such as the one for putting bounds on the normal mean \\(\\mu\\) when the variance is known: \\[ \\bar{x}_{\\rm obs} \\pm z_{1-\\alpha/2} \\frac{\\sigma_X}{\\sqrt{n}} \\,, \\] where \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\). We would argue that such equations obscure the reality what is actually happening when we construct confidence intervals: “moving” the sampling distribution for a statistic \\(Y\\) back and forth, by changing the value of a parameter \\(\\theta\\), until the value of \\(\\theta\\) becomes either too low or too high for the observed value \\(y_{\\rm obs}\\) to continue to be plausible. In traditional calculus-based probability and statistical inference classes, the focus is on introducing general techniques for constructing confidence intervals analytically. (While still requiring that at some point in the process, we utilize statistical tables.) A commonly introduced technique is the pivotal method, which for illustrative purposes we discuss in an example below. However, the pivotal method is not applicable in all situations, and thus other analytical methods exist as well (see, e.g., Chapter 9 of Casella and Berger 2002). The methodology we provide in this book fits within section 9.2.3 of Casella and Berger, entitled “Pivoting the CDF,” where the authors note that “even if [the equations \\(F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) = 1 - \\alpha/2\\) and \\(F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) = \\alpha/2\\)] cannot be solved analytically, we really only need to solve them numerically since the proof that we have a \\(1-\\alpha\\) confidence interval [does] not require an analytic solution.” Thus in this book, for the sake of consistency, we skip over describing analytical approaches to confidence interval construction and go straight to finding roots numerically in all cases where we cannot solve for the roots directly (as we can in the examples in Chapter 1). 2.13.1 Confidence Interval for the Normal Mean With Variance Known In Appendix B, we display a general-purpose R code that generates confidence intervals for arbitrary sampling distributions. Here, we recreate aspects of that code and focus specifically on the case of constructing a two-sided confidence interval for the normal mean \\(\\mu\\) when the variance \\(\\sigma^2\\) is known. (As the reader can see, when we focus on a specific case, the code simplifies substantially!) confint &lt;- function(y.obs,sigma2,n,alpha=0.05) { f &lt;- function(mean,sigma2,n,y.obs,q) { pnorm(y.obs,mean=mean,sd=sqrt(sigma2/n))-q } lo &lt;- uniroot(f,interval=c(-10000,10000),sigma2,n,y.obs,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(-10000,10000),sigma2,n,y.obs,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 25 X &lt;- rnorm(n,mean=0,sd=1) confint(mean(X),1,n) ## [1] -0.4875493 0.2964475 In this code above, we search along the “\\(\\mu\\) axis” (or along the values of mean) from \\(\\mu = -10,000\\) to \\(\\mu = 10,000\\) to see where the function \\(F_Y(y_{\\rm obs} \\vert \\theta_q) - q\\) crosses zero, where \\(q\\) is either \\(\\alpha/2\\) or \\(1-\\alpha/2\\). (Making the range of possible \\(\\mu\\) values large has no noticeable impact on computation time and helps ensure that the root will not lie outside the specified interval.) We test the code by sampling \\(n = 25\\) data from a standard normal distribution \\(\\mathcal{N}(0,1)\\), and passing the statistic \\(y_{\\rm obs} = \\bar{x}_{\\rm obs}\\) to the confint() function. We find that the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_H] = [-0.488,0.296]\\), which overlaps the true value. (See Figure 2.12.) Figure 2.12: Sampling distributions for \\(Y = \\bar{X} = \\sum_{i=1}^n X_i\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,1)\\), and where (left) \\(\\mu=-0.488\\) and (right) \\(\\mu=0.296\\). We observe \\(y_{\\rm obs} = -0.096\\) and we want to construct a 95% confidence interval. \\(\\mu=-0.488\\) is the smallest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.975) = -0.096\\), while \\(\\mu=0.296\\) is the largest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.025) = -0.096\\). We note that to verify the “coverage” of intervals, i.e., the proportion of the intervals that overlap the true value, we can simply wrap the calls to the random data generator and confint() with a for loop, and save the lower and upper bounds for each generated dataset. set.seed(101) num.sim &lt;- 10000 n &lt;- 25 lower &lt;- rep(NA,num.sim) upper &lt;- rep(NA,num.sim) for ( ii in 1:num.sim ) { X &lt;- rnorm(n,mean=0,sd=1) b &lt;- confint(mean(X),1,n) lower[ii] &lt;- b[1] upper[ii] &lt;- b[2] } We can then see how often the true value is within the bounds: truth &lt;- 0 in.bound &lt;- (lower &lt;= truth) &amp; (upper &gt;= truth) cat(&quot;The empirical coverage is &quot;,sum(in.bound)/num.sim,&quot;\\n&quot;) ## The empirical coverage is 0.9518 Here, if lower &lt;= truth, then TRUE is returned, as is the case if upper &gt;= truth. &amp; is the logical and operator, which returns TRUE if the input is TRUE &amp; TRUE and returns FALSE otherwise. R treats TRUE as equivalent to 1 (and FALSE as equivalent to 0), so sum(in.bound) returns the number of final TRUE values, i.e., the number of evaluated intervals that overlap the true value. Our estimated coverage is 0.9518. We do not expect a value of exactly 0.95 given that we only run a finite number of simulations and thus the coverage will exhibit random variation; however, we can say that this value is “close” to 0.95 and thus almost certainly compatible with 0.95. Once we discuss the binomial distribution in Chapter 3, we will more easily be able to quantify the notion of being “close” to the expected value. 2.13.2 Confidence Interval for the Normal Mean With Variance Unknown We extend the previous example by working with the case where the variance is unknown. confint &lt;- function(y.obs,s2,n,alpha=0.05) { f &lt;- function(mean,s2,n,y.obs,q) { pt((y.obs-mean)/sqrt(s2/n),n-1)-q } lo &lt;- uniroot(f,interval=c(-10000,10000),s2,n,y.obs,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(-10000,10000),s2,n,y.obs,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 25 X &lt;- rnorm(n,mean=0,sd=1) confint(mean(X),var(X),n) ## [1] -0.4483553 0.2572790 The code above is largely equivalent to that in the previous example, with the only changes being swapping in the call to pt() (and altering the argument in that call) in place of the call to pnorm(), and replacing the known value of \\(\\sigma^2\\) (sigma2 in the previous example) with \\(s_{\\rm obs}^2\\) (s2). We find that the interval is \\([\\hat{\\theta}_L,\\hat{\\theta}_H] = [-0.448,0.257]\\), which is similar to the interval derived in the last example and which still overlaps the true value. (See Figure 2.13.) Figure 2.13: Sampling distributions for \\(Y = \\bar{X} = \\sum_{i=1}^n X_i\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and where (left) \\(\\mu=-0.448\\) and (right) \\(\\mu=0.257\\). We observe \\(y_{\\rm obs} = -0.096\\) and \\(s_{\\rm obs}^2 = 0.855\\) and we want to construct a 95% confidence interval. \\(\\mu=-0.448\\) is the smallest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.975) = -0.096\\), while \\(\\mu=0.257\\) is the largest value of \\(\\mu\\) such that \\(F_Y^{-1}(0.025) = -0.096\\). 2.13.3 Confidence Interval for the Normal Variance Below, we adapt our confidence-interval code to the problem of estimating an interval for the variance \\(\\sigma^2\\). confint &lt;- function(y.obs,n,alpha=0.05) { f &lt;- function(sigma2,n,y.obs,q) { pchisq(y.obs*(n-1)/sigma2,n-1)-q } lo &lt;- uniroot(f,interval=c(1.e-6,10000),n,y.obs,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(1.e-6,10000),n,y.obs,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 25 X &lt;- rnorm(n,mean=0,sd=1) confint(var(X),n) ## [1] 0.4454088 1.4138974 The changes made to the code above include swapping in pchisq(), removing the variable mean, removing the variable s2 (as y.obs itself is now \\(s_{\\rm obs}^2\\)), and adjusting the lower bound on the interval (since \\(\\sigma^2 &gt; 0\\)). We find that the interval is \\([\\widehat{\\sigma}_L^2,\\widehat{\\sigma}_H^2] = [0.445,1.414]\\), which overlaps the true value. (See Figure 2.14.) Figure 2.14: Sampling distributions for \\(Y = S^2\\), where \\(n = 25\\) and \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and where (left) \\(\\sigma^2=0.445\\) and (right) \\(\\sigma^2=1.414\\). We observe \\(y_{\\rm obs} = s_{\\rm obs}^2 = 0.731\\) and we want to construct a 95% confidence interval. \\(\\sigma^2=0.445\\) is the smallest value of \\(\\sigma^2\\) such that \\(F_Y^{-1}(0.975) = 0.731\\), while \\(\\mu=1.414\\) is the largest value of \\(\\sigma^2\\) such that \\(F_Y^{-1}(0.025) = 0.731\\). 2.13.4 Confidence Interval for the Mean of an Arbitrary Distribution: Using the CLT Let’s assume that we have a iid sample of \\(n\\) data drawn from the distribution \\[ f_X(x) = \\theta x^{\\theta-1} ~~ x \\in [0,1] \\,. \\] This distribution is decidedly not normal. Can we derive a confidence interval for the mean of this distribution, where the mean is \\[ E[X] = \\int_0^1 \\theta x x^{\\theta-1} dx = \\left. \\frac{\\theta}{\\theta+1} x^{\\theta+1} \\right|_0^1 = \\frac{\\theta}{\\theta+1} \\,? \\] If we wanted to try to do this “exactly,” then one workflow would be (a) to attempt to determine the sampling distribution of \\(Y = \\bar{X}\\) (utilizing, e.g., moment-generating functions), and then (b) code up a variant on the confint() codes given in the examples above. However, regarding (a): the mgf of the distribution is \\[ m_X(t) = E[e^{tX}] = \\theta \\int_0^1 x^{\\theta-1} e^{tx} dx \\,. \\] This looks suspiciously like a gamma-function integral, except the bounds here are 0 and 1, not 0 and \\(\\infty\\)…what we actually have (or, technically, what we would have after an appropriate variable substitution) is an incomplete gamma function integral. We cannot easily work with this…and so we turn to the Central Limit Theorem. If \\(n \\gtrsim 30\\), we may write that \\[ Y = \\bar{X} ~ \\dot\\sim ~ \\mathcal{N}(\\mu,s_{\\rm obs}^2/n) \\,. \\] (Recall that the CLT assumes that we know \\(\\sigma^2\\), but if we do not and use \\(S^2\\) instead, the result will still hold as \\(n \\rightarrow \\infty\\).) Thus we would use the confidence interval approach that we describe in the first example above, while swapping in \\(s_{\\rm obs}^2\\) for \\(\\sigma^2\\). confint &lt;- function(y.obs,s2,n,alpha=0.05) { f &lt;- function(mean,y.obs,s2,n,q) { pnorm(y.obs,mean=mean,sd=sqrt(s2/n))-q } lo &lt;- uniroot(f,interval=c(-10000,10000),y.obs,s2,n,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(-10000,10000),y.obs,s2,n,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 25 theta &lt;- 2 # so the mean is 2/3 X &lt;- rbeta(n,theta,1) # theta x^(theta-1) is a beta distribution confint(mean(X),var(X),n) ## [1] 0.5888963 0.7532609 The evaluated confidence interval is \\([0.589,0.753]\\) (which overlaps the true value \\(\\mu = 2/3\\)). Note, however, that because an approximate sampling distribution is being used, the coverage can (and will) deviate from expectation (e.g., 0.95). Let’s see how much the coverage deviates in one case via simulation: set.seed(101) num.sim &lt;- 10000 n &lt;- 25 theta &lt;- 2 lower &lt;- rep(NA,num.sim) upper &lt;- rep(NA,num.sim) for ( ii in 1:num.sim ) { X &lt;- rbeta(n,theta,1) b &lt;- confint(mean(X),var(X),n) lower[ii] &lt;- b[1] upper[ii] &lt;- b[2] } truth &lt;- 2/3 in.bound &lt;- (lower &lt;= truth) &amp; (upper &gt;= truth) cat(&quot;The empirical coverage is &quot;,sum(in.bound)/num.sim,&quot;\\n&quot;) ## The empirical coverage is 0.9359 The estimated coverage is 0.9359; it is not quite 0.95. Thus it appears that are evaluated confidence intervals do not overlap the true value as often as we would expect. Note that we would expect that as \\(n \\rightarrow 0\\), the approximation will become progressively worse, and that as \\(n \\rightarrow \\infty\\), the coverage should asymptotically approach \\(1 - \\alpha\\). 2.13.5 Historical Digression: the Pivotal Method The pivotal method has been an often-used analytical method for constructing confidence intervals wherein one “reformats” the algorithm such that one can ultimately utilize statistical tables to determine interval bounds. In the age of computers, the pivotal method has become unnecessary; the numerical root-finding algorithm presented in this chapter and in Appendix B constructs the same intervals (and does so even in situations where the pivotal method cannot be used…all that is required is that the sampling distribution of the statistic that we use depends on both the statistic itself and the parameter of interest). The steps of the pivotal method are as follows: Determine a pivotal quantity \\(Y\\), a statistic that is a function of both the observed data and the parameter \\(\\theta\\), and that has a sampling distribution that does not depend on \\(\\theta\\). (Note that, as hinted at above, there is no guarantee that a pivotal quantity exists in any given situation. But there may also be situations where we can define multiple pivotal quantities; if so, it does not matter which we adopt, as they all will help generate the same interval.) Determine constants \\(a\\) and \\(b\\) such that \\(P(a \\leq Y \\leq b) = 1-\\alpha\\). (Here, we are assuming that we are constructing a two-sided interval.) Rearrange the terms within the probability statement such that \\(\\theta\\) is alone in the middle: the quantities on either end are the interval bounds. Let’s demonstrate how this plays out in the situation where we sample \\(n\\) iid data from a normal distribution with known variance, and wish to construct an interval for \\(\\mu\\). As we know, in this situation, \\[ \\bar{X} \\sim \\mathcal{N}\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\,. \\] \\(\\bar{X}\\) is not a pivotal quantity: its sampling distribution depends on \\(\\mu\\). However… \\[ Y = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1) \\] is a pivotal quantity, as it depends on both \\(\\bar{X}\\) and \\(\\mu\\) and has a sampling distribution that does not depend on \\(\\mu\\). Thus step 1 is complete. As for step 2: \\[\\begin{align*} P(a \\leq Y \\leq b) = P(Y \\leq b) - P(Y \\leq a) &amp;= 1-\\alpha \\\\ \\Phi(b) - \\Phi(a) &amp;= 1-\\alpha \\,. \\end{align*}\\] OK…we appear stuck here, except that we can fall back upon the fact that the standard normal distribution is symmetric around zero, and thus we can say that \\(a = -b\\). That, and the fact that symmetry allows us to write that \\(\\Phi(-b) = 1 - \\Phi(b)\\), allows us to continue: \\[\\begin{align*} \\Phi(b) - \\Phi(a) &amp;= 1-\\alpha \\\\ \\Phi(b) - (1 - \\Phi(b)) &amp;= 1-\\alpha \\\\ 2\\Phi(b) - 1 &amp;= 1-\\alpha \\\\ \\Phi(b) &amp;= \\frac{2-\\alpha}{2} = 1 - \\frac{\\alpha}{2} \\\\ b &amp;= \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) = z_{1-\\alpha/2} \\\\ \\end{align*}\\] Historically, at this point, one would make use of a \\(z\\) table to determine that, e.g., \\(b = z_{0.975} = 1.96\\). So now we move on to step 3: \\[\\begin{align*} P\\left(-z_{1-\\alpha/2} \\leq Y \\leq z_{1-\\alpha/2}\\right) = P\\left(-z_{1-\\alpha/2} \\leq \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\alpha/2}\\right) &amp;= 1-\\alpha \\\\ \\Rightarrow P\\left(\\bar{X} - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right) &amp;= 1-\\alpha \\,. \\end{align*}\\] Thus our confidence interval is \\[ \\bar{x}_{\\rm obs} \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\,. \\] 2.14 Hypothesis Testing: Testing for Normality Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-sided test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bounds given by \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\), which are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps, e.g., \\(y_{\\alpha/2}\\) to either the lower or upper rejection region boundary by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven. In a conventional data analysis workflow, we might receive a sample of \\(n\\) iid data without any knowledge about the distribution from which the individual data are drawn, and we might want to test a hypothesis about, e.g., the population mean. There are potentially many hypothesis tests from which to choose, but the ones that are generally the most powerful (i.e., the ones that do the best at discriminating between a null hypothesis and a given alternative hypothesis) are the ones that assume a functional form for the distribution of the individual data. In the following two sections, we provide details about tests of population means and variances that are built upon the assumption that the individual data are iid draws from a normal distribution…but before we talk about those tests, we talk about hypothesis tests that allow us to decide whether our data are plausibly normally distributed in the first place. To be clear about the analysis workflow… If we perform a hypothesis test for which the null hypothesis is that the individual data are normally distributed, and we fail to reject this null hypothesis, we can feel free to use the hypothesis tests in the next two sections, regardless of the sample size \\(n\\). If we reject the null hypothesis but the sample size is sufficiently large (\\(n \\gtrsim 30\\)), and we wish to test hypotheses about the population mean, we can fall back on the central limit theorem. If we reject the null hypothesis that the data are normally distributed, and the sample size is small or if we wish to test hypotheses about population variances (or both), we should tread carefully if we decide to use the hypothesis testing frameworks presented in the next two sections, as the results we get might be very inaccurate. However, regarding the third point above, we have to keep in mind that as \\(n\\) gets larger and larger, tests of normality become more and more prone to rejecting the null even when the deviation of the true distribution from a normal is small, meaning that the hypothesis tests detailed in the next two sections might actually yield somewhat accurate results. Again, we need to tread carefully! Alternative approaches include trying so-called nonparametric tests (which we do not cover in this book), or trying to determine another distribution with which the data are consistent and building a test utilizing its properties, etc. In the end: if in doubt, simulate. Let’s assume that we have collected \\(n\\) iid data from some unknown (and unassumed) distribution \\(P\\). Could \\(P\\) be the normal distribution? There are several varieties of tests whose null hypotheses are “the data are sampled from the [insert name here] distribution.” The most well-known and often-used is the Kolmogorov-Smirnov test, or KS test. (We note that it is not necessarily the most powerful test among those that assess the consistency of data with named distributions, but it is easy to implement and simple to understand. The interested reader should investigate alternatives like, e.g., the Anderson-Darling test and Cramer-von Mises test, and for the specific case of testing for normality, the Shapiro-Wilk test, which we demonstrate below in an example.) The empirical cumulative distribution function (or ecdf) for a dataset is defined as \\[ F_{X,n}(x) = \\frac{1}{n} \\left(\\mbox{number of observed data} \\, \\leq x\\right) \\,. \\] We see immediately that \\(F_{X,n}(x)\\) behaves like a cdf, in the sense that \\(F_{X,n}(-\\infty) = 0\\) and \\(F_{X,n}(\\infty) = 1\\), etc. It is a monotonically increasing step function, with steps of size \\(1/n\\) taken at each datum’s coordinate \\(x_i\\). If we assume a particular distribution as the null distribution, with cdf \\(F_X(x)\\), then the KS test statistic is \\[ D_n = \\mbox{sup} \\vert F_{X,n}(x) - F_X(x) \\vert \\,. \\] We have seen the abbrevation “inf” before, signaling that we are taking the infimum, or smallest value, of a set; here, “sup” stands for supremum, the largest value of a set. So for the KS test, the test statistic is simply the largest distance observed between the empirical and null cdfs; if the null is wrong, we expect this distance to be large. Under the null, \\(K = \\sqrt{n}D_n\\) is assumed to be, at least approximately, sampled from the Kolmogorov distribution, whose cdf is \\[ P(K \\leq k) = \\frac{\\sqrt{2\\pi}}{k} \\sum_{x=1}^\\infty \\exp\\left(-\\frac{(2x-1)^2\\pi^2}{8k^2}\\right) \\,. \\] Having sampled \\(k_{obs} = \\sqrt{n}D_n\\), we can use this cdf to establish whether our not the test statistic falls into the rejection region. If it does, we reject the null hypothesis that the individual data are normally distributed. 2.14.1 Example of the KS Test in R Let’s suppose you are given a dataset that has the empirical distribution given in Figure 2.15. Figure 2.15: Histogram of a dataset with sample size \\(n = 30\\). Is it plausible that these data are normally distributed? The sample statistics for these data are \\(\\bar{X} = 100.6\\) and \\(S = 29.8\\). In Figure 2.16, we plot the empirical cdf for these data, overlaying the cdf for a \\(\\mathcal{N}(100.6,29.8^2)\\) distribution. Figure 2.16: Empirical cumulative distribution function of our dataset with the cdf for a \\(\\mathcal{N}(100.6,29.8^2)\\) overlaid as the red dashed line. The largest difference between the empirical cdf and the overlaid normal cdf in Figure 2.16 occurs at \\(x \\approx 93\\). Is this difference large enough for us to decide the data are not normally distributed with the inferred parameters \\(\\mu = 100.6\\) and \\(\\sigma = 29.8\\)? ks.test(x,&quot;pnorm&quot;,100.6,29.8) ## ## Exact one-sample Kolmogorov-Smirnov test ## ## data: x ## D = 0.13858, p-value = 0.5649 ## alternative hypothesis: two-sided By default, ks.test() performs a two-sided test; type ?ks.test in the R console to see how to specify alternatives. According to this test, the \\(p\\)-value is 0.56. We formally introduce \\(p\\)-values below, in the next section; it suffices to say here that if \\(p &lt; \\alpha\\), where \\(\\alpha\\) is the user-specified Type I error, we reject the null, so here we fail to reject the null hypothesis that the data are normally distributed. In truth, the simulated data are sampled from a gamma distribution (see Chapter 4); with a larger sample size, the test becomes more better able to differentiate between normally distributed and gamma-distributed data, so eventually we would reject the null hypothesis. 2.14.2 Testing for Normality: Shapiro-Wilk Test The Shapiro-Wilk test, developed in 1965, tests the null hypothesis that our \\(n\\) iid data are sampled from a normal distribution. The test statistic \\(W\\), which utilizes order statistics (a concept we introduce in Chapter 3), is complicated and will not be reproduced here. It suffices for now for us to make two points. - The Shapiro-Wilk test is better able to reject the null hypothesis that our data are normally distributed than the KS test. (This makes sense because the SW test is built to be more “specific”\\(-\\)it is just used to test for normality). - The sampling distribution for \\(W\\) is non-analytic and thus the rejection region is estimated numerically, and thus the Shapiro-Wilk test cannot be used in R with samples of size \\(&gt;\\) 5000. Below, we demonstrate the use of the Shapiro-Wilk test with the gamma-distributed data of the first example above. shapiro.test(x) ## ## Shapiro-Wilk normality test ## ## data: x ## W = 0.9508, p-value = 0.1776 We see that while the KS test returned a \\(p\\)-value of 0.56, the Shapiro-Wilk test returns the value 0.18. This value is still larger than \\(\\alpha = 0.05\\), so we still fail to reject the null. 2.15 Hypothesis Testing: Population Mean Assume we are given a data sample \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)\\), where \\(\\sigma^2\\) is known. Because \\(Y = \\bar{X}\\) is the MLE for the normal mean, we identify it as a plausible statistic for testing hypotheses about \\(\\mu\\). Refer back to the table in the hypothesis testing section of Chapter 1. We first observe that \\(E[Y] = E[\\bar{X}] = \\mu\\), so \\(E[Y]\\) increases with \\(\\mu\\)…and thus the rejection regions for a two-tail test are \\[ y_{\\rm obs} = \\bar{x}_{\\rm obs} \\leq y_{\\alpha/2} ~~~ \\mbox{and} ~~~ y_{\\rm obs} = \\bar{x}_{\\rm obs} \\geq y_{1-\\alpha/2} \\,, \\] where \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\mu_o,\\sigma^2) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\mu_o,\\sigma^2) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] Can we solve for \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) analytically? The answer is no: the pdf for our test statistic is \\(\\mathcal{N}(\\mu,\\sigma^2/n)\\), and its cdf contains the error function; this is not a cdf we can work with by hand. But finding a numerical solution is straightforward: for instance, \\[ y_{\\alpha/2} = F_Y^{-1}\\left(\\frac{\\alpha}{2} \\vert \\mu_o,\\sigma^2\\right) \\] can be numerically evaluated using the R function qnorm(). What happens if the variance is unknown? In this case, we would borrow (and slightly amend) a result from the confidence interval section above, namely that \\[ F_Y(y) = F_{T(n-1)}\\left( \\frac{y-\\mu_o}{s_{\\rm obs}/\\sqrt{n}} \\right) \\,, \\] which means that for instance, \\[ y_{\\alpha/2} = \\mu_o + \\frac{s_{\\rm obs}}{\\sqrt{n}} F_{T(n-1)}^{-1}\\left(\\frac{\\alpha}{2}\\right) \\,, \\] which can be numerically evaluated using the R function qt(). (See Figure 2.17.) We summarize the evaluation of rejection region boundaries for population mean tests in the table below, but will note here that these boundaries are completely equivalent to the boundaries given in the so-called z- and t-tests that the reader learned about in introductory statistics settings. Type Rejection Region(s) R Code for Normal Distribution two-tail \\(y_{\\rm obs} \\leq y_{\\alpha/2}\\) and variance known: \\(y_{\\rm obs} \\geq y_{1-\\alpha/2}\\) y.q &lt;- qnorm(q,mean=mu.o,sd=sqrt(sigma2/n)) lower-tail \\(y_{\\rm obs} \\leq y_{\\alpha}\\) variance unknown: upper-tail \\(y_{\\rm obs} \\geq y_{1-\\alpha}\\) y.q &lt;- mu.o + sqrt(s2/n)*qt(q,n-1) Figure 2.17: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with \\(\\mu_o = 0\\) and \\(s_{\\rm obs}^2 = 1\\)), assuming \\(\\alpha = 0.05\\) and \\(n-1 = 5\\) degrees of freedom. There are two new hypothesis-test-related concepts that we introduce in this chapter. The first is the concept of the p-value. This is the probability of observing a hypothesis test statistic value \\(y_{\\rm obs}\\) or a more “extreme” value (i.e., a value even further from the null), given that the null hypothesis is true. We reject the null when \\(p &lt; \\alpha\\), the user-specified Type I error. See the example below for more information about \\(p\\)-values. Here, it suffices to say that when you can compute a \\(p\\)-value, you should: it is more informative than simply saying that the test statistic does, or does not, lie in the rejection region. (Below we provide R code for computing \\(p\\)-values for the specific case of performing a normal population mean test.) Type Formula R Code for Normal Distribution variance known: two-tail 2 \\(\\cdot\\) min(\\(F_Y(y_{\\rm obs} \\vert \\theta_o),\\) p &lt;- 2*min(c(pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)), \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\)) 1-pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)))) lower-tail \\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\) p &lt;- pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)) upper-tail \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\) p &lt;- 1-pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)) variance unknown: two-tail p &lt;- 2*min(c(pt((y.obs-mu.o)/sqrt(s2/n),n-1), 1-pt((y.obs-mu.o)/sqrt(s2/n),n-1))) lower-tail p &lt;- pt((y.obs-mu.o)/sqrt(s2/n),n-1) upper-tail p &lt;- 1-pt((y.obs-mu.o)/sqrt(s2/n),n-1) The second new concept that we introduce here is hypothesis test power. In words, it is the probability of rejecting the null hypothesis given an arbitrary parameter value \\(\\theta\\): \\[ power = P(\\mbox{reject}~H_o \\vert \\theta) \\,. \\] The power is thus a function of \\(\\theta\\), and its value is \\(1-\\beta(\\theta,\\alpha)\\), where \\(\\beta(\\theta,\\alpha)\\) is the Type II error. We note that if we set \\(\\theta = \\theta_o\\), the null hypothesis value, then the test power is by definition \\(\\alpha\\) (the probability of rejecting the null if the null is correct). (Below we provide R code for computing the test power for the specific case of performing a normal population mean test. In the formulas, lo maps to \\(\\alpha/2\\) if \\(E[Y]\\) increases with \\(\\theta\\) [as it does for the normal population mean], otherwise it maps to \\(1-\\alpha/2\\), with hi analogously mapping to either \\(1-\\alpha/2\\) or \\(\\alpha/2\\).) Type Formula R Code for Normal Distribution variance known: two-tail \\(F_Y(y_{\\rm lo} \\vert \\theta) +\\) power &lt;- pnorm(y.lo,mean=mu,sd=sqrt(sigma2/n)) + \\(1-F_Y(y_{\\rm hi} \\vert \\theta)\\) (1-pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n))) lower-tail \\(F_Y(y_{\\rm lo} \\vert \\theta)\\) power &lt;- pnorm(y.lo,mean=mu,sd=sqrt(sigma2/n)) upper-tail \\(1-F_Y(y_{\\rm hi} \\vert \\theta)\\) power &lt;- 1-pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n)) variance unknown: two-tail power &lt;- pt((y.lo-mu)/sqrt(s2/n),n-1) + (1-pt((y.hi-mu)/sqrt(s2/n),n-1)) lower-tail power &lt;- pt((y.lo-mu)/sqrt(s2/n),n-1) upper-tail power &lt;- 1-pt((y.hi-mu)/sqrt(s2/n),n-1) 2.15.1 Testing a Hypothesis About the Normal Mean with Variance Known Let’s assume that we have sampled \\(n = 25\\) iid data from a normal distribution whose variance is \\(\\sigma^2 = 1\\). We wish to test the null hypothesis \\(H_o : \\mu = \\mu_o = 2\\) versus the alternative \\(H_a : \\mu \\neq \\mu_o\\). What are the rejection regions for this test? What is the \\(p\\)-value if we observe \\(\\bar{x}_{\\rm obs} = 1.404\\)? Last, what is the power of this test for \\(\\mu = 1.8\\)? We assume the level of the test is \\(\\alpha = 0.05\\). To answer each of these questions, we will refer to the tables above and write down appropriate code for a two-tail test where the variance is known. First we look at the rejection regions: \\[ y_{\\rm obs} \\leq y_{\\alpha/2} ~~~ \\mbox{and} ~~~ y_{\\rm obs} \\geq y_{1-\\alpha/2} \\,, \\] for which the R function calls are n &lt;- 25 alpha &lt;- 0.05 mu.o &lt;- 2 sigma2 &lt;- 1 qnorm(alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) ## [1] 1.608007 qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) ## [1] 2.391993 The rejection region boundaries are thus 1.608 and 2.392 (values that we note lie symmetrically around \\(\\mu_o = 2\\); we also note that the placement of these boundaries does not depend on the observed statistic value \\(y_{\\rm obs} = 1.404\\)). Thus we reject the null if \\(y_{\\rm obs} \\leq 1.608\\) or \\(y_{\\rm obs} \\geq 2.392\\). See Figure 2.18, where the rejection regions are displayed in red. Next, we compute the \\(p\\)-value, noting that we already know that is has to be less than 0.05, since the observed value of 1.404 lies in the rejection region. From the table above, we extract the relevant code: n &lt;- 25 y.obs &lt;- 1.404 mu.o &lt;- 2 sigma2 &lt;- 1 2*min(c(pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)), 1-pnorm(y.obs,mean=mu.o,sd=sqrt(sigma2/n)))) ## [1] 0.002882484 The \\(p\\)-value is 0.0029, which is indeed \\(\\leq \\alpha\\). This value is to be interpreted as the probability that we would observe a value as far or farther from 2 as 1.404 (and 2.596, by symmetry) is 0.29 percent, if the null is correct. That is sufficiently small that we conclude that the null hypothesis is indeed incorrect. Last, we look at the test power assuming \\(\\mu = 1.8\\). The appropriate code is n &lt;- 25 alpha &lt;- 0.05 mu.o &lt;- 2 mu &lt;- 1.8 sigma2 &lt;- 1 y.lo &lt;- qnorm(alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) y.hi &lt;- qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) pnorm(y.lo,mean=mu,sd=sqrt(sigma2/n)) + (1-pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n))) ## [1] 0.170075 pnorm(y.lo,mean=mu,sd=sqrt(sigma2/n)) # the probability that bar(X) &lt;= 1.608 ## [1] 0.1685367 (1-pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n))) # the probability that bar(X) &gt;= 2.392 ## [1] 0.001538375 The test power is 0.170: if \\(\\mu\\) is equal to 1.8, then 17.0 percent of the time we would sample a value of our hypothesis test statistic that lies in the rejection region. The farther \\(\\mu\\) is from \\(\\mu_o\\), the higher this percentage gets. We note that in the last two lines above, we break up the power calculation into a probability of rejecting the null by observing a value of the statistic less than or equal to 1.608, and a probability of rejecting by observing a value greater than or equal to 2.392. As \\(\\mu = 1.8\\) is much closer to 1.608, we expect a much greater rate of rejection at this lower figure, and that is what we observe: very rarely will we end up rejecting the null by observing a value of the statistic that is greater than or equal to 2.392. Figure 2.18: The sampling distribution \\(f_Y(y)\\) (blue curve), rejection regions (red polygons), and observed statistic value \\(y_{ m obs}\\) (vertical green line) for \\(Y = \\bar{X}\\) given \\(n = 25\\) iid data that are assumed to be sampled from a \\(\\mathcal{N}(2,1)\\) distribution under the null hypothesis. We perform a two-tailed test, with \\(\\alpha = 0.05\\), and the variance \\(\\sigma^2 = 1\\) is assumed known. The value we observe is in the rejection region, thus we reject the null hypothesis and conclude that \\(\\mu \\neq 2\\). 2.15.2 Testing a Hypothesis About the Normal Mean with Variance Unknown Let’s assume that we have sampled \\(n = 15\\) iid data from a normal distribution whose variance is unknown$. We wish to test the null hypothesis \\(H_o : \\mu = \\mu_o = 4\\) versus the alternative \\(H_a : \\mu &gt; \\mu_o\\). What is the rejection region for this test? What is the \\(p\\)-value if we observe \\(\\bar{x}_{\\rm obs} = 4.5\\)? Last, what is the power of this test for \\(\\mu = 4.5\\)? We assume the level of the test is \\(\\alpha = 0.05\\) and that the sample variance is \\(s_{\\rm obs}^2 = 3\\). To answer these questions, we will refer to the tables above and write down appropriate code for an upper-tail test where the variance is unknown. First we look at the rejection region: \\[ y_{\\rm obs} \\geq y_{1-\\alpha/2} \\,, \\] for which the R function call is n &lt;- 15 alpha &lt;- 0.05 mu.o &lt;- 4 s2 &lt;- 3 mu.o + sqrt(s2/n)*qt(1-alpha,n-1) ## [1] 4.787682 The rejection region boundary is thus 4.787; we reject the null if \\(y_{\\rm obs} = \\bar{x}_{\\rm obs} \\geq 4.787\\). See Figure 2.19, in which the rejection region is displayed in red. Next, we compute the \\(p\\)-value, noting that we already know that is has to be greater than 0.05, since the observed value of 4.5 does not lie in the rejection region. From the tables above, we extract the relevant code: n &lt;- 15 y.obs &lt;- 4.5 mu.o &lt;- 4 s2 &lt;- 3 1-pt((y.obs-mu.o)/sqrt(s2/n),n-1) ## [1] 0.1411855 The \\(p\\)-value is 0.141, which is indeed \\(&gt; \\alpha\\). The probability that we would observe a value as far or farther from 4 as 4.5 is 14.1 percent, if the null is correct. That is sufficiently large that we fail to reject the null: we cannot conclude that we have sufficient data so as to reject the idea that \\(\\mu = 4\\). Last, we look at the test power assuming \\(\\mu = 4.5\\). The appropriate code is n &lt;- 15 alpha &lt;- 0.05 mu.o &lt;- 4 mu &lt;- 4.5 s2 &lt;- 3 y.hi &lt;- qnorm(1-alpha,mean=mu.o,sd=sqrt(sigma2/n)) 1-pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n)) ## [1] 0.6147183 The test power is 0.159: if \\(\\mu\\) is equal to 4.5, then 15.9 percent of the time we would sample a value of our hypothesis test statistic that lies in the rejection region. The farther \\(\\mu\\) is from \\(\\mu_o\\) (in the positive direction), the higher this percentage gets. See Figure 2.20. Figure 2.19: The sampling distribution \\(f_Y(y)\\) (blue curve), rejection region (red polygon), and observed statistic value \\(y_{ m obs}\\) (vertical green line) for \\(Y = \\bar{X}\\) given \\(n = 15\\) iid data that are assumed to be sampled from a normal distribution with mean \\(\\mu_o = 4\\) and variance unknown under the null hypothesis. We perform an upper-tailed test, with \\(\\alpha = 0.05\\). The value we observe is not in the rejection region, thus we fail to reject the null hypothesis and conclude that \\(\\mu = 4\\) is a plausible value. Figure 2.20: The power curve for the upper-tail test \\(H_o : \\mu = \\mu_o = 4\\) versus \\(H_a : \\mu &gt; \\mu_o\\). The red dot at \\(\\mu = 4\\) indicates the test power under the null: the power is \\(\\alpha\\), as it should be given the definition of test power. We see that \\(n = 15\\) is a sufficient amount of data to clearly disambiguate between \\(\\mu_o = 4\\) and, e.g., \\(\\mu \\gtrsim 5\\). We note that if the alternative hypothesis is \\(\\mu &lt; \\mu_o\\), the curve would be reversed\\(-\\)it would rise towards the left\\(-\\)and if the alternative hypothesis is \\(\\mu \\neq \\mu_o\\), the power curve would have a U shape, with the minimum power being \\(\\alpha\\) at \\(\\mu = \\mu_o\\). 2.15.3 Hypothesis Test About the Mean of an Arbitrary Distribution: Using the CLT Let’s harken back to the example in the confidence interval section above. In that example, we assumed that we sampled \\(n\\) iid data from the decidedly not normal distribution \\[ f_X(x) = \\theta x^{\\theta-1} ~~ x \\in [0,1] \\,, \\] with \\(\\theta &gt; 0\\), and for which \\(E[X] = \\mu = \\theta/(\\theta+1)\\). We find that because we cannot express the sampling distribution of \\(Y = \\bar{X}\\) analytically, our only path to determining a confidence interval is by making use of the Central Limit Theorem. Recall that confidence interval construction and the performance of hypothesis tests are “two sides of the same coin”: in both we are solving for roots of equations, and the only difference is in terms of what is fixed (the observed statistic for confidence intervals and the null hypothesis value \\(\\theta_o\\) for hypothesis tests) and what varies (the parameter value \\(\\theta\\) for confidence intervals and the threshold statistic values for hypothesis tests). Thus what “works” for confidence intervals will work for hypothesis tests, meaning that we can utilize the CLT to test the null hypothesis \\(H_o : \\theta = \\theta_o\\). (So long as \\(n \\gtrsim 30\\)!) To find the rejection region(s), \\(p\\)-value, and power for tests of the mean when the CLT is involved, we utilize the same codes as we utilize for the “variance known” case (i.e., the ones based on pnorm() and qnorm()), except that instead of plugging in \\(\\sigma^2\\) (i.e., sigma2), we would plug in the sample variance \\(s_{\\rm obs}^2\\) (i.e., s2). For instance: n &lt;- 25 alpha &lt;- 0.05 theta.o &lt;- 2 mu.o &lt;- theta.o/(theta.o+1) s2 &lt;- 0.0440 qnorm( alpha/2,mean=mu.o,sd=sqrt(s2/n)) ## [1] 0.5844416 qnorm(1-alpha/2,mean=mu.o,sd=sqrt(s2/n)) ## [1] 0.7488918 The rejection region boundaries are thus 0.5844 and 0.7489. We note that, as is the case for confidence interval coverage, the hypothesis test performance is only approximate, i.e., the Type I error rate is not exactly \\(\\alpha\\), because the distribution of \\(\\bar{X}\\) is not exactly normal. However, as we do in the confidence interval case, we can perform simulations to assess just how far off our actual Type I error rate \\(\\alpha\\) is for any given analysis setting. set.seed(101) num.sim &lt;- 10000 # repeat the data-generating process 10,000 times n &lt;- 25 alpha &lt;- 0.05 theta.o &lt;- 2 # the mean is 3/(3+1) = 3/4 mu.o &lt;- theta.o/(theta.o+1) typeIerr &lt;- 0 for ( ii in 1:num.sim ) { X &lt;- rbeta(n,theta.o,1) y.obs &lt;- mean(X) s2 &lt;- var(X) y.lo &lt;- qnorm( alpha/2,mean=mu.o,sd=sqrt(s2/n)) y.hi &lt;- qnorm(1-alpha/2,mean=mu.o,sd=sqrt(s2/n)) if ( y.obs &lt;= y.lo || y.obs &gt;= y.hi ) typeIerr = typeIerr+1 } cat(&quot;The observed proportion of Type I errors =&quot;,typeIerr/num.sim,&quot;\\n&quot;) ## The observed proportion of Type I errors = 0.064 We observe an empirical Type I error rate of 0.0640. This is sufficient far from the expected value of 0.05 that we cannot conclude that the distribution of the sample mean is approximately normal. 2.15.4 Extension to Testing With Two Data Samples: the t Test Let’s assume that we are given two independent data samples: \\[\\begin{align*} \\{U_1,\\ldots,U_{n_U}\\} &amp;\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu_U,\\sigma_U^2) \\\\ \\{V_1,\\ldots,V_{n_V}\\} &amp;\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu_V,\\sigma_V^2) \\,. \\end{align*}\\] The sample means are \\(\\bar{U}\\) and \\(\\bar{V}\\), respectively, and we know that \\[ \\bar{U} \\sim \\mathcal{N}\\left(\\mu_U,\\frac{\\sigma_U^2}{n_U}\\right) ~~\\mbox{and}~~ \\bar{V} \\sim \\mathcal{N}\\left(\\mu_V,\\frac{\\sigma_V^2}{n_V}\\right) \\,. \\] In the two-sample t test, the null hypothesis \\(H_o\\) is that \\(\\mu_U = \\mu_V\\). Thus a natural test statistic is \\(\\bar{U}-\\bar{V}\\), which, as the reader may confirm using the method of moment-generating functions, is normally distributed: \\[ \\bar{U} - \\bar{V} \\sim \\mathcal{N}\\left(\\mu_U-\\mu_V,\\frac{\\sigma_U^2}{n_U}+\\frac{\\sigma_V^2}{n_V}\\right) \\,. \\] However, when neither variance is known, we cannot model the observed data with the normal distribution; instead, in Welch’s t test, we assume the following: \\[ T = \\frac{\\bar{U}-\\bar{V}}{S_\\Delta} = \\frac{\\bar{U}-\\bar{V}}{\\sqrt{\\frac{S_U^2}{n_U}+\\frac{S_V^2}{n_V}}} \\sim t({\\nu}) \\,, \\] i.e., \\(T\\) is sampled from a \\(t\\) distribution with \\(\\nu\\) degrees of freedom, where \\(\\nu\\) is estimated using the Welch-Satterthwaite equation: \\[ \\nu \\approx \\frac{S_\\Delta^4}{\\frac{S_U^4}{n_U^2(n_U-1)} + \\frac{S_V^4}{n_V^2(n_V-1)}} \\,. \\] The rule-of-thumb for this approximation to hold is that both \\(n_U\\) and \\(n_V\\) are larger than 5. Note that in some applications, one might see \\(\\nu\\) rounded off to an integer, but this is not necessary: one can work with the \\(t\\) distribution’s probability density function just fine even if \\(\\nu\\) in not an integer! (The rounding-off is done to allow one to use \\(t\\) tables to estimate \\(p\\)-values.) Can we “reformat” this test so as to be consistent with how we perform one-sample tests? The answer is yes. Let the test statistic be \\(Y = \\bar{U}-\\bar{V} = S_{\\Delta} T\\), and let the null hypothesis be \\(H_o : \\mu_U - \\mu_V = 0\\). Then \\[ F_Y(y) = P(Y \\leq y) = P(S_{\\Delta} T \\leq y) = P\\left(T \\leq \\frac{y}{S_{\\Delta}}\\right) = F_{T(\\nu)}\\left(\\frac{y}{S_{\\Delta}}\\right) \\,, \\] and thus the rejection-region boundaries are given by \\[ y_q = S_{\\Delta} F_{T(\\nu)}^{-1}(q) \\,, \\] where \\(q = \\alpha/2\\) or \\(1-\\alpha/2\\). In R code, these boundaries are given by y.lo &lt;- S.Delta * qt(alpha/2,nu) y.hi &lt;- S.Delta * qt(1-alpha/2,nu) Additionally, the \\(p\\)-values and power are given by p &lt;- 2*min(c(pt(y.obs/S.Delta,nu),1-pt(y.obs/S.Delta,nu))) power &lt;- pt((y.lo-mu.Delta)/S.Delta,nu) + (1-pt((y.hi-mu.Delta)/S.Delta,nu)) where mu.Delta is the specified alternative value for \\(\\mu_U-\\mu_V\\). We leave it as an exercise to the reader to generalize the expressions and code above for cases in which the null hypothesis value differs from zero. We note that in R, the above code yields equivalent results to the function t.test(). As a last point: the reader may ask “what do I do if I have three or more samples and I want to test whether all of them have the same mean, with the alternative being that at least one of the means is different?” In such a situation, one would employ a one-way analysis of variance test, or an ANOVA test; we introduce this test in the final section of this chapter. 2.15.5 More About p-Values As described above, the \\(p\\)-value is the probability of observing a hypothesis test statistic value \\(y_{\\rm obs}\\) or a more extreme value, i.e., one further from the null value. In mathematical terms, for a continuous sampling distribution, a \\(p\\)-value is \\[\\begin{align*} p = \\mbox{min}\\left[ \\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta_o) dy , \\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta_o) dy \\right] ~~~&amp; \\mbox{(two-tail)} \\\\ p = \\int_{-\\infty}^{y_{\\rm obs}} f_Y(y \\vert \\theta_o) dy ~~~&amp; \\mbox{(lower-tail)} \\\\ p = \\int_{y_{\\rm obs}}^\\infty f_Y(y \\vert \\theta_o) dy ~~~&amp; \\mbox{(upper-tail)} \\,. \\end{align*}\\] If the null hypothesis is correct, then \\(p\\) is sampled uniformly between 0 and 1, i.e., 0.83 is just as likely to be observed as 0.14 and as 0.23, etc. This makes sense: the probability that \\(p &lt; \\alpha\\) is exactly \\(\\alpha\\), the Type I error rate. If we set \\(\\alpha\\) to 0.05, then when the null is true, \\(p\\) will be less than \\(\\alpha\\) 5% of the time, i.e., we will wrongly make the decision to reject a true null on average one time in every 20 hypothesis tests we perform. (See Figure 2.21.) set.seed(101) n &lt;- 20 mu.o &lt;- 3 sigma2 &lt;- 1 Y &lt;- rnorm(10000,mean=mu.o,sd=sqrt(sigma2/n)) # direct sampling of test statistic p.val &lt;- pnorm(Y,mean=mu.o,sd=sqrt(sigma2/n)) hist(p.val,main=NULL,col=&quot;blue&quot;,xlab=&quot;p&quot;,breaks=seq(0,1,by=0.05)) abline(h=500,col=&quot;red&quot;,lty=2) Figure 2.21: The empirical distribution of \\(p\\)-values under the null hypothesis, for a lower-tail test with \\(n = 20\\), \\(\\mu_o = 3\\), and \\(\\sigma^2 = 1\\). We can see that the \\(p\\)-values are uniformly distributed between 0 and 1, meaning that when the null is true, the probability of sampling a \\(p\\)-value less than \\(\\alpha\\) is exactly \\(\\alpha\\). Because we are performing a simulation, the number of observations falling into each histogram bin is a random variable. Because of this, we observe some amount of “noise” in Figure 2.21 (i.e., some amount of variation around the dashed red line)…but nevertheless we can visually infer that the sampling distribution for \\(p\\) is uniform (i.e., flat). (For completeness, the number of counts in each bin are collectively sampled from a multinomial distribution, where the probability of a count being recorded in any given bin being the reciprocal of the number of bins. We discuss the multinomial distribution in Chapter 3.) Now, what if the null hypothesis is not correct? What will happen to the distribution of \\(p\\)-values? The answer depends on the circumstance: if we are performing an lower-tail test (with \\(E[Y]\\) increasing with \\(\\theta\\)) and the true mean is larger than the hypothesized mean, then the \\(p\\)-values will skew towards 1: we are less likely to reject the null when its value is closer to the tail of interest than the true value is. If the true mean is smaller than the hypothesized mean, the \\(p\\)-values will skew towards 0. See Figure 2.22 for a demonstration of the latter point. set.seed(101) n &lt;- 20 mu.o &lt;- 3 mu &lt;- 2.75 sigma2 &lt;- 1 Y &lt;- rnorm(10000,mean=mu,sd=sqrt(sigma2/n)) # we sample with mu... p.val &lt;- pnorm(Y,mean=mu.o,sd=sqrt(sigma2/n)) # ...but test with mu.o df &lt;- data.frame(p.val=p.val) ggplot(data=df,aes(x=p.val)) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0,1,by=0.05)) + geom_hline(yintercept=500,col=&quot;red&quot;,lty=2) + labs(x=&quot;p&quot;,y=&quot;Frequency&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.22: The empirical distribution of \\(p\\)-values when the stated alternative hypothesis is consistent with the truth. We can see that the \\(p\\)-values towards 0, meaning that when the alternative hypothesis is true, the probability of sampling a \\(p\\)-value less than \\(\\alpha\\) can be \\(\\gg \\alpha\\). cat(&quot;The proportion of p-values &lt; 0.05 is &quot;,sum(p.val&lt;0.05)/10000,&quot;\\n&quot;) ## The proportion of p-values &lt; 0.05 is 0.2946 In Figure 2.22, 29.46% of the counts are observed as having value less than \\(\\alpha = 0.05\\)…so the test power is 0.2946. As the true mean gets smaller and smaller than \\(\\mu_o = 3\\), the number of counts in the first bin increases…which is equivalent to saying that the test power increases. We will make three important statements about \\(p\\)-values here, ones that are good for all readers to remember. The p-value is not the probability that the null hypothesis is correct! A hypothesis is not a random variable, and there is thus no probability associated with it. The \\(p\\)-value is the probability of observing a more extreme hypothesis test statistic than what we actually observe, conditioned on the null hypothesis being correct. You cannot select the Type I error rate \\(\\alpha\\) after computing the p-value. One specifies the Type I error rate \\(\\alpha\\) before data are analyzed; specifying it afterwards opens up the analysis procedure to bias. “My \\(p\\)-value is 0.09…so I’ll set \\(\\alpha = 0.1\\) so that I can reject the null.” You cannot perform multiple hypothesis tests without correcting your result. Suppose that we perform 100 independent hypothesis tests, each with \\(\\alpha = 0.05\\). Even if the null is true for every test, we will end up rejecting the null five or so times. (Not exactly five times: the number of rejections we observe is a random variable with mean five.) That we will reject an increasing number of null hypotheses with an increasing number of tests is inevitable: this is a feature of hypothesis testing, not a bug. When performing multiple tests, we need to apply a correction factor to each observed \\(p\\)-value, such as the Bonferroni correction (e.g., if \\(m\\) is the number of conducted tests, reset \\(\\alpha\\) to \\(\\alpha/m\\)) or the less conservative false-discovery rate (FDR) correction. We discuss these corrections in more detail in Chapter 5. (We note that terms used for conducting multiple tests without applying a correction for multiple comparisons include p-hacking and data dredging.) 2.15.6 How Many Data Do We Need to Achieve a Given Test Power? We can phrase a typical experimental design exercise as follows. “We will collect data that we assume are iid from a normal distribution with specified mean \\(\\mu\\) and specified variance \\(\\sigma^2\\).” (Alternatively, it could be a specification of \\(s_{\\rm obs}^2\\), motivating the use of \\(t\\) distribution.) “How many data do we need to collect to achieve a test power of 0.8, assuming \\(\\alpha = 0.05\\) and assuming that the null hypothesis is \\(H_o : \\mu = \\mu_o\\) and the alternative hypothesis is \\(H_a : \\mu &gt; \\mu_o\\)?” To answer this question, we will start with math, and then transition into code. We refer back to the power table above and note that for an upper-tail test, \\[ \\mbox{power}(\\theta) = 1 - F_Y(y_{\\rm hi} \\vert \\theta) \\,. \\] For the specific case of the normal distribution, we know that \\(Y = \\bar{X}\\), that \\(\\bar{X} \\sim \\mathcal{N}(\\mu,\\sigma^2/n)\\), and thus that \\(E[Y] = E[\\bar{X}] = \\mu\\) increases with \\(\\mu\\). Thus \\(y_{\\rm hi} = y_{1-\\alpha/2}\\), and we can write \\[ \\mbox{power}(\\theta) = 1 - F_Y(y_{\\rm 1-\\alpha/2} \\vert \\mu,\\sigma^2/n) \\,. \\] Now we substitute in our target power value and solve for \\(n\\): \\[\\begin{align*} 1 - F_Y(y_{1-\\alpha/2} \\vert \\mu,\\sigma^2/n) &amp;= 0.8 \\\\ \\Rightarrow ~~~ F_Y(y_{1-\\alpha/2} \\vert \\mu,\\sigma^2/n) &amp;= 0.2 \\\\ \\Rightarrow ~~~ &amp;\\mbox{...?} \\,. \\end{align*}\\] The issue is two-fold, as it turns out: first, \\(n\\) appears to the right of the condition symbol, which means that we will not be solving this by hand but rather numerically via root-finding. But there’s another less-readily apparent issue as well: \\(y_{1-\\alpha/2}\\) itself depends on \\(n\\)! Can we still pursue root-finding? Yes…we simply will have a more complicated expression. We transition here to code. We refer back to the rejection-region table and note that for an upper-tail normal mean test, \\(y_{1-\\alpha/2}\\) is qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)) # the null enters here The analogous code for test power is 1 - pnorm(y.hi,mean=mu,sd=sqrt(sigma2/n)) # the alternative enters here Combining the two expressions, we get 1 - pnorm(qnorm(1-alpha/2,mean=mu.o,sd=sqrt(sigma2/n)),mean=mu,sd=sqrt(sigma2/n)) To solve the problem at hand, we would subtract 0.8 from this expression, set the result equal to zero, and utilize uniroot() to solve for \\(n\\): # alpha &lt;- 0.05 ; mu.o &lt;- 8 ; mu &lt;- 9 ; sigma2 &lt;- 6 f &lt;- function(n) { pnorm(qnorm(0.975,mean=8,sd=sqrt(6/n)),mean=9,sd=sqrt(6/n)) - 0.2 } ceiling(uniroot(f,interval=c(1,1000))$root) ## [1] 48 Note how we use the function ceiling() here, which rounds up the result to the next higher integer. We do this because the sample size should be integer, and because we want the power to be at least 0.8; if we round our result down (via the floor() function), the power will be less than 0.8. In sample size-determination exercises, always round the final result up! 2.16 Hypothesis Testing: Population Variance When performing hypothesis tests for the population mean \\(\\mu\\), above, we adopt \\(\\bar{X}\\) as our hypothesis test statistic because it is the MLE for \\(\\mu\\); we do not theoretically justify the choice (by showing, e.g., that its use leads us to defining the most powerful test out of a set of possible alternatives). Here, we follow similar logic: \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), so we will adopt \\(Y = S^2\\) as our test statistic when testing hypotheses about \\(\\sigma^2\\). Assume that we are given \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) that are each sampled from a normal distribution with (unknown) mean \\(\\mu\\) and variance \\(\\sigma^2\\). Refer back to the table in the hypothesis testing section of Chapter 1. We observe that \\(E[Y] = E[S^2] = \\sigma^2\\), so \\(E[Y]\\) increases with \\(\\sigma^2\\). Thus the rejection regions for, e.g., a two-tail test are \\[ y_{\\rm obs} = s_{\\rm obs}^2 \\leq y_{\\alpha/2} ~~~ \\mbox{and} ~~~ y_{\\rm obs} = s_{\\rm obs}^2 \\leq y_{1-\\alpha/2} \\] where \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\) are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\sigma_o^2) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\sigma_o^2) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] As is the case for the population mean, we have to borrow (and amend) a result from the confidence interval section to proceed here, namely that \\[ F_Y(y) = F_{W(n-1)}\\left(\\frac{(n-1)y}{\\sigma_o^2}\\right) \\,, \\] where \\(F_{W(n-1)}\\) is the cdf for a chi-square distribution for \\(n-1\\) degrees of freedom. So, for instance, \\[ y_{1-\\alpha/2} = \\frac{\\sigma_o^2}{n-1}F_{W(n-1)}^{-1}\\left(1-\\frac{\\alpha}{2}\\right) \\,. \\] As is also the case for the population mean, here we do not work with the cdfs by hand. The following summarizes the evaluation of rejection region boundaries (see Figure 2.23): Type Rejection Region(s) R Code for Normal Distribution two-tail \\(y_{\\rm obs} \\leq y_{\\alpha/2}\\) and y.q &lt;- (sigma2.o/(n-1))*qchisq(q,n-1) \\(y_{\\rm obs} \\geq y_{1-\\alpha/2}\\) lower-tail \\(y_{\\rm obs} \\leq y_{\\alpha}\\) upper-tail \\(y_{\\rm obs} \\geq y_{1-\\alpha}\\) \\(p\\)-values: Type Formula R Code for Normal Distribution two-tail 2 \\(\\cdot\\) min(\\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\), p &lt;- 2*min(c(pchisq((n-1)*y.obs/sigma2.o,n-1), \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\)) 1-pchisq((n-1)*y.obs/sigma2.o,n-1))) lower-tail \\(F_Y(y_{\\rm obs} \\vert \\theta_o)\\) p &lt;- pchisq((n-1)*y.obs/sigma2.o,n-1) upper-tail \\(1-F_Y(y_{\\rm obs} \\vert \\theta_o)\\) p &lt;- 1-pchisq((n-1)*y.obs/sigma2.o,n-1) and power functions: Type Formula R Code for Normal Distribution two-tail \\(F_Y(y_{\\rm lo} \\vert \\theta) +\\) power &lt;- pchisq((n-1)*y.lo/sigma2,n-1) + \\(1-F_Y(y_{\\rm hi} \\vert \\theta)\\) (1-pchisq((n-1)*y.hi/sigma2,n-1)) lower-tail \\(F_Y(y_{\\rm lo} \\vert \\theta)\\) power &lt;- pchisq((n-1)*y.lo/sigma2,n-1) upper-tail \\(1-F_Y(y_{\\rm hi} \\vert \\theta)\\) power &lt;- 1-pchisq((n-1)*y.hi/sigma2,n-1) where for the specific case of the normal population variance, \\(y_{\\rm lo}\\) maps to \\(y_{\\alpha/2}\\) and \\(y_{\\rm hi}\\) maps to \\(y_{1-\\alpha/2}\\). Figure 2.23: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming \\(\\alpha = 0.1\\) and \\(n-1 = 3\\) degrees of freedom. 2.16.1 Testing a Hypothesis About the Normal Population Variance Let’s assume that we have sampled \\(n = 14\\) iid data from a normal distribution of unknown mean and variance. We wish to test the null hypothesis \\(H_o : \\sigma^2 = \\sigma_o^2 = 4\\) versus the alternative \\(H_a : \\sigma^2 &gt; \\sigma_o^2\\). What are the rejection regions for this test? What is the \\(p\\)-value if we observe \\(s_{\\rm obs}^2 = 5.5\\)? Last, what is the power of this test for \\(\\sigma^2 = 8\\)? We assume the level of the test is \\(\\alpha = 0.05\\). To answer each of these questions, we refer to the tables above. As the rejection region, we note that \\(y_{\\rm obs} \\geq y_{1-\\alpha}\\), for which the R function call is alpha &lt;- 0.05 n &lt;- 14 sigma2.o &lt;- 4 (sigma2.o/(n-1))*qchisq(1-alpha,n-1) ## [1] 6.880625 The rejection region boundary is thus 6.881: we reject the null hypothesis if \\(y_{\\rm obs} = s_{\\rm obs}^2 \\geq 6.881\\). Next, we compute the \\(p\\)-value, noting that we already know that is has to be greater than 0.05, since the observed value of 5.5 does not lie in the rejection region. The relevant code is y.obs &lt;- 5.5 n &lt;- 14 sigma2.o &lt;- 4 1-pchisq((n-1)*y.obs/sigma2.o,n-1) ## [1] 0.1623236 The \\(p\\)-value is 0.162, which is indeed \\(&gt; \\alpha\\). There is thus a 16.2 percent chance that we would sample a variance value of 5.5 or greater if the null is correct. This is too high a percentage chance for us to decide to reject the idea that \\(\\sigma^2 = 4\\). Last, we look at the test power assuming \\(\\sigma^2 = 8\\): alpha &lt;- 0.05 n &lt;- 14 sigma2.o &lt;- 4 sigma2 &lt;- 8 y.hi &lt;- (sigma2.o/(n-1))*qchisq(1-alpha,n-1) 1-pchisq((n-1)*y.hi/sigma2,n-1) ## [1] 0.5956563 The test power is 0.596: if \\(\\sigma^2\\) is equal to 8, then 59.6 percent of the time we would sample a value of \\(y_{\\rm obs}\\) that lies in the rejection region. This is not a particularly high value; if being able to differentiate between \\(\\sigma_o^2 = 4\\) and \\(\\sigma^2 = 8\\) is an important research goal, we would want to collect more data! In the previous section, we show in an example how to determine the sample size that is needed to achieve a particular power. Here we will repeat that exercise, but with a twist: we will visualize the power of the test as a function of \\(n\\). Because we are not solving for \\(n\\) for a particular power value, there is no need to utilize uniroot() here; we simply need to loop over calls to qchisq() and pchisq(). (But note that we don’t actually “loop,” as R’s vectorization capabilities allow us to simply pass vectors of sample-size values into the function calls, with vectors of results being returned.) See Figure 2.24. alpha &lt;- 0.05 sigma2.o &lt;- 4 sigma2 &lt;- 8 n &lt;- 2:100 y.hi &lt;- (sigma2.o/(n-1))*qchisq(1-alpha,n-1) power &lt;- 1-pchisq((n-1)*y.hi/sigma2,n-1) df &lt;- data.frame(n,power) ggplot(data=df,aes(x=n,y=power)) + geom_line(col=&quot;blue&quot;,lwd=1) + geom_line(data=data.frame(x=c(0,14,14,14),y=c(0.596,0.596,0,0.596)), aes(x=x,y=y),col=&quot;red&quot;,lty=2,lwd=0.85) + geom_point(x=14,y=0.596,size=4,col=&quot;red&quot;) + geom_hline(yintercept=0,lty=2,col=&quot;blue&quot;) + geom_hline(yintercept=1,lty=2,col=&quot;blue&quot;) + labs(x=&quot;n&quot;,y=&quot;Test Power&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.24: The test power as a function of sample size \\(n\\) assuming \\(\\sigma_o^2 = 4\\), \\(\\sigma^2 = 8\\), and \\(\\alpha = 0.05\\). The red dot indicates the test power for \\(n = 14\\), which as shown in the text is 0.596. 2.16.2 Extension to Testing With Two Data Samples: the F Test As was the case above for the two-sample \\(t\\) test, described above, we assume we are given two independent data samples: \\[\\begin{align*} \\{U_1,\\ldots,U_{n_U}\\} &amp;\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu_U,\\sigma_U^2) \\\\ \\{V_1,\\ldots,V_{n_V}\\} &amp;\\stackrel{iid}{\\sim} \\mathcal{N}(\\mu_V,\\sigma_V^2) \\,, \\end{align*}\\] with sample variances \\(S_U^2\\) and \\(S_V^2\\), respectively. Given what we know about the distribution of sample variance values, we can write that \\[ W_U = \\frac{(n_U-1)S_U^2}{\\sigma_U^2} \\sim \\chi_{n_U-1}^2 ~~\\mbox{and}~~ W_V = \\frac{(n_V-1)S_V^2}{\\sigma_V^2} \\sim \\chi_{n_V-1}^2 \\,. \\] The ratio of \\(W_U/(n_U-1)\\) to \\(W_V/(n_V-1)\\) is \\[ F = \\left. \\left(\\frac{\\frac{(n_U-1)S_U^2}{\\sigma_U^2}}{n_U-1}\\right) \\right/ \\left(\\frac{\\frac{(n_V-1)S_U^2}{\\sigma_V^2}}{n_V-1}\\right) = \\frac{S_U^2/\\sigma_U^2}{S_V^2/\\sigma_V^2} \\sim F_{n_U-1,n_V-1} \\,, \\] where \\(F_{n_U-1,n_V-1}\\) is the F distribution for \\(n_U-1\\) numerator, and \\(n_V-1\\) denominator, degrees of freedom. Examining the equation above, we see that \\(F\\) is an appropriate statistic for testing hypotheses about the relationship between the variances of both samples. Let the test statistic be \\[ Y = \\frac{s_U^2}{s_V^2} \\,. \\] (Note that it doesn’t matter which sample is identified as \\(U\\) and which is identified as \\(V\\) so long as care is taken to not accidentally “reverse” the samples when coding the test.) As was the case with the two-sample \\(t\\) test, we observe \\(Y\\) but only know the sampling distribution for \\(F\\), so we need to enact a variable transformation: \\[ F_Y(y) = P(Y \\leq y) = P\\left(\\frac{\\sigma_U^2}{\\sigma_V^2}F \\leq y \\right) = P\\left( F \\leq \\frac{\\sigma_V^2}{\\sigma_U^2} y \\right) = F_{F(n_U-1,n_V-1)}\\left(\\frac{\\sigma_V^2}{\\sigma_U^2} y\\right) \\,. \\] Thus the rejection-region boundaries are given by \\[ y_q = \\frac{\\sigma_{U,o}^2}{\\sigma_{V,o}^2} F_{F(n_U-1,n_V-1)}^{-1}(q) \\,, \\] where \\(q\\) is, e.q., \\(\\alpha/2\\), etc. In R code, the boundaries for a two-tail test are given by y.lo &lt;- (sigmaU2.o/sigmaV2.o) * qf(alpha/2,n.U-1,n.V-1) y.hi &lt;- (sigmaU2.o/sigmaV2.o) * qf(1-alpha/2,n.U-1,n.V-1) with the \\(p\\)-values and power given by p &lt;- 2*min(c(pf(sigmaV2.o*y.obs/sigmaU2.o,n.U-1,n.V-1), 1-pf(sigmaV2.o*y.obs/sigmaU2.o,n.U-1,n.V-1))) power &lt;- pf(sigmaV2*y.lo/sigmaU2,n.U-1,n.V-1) + (1-pf(sigmaV2*y.hi/sigmaU2,n.U-1,n.V-1)) where \\(\\sigma_U^2/\\sigma_V^2\\) is the specified alternative variance ratio. We leave it as an exercise to the reader to derive appropriate expressions for one-tail tests. We note that the above code yields equivalent results to the R function var.test(). 2.17 Simple Linear Regression Thus far in this chapter, we have generally assumed that we have been given a sample of data \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim} \\mathcal{N}(\\mu,\\sigma^2)\\), or, sometimes, two such samples that are independent of each other. Here we will move beyond this assumption to the case where our data consists of tuples \\(\\{(x_1,Y_1),\\ldots,(x_n,Y_n)\\}\\) and where we want to determine (or learn) the association between the variables \\(x_i\\) and \\(Y_i\\). (If there is one! See Figure 2.25. Also, note the use of the term “association”: a relationship might exist, but it is not necessarily the case that \\(x\\) “causes” \\(Y\\); to determine causation, one would utilize methods of causal inference, which are beyond the scope of this book.) In other words, we want to regress \\(\\mathbf{Y}\\) upon \\(\\mathbf{x}\\). \\(\\mathbf{x} = \\{x_1,\\ldots,x_n\\}\\): the independent variables (or predictors or covariates or features) \\(\\mathbf{Y} = \\{Y_1,\\ldots,Y_n\\}\\): the dependent variables (or the response variables) set.seed(202) x &lt;- runif(40,min=0,max=10) Y &lt;- 4 + x/2 + rnorm(40) df &lt;- data.frame(x=x,Y=Y) ggplot(data=df,aes(x=x,y=Y)) + geom_point(col=&quot;blue&quot;,size=3) + geom_line(data=data.frame(x=c(0,10),y=c(4,9)),aes(x=x,y=y), lty=2,col=&quot;red&quot;,lwd=0.85) + labs(y=&quot;Y | x&quot;) + coord_cartesian(ylim=c(0,11.5)) + theme(axis.title=element_text(size = rel(1.25))) Figure 2.25: Illustration of the setting for simple linear regression. The data \\(Y \\vert x\\) (blue points) are randomly distributed around the true regression line \\(y \\vert x = 4 + x/2\\) (red dashed line), with the error term \\(\\epsilon_i \\sim \\mathcal{N}(y \\vert x,1)\\). If the relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\) is deterministic, then there exists a function \\(f\\) such that \\(y_i = f(x_i)\\) for all \\(i\\). As statisticians, we are more interested in learning non-deterministic associations (so-called stochastic, probabilistic, or statistical relationships) between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\). One standard model for the data-generating process is \\[ Y_i \\vert x_i = f(x_i) + \\epsilon_i \\,, \\] where \\(\\{\\epsilon_1,\\ldots,\\epsilon_n\\}\\) are random variables with \\(E[\\epsilon_i] = 0\\) and uncorrelated variances \\(V[\\epsilon_i] = \\sigma^2\\) (i.e., the variances are the same for all \\(i\\), i.e., they are homoscedastic). (Note that we are not saying anything about the distribution of \\(\\epsilon_i\\) yet; we are just stating assumptions about its expected value and variance.) Now we can say why the independent variables are represented in lower case while the dependent variables are in upper case: we consider the independent variables to be fixed, while the dependent variables are random variables because the \\(\\epsilon_i\\)’s are random variables. We now suppose that there is a linear relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\) such that \\[\\begin{align*} Y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\ E[Y_i] &amp;= E[\\beta_0 + \\beta_1 x_i + \\epsilon_i] = \\beta_0 + \\beta_1 x_i + E[\\epsilon_i] = \\beta_0 + \\beta_1 x_i \\\\ V[Y_i] &amp;= V[\\beta_0 + \\beta_1 x_i + \\epsilon_i] = V[\\epsilon_i] = \\sigma^2 \\,. \\end{align*}\\] This is a simple linear regression model that is meant to represent the data-generating process. (The word “simple” follows from the fact that there is only one set of independent variables. Also, note that this is a linear model because it is linear in the parameters; e.g., \\(\\beta_0 + \\beta_1x_i^2\\) is also a linear model.) \\(\\beta_0\\) is dubbed the intercept and \\(\\beta_1\\) is the slope. In analyses, we are generally interested in estimating \\(\\beta_1\\), as it tells us the average change in \\(Y\\) given a one-unit change in \\(x\\). Once a simple linear regression model is learned, we can make predictions \\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] and we can compute residuals \\[ e_i = Y_i - \\hat{Y}_i \\] The goal in the estimation process is to make the residuals as small as possible. Since negative values of \\(e_i\\) are as “bad” as positive values, we estimate \\(\\beta_0\\) and \\(\\beta_1\\) by minimizing the sum of squared errors or SSE: \\(\\sum_{i=1}^n e_i^2\\). This is the so-called ordinary least squares estimator, or OLS. Our use of the OLS estimator for \\(\\beta_0\\) and \\(\\beta_1\\) (as opposed to, e.g., \\(\\sum_{i=1}^n \\vert e_i \\vert\\)) is motivated by the Gauss-Markov theorem, which states that the unbiased OLS estimator is the best estimator we can use if the \\(\\epsilon_i\\)’s are uncorrelated, have equal variances, and expected values of zero (hence motivating the assumptions we made above). Note that we have still not said anything about the distribution of the \\(\\epsilon_i\\)’s: the Gauss-Markov theorem does not mandate that they have a particular distribution! (As an aside: what happens if, e.g., we have data for which the variances are unequal, or heteroscedastic? We simply lose the ability to make statements like “the OLS estimator is the best linear unbiased estimator.” We can always learn the simple linear model we define above…a computer cannot stop us from doing so. It just may not be the best model choice…for instance, weighted OLS regression, where the weighting given each datum is related to how uncertain its value is, may provide better estimates.) To estimate \\(\\beta_0\\) and \\(\\beta_1\\), we take partial derivatives of \\[ \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), setting the result to zero, and solving for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), respectively. Here, we will quote results, and provide the estimate of \\(\\sigma^2\\), while leaving the derivations as exercises to the reader: Parameter Estimate Expected Value Variance \\(\\beta_0\\) \\(\\bar{Y}-\\hat{\\beta}_1\\bar{x}\\) \\(\\beta_0\\) \\(\\sigma^2\\left(\\frac{1}{n}+\\frac{\\bar{x}^2}{S_{xx}}\\right)\\) \\(\\beta_1\\) \\(\\frac{S_{xY}}{S_{xx}}\\) \\(\\beta_1\\) \\(\\frac{\\sigma^2}{S_{xx}}\\) \\(\\sigma^2\\) \\(\\frac{1}{n-2}\\) \\(\\sum_{i=1}^n\\) \\(e_i^2\\) \\(=\\frac{SSE}{n-2}\\) \\(\\sigma^2\\) \\(\\frac{2\\sigma^4}{n-2}\\) The quantities \\(S_{xx}\\) and \\(S_{xY}\\) (and \\(S_{YY}\\)) are shorthand for the following: \\[\\begin{align*} S_{xx} &amp;= \\sum_{i=1}^n (x_i-\\bar{x})^2 = \\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2 \\\\ S_{xY} &amp;= \\sum_{i=1}^n (x_i-\\bar{x})(Y_i-\\bar{Y}) = \\left(\\sum_{i=1}^n x_iY_i\\right) - n\\bar{x}\\bar{Y} \\\\ S_{YY} &amp;= \\sum_{i=1}^n (Y_i-\\bar{Y})^2 = \\left(\\sum_{i=1}^n Y_i^2\\right) - n\\bar{Y}^2 \\,. \\end{align*}\\] In Figure 2.26, we display the estimated regression line for the data shown above in Figure 2.25. Figure 2.26: Same as the previous figure, with the estimated regression line \\(\\hat{y} = 4.52 + 0.46 x\\) overlaid (solid red line). Ultimately, we are interested in determining if there is a statistically significant relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{Y}\\), a question that can be answered via hypothesis testing: \\[\\begin{align*} H_o&amp;: \\beta_1 = 0 \\implies Y_i = \\beta_0 + \\epsilon_i \\\\ H_a&amp;: \\beta_1 \\neq 0 \\implies Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{align*}\\] In order to carry out hypothesis testing, however, we have to make a distributional assumption regarding the \\(\\epsilon_i\\)’s. (Finally!) The standard (and simplest!) assumption is that \\[ \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2) \\,, \\] which means that \\[ Y_i \\vert x_i \\sim \\mathcal{N}(\\beta_0+\\beta_1x_i,\\sigma^2) \\] Adding this assumption to the model does not affect estimation; if, for instance, we were to derive the MLE estimators for \\(\\beta_0\\) and \\(\\beta_1\\), we would find that they are identical to the OLS estimators given above. Does our distributional assumption for the \\(\\epsilon_i\\)’s allow us to easily specify the distribution of, e.g., \\(\\hat{\\beta}_1\\)? The answer is yes. We begin by showing that \\(\\hat{\\beta}_1\\) can be written as a linear function of the dependent variables: \\[\\begin{align*} \\hat{\\beta}_1 &amp;= \\frac{\\left(\\sum_{i=1}^n Y_i x_i\\right) - n \\bar{x}\\bar{Y}}{\\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2} \\\\ &amp;= \\frac{1}{k} \\left[ \\left(\\sum_{i=1}^n Y_i x_i\\right) - n \\bar{x}\\bar{Y} \\right] \\\\ &amp;= \\frac{1}{k} \\left[ \\left(\\sum_{i=1}^n Y_i x_i\\right) - \\bar{x} \\sum_{i=1}^n Y_i \\right] \\\\ &amp;= \\frac{1}{k} \\sum_{i=1}^n (x_i - \\bar{x}) Y_i = \\sum_{i=1}^n \\left( \\frac{x_i-\\bar{x}}{k}\\right) Y_i = \\sum_{i=1}^n a_i Y_i \\,. \\end{align*}\\] Because the \\(Y_i\\)’s are normally distributed, we know (via the method of moment-generating functions) that \\(\\hat{\\beta}_1\\) is also normally distributed, with mean \\[ E[\\hat{\\beta}_1] = \\sum_{i=1}^n a_i E[Y_i] = \\cdots = \\beta_1 \\] and variance \\[ V[\\hat{\\beta}_1] = \\sum_{i=1}^n a_i^2 V[Y_i] = \\sum_{i=1}^n a_i^2 \\sigma^2 = \\cdots = \\frac{\\sigma^2}{\\left(\\sum_{i=1}^n x_i^2\\right) - n\\bar{x}^2} \\,. \\] We can finally perform the hypothesis test. If \\(\\sigma^2\\) is unknown, the standardized slope is assumed to be \\(t\\)-distributed for \\(n-2\\) degrees of freedom: \\[ \\frac{\\hat{\\beta}_1 - \\beta_1}{se({\\hat{\\beta}_1})} = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\sqrt{V[{\\hat{\\beta}_1}]}} \\sim t_{n-2} \\,. \\] To reiterate: The assumption that \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) allows us to test hypotheses about, e.g., \\(\\beta_1\\). If this assumption is violated, then OLS regression is still the best linear unbiased estimator. If, in addition, the assumption that \\(\\epsilon_i\\)’s are homoscedastic and uncorrelated is violated, the OLS regression will no longer be the best linear unbiased estimator, and we would need to seek alternatives, such as weighted OLS regression. Last…if, in addition, the assumption of \\(E[\\epsilon_i] = 0\\) is violated, then using OLS will be a biased estimator. At this point, we would generally conclude that there is a better, nonlinear representation of the data-generating process that we should use. 2.17.1 Correlation: the Strength of Linear Association We can measure the strength of a linear association via the metric of correlation. We define the correlation between a pair of random variables \\(X\\) and \\(Y\\) as \\[ \\rho_{XY} = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{V[X]V[Y]}}\\,, \\] with \\(\\vert \\rho_{XY} \\vert \\leq 1\\). (We will discuss correlation and the related concept of covariance more in depth in Chapter 6. For now, just treat the equation above as a definition. The main thing to keep in mind for now is that if \\(X\\) and \\(Y\\) are independent random variables, \\(\\rho_{XY} = 0\\), while if \\(X = Y\\) or \\(X = -Y\\), \\(\\rho_{XY} = 1\\) and \\(-1\\) respectively.) If instead of a pair of random variables we have a set of tuples, we can still define a correlation, which we can estimate using the Pearson correlation coefficient: \\[ R = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(Y_i-\\bar{Y})}{\\left[\\sum_{i=1}^n (x_i-\\bar{x})^2\\right]\\left[\\sum_{i=1}^n (Y_i-\\bar{Y})^2\\right]} = \\frac{S_{xY}}{\\sqrt{S_{xx}S_{YY}}} = \\hat{\\beta}_1 \\sqrt{\\frac{S_{xx}}{S_{YY}}} \\,. \\] The last equality follows from the fact that \\(\\hat{\\beta}_1 = S_{xY}/S_{xx}\\). It follows from this expression that \\(R\\) and \\(\\hat{\\beta}_1\\) have the same sign. Related to the correlation is the coefficient of determination, or \\(R^2\\), a quantity ranging from 0 to 1 that is interpreted as the proportion of the variation in the dependent variable that is predictable from the independent variable. The closer \\(R^2\\) is to 1, the more strictly linear is the association between \\(x\\) and \\(Y\\). For completeness, we mention “adjusted \\(R^2\\),” as this is an oft-quoted output from R’s linear model function, lm(). (See below for example usage of lm().) Adjusted \\(R^2\\) attempts to correct for the tendency of \\(R^2\\) to over-optimistically indicate the quality of fit of a linear model. There are several formulae for computing adjusted \\(R^2\\); R uses Wherry’s formula: \\[ \\mbox{Adj.}~R^2 = 1 - (1-R^2)\\frac{n-1}{n-p-1} \\,, \\] where \\(p\\) is the number of independent variables (with \\(p = 1\\) for simple linear regression). 2.17.2 The Expected Value of the Sum of Squared Errors (SSE) Here we show that \\(SSE/(n-2)\\) is an unbiased estimator of \\(\\sigma^2\\). This is a non-trivial algebraic exercise, and thus we will leave the calculation of \\(V[\\hat{\\sigma^2}] = V[SSE/(n-2)] = 2\\sigma^4/(n-2)\\) as an exercise to the (masochistic) reader. We start by writing that \\[\\begin{align*} SSE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 &amp;= \\sum_{i=1}^n [Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 \\\\ &amp;= \\sum_{i=1}^n [Y_i - (\\bar{Y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i)]^2 \\\\ &amp;= \\sum_{i=1}^n [(Y_i - \\bar{Y}) - \\hat{\\beta}_1 (x_i-\\bar{x})]^2 \\\\ &amp;= \\sum_{i=1}^n [(Y_i - \\bar{Y})^2 - 2 \\hat{\\beta}_1 (x_i-\\bar{x})(Y_i - \\bar{Y}) + \\hat{\\beta}_1^2(x_i-\\bar{x})^2] \\\\ &amp;= S_{YY} - 2 \\hat{\\beta}_1 S_{xY} + \\hat{\\beta}_1^2 S_{xx} \\\\ &amp;= S_{YY} - 2 \\hat{\\beta}_1 (S_{xx} \\hat{\\beta}_1) + \\hat{\\beta}_1^2 S_{xx} \\\\ &amp;= S_{YY} - \\hat{\\beta}_1^2 S_{xx} \\,. \\end{align*}\\] To find the expected value of this difference, we look first at \\(S_{YY}\\) and then at \\(\\hat{\\beta}_1^2 S_{xx}\\). \\[\\begin{align*} E[S_{YY}] &amp;= E\\left[ \\sum_{i=1}^n (Y_i - \\bar{Y})^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\left(\\beta_0 + \\beta_1x_i + \\epsilon_i - \\beta_0 - \\beta_1 \\bar{x} - \\bar{\\epsilon} \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\left(\\beta_1(x_i -\\bar{x}) + (\\epsilon_i - \\bar{\\epsilon}) \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n \\beta_1^2(x_i -\\bar{x})^2 + \\sum_{i=1}^n 2 \\beta_1 (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon}) + \\sum_{i=1}^n (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\\\ &amp;= E\\left[ \\beta_1^2 S_{xx} + 2 \\beta_1 \\sum_{i=1}^n (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon}) + \\sum_{i=1}^n (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\\\ &amp;= \\beta_1^2 S_{xx} + 2 \\beta_1 \\sum_{i=1}^n (x_i - \\bar{x}) E\\left[ \\epsilon_i - \\bar{\\epsilon} \\right] + \\sum_{i=1}^n E\\left[ (\\epsilon_i - \\bar{\\epsilon})^2 \\right] \\,. \\end{align*}\\] Since \\(E[\\epsilon_i] = E[\\bar{\\epsilon}] = 0\\), the middle term vanishes. As for the right-most term: \\[\\begin{align*} \\sum_{i=1}^n E\\left[ (\\epsilon_i - \\bar{\\epsilon})^2 \\right] &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 - 2\\epsilon_i \\bar{\\epsilon} + \\bar{\\epsilon}^2 \\right] \\\\ &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 \\right] - 2 \\sum_{i=1}^n E\\left[\\epsilon_i \\bar{\\epsilon}\\right] + \\sum_{i=1}^n E\\left[\\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n E\\left[ \\epsilon_i^2 \\right] - 2 E\\left[ \\bar{\\epsilon} \\sum_{i=1}^n \\epsilon_i \\right] + E\\left[ \\sum_{i=1}^n \\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n V[\\epsilon_i] + \\sum_{i=1}^n \\left(E[\\epsilon_i]\\right)^2 - 2 n E\\left[ \\bar{\\epsilon}^2 \\right] + n E\\left[ \\bar{\\epsilon}^2\\right] \\\\ &amp;= \\sum_{i=1}^n \\sigma^2 - n E\\left[ \\bar{\\epsilon}^2 \\right] \\\\ &amp;= n \\sigma^2 - n \\frac{\\sigma^2}{n} = (n-1)\\sigma^2 \\,. \\end{align*}\\] So at this point, we have that \\[ E[S_{YY}] = S_{xx} \\beta_1^2 + (n-1)\\sigma^2 \\,. \\] Now let’s look at \\(E[\\hat{\\beta}_1^2 S_{xx}]\\): \\[\\begin{align*} E\\left[\\hat{\\beta}_1^2 S_{xx}\\right] &amp;= S_{xx} E\\left[\\hat{\\beta}_1^2\\right] \\\\ &amp;= S_{xx} \\left( V\\left[ \\hat{\\beta}_1 \\right] + \\left( E\\left[ \\hat{\\beta}_1 \\right] \\right)^2 \\right) \\\\ &amp;= S_{xx} \\left( \\frac{\\sigma^2}{S_{xx}} + \\beta_1^2 \\right) \\\\ &amp;= \\sigma^2 + S_{xx} \\beta_1^2 \\,. \\end{align*}\\] So \\[ E[SSE] = S_{xx} \\beta_1^2 + (n-1)\\sigma^2 - \\sigma^2 - S_{xx} \\beta_1^2 = (n-2)\\sigma^2 \\,. \\] Thus \\(SSE/(n-2)\\) is an unbiased estimator of \\(\\sigma^2\\). 2.17.3 Linear Regression in R Below, we show the results of running a linear regression using R. The data are the same as used to produce Figure 2.26, with the parameter estimates output by lm() mapping directly to the solid red line in that figure. # lm(): &quot;linear model&quot; # y~x : this is a &quot;model formula&quot; that in words translates as # &quot;regress the response data y upon the predictor data x&quot; # or &quot;estimate beta_0 and beta_1 for the model E[Y] = beta_0 + beta_1 x&quot; lm.out = lm(y~x) # show a summary of the model summary(lm.out) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 The model residuals \\(Y_i - \\hat{Y}_i\\) are summarized via five numbers (minimum, maximum, and median, along with the 25\\(^{\\rm th}\\) and 75\\(^{\\rm th}\\) percentile values). If, e.g., the median is substantially different from zero, it is possible that the assumption that \\(Y \\vert x\\) is normally distributed is violated…and that it is also possible that the assumption \\(E[\\epsilon_i] = 0\\) is violated as well. Under Coefficients, there are four numerical columns. The first shows \\(\\hat{\\beta}_0\\) ((Intercept)) and \\(\\hat{\\beta}_1\\) (x), the second shows the estimated standard errors for these statistics, the third is simply the ratio of the values in the first and second columns, and the fourth is the \\(p\\)-value. The third and fourth columns are distribution-specific: the hypothesis test being carried out has a null of zero, and the t value is assumed to be \\(t\\)-distributed. The \\(p\\)-values in the fourth column are \\(\\ll \\alpha = 0.05\\), which lead us to decide that both the intercept and the slope are truly non-zero. (In our simulation, they are: \\(\\beta_0 = 4\\) and \\(\\beta_1 = 0.5\\).) The Residual standard error shows the value of \\(\\hat{\\sigma} = \\sqrt{SSE/(n-2)}\\), while the “38 degrees of freedom” indicates that \\(n = 40\\). We note that in this simulation, \\(\\sigma^2 = 1\\). The meanings behind the R-squared values are explained above in the correlation example. In words, approximately 60% of the variation exhibited by the data is accounted for with the linear model. Last, the line with the phrase F-statistic shows the result of a hypothesis test in which the null is that \\(\\beta_0 = \\beta_1 = 0\\) and the alternative is that at least one of the coefficients is truly non-zero. The numbers shown will only truly make sense after we cover one-way analysis of variance in the next section. 2.18 One-Way Analysis of Variance In the simple linear regression setting, our data consists of continuously distributed independent variables \\(\\mathbf{x}\\) and dependent variables \\(\\mathbf{Y}\\). What if, instead, the independent variables come in groups? For instance, we might wish to determine the time a person needs to accomplish a task after eating either chocolate or vanilla ice cream. To do this, we might map the eating of chocolate to a particular value of \\(x\\), say \\(x = 0\\), and the eating of vanilla to another value of \\(x\\), say \\(x = 1\\). Then, if we continue to adopt a simple linear regression framework, we have that \\[\\begin{align*} Y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\ E[Y_i \\vert x_i = 0] &amp;= \\beta_0 \\\\ E[Y_i \\vert x_i = 1] &amp;= \\beta_0 + \\beta_1 \\,. \\end{align*}\\] To see if there is a difference in task-accomplishment time between groups, we test the hypotheses \\(H_o: \\beta_1 = 0\\) versus \\(H_a: \\beta_1 \\neq 0\\). As the value of the slope is meaningless, beyond whether or not it is zero (because the mapping from group characteristics to values of \\(x\\) is arbitrary), we would not run a conventional linear regression analysis; rather, if we assume that \\(\\bar{Y} \\vert x_i\\) is normally distributed, we would run a two-sample \\(t\\) test like the Welch’s \\(t\\) test described earlier in this chapter to determine whether \\(\\bar{Y} \\vert x_i=0\\) is drawn from a distribution with the same population mean as \\(\\bar{Y} \\vert x_i=1\\). What if there are more than two groups (or treatments)? For instance, what if we redo the experiment but add strawberry ice cream? When the number of groups is greater than two, we move from the two-sample \\(t\\) test to one-way analysis of variance (or one-way ANOVA; the “one-way” indicates that there is a single independent variable, which in our example is ice cream flavor). See Figure 2.27. Figure 2.27: Illustration of the setting for a one-way analysis of variance. To the left, the spread of the data within each group is large compared to the differences in means between each group, so an ANOVA is less likely to reject the null hypothesis that \\(\\mu_0 = \\mu_1 = \\mu_2\\). To the right, the spread of data within groups is small relative to the differences in means between groups, so an ANOVA is more likely to reject the null hypothesis. Let the index \\(i\\) denote a particular group (chocolate ice-cream eaters, etc.) and let the index \\(j\\) denote different data within the same group. Let \\(n_i\\) denote the sample size within each group, and let \\(k\\) denote the total number of groups. The total sum of squares is defined as the sum of squared differences between \\(Y_i\\) and \\(\\bar{Y}\\), which in this setting can be expressed via a double summation, one over data within a group, and another over groups: \\[ \\mbox{Total SS} = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y})^2 = \\sum_{i=1}^k \\sum_{j=1}^{n_i} (Y_{ij}-\\bar{Y}_{i\\bullet})^2 + \\sum_{i=1}^k n_i(\\bar{Y}_{i\\bullet}-\\bar{Y})^2 = SSE + SST \\,, \\] where \\(\\bar{Y}_{i\\bullet}\\) is the average value for group \\(i\\), \\(SSE\\) has its usual meaning as the sum of squared errors (although here \\(\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) is replaced with the estimated group mean \\(\\bar{Y}_{i\\bullet}\\)), and \\(SST\\) is the sum of squared average treatment effects. If we think through what these values represent, we see that large values of \\(SSE\\) meant hat the data are spread widely within each group, which makes it less likely that we would detect a difference between the group means, whereas if \\(SST\\) is large, the group means are widely separated from the overall mean, making differences easier to detect. This motivates using the following as a test statistic: \\[ \\frac{SST}{SSE} \\,. \\] However, we do not know the distribution of this statistic. What we do know, however, is that under the null \\[\\begin{align*} W_T = \\frac{SST}{\\sigma^2} &amp;\\sim \\chi_{k-1}^2 \\\\ W_E = \\frac{SSE}{\\sigma^2} &amp;\\sim \\chi_{n-k}^2 \\,. \\end{align*}\\] And we know that \\[ F = \\frac{W_1/\\nu_1}{W_2/\\nu_2} \\sim F_{\\nu_1,\\nu_2} \\,, \\] i.e., the ratio of chi-square distributed random variables, each divided by their respective number of degrees of freedom, is \\(F\\)-distributed for \\(\\nu_1\\) numerator and \\(\\nu_2\\) denominator degrees of freedom. Thus \\[ F = \\frac{SST/(k-1)}{SSE/(n-k)} = \\frac{MST}{MSE} \\sim F_{k-1,n-k} \\,. \\] In the ANOVA hypothesis test, the null hypothesis \\(H_o\\) is that \\(E[Y_1] = E[Y_2] = \\cdots = E[Y_k]\\), and the alternative hypothesis is that at least one mean is different from the others. If we test at level \\(\\alpha\\), we reject the null hypothesis if \\(F &gt; F_{1-\\alpha,k-1,n-k}\\). In R, the boundary of the rejection region would be given by qf(1-alpha,k-1,n-k) while the \\(p\\)-value would be given by 1-pf(F.obs,k-1,n-k). 2.18.1 One-Way ANOVA: the Statistical Model In the notes above, we build up the test statistic for the one-way ANOVA, but we do not discuss the underlying statistical model. We can write down the model as \\[ Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij} \\,, \\] where \\(i\\) denotes the treatment group and \\(j\\) denotes an observed datum within group \\(i\\). \\(\\mu\\) is the overall mean response, while \\(\\tau_i\\) is the deterministic effect of treatment in group \\(i\\). (\\(\\tau\\) is the Greek letter tau, which is pronounced “tao” [rhyming with “ow,” the expression of pain]. Recall that by “deterministic” we mean that \\(\\tau_i\\) is not random.) The error terms \\(\\epsilon_{ij}\\) are independent, normally distributed, and homoscedastic with variance \\(\\sigma^2\\). Recall that \\(\\bar{Y}_{i\\bullet}\\) is the sample mean within group \\(i\\). We know it is distributed normally (by assumption) but what are its expected value and variance? The expected value is \\[ E[\\bar{Y}_{i\\bullet}] = E\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} Y_{ij}\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i} E[Y_{ij}] = \\frac{1}{n_i}\\sum_{j=1}^{n_i} E[\\mu + \\tau_i + \\epsilon_{ij}] = \\mu + \\tau_i + E[\\epsilon_{ij}] = \\mu+\\tau_i \\,, \\] while the variance is \\[ V[\\bar{Y}_{i\\bullet}] = V\\left[\\frac{1}{n_i}\\sum_{j=1}^{n_i} Y_{ij}\\right] = \\frac{1}{n_i}\\sum_{j=1}^{n_i^2} V[Y_{ij}] = \\frac{1}{n_i^2}\\sum_{j=1}^{n_i} V[\\mu + \\tau_i + \\epsilon_{ij}] = V[\\epsilon_{ij}] = \\sigma^2 \\,. \\] Thus \\(Y_{ij} \\sim \\mathcal{N}(\\mu+\\tau_i,\\sigma^2)\\). 2.18.2 Linear Regression in R: Redux The last example of the last section showed the output from lm(), R’s linear model function: summary(lm.out) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 In that example, we indicated that the numbers on the line beginning F-statistic would make more sense after we covered one-way analysis of variance. There are two coefficients in this model, which is analogous to two “treatment groups.” Hence \\(k = 2\\), \\(SST/\\sigma^2 \\sim \\chi_{k-1=1}^2\\), and \\(SSE/\\sigma^2 \\sim \\chi_{n-k=38}^2\\). This explains on 1 and 38 DF. The \\(F\\) statistic is \\(MST/MSE = 61.76\\), which under the null is sampled from an \\(F\\) distribution with 1 numerator and 38 denominator degrees of freedom. This is an extreme value under the null, as indicated by the \\(p\\)-value 1.749 \\(\\times 10^{-9}\\)…so we conclude that either the intercept \\(\\beta_0\\) or the slope \\(\\beta_1\\) or both are non-zero. But there’s more. To access more information about the \\(F\\) test that is carried out, we can call the anova() function. anova(lm.out) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 59.208 59.208 61.756 1.749e-09 *** ## Residuals 38 36.432 0.959 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This function call outputs information we could not see above: the \\(SST\\) (the first row of Sum Sq), \\(SSE\\) (second row), \\(MSE\\) (the first row of Mean Sq), and \\(MST\\) (second row). The F value is the ratio \\(MSE/MST\\); here, that is 59.208/0.959 = 61.756. "],["the-binomial-and-related-distributions.html", "3 The Binomial (and Related) Distributions 3.1 Motivation 3.2 Probability Mass Function 3.3 Cumulative Distribution Function 3.4 Linear Functions of Binomial Random Variables 3.5 Order Statistics 3.6 Point Estimation 3.7 Confidence Intervals 3.8 Hypothesis Testing 3.9 Logistic Regression 3.10 Naive Bayes Regression 3.11 The Beta Distribution 3.12 The Multinomial Distribution 3.13 Chi-Square-Based Hypothesis Testing", " 3 The Binomial (and Related) Distributions 3.1 Motivation Let’s assume we are holding a coin. It may be fair (meaning that the probabilities of observing heads or tails in a given flip of the coin are each 0.5)…or perhaps not. We decide that we are going to flip the coin some fixed number of times \\(k\\), and we will record the outcome of each flip: \\[ \\mbox{Results of each flip:}~~\\mathbf{Y} = \\{Y_1,Y_2,\\ldots,Y_k\\} \\,, \\] where, e.g., \\(Y_1 = 1\\) if we observe heads or \\(0\\) if we observe tails. This is an example of a Bernoulli process, where “process” denotes a sequence of observations, and “Bernoulli” indicates that there are two possible discrete outcomes. A binomial experiment is one that generates Bernoulli process data through the running of \\(k\\) trials (e.g., \\(k\\) separate coin flips). The properties of such an experiment are as follows: The number of trials \\(k\\) is fixed in advance. Each trial has two possible outcomes, generically denoted as \\(S\\) (success) or \\(F\\) (failure). The probability of success remains \\(p\\) throughout the experiment. The outcome of each trial is independent of the outcomes of the others. The random variable of interest for a binomial experiment is the number of observed successes. A closely related alternative to a binomial experiment is a negative binomial experiment, wherein the number of successes \\(s\\) is fixed in advance, instead of the number of trials \\(k\\), and the random variable of interest is the number of failures that we observe before achieving \\(s\\) successes. A simple example would be flipping a coin until \\(s\\) heads are observed and recording the overall number of tails that are seen. As a side note to the third point above, about the probability of success remaining \\(p\\) throughout the experiment: binomial and negative binomial experiments rely on sampling with replacement…if we observe a head for a given coin flip, we can indeed observe heads again in the future. In the real world, however, the reader will observe instances where, e.g., a binomial distribution is used to model experiments featuring sampling without replacement: we have \\(K = 100\\) widgets, of which ten are defective; we check one to see if it is defective (with probability \\(p = 0.1\\)) and set it aside, then check another (with probability either 10/99 or 9/99, depending on the outcome of the first trial), etc. The rule-of-thumb for using the binomial distribution to model data in such a situation is that it is fine if the number of trials \\(k \\lesssim K/10\\). However, in the age of computers, there is no reason to apply the binomial distribution when we can apply the hypergeometric distribution instead. We will return to this point later in this chapter. As a side note to the fourth point above, about the outcome of each trial being independent of the outcomes of the others: in a general process, each datum can be dependent on the data observed previously. How each datum is dependent on previous data defines the type of process that is observed: Markov processes, Gaussian processes, etc. A Bernoulli process is said to be a memoryless process and it is comprised of iid data. 3.2 Probability Mass Function Let’s focus first on the outcome of a binomial experiment, with the random variable \\(X\\) being the number of observed successes in \\(k\\) trials. What is the probability of observing \\(X=x\\) successes, if the probability of observing a success in any one trial is \\(p\\)? \\[\\begin{align*} \\mbox{$x$ successes}&amp;: p^x \\\\ \\mbox{$k-x$ failures}&amp;: (1-p)^{k-x} \\,. \\end{align*}\\] So \\(P(X=x) = p^x (1-p)^{k-x}\\)…but, no, this isn’t right. Let’s think this through. Assume \\(k = 2\\). The sample space of possible experimental outcomes is \\[ \\Omega = \\{ SS, SF, FS, FF \\} \\,. \\] If \\(p\\) = 0.5, then we can see that the probability of observing one success in two trials is 0.5…but our equation tells us that \\(P(X=1) = (0.5)^1 (1-0.5)^1 = 0.25\\). What are we missing? We are missing that there are two ways of observing a single success…and we need to count both. Because we ultimately do not care about the order in which successes and failures are observed, we utilize counting via combination: \\[ \\binom{k}{x} = \\frac{k!}{x! (k-x)!} \\,, \\] where the exclamation point represents the factorial function \\(x! = x(x-1)(x-2)\\cdots 1\\). So now we can correctly write down the probability \\(P(X=x)\\): \\[ P(X=x) = \\binom{k}{x} p^x (1-p)^{k-x} ~~~ x \\in \\{0,\\ldots,k\\} \\,. \\] This is the binomial probability mass function. We denote the distribution of the random variable \\(X\\) as \\(X \\sim\\) Binomial(\\(k\\),\\(p\\)). Note a special case: if \\(k = 1\\), the distribution is dubbed a Bernoulli distribution. Recall: a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) \\(0 \\leq P(X=x) \\leq 1\\) and (b) \\(\\sum_x P(X=x) = 1\\), where the sum is over all possible values of \\(x\\). (Note that it is conventional to denote the number of trials as \\(n\\), not \\(k\\). However, \\(n\\) is also conventionally used to denote the sample size in any experiment, so to avoid confusion, we use \\(k\\) to denote the number of trials here.) In Figure 3.1, we display three binomial probability mass functions, one each for success probabilities 0.1 (red, to the left), 0.5 (green, to the center), and 0.8 (blue, to the right). This figure indicates an important aspect of the binomial pmf, namely that it can attain a shape akin to that of a normal distribution, if \\(p\\) is such that any truncation observed at values \\(x=0\\) or at \\(x=k\\) is not apparent. In fact, a binomial random variable converges in distribution to a normal random variable in certain limiting situations, as we indicate in an example below. Figure 3.1: Binomial probability mass functions for number of trials \\(k = 10\\) and success probabilities \\(p = 0.1\\) (red), 0.5 (green), and 0.8 (blue). Recall: the expected value of a discretely distributed random variable is \\[ E[X] = \\sum_x x P(X=x) = \\sum_x x p_X(x) \\,, \\] where the sum is over all \\(x\\) for which \\(P(X = x) &gt; 0\\). For the binomial distribution, the expected value is \\[ E[X] = \\sum_{x=0}^k x \\binom{k}{x} p^x (1-p)^{k-x} \\,. \\] At first, this does not appear to be easy to evaluate. One possibility is to utilize the binomial theorem, \\[ (x+y)^k = \\sum_{i=0}^k \\binom{k}{x} x^i y^{k-i} \\,, \\] but this will not help here, as the \\(x\\) in the expression above for \\(E[X]\\) is not raised to a power. Another trick in our arsenal is to pull constants out of the summation such that whatever is left as the summand is a pmf (and thus sums to 1). Let’s try this here: \\[\\begin{align*} E[X] &amp;= \\sum_{x=0}^k x \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=1}^k x \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=1}^k \\frac{k!}{(x-1)!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= kp \\sum_{x=1}^k \\frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\,. \\end{align*}\\] The summation appears almost like that of a binomial random variable. Let’s set \\(y = x-1\\). Then \\[\\begin{align*} E[X] &amp;= kp \\sum_{x=1}^k \\frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\\\ &amp;= kp \\sum_{y=0}^{k-1} \\frac{(k-1)!}{y!(k-(y+1))!} p^y (1-p)^{k-(y+1)} \\\\ &amp;= kp \\sum_{y=0}^{k-1} \\frac{(k-1)!}{y!((k-1)-y)!} p^y (1-p)^{(k-1)-y} \\,. \\end{align*}\\] The summand is now the pmf for the random variable \\(Y \\sim\\) Binomial(\\(k-1\\),\\(p\\)), summed over all values of \\(y\\) in the domain of the distribution. Thus the summation evaluates to 1, and thus \\(E[X] = kp\\). Binomial Distribution - R Functions quantity R function call PMF dbinom(x,k,p) CDF pbinom(x,k,p) Inverse CDF qbinom(q,k,p) \\(n\\) iid random samples rbinom(n,k,p) A negative binomial experiment is governed by the negative binomial distribution, whose pmf is \\[ p_X(x) = \\binom{x+s-1}{x} p^s (1-p)^x ~~~ x \\in \\{0,1,\\ldots,\\infty\\} \\] The form of this pmf follows from the fact that the underlying Bernoulli process would consist of \\(x+s\\) data, with the last datum being the observed success that ends the experiment. The first \\(x+s-1\\) data would feature \\(s-1\\) successes and \\(x\\) failures, with the order of success and failure not mattering…so we can view these data as being binomially distributed (albeit with \\(x\\) represent failures…hence the “negative” in negative binomial!): \\[ p_X(x) = \\underbrace{\\binom{x+s-1}{x} p^{s-1} (1-p)^x}_{\\mbox{first $x+s-1$ trials}} \\cdot \\underbrace{p}_{\\mbox{last trial}} \\,. \\] Note the special case of \\(s = 1\\): this is dubbed the geometric distribution. Figure 3.2: Negative binomial probability mass functions for the number of successes \\(s = 2\\) and success probabilities \\(p = 0.7\\) (red), 0.5 (green), and 0.2 (blue). Negative Binomial Distribution - R Functions quantity R function call PMF dnbinom(x,s,p) CDF pnbinom(x,s,p) Inverse CDF qnbinom(q,s,p) \\(n\\) iid random samples rnbinom(n,s,p) 3.2.1 Binomial Distribution: Normal Approximation In certain limiting situations, a binomial random variable converges in distribution to a normal random variable, i.e., if \\[ P\\left(\\frac{X-\\mu}{\\sigma} &lt; a \\right) = P\\left(\\frac{X-kp}{\\sqrt{kp(1-p)}} &lt; a \\right) \\approx P(Z &lt; a) = \\Phi(a) \\,, \\] then we can state that at least approximately \\(X \\sim \\mathcal{N}(kp,kp(1-p))\\). Now, what do we mean by “certain limiting situations”? For instance, if \\(p\\) is close to zero or one, then the binomial distribution is truncated at 0 or at \\(k\\), and the shape of the pmf does not appear to be like that of a normal pdf. One conventional rule-of-thumb is that the normal approximation is adequate if \\[ k &gt; 9\\left(\\frac{\\mbox{max}(p,1-p)}{\\mbox{min}(p,1-p)}\\right) \\,. \\] The reader might question why we would mention this approximation at all: if we have binomially distributed data and a computer, then we need not ever utilize such an approximation to, e.g., compute probabilities. This point is correct (and is the reason why, for instance, we do not mention the so-called continuity correction here; our goal is not to compute probabilities). The reason we mention this is that, as we will see, this approximation underlies a commonly used hypothesis test framework, and thus needs to be mentioned for completeness. 3.2.2 Variance of a Binomial Random Variable The variance of a random variable is given by the shortcut formula that we have been using since Chapter 1: \\(V[X] = E[X^2] - (E[X])^2\\). So we would expect that we would need to compute \\(E[X^2]\\) here, since we already know that \\(E[X] = kp\\). But for reasons that will become apparent below, it is actually far easier for us to compute \\(E[X(X-1)]\\) and work with that to eventually derive the variance: \\[\\begin{align*} E[X(X-1)] &amp;= \\sum_{x=0}^k x(x-1) \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=0}^k x(x-1) \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=2}^k \\frac{k!}{(x-2)!(k-x)!} p^x (1-p)^{k-x} \\\\ &amp;= k(k-1) p^2 \\sum_{x=2}^k \\frac{(k-2)!}{(x-2)!(k-x)!} p^{x-2} (1-p)^{k-x} \\,. \\end{align*}\\] The advantage to using \\(x(x-1)\\) was that it matches the first two terms of \\(x! = x(x-1)\\cdots(1)\\), allowing easy cancellation. If we set \\(y = x-2\\), we find that the summand above will, in a similar manner as in the calculation of \\(E[X]\\), become the pmf for the random variable \\(Y \\sim\\) Binomial(\\(k-2,p\\))…and thus the summation will evaluate to 1. So \\(E[X(X-1)] = E[X^2] - E[X] = k(k-1)p^2\\), and \\(E[X^2] = k^2p^2-kp^2 + kp = V[X] + (E[X])^2\\), and \\(V[X] = k^2p^2-kp^2+kp-k^2p^2 = kp-kp^2 = kp(1-p)\\). Done. 3.2.3 The Expected Value of a Negative Binomial Random Variable The calculation for the expected value \\(E[X]\\) for a negative binomial random variable is similar to that for a binomial random variable: \\[\\begin{align*} E[X] &amp;= \\sum_{x=0}^{\\infty} x \\binom{x+s-1}{x} p^s (1-p)^x \\\\ &amp;= \\sum_{x=0}^{\\infty} x \\frac{(x+s-1)!}{(s-1)!x!} p^s (1-p)^x \\\\ &amp;= \\sum_{x=1}^{\\infty} \\frac{(x+s-1)!}{(s-1)!(x-1)!} p^s (1-p)^x \\,. \\end{align*}\\] Let \\(y = x-1\\). Then \\[\\begin{align*} E[X] &amp;= \\sum_{y=0}^{\\infty} \\frac{(y+s)!}{(s-1)!y!} p^s (1-p)^{y+1} \\\\ &amp;= \\sum_{y=0}^{\\infty} s(1-p) \\frac{(y+s)!}{s!y!} p^s (1-p)^y \\\\ &amp;= \\sum_{y=0}^{\\infty} \\frac{s(1-p)}{p} \\frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\\\ &amp;= \\frac{s(1-p)}{p} \\sum_{y=0}^{\\infty} \\frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\\\ &amp;= \\frac{s(1-p)}{p} \\,. \\end{align*}\\] The summand is that of a negative binomial distribution for \\(s+1\\) successes, hence the summation is 1, and thus \\(E[X] = s(1-p)/p\\). 3.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as \\(F_X(x) = \\sum_{y\\leq x} p_Y(y)\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). For the binomial distribution, the cdf is \\[ F_X(x) = \\sum_{y=0}^{\\lfloor x \\rfloor} p_Y(y) = \\sum_{y=0}^{\\lfloor x \\rfloor} \\binom{k}{y} p^y (1-p)^{k-y} \\,, \\] where \\(\\lfloor x \\rfloor\\) denotes the largest integer that is less than or equal to \\(x\\) (e.g., if \\(x\\) = 6.75, \\(\\lfloor x \\rfloor\\) = 6). In general, there is no closed-form representation for the binomial cdf (i.e., one cannot replace the summation with a formula). Also, because a pmf is defined at discrete values of \\(x\\), its associated cdf is a step function, as illustrated in the left panel of Figure 3.3. As we can see in this figure, the cdf steps up at each value of \\(x\\) in the domain of \\(p_X(x)\\), and unlike the case for continuous distributions, the form of the inequalities in a probabilistic statement matter: \\(P(X &lt; x)\\) and \\(P(X \\leq x)\\) may not be the same, if \\(x\\) is in the domain of \\(p_X(x)\\). Recall: an inverse cdf function \\(F_X^{-1}(\\cdot)\\) takes as input the total probability \\(q \\in [0,1]\\) in the range \\((-\\infty,x]\\) and returns the value of \\(x\\). A discrete distribution has no unique inverse cdf; it is convention to utilize the generalized inverse cdf, \\(x = F_X^{-1}(q) = \\mbox{inf}\\{x : F_X(x) \\geq q\\}\\), where “inf” indicates the return the smallest value of \\(x\\) such that \\(F_X(x) \\geq q\\). In the right panel of Figure 3.3, we display the inverse cdf for the same distribution used to generate the figure in the left panel (\\(k=4\\) and \\(p=0.5\\)). Like the cdf, the inverse cdf for a discrete distribution is a step function. Below, in an example, we show how we adapt the inverse transform sampler algorithm of Chapter 1 to accommodate the step-function nature of an inverse cdf. Figure 3.3: Illustration of the cumulative distribution function \\(F_X(x)\\) (left) and inverse cumulative distribution function \\(F_X^{-1}(q)\\) (right) for a binomial distribution with number of trials \\(k = 4\\) and probability of success \\(p=0.5\\). 3.3.1 Computing Probabilities Because there is no closed-form expression for the binomial cdf, and because computing the binomial pmf for a range of values of \\(x\\) can be laborious, we almost always utilize R shortcut functions from the beginning when computing probabilities. If \\(X \\sim\\) Binomial(10,0.6), which is \\(P(4 \\leq X &lt; 6)\\)? We first note that due to the form of the inequality, we do not include \\(X=6\\) in the computation. Thus \\(P(4 \\leq X &lt; 6) = p_X(4) + p_X(5)\\), which equals \\[ \\binom{10}{4} (0.6)^4 (1-0.6)^6 + \\binom{10}{5} (0.6)^5 (1-0.6)^5 \\,. \\] Even computing this is unnecessarily laborious; instead, we call on R: dbinom(4,size=10,prob=0.6) + dbinom(5,size=10,prob=0.6) ## [1] 0.3121349 We can also utilize cdf functions here: \\(P(4 \\leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \\leq 5) - P(X \\leq 3) = F_X(5) - F_X(3)\\), which in R is computed via pbinom(5,size=10,prob=0.6) - pbinom(3,size=10,prob=0.6) ## [1] 0.3121349 \\(X \\sim\\) Binomial(10,0.6), what is the value of \\(a\\) such that \\(P(X \\leq a) = 0.9\\)? First, we set up the inverse cdf formula: \\[ P(X \\leq a) = F_X(a) = 0.9 ~~ \\Rightarrow ~~ a = F_X^{-1}(0.9) \\] Note that we didn’t do anything differently here than we would have done in a continuous distribution setting…and we can proceed directly to R because it utilizes the generalized inverse cdf algorithm. qbinom(0.9,size=10,prob=0.6) ## [1] 8 3.3.2 Sampling Data While we would always utilize R shortcut functions like rbinom() when they exist, there may be instances when we need to code our own functions for sampling data from discrete distributions. The code below shows such a function for an arbitrary probability mass function. x &lt;- c(1,2,4,8) # domain of x p.x &lt;- c(0.2,0.35,0.15,0.3) # p(x) F.x &lt;- cumsum(p.x) # cumulative sum -&gt; produces F(x) set.seed(235) n &lt;- 100 q &lt;- runif(n) # we still ultimately need runif! i &lt;- findInterval(q,F.x)+1 # the output is [0,3] and not [1,4] # 0 means q is between Fx[1] and Fx[2], etc. x.sample &lt;- x[i] # Plotting the output is a bit tricky: we override the default definition of breaks df.sample &lt;- data.frame(x.sample=x.sample) df.true &lt;- data.frame(x=x-0.5,xend=x+0.5,y=p.x,yend=p.x) ggplot(data=df.sample,aes(x=x.sample,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0.5,8.5,by=1)) + geom_segment(data=df.true,aes(x=x,xend=xend,y=y,yend=yend),col=&quot;red&quot;,lwd=1) + labs(x=&quot;x&quot;) + coord_cartesian(xlim=c(0,9)) + theme(axis.title=element_text(size = rel(1.25))) Figure 3.4: Histogram of \\(n = 100\\) iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of \\(x\\). 3.4 Linear Functions of Binomial Random Variables Let’s assume we are given \\(n\\) iid binomial random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Binomial(\\(k\\),\\(p\\)). Can we determine the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Yes, we can…via the method of moment-generating functions. Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(E[e^{tX}]\\). If \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) os the mgf for a known family of distributions, then we can immediately identify the distribution of \\(Y\\) and the parameters of that distribution. The mgf for the binomial distribution is \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^k e^{tx} \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\sum_{x=0}^k \\binom{k}{x} (pe^t)^x (1-p)^{k-x} \\,. \\end{align*}\\] Here we do utilize the binomial theorem (unlike when we were computing \\(E[X]\\)), \\[ (x+y)^k = \\sum_{i=0}^k \\binom{k}{x} x^i y^{k-i} \\,, \\] to write down that \\[ m_X(t) = [pe^t + (1-p)]^k \\,, \\] or \\((pe^t+q)^k\\), where \\(q = 1-p\\). The mgf for \\(Y = \\sum_{i=1}^n X_i\\) is thus \\[ m_Y(t) = (pe^t + (1-p))^{k} (pe^t + (1-p))^{k} \\cdots (pe^t + (1-p))^{k} = (pe^t + (1-p))^{nk}\\,. \\] We can see that this has the form of a binomial mgf, and furthermore that \\(Y \\sim\\) Binomial(\\(nk\\),\\(p\\)), with expected value \\(E[Y] = nkp\\) and variance \\(V[Y] = nkp(1-p)\\). This makes sense, as the act of summing binomial data is equivalent to concatenating \\(n\\) separate Bernoulli processes into one longer Bernoulli process…whose data can subsequently be modeled using a binomial distribution. While we can identify the distribution of the sum, we cannot identify the distribution of the sample mean. To see this, we will attempt to use the mgf method again, this time plugging in \\(a_i = 1/n\\) instead of \\(a_i = 1\\). We find that \\[ m_{\\bar{X}}(t) = (pe^{t/n} + (1-p))^{nk} \\,. \\] Changing \\(t\\) to \\(t/n\\) has the effect of creating an mgf that does not have the form of any known mgf…thus we cannot identify the family of distribution for \\(\\bar{X}\\). To learn more about the distribution of \\(\\bar{X}\\), we have two alternatives: If \\(n \\gtrsim 30\\), we can utilize the Central Limit Theorem to state that \\[ \\bar{X} \\stackrel{d}{\\rightarrow} X&#39; \\sim \\mathcal{N}\\left(kp,\\frac{kp(1-p)}{n}\\right) \\,, \\] i.e., that the random variable \\(\\bar{X}\\) converges in distribution to a normal random variable with mean \\(E[\\bar{X}] = \\mu = kp\\) and variance \\(V[\\bar{X}] = \\sigma^2/n = kp(1-p)/n\\). We can perform a general transformation to express the probability mass function for \\(\\bar{X}\\) (i.e., so as to provide all the values of \\(p_{\\bar{X}}(\\bar{x})\\), which is not a “named” distribution). We show how to derive this pmf for \\(\\bar{X}\\) in an example below; with it, we can tackle computing confidence intervals and performing hypothesis tests for the population mean. (We note in passing that we cannot utilize this same technique to easily determine the pmf for the sample variance \\(S^2\\); if we wish to construct confidence intervals, etc., for the population variance, we have to fall back on simulations.) 3.4.1 The Moment-Generating Function for a Geometric Random Variable Recall that a geometric distribution is equivalent to a negative binomial distribution with number of successes \\(s = 1\\); the probability mass function is thus \\[ p_X(x) = p (1-p)^x \\,, \\] with \\(x = \\{0,1,\\ldots\\}\\) and \\(p \\in [0,1]\\). The moment-generating function for a geometric random variable is \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^{\\infty} e^{tx} p (1-p)^x \\\\ &amp;= p \\sum_{x=0}^\\infty e^{tx} (1-p)^x \\\\ &amp;= p \\sum_{x=0}^\\infty [e^t(1-p)]^x \\\\ &amp;= p \\frac{1}{1-e^t(1-p)} \\,. \\end{align*}\\] The jump from the penultimate to the last line follows from knowing the formula for the sum of an infinite geometric series. We note that in the same way that the sum of Bernoulli random variables is binomially distributed (for \\(n\\) trials), the sum of geometric random variables is negative binomially distributed (for \\(s\\) successes). That means that the mgf for a negative binomial random variable is \\[ m_Y(t) = \\prod_{i=1}^s m_{X_i}(t) = \\prod_{i=1}^s \\frac{p}{1-e^t(1-p)} = \\left(\\frac{p}{1-e^t(1-p)}\\right)^s \\,. \\] We also note that it is far easier to derive the mgf for the negative binomial in this fashion than via direct computation of \\(E[e^{tX}]\\). 3.4.2 The Probability Mass Function for the Sample Mean Let’s assume we are given \\(n\\) iid binomial random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Binomial(\\(k\\),\\(p\\)). As we observe above, the distribution of the sum \\(Y = \\sum_{i=1}^k X_i\\) is binomial with mean \\(nkp\\) and variance \\(nkp(1-p)\\). The sample mean is \\(\\bar{X} = Y/n\\), and so \\[ F_{\\bar{X}}(\\bar{x}) = P(\\bar{X} \\leq \\bar{x}) = P(Y \\leq n\\bar{x}) = \\sum_{y=0}^{n\\bar{x}} p_Y(y) \\,. \\] (We note that \\(n\\bar{x}\\) is integer-valued by definition; we do not need to round down here.) Because we are dealing with a pmf, we cannot simply take the derivative of \\(F_{\\bar{X}}(\\bar{x})\\) to find \\(f_{\\bar{X}}(\\bar{x})\\)…but what we can do is assess the jump in the cumulative distribution function at each step, because that is the pmf. In other words, we can compute \\[ f_{\\bar{X}}(\\bar{x}) = P(Y \\leq n\\bar{x}) - P(Y \\leq n\\bar{x}-1) \\] and store this as a numerically expressed pmf for \\(\\bar{X}\\). See Figure 3.5. But it turns out we can say more about this pmf, by looking at the problem in a different way. We know that \\(Y \\sim\\) Binomial\\((nkp,nkp(1-p))\\) and thus that \\(Y \\in [0,1,\\ldots,nk]\\). When we compute the quotient \\(\\bar{X} = Y/n\\), all we are doing is redefining the domain of the pmf from being \\([0,1,\\ldots,nk]\\) to being \\([0,1/n,2/n,\\ldots,k]\\). We do not actually change the probability masses! So we can write \\[ p_{\\bar{X}}(\\bar{x}) = \\binom{nk}{n\\bar{x}} p^{n\\bar{x}} (1-p)^{nk-n\\bar{x}} ~~ \\bar{x} \\in [0,1/n,2/n,\\ldots,k] \\,. \\] This pmf has the functional form of a binomial pmf…but not the domain of a binomial pmf. For that reason, we cannot say that \\(\\bar{X}\\) is binomially distributed. The pmf has a functional form, it has a domain, but it has no known “name” and thus has no tabulated properties that one can just look up when doing calculations. (However, we do know that the expected value is \\(E[\\bar{X}] = \\mu = kp\\) and the variance is \\(V[\\bar{X}] = \\sigma^2/n = kp(1-p)/n\\).) k &lt;- 10 n &lt;- 10 # we perform n = 10 sets of k = 10 coin flips p &lt;- 0.6 # assume the coin is slightly unfair x.bar &lt;- 0:(n*k+1)/k p.x.bar &lt;- dbinom(n*x.bar,size=n*k,prob=p) df &lt;- data.frame(x.bar=x.bar,p.x.bar=p.x.bar) ggplot(data=df,aes(x=x.bar,y=p.x.bar)) + geom_point(col=&quot;blue&quot;,size=4) + labs(x=&quot;x.bar&quot;,y=expression(p[X]*&quot;(x.bar)&quot;)) + coord_cartesian(xlim=c(4,8)) + theme(axis.title=element_text(size = rel(1.25))) Figure 3.5: Probability mass function for the sample mean of \\(n = 10\\) iid binomial random variables, for \\(k = 10\\) and \\(p = 0.6\\). 3.5 Order Statistics Let’s suppose that we have sampled \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) from some arbitrary distribution. Previously, we have summarized such data with the sample mean and the sample variance. However, there are other summary statistics, some of which are only calculable if we sort the data into ascending order: \\(\\{X_{(1)},\\ldots,X_{(n)}\\}\\). These are dubbed order statistics and the \\(j^{th}\\) order statistic is the sample’s \\(j^{th}\\) smallest value (i.e., the smallest-valued datum in the sample is \\(X_{(1)}\\) and the largest-valued datum is \\(X_{(n)}\\)). Examples of statistics based on ordering include \\[\\begin{align*} \\mbox{Range:}&amp; ~~X_{(n)} - X_{(1)} \\\\ \\mbox{Median:}&amp; ~~X_{(n+1)/2} ~ \\mbox{if $n$ is odd} \\\\ &amp; ~~(X_{n/2}+X_{(n+1)/2})/2 ~ \\mbox{if $n$ is even} \\,. \\end{align*}\\] The most important point to keep in mind is that the probability mass and density functions for order statistics differ from the pmfs and pdfs for their constituent data. For instance, if we sample \\(n\\) data from a \\(\\mathcal{N}(0,1)\\) distribution, we would not expect the minimum value to be distributed the same way; if anything, the mean should become more negative, and the variance should decrease, as \\(n\\) increases. So: why are we discussing order statistics here, in the middle of a discussion of the binomial distribution? It is because we can derive, e.g., the pdf for an order statistic of a continuous distribution using the binomial pmf. Figure 3.6: If we have, e.g., a probability density function \\(f_X(x)\\) whose domain is \\([a,b]\\), and we view success as sampling a datum less than a given value \\(x\\), then the number of data sampled to the left of \\(x\\) is a binomial random variable with number of trials \\(k\\) and success probability \\(p = F_X(x)\\). See Figure 3.6. Without loss of generality, we can assume that \\(f_X(x) &gt; 0\\) for \\(x \\in [a,b]\\) and that we sample \\(n\\) data from this distribution. The number of data \\(X\\) that have value less than some arbitrarily chosen \\(x\\) is a random variable: \\[ Y \\sim \\mbox{Binomial}(n,p=F_X(x)) \\] What is the probability that the \\(j^{th}\\) ordered datum has a value \\(\\leq x\\)? That’s equivalent to asking for the probability that \\(Y \\geq j\\), i.e., did we see at least \\(j\\) successes in \\(n\\) trials? \\[ P(X_{(j)} \\leq x) = P(Y \\geq j) = \\sum_{i=j}^n \\binom{n}{i} [F_X(x)]^j [1 - F_X(x)]^{n-j} \\,. \\] This expression defines the cdf for the \\(j^{th}\\) ordered datum: \\(F_{(j)}(x)\\). Recall: a continuous distribution’s pdf is the derivative of its cdf. Leaving aside algebraic details, we can write down the pdf for \\(X_{(j)}\\): \\[ f_{(j)}(x) = \\frac{d}{dx}F_{(j)}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j} \\,, \\] and write down simplified expressions for the pdfs for the minimum and maximum data values: \\[ f_{(1)}(x) = n f_X(x) [1 - F_X(x)]^{n-1} ~~\\mbox{and}~~ f_{(n)}(x) = n f_X(x) [F_X(x)]^{n-1} \\,. \\] 3.5.1 Distribution of the Minimum Value Sampled from an Exponential Distribution The probability density function for an exponential random variable is \\[ f_X(x) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right) \\,, \\] for \\(x \\geq 0\\) and \\(\\theta &gt; 0\\), and the expected value of \\(X\\) is \\(E[X] = \\theta\\). What is the pdf for the smallest value among \\(n\\) iid data sampled from an exponential distribution? What is the expected value for the smallest value? First, if we do not immediately recall the cumulative distribution function \\(F_X(x)\\), we derive it: \\[ F_X(x) = \\int_0^x \\frac{1}{\\theta} e^{-y/\\theta} dy = 1 - e^{-x/\\theta} \\,. \\] Then we plug into the expression of the pdf of the minimum datum as given above: \\[\\begin{align*} f_{(1)}(x) &amp;= n \\frac{1}{\\theta} e^{-x/\\theta} \\left[ 1 - (1-e^{-x/\\theta}) \\right]^{n-1} \\\\ &amp;= n \\frac{1}{\\theta} e^{-x/\\theta} e^{-(n-1)x/\\theta} \\\\ &amp;= \\frac{n}{\\theta} e^{-nx/\\theta} \\,. \\end{align*}\\] The expected value is thus \\[ E[X_{(1)}] = \\int_0^\\infty x \\frac{n}{\\theta} e^{-nx/\\theta} dx \\,. \\] We recognize this as almost having the form of a gamma-function integral: \\[ \\Gamma(u) = \\int_0^\\infty x^{u-1} e^{-x} dx \\,. \\] We affect a variable transformation \\(y = nx/\\theta\\); for this transformation, \\(dy = (n/\\theta)dx\\), and if \\(x = 0\\) or \\(\\infty\\), \\(y = 0\\) or \\(\\infty\\) (meaning the integral bounds are unchanged). Our new integral is \\[ E[X] = \\int_0^\\infty \\frac{\\theta y}{n} \\frac{n}{\\theta} e^{-y} \\frac{\\theta}{n} dy = \\frac{\\theta}{n} \\int_0^\\infty y e^{-y} dy = \\frac{\\theta}{n} \\Gamma(2) = \\frac{\\theta}{n} 1! = \\frac{\\theta}{n} = \\frac{E[X]}{n} \\,. \\] 3.5.2 Distribution of the Median Value Sampled from a Uniform(0,1) Distribution The probability density function for a Uniform(0,1) distribution is \\[ f_X(x) = 1 \\] for \\(x \\in [0,1]\\). The cdf for this distribution is thus \\[ F_X(x) = \\int_0^x 1 dy = x \\,. \\] Let’s assume that we sample \\(n\\) iid data from this distribution, where \\(n\\) is an odd number. The index of the median value is thus \\((n+1)/2\\), and if we plug into the general expression for the pdf of the \\(m^{th}\\) ordered datum, we find that \\[\\begin{align*} f_{(n+1)/2} &amp;= \\frac{n!}{\\left(\\frac{n+1}{2}-1\\right)!\\left(n - \\frac{n+1}{2}\\right)!} \\cdot 1 \\cdot x^{\\left(\\frac{n+1}{2}\\right)-1} \\cdot (1-x)^{n - \\left(\\frac{n+1}{2}\\right)} \\\\ &amp;= \\frac{n!}{2\\left(\\frac{n-1}{2}\\right)!} x^{\\left(\\frac{n-1}{2}\\right)} (1-x)^{\\left(\\frac{n-1}{2}\\right)} \\,. \\end{align*}\\] This is a beta distribution with parameters \\(\\alpha = \\beta = (n+1)/2\\). The median value has expected value 1/2 and a variance that shrinks with \\(n\\). 3.6 Point Estimation In the first two chapters, we introduced a number of concepts related to point estimation, the act of using statistics to make inferences about a population parameter \\(\\theta\\). These concepts include assessing estimator bias, variance, mean-squared error, and consistency; utilizing the metric of Fisher information to determine the lower bound on the variance for unbiased estimators (the Cramer-Rao Lower Bound, or CRLB); and defining estimators via the maximum likelihood algorithm, which generates estimators that are at least asymptotically unbiased and at least asymptotically reach the CRLB, and which converge in distribution to normal random variables. We will review these concepts in the context of estimating population quantities for binomial distributions below, in the body of the text and in examples. Here, we introducing another means by which to define an estimator. The minimum variance unbiased estimator (or MVUE) is the estimator that has the smallest variance among all unbiased estimators of \\(\\theta\\). (The MVUE may achieve the CRLB, but is not required to; if it doesn’t, we know that there is no estimator that achieves the CRLB. On the other hand, if we have an unbiased estimator that achieves the CRLB, we know that that estimator is the MVUE!) The reader’s first thought might be “well, why didn’t we use this estimator in the first place…after all, MLE is not guaranteed to yield an unbiased estimator, so why have we deferred discussing the MVUE?” The primary reasons are that MVUEs are sometimes not definable (i.e., one can reach an insurmountable roadblock when carrying out the mathematical derivation), and unlike MLEs, they do not abide by the invariance property. (For instance, if \\(\\hat{\\theta}_{MLE} = \\bar{X}\\), then \\(\\hat{\\theta^2}_{MLE} = \\bar{X}^2\\), but if \\(\\hat{\\theta}_{MVUE} = \\bar{X}\\), it is not necessarily the case that \\(\\hat{\\theta^2}_{MVUE} = \\bar{X}^2\\).) However, we should always at least try to define the MVUE, because if we can, it will be at least equal the performance of, if not do better than, the MLE, in terms of bias and/or variance. There are two steps to carry out when deriving the MVUE: determining a sufficient statistic for \\(\\theta\\); and correcting any bias that is observed when we utilize the sufficient statistic as our initial estimator. A sufficient statistic for a parameter \\(\\theta\\) captures all information about \\(\\theta\\) contained in the sample. In other words, any additional statistic, beyond the sufficient statistic, will not provide any additional information about \\(\\theta\\). (This does not mean that a sufficient statistic is necessarily unique: any function of a sufficient statistic is also a sufficient statistic. It just means that any additional statistic that is not a function of the sufficient statistic will not help us when we try to estimate \\(\\theta\\).) Given this definition, it stands to reason that either a sufficient statistic, or a function of a sufficient statistic, will be the optimal estimator of \\(\\theta\\). The simplest way by which to identify a sufficient statistic is by writing down the likelihood function and factorizing it into two functions, one of which depends only on the observed data and the other of which depends on both the observed data and the parameter of interest: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = g(\\mathbf{x},\\theta) \\cdot h(\\mathbf{x}) \\,, \\] This is the so-called factorization criterion. Within \\(g(\\mathbf{x},\\theta)\\), the data will appear as part of, e.g., a summation (\\(\\sum_{i=1}^n x_i\\)) or a product (\\(\\prod_{i=1}^n x_i\\)), and we identify that summation or product as the sufficient statistic. Let’s assume we are given a sample \\(\\mathbf{X} = \\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim}\\) Binomial(\\(k\\),\\(p\\)). Factorizing the likelihood function yields the following: \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n \\binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \\underbrace{\\left[ \\prod_{i=1}^n \\binom{k}{x_i} \\right]}_{h(\\mathbf{x})} \\underbrace{p^{\\sum_{i=1}^n x_i} (1-p)^{nk-\\sum_{i=1}^n x_i}}_{g(\\sum_{i=1}^n x_i,p)} \\,. \\] By inspecting the function \\(g(\\mathbf{x},\\theta)\\), we determine that the sufficient statistic for binomially distributed data is \\(U = \\sum_{i=1}^n X_i\\). (The next step, in general, is to demonstrate whether the sufficient statistic is both minimally sufficient and complete; it needs to be both so that we can use it to determine the MVUE. It suffices to say here that the factorization criterion typically identifies statistics that are minimally sufficient and complete, particularly in the context of this course. See Chapter 7 for more details on how to determine if a sufficient statistic is a minimally sufficient statistic and if it is complete. Note that complete statistics are always minimally sufficient, but not necessarily vice-versa, so check for completeness first!) If \\(U\\) is a minimally sufficient and complete statistic, and there is a function \\(h(U)\\) that is an unbiased estimator for \\(\\theta\\) and that depends on the data only through \\(U\\), then \\(h(U)\\) is the MVUE for \\(\\theta\\). Here, given \\(U = \\sum_{i=1}^n X_i\\), we need to find a function \\(h(\\cdot)\\) such that \\(E[h(U)] = p\\). Earlier in this chapter, we determined that the distribution for the sum of iid binomial random variables is Binomial(\\(nk\\),\\(p\\)), and thus we know that this distribution has expected value \\(nkp\\). Thus \\[ E\\left[U\\right] = nkp ~\\implies~ E\\left[\\frac{U}{nk}\\right] = p ~\\implies~ h(U) = \\frac{U}{nk} = \\frac{\\bar{X}}{k} ~\\mbox{is the MVUE for}~p \\,. \\] The variance of \\(\\hat{p}\\) is \\[ V[\\hat{p}] = V\\left[\\frac{\\bar{X}}{k}\\right] = \\frac{1}{k^2}V[\\bar{X}] = \\frac{1}{k^2} \\frac{V[X]}{n} = \\frac{1}{k^2}\\frac{kp(1-p)}{n} = \\frac{p(1-p)}{nk} \\,. \\] We know that this variance abides by the restriction \\[ V[\\hat{p}] \\geq -\\frac{1}{nE\\left[\\frac{d^2}{dp^2} \\log p_X(X \\vert p) \\right]} = \\frac{1}{nI(p)} \\] But is it equivalent to the lower bound itself, the CRLB? (Note that in particular situations, the MVUE may have a variance larger than the CRLB; when this is the case, unbiased estimators that achieve the CRLB simply do not exist.) For the binomial distribution, \\[\\begin{align*} p_{X \\vert P}(x \\vert p) &amp;= \\binom{k}{x} p^{x} (1-p)^{k-x} \\\\ \\log p_{X \\vert P}(x \\vert p) &amp;= \\log \\binom{k}{x} + x \\log p + (k-x) \\log (1-p) \\\\ \\frac{d}{dp} \\log p_{X \\vert P}(x \\vert p) &amp;= 0 + \\frac{x}{p} - \\frac{k-x}{(1-p)} \\\\ \\frac{d^2}{dp^2} \\log p_{X \\vert P}(X \\vert p) &amp;= -\\frac{x}{p^2} - \\frac{k-x}{(1-p)^2} \\\\ E\\left[\\frac{d^2}{dp^2} \\log p_{X \\vert P}(X \\vert p)\\right] &amp;= -\\frac{1}{p^2}E[X] - \\frac{1}{(1-p)^2}E[k-X] \\\\ &amp;= -\\frac{kp}{p^2}-\\frac{k-kp}{(1-p)^2} \\\\ &amp;= -\\frac{k}{p}-\\frac{k}{1-p} = -\\frac{k}{p(1-p)} \\,. \\end{align*}\\] The lower bound on the variance is thus \\(p(1-p)/(nk)\\), and so here the MVUE does achieve the CRLB. We cannot define a better estimator for \\(p\\) than \\(\\bar{X}/k\\)! 3.6.1 The Minimum Variance Unbiased Estimator for the Exponential Mean The exponential distribution is \\[ f_X(x) = \\frac{1}{\\theta} \\exp\\left(-\\frac{x}{\\theta}\\right) \\,, \\] where \\(x \\geq 0\\) and \\(\\theta &gt; 0\\), and where \\(E[X] = \\theta\\) and \\(V[X] = \\theta^2\\). Let’s assume that we have \\(n\\) iid data drawn from this distribution. Can we define the MVUE for \\(\\theta\\)? For \\(\\theta^2\\)? The likelihood function is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\frac{1}{\\theta^n}\\exp\\left(-\\frac{1}{\\theta}\\sum_{i=1}^n x_i \\right) = h(\\mathbf{x}) \\cdot g(\\theta,\\mathbf{x}) \\,. \\] Here, there are no terms that are functions of only the data, so \\(h(\\mathbf{x}) = 1\\) and thus the sufficient statistic is \\(U = \\sum_{i=1}^n x_i\\). We compute the expected value of \\(U\\): \\[ E[U] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n \\theta = n\\theta \\,. \\] The expected value of \\(U\\) is not \\(\\theta\\), so \\(U\\) is not unbiased…but we can see immediately that \\(E[U/n] = \\theta\\), so that \\(U/n\\) is unbiased. Thus the MVUE for \\(\\theta\\) is \\(\\hat{\\theta}_{MVUE} = U/n = \\bar{X}\\). Note that the MVUE does not possess the invariance property…it is not necessarily the case that \\(\\hat{\\theta^2}_{MVUE} = \\bar{X}^2\\). Let’s propose a function of \\(U\\) and see if we can use that to define \\(\\hat{\\theta^2}_{MVUE}\\): \\(h(U) = U^2/n^2 = \\bar{X}^2\\). (To be clear, we are simply proposing a function and seeing if it helps us define what we are looking for. It might not. If not, we can try again with another function of \\(U\\).) Utilizing what we know about the sample mean, we can write down that \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{V[X]}{n} + (E[X])^2 = \\frac{\\theta^2}{n}+\\theta^2 = \\theta^2\\left(\\frac{1}{n} + 1\\right) \\,. \\] So \\(\\bar{X}^2\\) itself is not an unbiased estimator of \\(\\theta^2\\)…but we can see that \\(\\bar{X}^2/(1/n+1)\\) is. Hence \\[ \\hat{\\theta^2}_{MVUE} = \\frac{\\bar{X}^2}{\\left(\\frac{1}{n}+1\\right)} \\,. \\] 3.6.2 Sufficient Statistics for the Normal Distribution If we have \\(n\\) iid data drawn from a normal distribution with unknown mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\), then the factorized likelihood is \\[ \\mathcal{L}(\\mu,\\sigma^2 \\vert \\mathbf{x}) = \\underbrace{(2 \\pi \\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right)\\exp\\left(\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^n x_i\\right)\\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)}_{g(\\sum x_i^2, \\sum x_i,\\mu,\\sigma)} \\cdot \\underbrace{1}_{h(\\mathbf{x})} \\,. \\] Here, we identify \\(\\sum x_i^2\\) and \\(\\sum x_i\\) as joint sufficient statistics: we need two pieces of information to jointly estimate \\(\\mu\\) and \\(\\sigma^2\\). (To be clear: it is not necessarily the case that one of the parameters matches up to one of the sufficient statistics…rather, the two statistics are jointly sufficient for estimation.) We thus cannot proceed further to define an MVUE for \\(\\mu\\) or for \\(\\sigma^2\\). (Note that we can proceed if we happen to know either \\(\\mu\\) or \\(\\sigma^2\\); if one of these values is fixed, then we can turn the crank so as to find the MVUE for the other parameter.) 3.6.3 The Maximum Likelihood Estimator for the Binomial Proportion Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). The likelihood function for \\(n\\) iid binomial random variables \\(\\{X_1,\\ldots,X_n\\}\\) is \\[ \\mathcal{L}(p \\vert \\mathbf{x}) = \\prod_{i=1}^n \\binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \\left[\\prod_{i=1}^n \\binom{k}{x_i} \\right] p^{\\sum_{i=1}^n x_i} (1-p)^{nk-\\sum_{i=1}^n x_i} \\,. \\] Recall that the value \\(\\hat{p}_{MLE}\\) that maximizes \\(\\mathcal{L}(p \\vert x)\\) also maximizes \\(\\ell(p \\vert x) = \\log \\mathcal{L}(p \\vert x)\\), which is considerably easier to work with: \\[\\begin{align*} \\ell(p \\vert \\mathbf{x}) &amp;= \\sum_{i=1}^n \\log \\left[\\binom{k}{x_i}\\right] + \\left(\\sum_{i=1}^n x_i\\right) \\log p + \\left(nk - \\sum_{i=1}^n x_i\\right) \\log (1-p) \\\\ \\frac{d}{dp} \\ell(p \\vert \\mathbf{x}) &amp;= 0 + \\frac{1}{p} \\sum_{i=1}^n x_i - \\frac{1}{1-p} \\left(nk - \\sum_{i=1}^n x_i\\right) = 0 \\,. \\end{align*}\\] After rearranging terms, we find that \\[ p = \\frac{1}{nk}\\sum_{i=1}^n x_i ~\\implies~ \\hat{p}_{MLE} = \\frac{\\bar{X}}{k} \\,. \\] The MLE matches the MVUE, thus we know that the MLE is unbiased and we know that it achieves the CRLB. A useful property of MLEs is the invariance property, whereby the MLE for a function of \\(\\theta\\) is given by applying the same function to the MLE itself. Thus the MLE for the population mean \\(E[X] = \\mu = kp\\) is \\(\\bar{X}\\); and the MLE for the population variance \\(V[X] = \\sigma^2 = kp(1-p)\\) is \\(\\bar{X}(1-\\bar{X}/k)\\). Last, we recall that asymptotically, \\(\\hat{p}_{MLE}\\) converges in distribution to a normal random variable: \\[ \\hat{p}_{MLE} \\stackrel{d}{\\rightarrow} X&#39; \\sim \\mathcal{N}\\left(p,\\frac{1}{nI(p)} = \\frac{p(1-p)}{nk}\\right) \\,. \\] 3.7 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by, e.g., solving for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The construction of confidence intervals thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) to \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 14 of Chapter 1.) The only new element to consider here regarding the construction of confidence intervals is that now the sampling distribution for our adopted statistic (\\(Y = \\sum_{i=1}^n X_i\\), where the \\(X_i\\)’s are iid samples from a binomial distribution) is discrete as opposed to continuous. As it turns out, this does not impact our use of uniroot(): even though \\(F_Y(y_{\\rm obs})\\) is discrete, the proportion \\(p\\) itself is still continuous, and we can tune its value so that \\(F_Y(y_{\\rm obs})\\) will match, e.g., \\(\\alpha/2\\) or \\(1-\\alpha/2\\) with arbitrary precision. What the discreteness of the sampling distribution does impact is the coverage, or the proportion of intervals that overlap the true value. We illustrate the impact of discreteness on coverage below in an example; in short, the smaller the value of \\(nk\\), the further the estimates of coverage deviate from expectation (both positively and negatively). When the number of data are small, use simulations to estimate the true coverage! Before going to the examples, we note here that discrete distributions like the binomial have historically been difficult to work with analytically. Because of this, a number of algorithms have been developed through the years for constructing confidence intervals for binomial proportions. It is our opinion that there is no particular reason to utilize any of these algorithms when one can compute exact intervals in the manner we do below and can utilize simulations to assess interval coverage. However, given the ubiquity of the most commonly seen approximation interval, the Wald interval, we will illustrate its use in an example below. 3.7.1 Confidence Interval for the Binomial Proportion As we did in Chapter 2, below we will adapt the general-purpose R code for constructing confidence intervals that we provide in Appendix B to a specific problem: here, that problem is putting a confidence interval on the binomial proportion \\(p\\). Assume that we sample \\(n\\) iid data from a binomial distribution with number of trials \\(k\\) and proportion \\(p\\). Then, as shown above, \\(Y = \\sum_{i=1}^n X_i \\sim\\) Binom(\\(nk,p\\)); our observed test statistic is \\(y_{\\rm obs} = \\sum_{i=1}^n x_i\\). For this statistic, \\(E[Y] = nkp\\) increases with \\(p\\), so \\(p_{1-\\alpha/2}\\) maps to the lower bound, while \\(p_{\\alpha/2}\\) maps to the upper bound. confint &lt;- function(y.obs,nk,alpha=0.05) { f &lt;- function(prob,y.obs,nk,q) { pbinom(y.obs,size=nk,prob=prob)-q } lo &lt;- uniroot(f,interval=c(0,1),y.obs,nk,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(0,1),y.obs,nk,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 12 k &lt;- 5 p &lt;- 0.4 X &lt;- rbinom(n,size=k,prob=p) cat(&quot;The observed test statistic is&quot;,sum(X),&quot;\\n&quot;) ## The observed test statistic is 22 confint(sum(X),n*k) ## [1] 0.2607037 0.5010387 We find that the interval is \\([\\hat{p}_L,\\hat{p}_H] = [0.261,0.501]\\), which overlaps the true value of 0.4. (See Figure 3.7.) Note that, unlike in Chapter 2, the interval over which we search for the root is [0,1], which is the range of possible values for \\(p\\). Figure 3.7: Probability mass functions for binomial distributions with \\(n=12\\) and \\(k=5\\) and (left) \\(p=0.261\\) and (right) \\(p=0.501\\). We observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 22\\) successes and we want to construct a 95% confidence interval. \\(p=0.261\\) is the smallest value of \\(p\\) such that \\(F_Y^{-1}(0.975) = 22\\), while \\(p=0.501\\) is the largest value of \\(p\\) such that \\(F_Y^{-1}(0.025) = 22\\). In Chapter 2, we find that when we are constructing exact intervals with continuous sampling distributions (“exact” meaning that the sampling distribution is the correct one, not an approximation), the coverage is what we expect it to be: in any finite simulation, the proportion of intervals that overlap (or cover) the true value is “close” to \\(1-\\alpha\\). Here, we are dealing with a discrete sampling distribution, and we expect that discreteness will affect the coverage. Let’s check to see if our expectation is correct. In Figure 3.8, we show the results of simulating data to assess coverage for \\(k = 5\\) and for \\(n = 12\\), 120, and 1200 (from left to right). The solid blue lines indicate the expected coverage, 0.95, while the dashed blue lines indicate the range of values in which we expect to see \\(\\approx\\) 95% of the simulated values. We determine the placement of the dashed blue lines as follows: we simulate \\(k = 10000\\) intervals, where the expected coverage is \\(p = 0.95\\). Let \\(X\\) be the number of simulations in which the confidence intervals that actually overlap the true parameter value. Then \\(E[X] = kp\\) (here, 9500), \\(V[X] = kp(1-p)\\) (here, 475), and \\(\\sigma_X = \\sqrt{V[X]}\\) (here, 21.79). The dashed lines are thus placed at the values 0.9500 \\(\\pm\\) 0.0043. As we can see, the proportion of simulated values observed between the lines increases as \\(n\\) increases, i.e., as the effect of discreteness decreases. Figure 3.8: Results of simulations (described in the text) for \\(k = 5\\) and \\(n = 12\\) (left), 120 (center), and 1200 (right). The expected proportion of simulation results lying between the two dashed lines is \\(\\approx\\) 95 percent; we see that as \\(n\\) increases, i.e., as the effect of sampling distribution discreteness decreases, we achieve that expectation. 3.7.2 Confidence Interval for the Negative Binomial Proportion Let’s assume that we have performed \\(n = 10\\) separate negative binomial trials, each with a target number of successes \\(s\\), and recorded the number of failures \\(X_1,\\ldots,X_{n}\\) for each. Further, assume that the success proportion is \\(p\\). Below we will show how to compute the confidence interval for \\(p\\), but before we start, we recall that \\(Y = \\sum_{i=1}^n X_i\\) is a negative binomially distributed random variable for \\(ns\\) successes and success proportion \\(p\\). Here, \\(E[Y] = s(1-p)/p\\)…as \\(p\\) increases, \\(E[Y]\\) decreases. Thus when we adapt the confidence interval code we use for binomially distributed data, we need to switch the mapping of \\(p_{1-\\alpha/2}\\) and \\(p_{\\alpha/2}\\) to point to the upper and lower bounds, respectively. confint &lt;- function(y.obs,ns,alpha=0.05) { f &lt;- function(prob,y.obs,ns,q) { pnbinom(y.obs,size=ns,prob=prob)-q } # Note the switch in the mapping! hi comes first here. hi &lt;- uniroot(f,interval=c(0.0001,1),y.obs,ns,1-alpha/2)$root lo &lt;- uniroot(f,interval=c(0.0001,1),y.obs,ns,alpha/2)$root return(c(lo,hi)) } set.seed(101) n &lt;- 12 s &lt;- 5 p &lt;- 0.4 X &lt;- rnbinom(n,size=s,prob=p) confint(sum(X),n*s) ## [1] 0.3255305 0.4822869 The confidence interval is \\([\\hat{p}_L,\\hat{p}_H] = [0.326,0.482]\\), which overlaps the true value 0.4. We note that in the code, we change the lower bound on the interval from 0 (in the binomial case) to 0.0001 (something suitably small but non-zero): a success proportion of 0 maps to an infinite number of failures, which R cannot tolerate! Figure 3.9: Probability mass functions for negative binomial distributions with \\(n=12\\) and \\(s=5\\) and (left) \\(p=0.326\\) and (right) \\(p=0.482\\). We observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 88\\) failures and we want to construct a 95% confidence interval. \\(p=0.326\\) is the smallest value of \\(p\\) such that \\(F_Y^{-1}(0.025) = 88\\), while \\(p=0.482\\) is the largest value of \\(p\\) such that \\(F_Y^{-1}(0.975) = 88\\). 3.7.3 Large-Sample Confidence Intervals for the Binomial Proportion Let’s assume that we have sampled \\(n\\) iid binomial variables with number of trials \\(k\\) and success proportion \\(p\\). When \\(k\\) is sufficiently large, we can assume that \\(\\bar{X}\\) has a distribution whose shape is approximately that of a normal distribution, with mean \\(E[\\bar{X}] = kp\\) and with variance and standard error \\[ V[\\bar{X}] = \\frac{kp(1-p)}{n} ~~~ \\mbox{and} ~~~ se(\\bar{X}) = \\sqrt{V[\\bar{X}]} = \\sqrt{\\frac{kp(1-p)}{n}} \\,. \\] Furthermore, we can assume that \\(\\hat{p} = \\bar{X}/k\\) is approximately normally distributed, with mean \\(p\\), variance \\(V[\\hat{p}] = V[\\bar{X}]/k^2 = p(1-p)/nk\\), and standard error \\(\\sqrt{p(1-p)/nk}\\). Given this information, it is simple to express, e.g., an approximate two-sided \\(100(1-\\alpha)\\)% confidence interval for \\(p\\): \\[ \\hat{p} \\pm z_{1-\\alpha/2} se(\\hat{p}) ~~ \\Rightarrow ~~ \\left[ \\hat{p} - z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{nk}} \\, , \\, \\hat{p} + z_{1-\\alpha/2} \\sqrt{\\frac{p(1-p)}{nk}} \\right] \\,, \\] where \\(z_{1-\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)\\). However, we don’t actually know the true value of \\(p\\)…so we plug in \\(p = \\hat{p}\\). This is the so-called Wald interval that is typically provided to students in introductory statistics courses, although it is typically provided assuming \\(n = 1\\) and assuming that \\(\\alpha = 0.05\\): \\[ \\left[ \\hat{p} - 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{k}} \\, , \\, \\hat{p} + 1.96 \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{k}} \\right] \\,, \\] where in this case \\(\\hat{p} = X/k\\). 3.8 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-sided test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bounds given by \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\), which are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps, e.g., \\(y_{\\alpha/2}\\) to either the lower or upper rejection region boundary by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven. In the previous chapter, we utilized \\(\\bar{X}\\) when testing hypotheses about the normal mean \\(\\mu\\). This is a principled choice for a test statistic\\(-\\)after all, \\(\\bar{X}\\) is the MLE for \\(\\mu-\\)but we do not yet know whether or not we can choose a better one. Can we differentiate hypotheses more easily if we use a test statistic other than \\(\\bar{X}\\)? To help answer this question, we now introduce a method for defining the most powerful test of a simple null hypothesis versus a simple alternative hypothesis: \\[ H_o : \\theta = \\theta_o ~~\\mbox{and}~~ H_a : \\theta = \\theta_a \\,. \\] Note that the word “simple” has a precise meaning here: it means that when we set \\(\\theta\\) to a particular value, we are completely fixing the shape and location of the pmf or pdf from which data are sampled. If, for instance, we are dealing with a normal distribution with unknown variance \\(\\sigma^2\\), the hypothesis \\(\\mu = \\mu_o\\) would not be simple, since the width of the pdf can still vary: the shape is not completely fixed. (The hypothesis \\(\\mu = \\mu_o\\) with variance unknown is dubbed a composite hypothesis. We will examine how to work with composite hypotheses in the next chapter.) For a given test level \\(\\alpha\\), the Neyman-Pearson lemma states that the test that maximizes the power has a rejection region of the form \\[ \\frac{\\mathcal{L}(\\theta_o \\vert \\mathbf{x})}{\\mathcal{L}(\\theta_a \\vert \\mathbf{x})} \\leq c(\\alpha) \\,, \\] where \\(c\\) is a constant whose value depends on \\(\\alpha\\) that we have to determine. While this formulation initially appears straightforward, it is in fact not necessarily clear how to derive \\(c(\\alpha)\\). To do so, we will make use of sufficient statistics and their sampling distributions. (And in fact, it will turn out that we don’t actually determine the quantity \\(c(\\alpha)\\) at all! This will make more sense below.) Let’s illustrate how we would use the NP lemma to construct a hypothesis test for \\(H_o : \\theta = \\theta_o\\) versus \\(H_a : \\theta = \\theta_a\\) assuming that we observe \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) that are drawn from some distribution \\(P(\\theta)\\) (where \\(\\theta\\) is the one and only freely varying parameter). The steps are as follows: we factorize the likelihood \\(\\mathcal{L}(\\theta \\vert \\mathbf{x})\\) to determine the sufficient statistic \\(U\\); we write down the likelihood ratio, which is a function of \\(U\\) (and we simplify the ratio as much as possible by, e.g., cancelling constants that appear in both the numerator and denominator); we use the ratio to determine, given the specific values \\(\\theta_o\\) and \\(\\theta_a\\), how \\(U\\) must change to drive the ratio to zero; that tells us whether the rejection region is \\(U \\leq u_\\alpha\\) or \\(U \\geq u_{1-\\alpha}\\); and last we use the sampling distribution for \\(U\\) to determine \\(u_\\alpha\\) or \\(u_{1-\\alpha}\\). Let’s illustrate the process using the binomial distribution. Our goal is to construct the most powerful test of the simple hypotheses \\(H_o: p = p_o\\) and \\(H_a: p = p_a\\), where \\(p_a &gt; p_o\\). For simplicity, let’s initially assume that we observe a single datum \\(X \\sim\\) Binomial(\\(k,p\\)). We start by factorizing the likelihood to determine the sufficient statistic. This yields \\[ \\mathcal{L}(p \\vert x) = p^x (1-p)^{1-x} = g(p,x) \\times 1 \\,. \\] Thus the sufficient statistic is, as we expect, \\(U = X\\). The next step is to write down the likelihood ratio: \\[ \\frac{\\mathcal{L}(\\theta_o \\vert x)}{\\mathcal{L}(\\theta_a \\vert x)} = \\frac{\\binom{k}{x} p_o^x (1-p_o)^{k-x}}{\\binom{k}{x} p_a^x (1-p_a)^{k-x}} \\propto \\left(\\frac{p_o(1-p_a)}{p_a(1-p_o)}\\right)^x = \\left(\\frac{p_o(1-p_a)}{p_a(1-p_o)}\\right)^u \\,. \\] How must \\(u\\) change so that the ratio is driven toward zero? If \\(p_a &gt; p_o\\), the ratio will go to zero as \\(u \\rightarrow \\infty\\). Thus the rejection region is \\(U \\geq u_{1-\\alpha}\\). At this point, we leave the domain of the NP lemma…from here on out, we are simply solving an inverse cdf problem: \\[\\begin{align*} F_U(u_{1-\\alpha} \\vert p_o) - (1-\\alpha) &amp;= 0 \\\\ \\Rightarrow ~~~ u_{1-\\alpha} &amp;= F_U^{-1}(1-\\alpha \\vert p_o) \\,. \\end{align*}\\] Here, \\(U\\) is binomially distributed, and because we cannot easily work with the generalized inverse cdf for the binomial distribution, we go directly to code: u.hi &lt;- qbinom(1-alpha,size=k,prob=p.o) # it turns out this is incorrect! However, we run into an issue that we will run into whether the sampling distribution is discrete, one that is best described via an example. Let’s suppose we flip a coin that we think is fair \\(k = 10\\) times, and we wish to test \\(p_o = 0.5\\) versus \\(p_a &gt; p_o\\). (Remember: we don’t have to specify the value of \\(p_a\\) in order to derive the rejection region…at this point, it suffices to state that \\(p_a &gt; p_o\\).) Our proposed rejection region, assuming \\(\\alpha = 0.05\\), is qbinom(0.95,size=10,prob=0.5) ## [1] 8 So we’d reject the null if \\(U \\geq 8\\)…but…the probability of sampling a value \\(U \\geq 8\\) is 1 - pbinom(8-1,size=10,prob=0.5) ## [1] 0.0546875 The probability, 0.055, is greater than \\(\\alpha\\). So we need to make what we dub a “discreteness correction”: u.hi &lt;- qbinom(1-alpha,size=k,prob=p.o) + 1 We need to make an analogous correction of \\(-1\\) whenever we derive lower bounds. Note how the rejection region boundary depends on the value of \\(p_o\\), but not on the value of \\(p_a\\). This means that the test we define above is the most-powerful test regardless of the value \\(p_a &gt; p_o\\). We have thus constructed a uniformly most powerful (or UMP) test for disambiguating the simple hypotheses \\(H_o : p = p_o\\) and \\(H_a : p = p_a &gt; p_o\\). It is typically the case that when we use the NP lemma to define a most powerful test for \\(\\theta_o\\) versus \\(\\theta_a\\), we end up inadvertently defining a UMP test as well. In general, when we have \\(n\\) iid binomial data, where \\(X_i \\sim\\) Binom(\\(k,p\\)), the sufficient statistic will be \\(U = \\sum_{i=1}^n X_i \\sim\\) Binom(\\(nk,p\\)), and the UMP tests for \\(p_o\\) versus \\(p_a\\) have rejection regions of the form Alternative Rejection Region(s) R Code for Binomial Distribution \\(p_a &lt; p_o\\) \\(u_{\\rm obs} \\leq u_{\\alpha}\\) u.lo &lt;- qbinom(alpha,size=n*k,prob=p.o) - 1 \\(p_a &gt; p_o\\) \\(u_{\\rm obs} \\geq u_{1-\\alpha}\\) u.hi &lt;- qbinom(1-alpha,size=n*k,prob=p.o) + 1 Note that we can combine these results to define the rejection regions for a two-tail test, while remembering that two-tail tests fall outside the domain of the NP lemma and thus we cannot make any guarantees about test power. As for the \\(p\\)-values and the test power, we can simply adapt previous results to write down Alternative Formula R Code for Binomial Distribution \\(p_a &lt; p_o\\) \\(F_U(u_{\\rm obs} \\vert p_o)\\) p &lt;- pbinom(u.obs,size=n*k,prob=p.o) \\(p_a &gt; p_o\\) \\(1-F_U(u_{\\rm obs}-1 \\vert p_o)\\) p &lt;- 1-pbinom(u.obs-1,size=n*k,prob=p.o) and Alternative Formula R Code for Binomial Distribution \\(p_a &lt; p_o\\) \\(F_U(u_\\alpha \\vert p_a)\\) power &lt;- pbinom(u.lo,size=k,prob=p.a) \\(p_a &gt; p_o\\) \\(1-F_U(u_{1-\\alpha}-1 \\vert p_a)\\) power &lt;- 1-pbinom(u.hi-1,size=k,prob=p.a) 3.8.1 Defining the Uniformly Most-Powerful Test for the Beta(1,\\(\\theta\\)) Distribution The Beta(1,\\(\\theta\\)) distribution is one that we have seen, albeit unnamed, before: \\[ f_X(x) = \\theta (1-x)^{\\theta-1} ~~ x \\in [0,1] \\,. \\] Here, we wish to test \\(H_o: \\theta = \\theta_o\\) versus the simple alternative \\(H_a: \\theta = \\theta_a\\) at level \\(\\alpha = 0.05\\). We assume \\(\\theta_a &lt; \\theta_o\\). Because we are testing a simple null versus a simple alternative, we can make use of the NP lemma. First, we factorize the likelihood in order to determine the sufficient statistic. (Note: it may seem trivial to identify that statistic as being \\(X\\), but it can be useful to lay out all the steps!) \\[ \\mathcal{L}(\\theta \\vert x) = f_X(x \\vert \\theta) = \\theta (1-x)^{\\theta-1} = g(\\theta,x) \\times 1 \\,, \\] so \\(U = X\\). Second, we examine the likelihood ratio: \\[ \\frac{\\mathcal{L}(\\theta_o \\vert x)}{\\mathcal{L}(\\theta_a \\vert x)} = \\frac{\\theta_o(1-x)^{\\theta_o-1}}{\\theta_a(1-x)^{\\theta_a-1}} \\propto (1-x)^{\\theta_o-\\theta_a} = (1-u)^{\\theta_o-\\theta_a} \\,. \\] Since we are assuming \\(\\theta_a &lt; \\theta_o\\) and since we know that \\(0 \\leq 1-u \\leq 1\\), it is the case that as \\(u \\rightarrow 1\\), the ratio will get smaller. Hence the rejection region for our test will be \\(U \\geq u_{1-\\alpha}\\). To determine \\(u_{1-\\alpha}\\) we need to solve \\[\\begin{align*} F_U(u_{1-\\alpha} \\vert \\theta_o) - (1-\\alpha) &amp;= 0 \\\\ \\Rightarrow ~~~ u_{1-\\alpha} &amp;= F_U^{-1}(1-\\alpha \\vert \\theta_o) \\,. \\end{align*}\\] The sampling distribution for \\(U\\) is that of \\(X\\): the original pdf itself. Thus \\[ F_U(u) = \\int_0^u \\theta (1-y)^{\\theta-1} dy = \\left. -(1-y)^\\theta \\right|_0^u = 1 - (1-u)^\\theta \\,, \\] and so \\[ 1 - (1-u_{1-\\alpha)}^{\\theta_o} = 1-\\alpha ~~~ \\Rightarrow ~~~ 1-u_{1-\\alpha} = \\alpha^{1/\\theta_o} ~~ \\Rightarrow ~~ u_{1-\\alpha} = 1 - \\alpha^{1/\\theta_o} \\,. \\] If \\(U \\geq u_{1-\\alpha}\\), we reject the null hypothesis that \\(\\theta = \\theta_o\\). For instance, if \\(\\theta_o = 1\\) and \\(\\alpha = 0.05\\), we reject the null if \\(U \\geq 1-(0.05)^1 = 0.95\\), and if \\(\\theta_o = 2\\), we reject the null if \\(U \\geq 1-(0.05)^{1/2} = 0.776\\), etc. We note that the rejection region does not depend on \\(\\theta_a\\), so we have defined a uniformly most powerful test of \\(\\theta\\) for the situation in which \\(\\theta_a &lt; \\theta_o\\). What is the test power? Recall that it is the probability of rejecting the null given \\(\\theta = \\theta_a\\), i.e., \\(P(U \\geq u_{1-\\alpha} \\vert \\theta = \\theta_a)\\). We can compute this directly given the relative simplicity of the pdf: \\[ power(\\theta_a) = \\int_{u_{1-\\alpha}}^1 \\theta_a (1-u)^{\\theta_a-1} = \\left. -(1-u)^{\\theta_a} \\right|_{u_{1-\\alpha}}^1 = (1-u_{1-\\alpha})^{\\theta_a} \\,. \\] For instance, if \\(\\theta_o = 1\\) and \\(u_{1-\\alpha} = 0.95\\), the power for \\(\\theta_a = 1/2\\) is \\(0.05^{1/2} = 0.224\\), while the power for \\(\\theta_a = 1/4\\) is \\(0.05^{1/4} = 0.473\\), etc. As \\(\\theta_a \\rightarrow 0\\), the power goes to 1. 3.8.2 Defining the Uniformly Most-Powerful Test of the Negative Binomial Proportion Let’s say that we are going to flip a coin until we observe \\(s = 20\\) heads, and that we will use the number of observed tails (or failures) to test \\(H_o : p = p_o = 0.5\\) versus \\(H_a : p = p_a = 0.6\\). What is the rejection region for this test, assuming \\(\\alpha = 0.1\\)? We then conduct the experiment of flipping the coin and we observe \\(X = 13\\) failures. What is the \\(p\\)-value, and what to we conclude about the coin? Last, what is the power of the test if the true proportion is \\(p = 0.6\\)? Let’s say that we flip a coin until we observe \\(s = 20\\) heads. The total number of failures is \\(X = 29\\). The distribution that governs this experiment is the negative binomial distribution, with \\(X \\sim\\) NBinom(\\(s,p\\)). Because we only conduct one experiment, we identify (without explicit factorization) that the sufficient statistic is \\(U = X\\), and we write down the likelihood ratio: \\[ \\frac{\\mathcal{L}(\\theta_o \\vert x)}{\\mathcal{L}(\\theta_a \\vert x)} = \\frac{\\binom{x+s-1}{x} p_o^s (1-p_o)^{x-s}}{\\binom{x+s-1}{x} p_a^s (1-p_a)^{x-s}} \\propto \\left(\\frac{1-p_o}{1-p_a}\\right)^x = \\left(\\frac{1-p_o}{1-p_a}\\right)^u \\,. \\] We have that \\(p_a &gt; p_o\\), so the ratio is \\(&gt; 1\\), so to drive the ratio towards zero, \\(u \\rightarrow 0\\)…so the rejection region is \\(U \\leq u_{\\alpha}\\). We combine this information along with the information in the binomial tables above to write the following: u.lo &lt;- qnbinom(alpha,size=s,prob=p.o) - 1 # rejection region p &lt;- pnbinom(u.obs,size=s,prob=p.o) # p-value power &lt;- pnbinom(u.lo,size=s,prob=p.a) # power for p = p.a We thus find that alpha &lt;- 0.1 s &lt;- 20 u.obs &lt;- 13 p.o &lt;- 0.5 p.a &lt;- 0.6 (u.lo &lt;- qnbinom(alpha,size=s,prob=p.o) - 1) # rejection region ## [1] 11 (p &lt;- pnbinom(u.obs,size=s,prob=p.o)) # p-value ## [1] 0.1481032 (power &lt;- pnbinom(u.lo,size=s,prob=p.a)) # power for p = p.a ## [1] 0.375243 The rejection region is \\(U \\leq u_{\\alpha} = 11\\), the \\(p\\)-value is 0.148 (hence we fail to reject the null…we cannot decide, given the observed data, that the coin is unfair), and the power is 0.375 (if the proportion \\(p\\) is truly 0.6, we would sample a value of \\(U\\) in the rejection region 37.5 percent of the time; it is not surprising that we fail to reject the null here…we simply don’t have enough data!). We note in passing that due to the discreteness of the sampling distribution, in the same way that we observe in the last section that estimates of confidence interval coverage are “noisy” in the small-sample limit (see Figure 3.8), we observe that estimates of \\(\\alpha\\) under the null are also “noisy.” This makes sense: confidence interval construction and hypothesis testing are intimately related: e.g., if the actual confidence interval coverage is too high, the rate of Type I errors will be reduced. Figure 3.10: The power curve for the uniformly most-powerful upper-tail test \\(s = 20\\) and \\(p_o = 0.5\\). The red dot indicates the theoretically expected power for \\(p = p_o\\); the observed offset from the curve is a by-product of working with a discrete rather than a continuous distribution. We also note that because \\(p_a\\) is not part of the definition of the rejection region, we can say that we have defined the uniformly most-powerful upper-tail test of the negative binomial proportion \\(p\\). 3.8.3 Defining the Uniformly Most-Powerful Test for the Normal Population Mean In Chapter 2, we use the statistic \\(\\bar{X}\\) as the basis for testing hypotheses about the normal population mean, \\(\\mu\\). We justified this choice because, at the very least, \\(\\hat{\\mu}_{MLE} = \\bar{X}\\), so it is a principled choice. However, beyond that, we did not (and indeed could not) provide any further justification. Is \\(\\bar{X}\\) the basis of the UMP for \\(\\mu\\)? The first thing to realize is that the NP lemma does not apply if there are two freely varying parameters. The NP lemma applies to simple hypotheses, where the hypotheses uniquely specify the population from which data are drawn. Here, even if we set \\(H_o: \\mu = \\mu_o\\) and \\(H_a: \\mu = \\mu_a\\), \\(\\sigma^2\\) can still freely vary. (So, technically, these hypotheses are composite hypotheses.) So to go further here, we must assume \\(\\sigma^2\\) is known. When \\(\\sigma^2\\) is known, we can factorize the likelihood as \\[ \\mathcal{L}(\\mu,\\sigma^2 \\vert \\mathbf{x}) = \\underbrace{\\exp\\left(\\frac{\\mu}{\\sigma^2}\\sum_{i=1}^n x_i\\right)\\exp\\left(-\\frac{n\\mu^2}{2\\sigma^2}\\right)}_{g(\\sum x_i,\\mu)} \\cdot \\underbrace{(2 \\pi \\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n x_i^2\\right)}_{h(\\mathbf{x})} \\,, \\] so we can identify the sufficient statistic as \\(U = \\sum_{i=1}^n X_i\\). The ratio of likelihoods is \\[ \\frac{\\mathcal{L}(\\mu_o \\vert \\mathbf{x})}{\\mathcal{L}(\\mu_a \\vert \\mathbf{x})} \\propto \\exp\\left(\\frac{(\\mu_o-\\mu_a)}{\\sigma^2}\\sum_{i=1}^n x_i\\right) = \\exp\\left(\\frac{\\mu_o-\\mu_a}{\\sigma^2}u\\right) \\,. \\] If \\(\\mu_a &lt; \\mu_o\\), then we require that \\(u \\rightarrow -\\infty\\) so that the ratio goes to zero, meaning the rejection region is \\(U \\leq u_{\\alpha}\\). If \\(\\mu_a &gt; \\mu_o\\), on the other hand, \\(u\\) has to increase towards \\(\\infty\\) and thus the rejection region is \\(U \\geq u_{1-\\alpha}\\). We know from using the method of moment-generating functions that the sum of \\(n\\) iid normal random variables is itself a normal random variable with mean \\(n\\mu\\) and variance \\(n\\sigma^2\\), so \\(U \\sim \\mathcal{N}(n\\mu,n\\sigma^2)\\)…which means that \\[ Z = \\frac{U - n\\mu}{\\sqrt{n}\\sigma} \\sim \\mathcal{N}(0,1) \\,. \\] Hence the rejection region, rendered as a function of \\(Z\\), is \\(Z \\leq z_{\\alpha} = \\Phi^{-1}(\\alpha)\\) if \\(\\mu_a &lt; \\mu_o\\) and \\(Z \\geq z_{1-\\alpha} = \\Phi^{-1}(1-\\alpha)\\) if \\(\\mu_a &gt; \\mu_o\\). So we can see that \\(U = \\sum_{i=1}^n X_i\\) is the statistic that forms the basis for the uniformly most-powerful tests of \\(\\mu\\) when \\(\\sigma^2\\) is known. “But wait. What about \\(\\bar{X}\\)?” Recall that a function of a sufficient statistic is itself a sufficient statistic, so we could also use \\(U&#39; = \\bar{X} = U/n\\) as the basis for the UMP test: \\[ Z = \\frac{U - n\\mu}{\\sqrt{n}\\sigma} = \\frac{U/n - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1) \\,. \\] We thus see that, at least when \\(\\sigma^2\\) is known, \\(\\bar{X}\\) (or functions of it) is the best statistic to use to test simple hypotheses. Now, what if \\(\\sigma^2\\) is unknown? We discuss this possibility in the next chapter, when we introduce the likelihood ratio test. 3.8.4 Large-Sample Tests of the Binomial Proportion Earlier in the chapter, we saw that if \\[ k &gt; 9\\left(\\frac{\\mbox{max}(p,1-p)}{\\mbox{min}(p,1-p)}\\right) \\,, \\] then, given a random variable \\(X \\sim\\) Binomial(\\(k\\),\\(p\\)), we can assume \\[ X \\stackrel{d}{\\rightarrow} X&#39; \\sim \\mathcal{N}(kp,kp(1-p)) \\,, \\] or that \\[ \\hat{p} = \\hat{p}_{MLE} = \\frac{X}{k} \\stackrel{d}{\\rightarrow} X&#39; \\sim \\mathcal{N}(p,p(1-p)/k) \\,. \\] In hypothesis testing, this approximation forms the basis of two different tests: \\[\\begin{align*} \\mbox{Score}~\\mbox{Test:} ~~ &amp; \\hat{p} \\sim \\mathcal{N}(p_o,p_o(1-p_o)/k) \\\\ \\mbox{Wald}~\\mbox{Test:} ~~ &amp; \\hat{p} \\sim \\mathcal{N}(p_o,\\hat{p}(1-\\hat{p})/k) \\,. \\end{align*}\\] The only difference between the two is in how the variance is estimated under the null. To carry out these tests, we can simply adapt the expressions for the rejection regions, \\(p\\)-values, and power given in Chapter 2 for tests of the normal population mean with variance known, with the primary change being the definition of the variance. We are including the details of the Wald and Score tests here for completeness, in that they are ubiquitous in introductory statistics settings. As they yield only approximate results, we would encourage the reader to always use the exact testing mechanisms described above in the main body of this section. 3.9 Logistic Regression In the last chapter, we introduced simple linear regression, in which we model the data-generating process as \\[ Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\,. \\] So as to be able to perform hypothesis testing (e.g., \\(H_o : \\beta_1 = 0\\) vs. \\(H_a : \\beta_1 \\neq 0\\)), we make the assumption that \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\), or equivalently, that \\(Y_i \\vert x_i \\sim \\mathcal{N}(\\beta_0+\\beta_1x_i,\\sigma^2)\\). When we make this assumption, we are implicitly stating that the response variable is continuous and that it can take on any value between \\(-\\infty\\) and \\(\\infty\\). But what if this doesn’t actually correctly represent the response variable? Maybe the \\(Y_i\\)’s are distances with values \\(\\geq 0\\). Maybe each \\(Y_i\\) belongs to one of several categories, and thus the \\(Y_i\\)’s are discretely valued. When provided data such as these, it is possible, though not optimal, to utilize simple linear regression. A better choice is to generalize the concept of linear regression, and to utilize this generalization so as to implement a more appropriate statistical model. To implement a generalized linear model (or GLM), we need to do two things: examine the \\(Y_i\\) values and select an appropriate distribution for them (discrete or continuous? what is the functional domain?); and define a link function \\(g(\\theta \\vert x)\\) that maps the line \\(\\beta_0 + \\beta_1 x_i\\), which has infinite range, into a more limited range (e.g., \\([0,\\infty)\\)). Suppose that, in an experiment, the response variable can take on the values “false” and “true.” To generalize linear regression so as to handle these data, we map “false” to 0 and “true” to 1, and assume that we can model the data-generating process using a Bernoulli distribution (i.e., a binomial distribution with \\(k = 1\\)): \\(Y \\sim\\) Bernoulli(\\(p\\)). We know that \\(0 &lt; p &lt; 1\\), so we adopt a link function that maps \\(\\beta_0 + \\beta_1 x\\) to the range \\((0,1)\\). There is no unique choice, but a conventional one is the logit function: \\[ g(p \\vert x) = \\log\\left[\\frac{p \\vert x}{1-p \\vert x}\\right] = \\beta_0 + \\beta_1 x ~\\implies~ p \\vert x = \\frac{\\exp\\left(\\beta_0+\\beta_1x\\right)}{1+\\exp\\left(\\beta_0+\\beta_1x\\right)} \\,. \\] Using the logit function to model dichotomous data is dubbed logistic regression. See Figure 3.11. Figure 3.11: An example of an estimated logistic regression line. (See the first example below for data details.) The blue line is a logit function, and it represents the probability that we would sample a datum of Class 1 as a function of \\(x\\). The red points represent the observed data (350 objects each in Class 0 and Class 1). The intersection of the green dashed lines indicates the intercept at \\(y = 0.542\\). (It is important to note here that when performing logistic regression, we are simply estimating \\(p \\vert x\\), which is the probability that we would sample a datum belonging to Class 1, given \\(x\\). How we choose to map \\(p \\vert x\\) to either 0 or 1 (i.e., the classification step), if we indeed choose to make that mapping, is not actually part of the logistic regression framework. We will indicate how to make this mapping when the response classes are balanced (i.e., when a model is trained on just as many Class 1 objects as Class 0 objects) in an example below. Going beyond that into more general classification situations is beyond the scope of this book. How do we learn a logistic regression model? In linear regression, we can determine values for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by formulae, the ones we derive when we minimize the sum of squared errors (SSE). For logistic regression, such simple formulae do not exist, and so we estimate \\(\\beta_0\\) and \\(\\beta_1\\) via numerical optimization of the likelihood function \\[ \\mathcal{L}(\\beta_0,\\beta_1 \\vert \\mathbf{y}) = \\prod_{i=1}^n p_{Y \\vert \\beta_0,\\beta_1}(y_i \\vert \\beta_0,\\beta_1) \\,, \\] where \\(p_Y(\\cdot)\\) is the Bernoulli probability mass function. Specifically, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the coordinates at which the bivariate likelihood surface achieves its maximum value. Note that because numerical optimization is an iterative process, logistic regression models are learned more slowly than simple linear regression models. Let’s step back for a moment. Why would we want to learn a logistic regression model in the first place? After all, it is a relatively inflexible model that might lack the ability to mimic the true behavior of \\(p \\vert x\\). The reason is that because we specify the model parameterization, we can examine the learned model and perform inference: we can examine how the response is affected by changes in the predictor variable’s values. This can be important in, e.g., scientific contexts, where explaining what a model does can be as important, if not more important, as its ability to predict response values. (This is in constrast to the vast majority of machine learning models, where you cannot specify the parameterized form of the model a priori, thus making the model a black box.) In simple linear regression, if we increase \\(x\\) by one unit, we can immediately infer how the response value changes: \\[ \\hat{Y}&#39; = \\hat{\\beta}_0 + \\hat{\\beta}_1 (x + 1) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_1 = \\hat{Y} + \\hat{\\beta}_1 \\,. \\] For logistic regression, the situation is not as straightforward, because \\(p\\) changes non-linearly as a function of \\(x\\). So we fall back on the concept of odds: \\[ O(x) = \\frac{p(x)}{1-p(x)} = \\exp\\left(\\hat{\\beta}_0+\\hat{\\beta}_1x\\right) \\,. \\] If, e.g., \\(O(x) = 4\\), then that means that, given \\(x\\), you are four times more likely to sample a success (1) than a failure (0). How does the odds change if we add one unit to \\(x\\)? \\[ O(x+1) = e^{\\hat{\\beta}_0+\\hat{\\beta}_1(x+1)} = \\exp\\left(\\hat{\\beta}_0 + \\hat{\\beta}_1x\\right) \\times \\exp\\left(\\hat{\\beta}_1\\right) = O(x) \\exp\\left(\\hat{\\beta}_1\\right) \\,. \\] The odds change by a factor of \\(\\exp\\left(\\hat{\\beta}_1\\right)\\), which can be greater than or less than one, depending on the sign of \\(\\hat{\\beta}_1\\). 3.9.1 Logistic Regression in R: Star-Quasar Classification When observed with a telescope, a star is a point-like object, as opposed to a galaxy, which has some angular extent. Telling stars and galaxies apart visually is thus, in relative terms, easy. However, if the galaxy has an inordinately bright core because the supermassive black hole at its center is ingesting matter at a high rate, that core can outshine the rest of the galaxy so much that the galaxy appears to be point-like. Such galaxies with bright cores are dubbed “quasars,” or “quasi-stellar objects,” and they are much harder to tell apart from stars. Below we run multiple logistic regression on a dataset with 500 stars and 500 quasars (so \\(n = 1000\\)). The response variable is a factor variable with two levels, which we dub “Class 0” (here, QSO) and “Class 1” (STAR). (The mapping of qualitative factors to quantitative levels is by default alphabetical; as STAR comes after QSO, STAR gets mapped to Class 1.) To keep the analysis in the realm of the “simple” we will utilize a single predictor variable, col.iz, that represents the difference in the object’s brightness at two infrared wavelengths. Note that we use glm() rather than lm() (which we use for simple linear regression), and that we specify family=binomial, which means “do logistic regression.” Also note that we follow statistical learning convention by splitting the data into a training set (which we use to learn the model) and a test set (which we use to assess the learned model). There is no unique specification for how to split data; here we randomly put 70% into the training set, with the remainder comprising the test set. # run simple logistic regression on training dataset log.out &lt;- glm(class~col.iz,data=df.train,family=binomial) summary(log.out) ## ## Call: ## glm(formula = class ~ col.iz, family = binomial, data = df.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7460 -1.1661 0.3405 1.1280 2.0606 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.16841 0.09265 1.818 0.06911 . ## col.iz -0.96729 0.31051 -3.115 0.00184 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 970.41 on 699 degrees of freedom ## Residual deviance: 958.16 on 698 degrees of freedom ## AIC: 962.16 ## ## Number of Fisher Scoring iterations: 4 The summary() of a learned logistic regression model is similar to, yet different from, that for a linear regression model. The first important lines are the ones indicating the “deviance residuals,” with the residual for the \\(i^{\\rm th}\\) datum defined as \\[ d_i = \\mbox{sign}(Y_i - \\hat{Y}_i) \\sqrt{-2[Y_i \\log \\hat{Y}_i + (1-Y_i)\\log(1-\\hat{Y}_i)]} \\,, \\] where \\[ \\hat{Y}_i = \\hat{p}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i)}{1 + \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i)} \\,. \\] The sum of the squared deviance residuals is equal to \\(-2 \\log \\mathcal{L}_{max}\\), where \\(\\mathcal{L}_{max}\\) is the joint likelihood of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), given \\(\\mathbf{x}\\). Here, the deviance residuals are seemingly well-balanced around zero. The coefficients can be translated to \\(y\\)-coordinate values as follows: the intercept is \\(e^{0.168}/(1+e^{0.168} = 0.542\\) (and is the probability the a sampled datum with col.iz equal to zero is a STAR), while the odds ratio is \\(O_{new}/O_{old} = e^{-0.967}\\) (so that every time col.iz increases by one unit, the probability that a sampled datum is a star is 0.380 times what it had been before: the higher the value of col.iz, the less and less likely that a sampled datum is actually a star, and the more and more likely that it is a quasar. The numbers in the other three columns of the coefficients section are estimated numerically using the behavior of the likelihood function (specifically, the rates at which it curves downward away from the surface’s maximum, along each parameter axis). Some details about how the numbers are calculated are given in an example in the “Covariance and Correlation” section of Chapter 6. It suffices to say here that under the assumption that \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are both normally distributed random variables, we fail to reject the null hypothesis that the intercept is 0 (or equivalently \\(y = 1/2\\)), while we do reject the null hypothesis that \\(\\beta_1 = 0\\). The null deviance is \\(-2 \\log \\mathcal{L}_{max,o}\\), where \\(\\mathcal{L}_{max,o}\\) is the maximum likelihood when \\(\\beta_1\\) is set to zero. The residual deviance is \\(-2 \\log \\mathcal{L}_{max}\\). The difference between these values (here, 970.41-958.16 = 12.25) is, under the null hypothesis that \\(\\beta_1 = 0\\), assumed to be chi-square-distributed for 699-698 = 1 degree of freedom. The \\(p\\)-value is thus 1 - pchisq(12.25,1) or 4.65 \\(\\times\\) 10\\(^{-4}\\): we reject the null hypothesis that \\(\\beta_1 = 0\\). (To be clear, this test is analogous to the \\(F\\)-test in linear regression, and is testing \\(H_o: \\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\) versus \\(H_a:\\) at least one of the \\(\\beta_i\\)’s is non-zero. Because the sampling distribution here is the chi-square distribution as opposed to the normal distribution, the \\(p\\)-value here will not match the \\(p\\)-value seen for col.iz in the coefficients section.) Last, the AIC, or Akaike Information Criterion, is \\(-2\\) times the model likelihood (or here, the deviance) plus two times the number of variables (here, 2, as the intercept is counted as a variable). Adding the number of variables acts to penalize those models with more variables: the improvement in the maximum likelihood has to be sufficient to justify added model complexity. Discussion of the mathematical details of AIC is beyond the scope of this book; it suffices to say that if we compute it when learning a suite of different models, we would select the model with the smallest value. 3.9.2 Logistic Regression in R: Star-Quasar Classification As indicated in the text above, classification, i.e., the step of predicting which class a datum belongs to, can be tricky when we train a logistic regression model given unbalanced classes. But even if the classes are balanced, there are numerous metrics that one can compute to indicate the overall quality of the model. Here, we simply show how to compute the misclassification rate or MCR for a model trained given class balance. # we assume the trained model details contained in log.out from above # output Class 1 probabilities for the test dataset log.prob &lt;- predict(log.out,newdata=df.test,type=&quot;response&quot;) # if the probability is &gt; 0.5, map the datum to Class 1 (STAR), etc. log.pred &lt;- ifelse(log.prob&gt;0.5,&quot;STAR&quot;,&quot;QSO&quot;) # output &quot;confusion matrix&quot; table(log.pred,df.test$class) ## ## log.pred QSO STAR ## QSO 150 146 ## STAR 0 4 Each row of the output confusion matrix corresponds to a predicted class: the algorithm predicts that we have observed 103+51 = 154 quasars and 47+99 = 146 stars. Each column corresponds to the true class: there are 150 quasars and 150 stars in the test dataset. There are 51 stars that are misclassified as quasars and 47 quasars that are misclassified as stars. Thus the rate of misclassification is (51+47)/300 = 0.327…the model predicts the wrong class 32.7 percent of the time. This seems sub-optimal, and it is…but it is a reflection that we are illustrating simple logistic regression. In a real-life setting, there is more than one predictor variable, and the misclassification rate is actually \\(\\approx\\) 15%. 3.10 Naive Bayes Regression The Naive Bayes model is the basis for perhaps the simplest probabilitic classifier, one that is oft-used for, e.g., detecting spam emails. The meanings of the words “Naive” and “Bayes” will become more clear below. (Note that one would often see this model referred to as the “Naive Bayes classifier.” However, as noted in the last section, there are actually two steps in classification, the first being the estimation of probabilities that a given datum belongs to each class, and the second being the mapping of those probabilities to class predictions. In the last section and here, we are only focusing on the generation of predicted probabilities…so we will call this section Naive Bayes Regression.) Let’s assume that we are in a similar setting as the one for logistic regression, but instead of having a response that is a two-level factor variable (i.e., one the represents two classes), the number of levels is \\(K \\geq 2\\). (So instead of just, e.g., “chocolate” and “vanilla” as our response variable values, we can add “strawberry” and other ice cream flavors too!) Also, we note that model is best understood is a multiple regression context, i.e., one where there is more than one predictor variable. (We denote the vector of predictor variable values associated with each response value as \\(\\mathbf{x}\\). This vector is of length \\(p\\).) The ultimate goal of the model is to assign conditional probabilities for each response class, given a datum \\(x\\): \\(p(C_k \\vert \\mathbf{x})\\), where \\(C_k\\) denotes “class \\(k\\).” How do we derive this quantity? The first step is to apply Bayes’ rule (hence the “Bayes” in “Naive Bayes”): \\[ p(C_k \\vert \\mathbf{x}) = \\frac{p(C_k) p(\\mathbf{x} \\vert C_k)}{p(\\mathbf{x})} \\propto p(C_k) p(\\mathbf{x} \\vert C_k) \\,. \\] We use the proportionality symbol \\(\\propto\\) here because we do not need to evaluate \\(p(\\mathbf{x})\\), which is a constant in any given analysis. The next step is to expand \\(p(\\mathbf{x} \\vert C_k)\\): \\[ p(\\mathbf{x} \\vert C_k) = p(x_1,\\ldots,x_p \\vert C_k) = p(x_1 \\vert x_2,\\ldots,x_p,C_k) p(x_2 \\vert x_3,\\ldots,x_p,C_k) \\cdots p(x_p \\vert C_k) \\,. \\] (This is just the multiplicative law of probability in action, as applied to a conditional probability. See section 1.4.) The right-most expression above is one that is difficult to evaluate in practice, given all the conditions that must be jointly applied…so this is where the “Naive” aspect of the model comes in. We simplify the expression by assuming (most often incorrectly!) that the predictor variables are all mutually independent, so that \\[ p(x_1 \\vert x_2,\\ldots,x_p,C_k) p(x_2 \\vert x_3,\\ldots,x_p,C_k) \\cdots p(x_p \\vert C_k) ~~ \\rightarrow ~~ p(x_1 \\vert C_k) \\cdots p(x_p \\vert C_k) \\] and thus \\[ p(C_k \\vert \\mathbf{x}) \\propto p(C_k) \\prod_{i=1}^p p(x_i \\vert C_k) \\,. \\] OK…but where do we go from here? We need to make further assumptions! We need to assign “prior probabilities” \\(p(C_k)\\) to each class. Common choices are \\(1/K\\) (we view each class as equally probable before we gather data) and \\(n_k/n\\) (the number of observed data of class \\(k\\) divided by the overall sample size). We also need to assume probability mass and density functions \\(p(x_i \\vert C_k)\\) for each predictor variable (pmfs if \\(x_i\\) is discrete and pdfs if \\(x_i\\) is continuous). For pmfs, standard assumptions are binomial or multinomial distributions, depending on the number of levels, with the observed proportions in each level informing the category probability estimate. For pdfs, the standard assumption is that the data are distributed normally, with \\(\\hat{\\mu} = \\bar{x_i}\\) and \\(\\hat{\\sigma^2} = s_i^2\\) (the sample variance). Ultimately, this model depends on a number of (perhaps unjustified) assumptions. Why would we ever use it? Because the assumption of mutual independence makes model evaluation fast. Naive Bayes is rarely the model underlying the best classifier for any given problem, but when speed is needed (such as in the identification of spam email), one’s choices are limited. (As as general rule: if a model is simple to implement, one should implement it, even if the a priori expectation is that another model will ultimately generate better results. One never knows…) 3.10.1 Naive Bayes Regression With Categorical Predictors Let’s assume that we have collected the following data: \\(x_1\\) \\(x_2\\) \\(Y\\) Yes Chocolate True Yes Chocolate False No Vanilla True No Chocolate True Yes Vanilla False No Chocolate False No Vanilla False We learn a Naive Bayes model given these data, in which we assume “False” is Class 0 and “True” is Class 1. If we have a new datum \\(\\mathbf{x}\\) = (“Yes”,“Chocolate”), what is the probability that the response is “True”? We seek the quantity \\[ p(C_1 \\vert \\mathbf{x}) = \\frac{p(C_1) p(\\mathbf{x} \\vert C_1)}{p(\\mathbf{x})} = \\frac{p(C_1) p(x_1 \\vert C_1) p(x_2 \\vert C_1)}{p(x_1,x_2)} \\,. \\] When we examine the data, we see that \\(p(C_1) = 3/7\\), as there are three data out of seven with the value \\(Y\\) = “True.” Now, given \\(C_1\\), at what rate do we observe “Yes”? (One time out of three…so \\(p(x_1 \\vert C_1) = 1/3\\).) What about “Chocolate”? (Two times out of three…so \\(p(x_2 \\vert C_1) = 2/3\\).) The numerator is thus \\(3/7 \\times 1/3 \\times 2/3 = 2/21\\). The denominator, \\(p(x_1,x_2)\\), is determined by utilizing the Law of Total Probability: \\[\\begin{align*} p(x_1,x_2) &amp;= p(x_1 \\vert C_1) p(x_2 \\vert C_1) p(C_1) + p(x_1 \\vert C_2) p(x_2 \\vert C_2) p(C_2) \\\\ &amp;= 2/21 + 2/4 \\times 2/4 \\times 4/7 = 2/21 + 4/28 = 2/21 + 3/21 = 5/21 \\,. \\end{align*}\\] And so now we know that \\[ p(\\mbox{True} \\vert \\mbox{Yes,Chocolate}) = \\frac{2/21}{5/21} = \\frac{2}{5} \\,. \\] 3.10.2 Naive Bayes Regression With Continuous Predictors Let’s assume you have the following data regarding credit-card defaults, where \\(x\\) represents the credit-card balance and \\(Y\\) is a categorical variable indicating whether a default has occurred: \\(x\\) 1487.00 324.74 988.21 836.30 2205.80 927.89 712.28 706.16 1774.69 \\(Y\\) Yes No No No Yes No No No Yes Let’s now suppose someone came along with a credit balance of $1,200. According to the Naive Bayes model, given this balance, what is the probability of a credit default? The Naive Bayes model in this particular case is \\[ p(Y \\vert x) = \\frac{p(x \\vert Y) p(Y)}{p(x \\vert Y) p(Y) + p(x \\vert N) p(N)} \\,, \\] where we can observe immediately that \\(p(Y) = 3/9 = 1/3\\) and \\(p(N) = 6/9 = 2/3\\). To compute, e.g., \\(p(x \\vert Y)\\), we first compute the sample mean and sample standard deviation for the observed data for which \\(Y\\) is “Yes”: x &lt;- c(1487.00,2205.80,1774.69) round(mean(x),2) ## [1] 1822.5 round(sd(x),2) ## [1] 361.78 The mean is $1,822.50 and the standard deviation is $361.78, so the probability density associated with observing $1,200 is dnorm(1200,mean=mean(x),sd=sd(x)) ## [1] 0.0002509367 The density is 2.51 \\(\\times\\) 10\\(^{-4}\\). As far as for those data for which \\(Y\\) is “No”: x &lt;- c(324.74,988.21,836.30,927.89,712.28,706.16) dnorm(1200,mean=mean(x),sd=sd(x)) ## [1] 0.0002748351 The density is 2.75 \\(\\times\\) 10\\(^{-4}\\). Thus \\[ p(Y \\vert x) = \\frac{2.51 \\cdot 0.333}{2.51 \\cdot 0.333 + 2.75 \\cdot 0.667} = \\frac{0.834}{0.834 + 1.834} = 0.313 \\,. \\] There is roughly a 1 in 3 chance that a person with a credit balance of $1,200 will default on their debt. 3.10.3 Naive Bayes Applied to Star-Quasar Data In the previous section, we show how one can map Class 1 probabilities generated by a multiple linear regression model to actual class predictions, and that by displaying the result via a confusion matrix, we can determine the model’s misclassification rate. We repeat that exercise here, for the Naive Bayes model, using functionality in R’s contributed e1071 package. library(e1071) nb.out &lt;- naiveBayes(class~.,data=df.train) # map probabilities directly to class predictions nb.pred &lt;- predict(nb.out,newdata=df.test,type=&quot;class&quot;) table(nb.pred,df.test$class) ## ## nb.pred QSO STAR ## QSO 144 42 ## STAR 6 108 We see that the Naive Bayes model performs poorly with stars: this indicates that the underlying assumptions of the Naive Bayes model hold (much!) better for quasars than they do for stars. 3.11 The Beta Distribution The beta distribution is a continuous distribution that is commonly used to model random variables with finite, bounded domains. Its probability density function is given by \\[ f_X(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\,, \\] where \\(x \\in [0,1]\\), \\(\\alpha\\) and \\(\\beta\\) are both \\(&gt; 0\\), and the normalization constant \\(B(\\alpha,\\beta)\\) is \\[ B(\\alpha,\\beta) = \\int_0^1 x^{\\alpha-1}(1-x)^{\\beta-1} dx = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\,. \\] (See Figure 3.12.) We have seen \\(\\Gamma(\\alpha)\\), the gamma function, before; it is defined as \\[ \\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha-1} e^{-x} dx \\,. \\] There are two things to note about the gamma function. The first is its recursive property: \\(\\Gamma(\\alpha+1) = \\alpha \\Gamma(\\alpha)\\). (This can be shown by applying integration by parts.) The second is that when \\(\\alpha\\) is a positive integer, the gamma function takes on the value \\((\\alpha-1)! = (\\alpha-1)(\\alpha-2) \\cdots 1\\). (Note that \\(\\Gamma(1) = 0! = 1\\).) Regarding the statement above about “model[ing] random variables with finite, bounded domains”: if a set of \\(n\\) iid random variable \\(\\mathbf{X}\\) has domain \\([a,b]\\), we can define a new set of random variables \\(\\mathbf{Y}\\) via the transformation \\[ \\mathbf{Y} = \\frac{\\mathbf{X}-a}{b-a} \\] such that the domain becomes \\([0,1]\\). We can model these newly defined data with the beta distribution. (Note the word can: we can model these data with the beta distribution, but we don’t have to, and it may be the case that there is another distribution bounded on the interval \\([0,1]\\) ultimately better describes the data-generating process. The beta distribution just happens to be commonly used.) But…in the end, what does this all have to do with the binomial distribution, the subject of this chapter? We know the binomial has a parameter \\(p \\in [0,1]\\) and the domain of the beta distribution is \\([0,1]\\), but is there more? Let’s write down the binomial pmf: \\[ \\binom{k}{x} p^x (1-p)^{k-x} = \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\,. \\] This pmf dictates the probability of observing a particular value of \\(x\\) given \\(k\\) and \\(p\\). But what if we turn this around a bit…and examine this function if we fix \\(k\\) and \\(x\\) and vary \\(p\\) instead? In other words, let’s examine the likelihood function \\[ \\mathcal{L}(p \\vert k,x) = \\frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\,. \\] We can see immediately that the likelihood has the form of a beta distribution if we set \\(\\alpha = x+1\\) and \\(\\beta = k-x+1\\): \\[ \\mathcal{L}(p \\vert k,x) = \\frac{\\Gamma(\\alpha+\\beta-1)}{\\Gamma(\\alpha)\\Gamma(\\beta)} p^{\\alpha-1} (1-p)^{\\beta-1} \\,, \\] except that the normalization term is not quite right: here we have \\(\\Gamma(\\alpha+\\beta-1)\\) instead of \\(\\Gamma(\\alpha+\\beta)\\). But that’s fine: there is no requirement that a likelihood function integrate to one over its domain. (Here, as the interested reader can verify, the likelihood function integrates to \\(1/(k+1)\\).) So, in the end, if we observe a random variable \\(X \\sim\\) Binomial(\\(k,p\\)), then the likelihood function \\(\\mathcal{L}(p \\vert k,x)\\) has the shape (if not the normalization) of a Beta(\\(x+1,k-x+1\\)) distribution. Figure 3.12: Three examples of beta probability density functions: Beta(2,2) (red), Beta(4,2) (blue), and Beta(2,3) (green). Beta Distribution - R Functions quantity R function call PDF dbeta(x,shape1,shape2) CDF pbeta(x,shape1,shape2) Inverse CDF qbeta(q,shape1,shape2) \\(n\\) iid random samples rbeta(n,shape1,shape2) 3.11.1 The Expected Value of a Beta Random Variable The expected value of a random variable sampled from a Beta(\\(\\alpha,\\beta\\)) distribution is \\[\\begin{align*} E[X] = \\int_0^1 x f_X(x) dx &amp;= \\int_0^1 x \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha+1,\\beta)} \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} dx \\\\ &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\int_0^1 \\frac{x^{\\alpha} (1-x)^{\\beta-1}}{B(\\alpha+1,\\beta)} dx \\\\ &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\,. \\end{align*}\\] The last result follows from the fact that the integrand is the pdf for a Beta(\\(\\alpha+1,\\beta\\)) distribution, and the integral is over the entire domain, hence the integral evaluates to 1. Continuing, \\[\\begin{align*} E[X] &amp;= \\frac{B(\\alpha+1,\\beta)}{B(\\alpha,\\beta)} \\\\ &amp;= \\frac{\\Gamma(\\alpha+1) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta+1)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{(\\alpha+\\beta)\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)} \\\\ &amp;= \\frac{\\alpha}{\\alpha+\\beta} \\,. \\end{align*}\\] Here, we take advantage of the recursive property of the gamma function. We can utilize a similar strategy to determine the variance of a beta random variable, starting by computing \\(E[X^2]\\) and utilizing the shortcut formula \\(V[X] = E[X]^2 - (E[X])^2\\). The final result is \\[ V[X] = \\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\,. \\] 3.11.2 The Sample Median of a Uniform(0,1) Distribution The Uniform(0,1) distribution is \\[ f_X(x) = 1 ~~~ x \\in [0,1] \\,. \\] Let’s assume that we draw \\(n\\) iid data from this distribution, with \\(n\\) odd. Then we can utilize a result from order statistics to write down the pdf of the sample median, which is the \\(j^{\\rm th}\\) order statstic (where \\(j = (n+1)/2\\)): \\[ f_{((n+1)/2)}(x) = \\frac{n!}{\\left(\\frac{n-1}{2}\\right)! \\left(\\frac{n-1}{2}\\right)!} f_X(x) \\left[ F_X(x) \\right]^{(n-1)/2} \\left[ 1 - F_X(x) \\right]^{(n-1)/2} \\,. \\] Given that \\[ F_X(x) = \\int_0^x dy = x ~~~ x \\in [0,1] \\,, \\] we can write \\[ f_{((n+1)/2)}(x) = \\frac{n!}{\\left(\\frac{n-1}{2}\\right)! \\left(\\frac{n-1}{2}\\right)!} x^{(n-1)/2} (1-x)^{(n-1)/2} \\,, \\] for \\(x \\in [0,1]\\). This function has both the form of a beta distribution (with \\(\\alpha = \\beta = (n+1)/2\\)) and the domain of a beta distribution, so \\(\\tilde{X} \\sim\\) Beta\\(\\left(\\frac{n+1}{2},\\frac{n+1}{2}\\right)\\). (In fact, we can go further and state a more general result: \\(X_{(j)} \\sim\\) Beta(\\(j,n-j+1\\)): all the order statistics for data drawn from a Uniform(0,1) distribution are beta-distributed random variables!) 3.11.3 Testing Hypotheses Using the Sample Median of a Uniform(0,1) Distribution Let’s assume that we draw \\(n\\) iid data, where \\(n\\) is an odd number, from a Uniform(\\(0,\\theta\\)) distribution, and that we wish to test the hypothesis \\(H_o: \\mu = \\mu_o = 1/2\\) versus \\(H_a: \\mu = \\mu_a &gt; \\mu_o\\) at level \\(\\alpha\\). (Let’s also assume that all the data we observe lie in the range \\([0,1]\\)…otherwise the null cannot be correct. We will return to this point in Chapter 5.) It sounds like we might use the Neymann-Pearson lemma here, but as we will see in Chapter 5, that would dictate a different test statistic than what we want to use here, which is the sample median (which is not a sufficient statistic). So here we will simply fall back on the methodology shown in Chapters 1 and 2, i.e., we will solve \\[ F_Y(y_{1-\\alpha} \\vert \\theta_o) - \\left(1 - \\alpha\\right) = 0 \\,, \\] where \\(Y = X_{((n+1)/2)}\\) is the sample median. We start by deriving the cdf \\(F_{((n+1)/2)}\\): \\[\\begin{align*} F_{((n+1)/2)}(x) &amp;= \\int_0^x f_{((n+1)/2)}(v) dv = \\ldots \\,. \\end{align*}\\] But, there’s an issue. While the expressions for the cdfs for the minimum and maximum values are comprised of single terms, the expression for the cdf of the median is comprised of a summation of terms. (Go back to the section on order statistics earlier in this chapter to verify this.) Hence we cannot work with this cdf by hand. So…what can we do? Utilize numerical integration, that’s what. Here is the code we need: # This computes the cdf of the sample median at coordinate x f &lt;- function(x,n) { j &lt;- (n-1)/2 # lfactorial(a) == log(a!) # exp(lfactorial(a)) == a! exp(lfactorial(n)-2*lfactorial(j))*x^j*(1-x)^j } # Find root of F_Y(y) - (1-alpha) g &lt;- function(y,n,alpha) { F.med &lt;- integrate(f,0,y,n=n) F.med$value - (1-alpha) } uniroot(g,interval=c(0,1),n=5,alpha=0.05)$root ## [1] 0.8107542 In words, the function f above computes \\(F_{((n+1)/2)}(x)\\), while the function g evaluates the function that we are trying to solve, specifically \\(F_{((n+1)/2)}(x) - (1-\\alpha) = 0\\). The call to uniroot() specifies that the root is between 0 and 1 (since \\(\\theta_o = 2\\mu_o = 1\\)). We see that if \\(n = 5\\), we would reject the null hypothesis that \\(\\mu_o = 1/2\\) if and only if \\(X_{((n+1)/2)} \\geq 0.811\\), i.e., if three of the five values are greater than 0.811. Something to take away from this example is to realize that just because we cannot write down an analytical expression (in this case, for the cdf of the median of \\(n\\) Uniform(0,1) random variables), we should not just give up, as it may be easy to implement numerical methods! (As it was in this case.) 3.12 The Multinomial Distribution Let’s suppose we are in a situation in which we are gathering categorical data. For instance, we might be throwing a ball and recording which of bins numbered 1 through \\(m\\) it lands in; categorizing the condition of old coins as “mint,” “very good,” “fine,” etc.; or classifying a galaxy as being a spiral galaxy, an elliptical galaxy, or an irregular galaxy. These are examples of multinomial trials, which is a generalization of the concept of binomial trials to situations where the number of possible outcomes is \\(m \\geq 2\\) rather than \\(m = 2\\). Earlier in this chapter, we wrote down the five properties of binomial trials. The analogous properties of a multinomial experiment are the following: It consists of \\(k\\) trials, with \\(k\\) chosen in advance. There are \\(m\\) possible outcomes for each trial. A trial may have no more than one realized outcome. The outcomes of each trial are independent. The probability of achieving the \\(i^{th}\\) outcome is \\(p_i\\), a constant quantity. The probabilities of each outcome sum to one: \\(\\sum_{i=1}^m p_i = 1\\). The number of trials that achieve a particular outcome is \\(X_i\\), with \\(\\sum_{i=1}^m X_i = k\\). The probability of any given outcome \\(\\{X_1,\\ldots,X_m\\}\\) is given by the multinomial probability mass function: \\[ p(x_1,\\ldots,x_m \\vert p_1,\\ldots,p_m) = \\frac{k!}{x_1! \\cdots x_m!}p_1^{x_1}\\cdots p_m^{x_m} \\,. \\] The distribution for any given \\(X_i\\) itself is binomial, with \\(E[X_i] = kp_i\\) and \\(V[X_i] = kp_i(1-p_i)\\). This makes intuitive sense, as one either observes \\(i\\) as a trial outcome (success), or something else (failure). However, the \\(X_i\\)’s are not independent random variables; the covariance between \\(X_i\\) and \\(X_j\\), a metric of linear dependence, is Cov(\\(X_i\\),\\(X_j\\)) = \\(-kp_ip_j\\) if \\(i \\neq j\\). (We will discuss the concept of covariance at length in Chapter 6.) This also makes intuitive sense: given that the number of trials \\(k\\) is fixed, observing more data that achieve one outcome will usually mean we will observe fewer data achieving any given other outcome. Multinomial Distribution - R Functions quantity R function call PMF dmultinom(x,m,p) (\\(x\\) and \\(p\\) are vectors of length \\(m\\)) \\(n\\) iid random samples rmultinom(n,m,p) (\\(p\\) is a vector of length \\(m\\)) 3.13 Chi-Square-Based Hypothesis Testing Imagine that we are scientists, sitting on a platform in the middle of a plain. We are recording the number of animals of a particular species that we see, and the azimuthal angle for each one. (An azimuthal angle is, e.g., \\(0^\\circ\\) when looking directly north, and 90\\(^\\circ\\), 180\\(^\\circ\\), and 270\\(^\\circ\\) as we look east, south, and west, etc.) The question we want to answer is, are the animals uniformly distributed as a function of angle? There is no unique way to answer this question. However, a very common approach is to bin the data and use a chi-square goodness-of-fit (GoF) hypothesis test. Let’s assume we’ve observed 100 animals and that we record the numbers seen in each of four quadrants: angle range 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) number of animals 28 32 17 23 There is nothing “special” about the choice of four quadrants\\(-\\)we could have chosen eight, etc.\\(-\\)but as we’ll see below, the chi-square GoF test is an “approximate” test and its results become more precise as the numbers of data in each bin get larger. So there is a tradeoff if we increase the number of quadrants: we might be able to detect smaller-scale non-uniformities, but are test results will become less precise! To perform a chi-square GoF test with these data, we first specify are null and alternative hypotheses: \\[\\begin{align*} H_o &amp;: p_1 = p_{1,o},\\cdots,p_m = p_{m,o} ~~ vs. ~~ H_a &amp;: \\mbox{at least two of the probabilities differ} \\,, \\end{align*}\\] where \\(p_{i,o}\\) is the null hypothesis proportion in bin \\(i\\). Here, \\(k = 100\\), \\(m = 4\\), and \\(p_{1,o} = p_{2,o} = p_{3,o} = p_{4,o} = 0.25\\)…we expect, under the null, to see 25 animals in each quadrant. To make the connection with the previous section of the chapter now, the null hypothesis is that the data are multinomially distributed with specified proportions being expected in each of the \\(m\\) defined bins. As we might imagine, performing a hypothesis test of the form given above would be computationally difficult; multinomial distributions are intrinsically high-dimensional and the data (the numbers of counts in each bin) are not iid. In 1900, the statistician Karl Pearson proposed a “workaround” for testing multinomial hypotheses, by utilizing the following as a test statistic: \\[ W = \\sum_{i=1}^m \\frac{(X_i-E[X_i])^2}{E[X_i]} = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} \\,. \\] This looks suspiciously familiar, yet “off,” at the same time. Above, we stated that the \\(X_i\\)’s in a multinomial experiment are themselves binomially distributed, and thus if we were to standardize them, we would get \\[ Z_i = \\frac{X_i - E[X_i]}{\\sqrt{V[X_i]}} = \\frac{X_i - kp_i}{\\sqrt{kp_i(1-p_i)}} \\,. \\] If \\(kp_i\\) is sufficiently large, then this standardized quantity is approximately standard normally distributed…which means that the square of this quantity is approximately chi-square distributed for one degree of freedom. So: if we assume that \\(k\\) is sufficiently large, and the \\(p_i\\)’s are sufficiently small, such that we can assume \\(kp_i(1-p_i) \\approx kp_i\\), then we assume that \\[ W = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} \\approx \\sum_{i=1}^m Z_i^2 \\stackrel{d}{\\rightarrow} W \\sim \\chi_{m-1}^2 \\,, \\] i.e., \\(W\\) converges in distribution to a random variable that is chi-square-distributed with \\(m-1\\) degrees of freedom. (We subtract 1 because only \\(m-1\\) of the multinomial probabilities can freely vary; the \\(m^{th}\\) one is constrained by the fact that the probabilities must sum to 1. This constraint is what makes multinomial data not iid.) The chi-square GoF test is an upper-tail test in which, in addition to the approximations made above, we also assume that we can ignore the correlations between the \\(X_i\\)’s in different bins. For this test, the rejection region boundary is \\(\\{ W &gt; \\chi_{1-\\alpha}^2 = F_W^{-1}(1-\\alpha) \\}\\) for \\(m-1\\) degrees of freedom (in R, qchisq(1-alpha,m-1)); the \\(p\\)-value is \\(1 - F_{W}(x^2)\\) (e.g., 1-pchisq(x.squared,m-1)); and the rule-of-thumb is that \\(kp_i\\) must be \\(\\geq\\) 5 in each bin for the test to yield a valid result. This last point underscores the tradeoff between splitting the data over more bins and test precision! In a chi-square GoF test, the inputs are the observed data and the hypothesized proportions. There are variations on this test in which the inputs are simply the observed data, variations that differ by how the data are collected: chi-square test of independence: the question is whether two variables are associated with each other in a population; subjects are selected and the values of two variables are recorded for each. For instance, we might select \\(k\\) people at random and record whether or not they have had Covid-19, and also record whether or not they initially had zero, one, or two vaccine shots. chi-square test of homogeneity: the question is whether the distribution of a single variable is the same for two subgroups of a population, with subjects being selected randomly from each subgroup separately. For instance, we might select \\(k\\) people under 20 years of age and ask if they prefer vanilla, chocolate, or strawberry ice cream, and then repeat the process for people of age 20 or over. Whether we perform a test of independence versus one of homogeneity affects the interpretation of results, but algorithmically the tests are identical. Under the null hypothesis, \\[ \\widehat{E[X_{ij}]} = \\frac{r_i c_j}{k} \\,, \\] i.e., the expected value in the cell in row \\(i\\) and column \\(j\\) is the product of the total number of data in row \\(i\\) and column \\(j\\), divided by the total number of data overall. Then, \\[ W = \\sum_{i=1}^r \\sum_{j=1}^c \\frac{(X_{ij} - \\widehat{E[X_{ij}]})^2}{\\widehat{E[X_{ij}]}} \\sim \\chi_{(r-1)(c-1)}^2 ~\\mbox{under~the~null~(by~assumption!)} \\,. \\] i.e., the test statistic \\(W\\) is, under the null, assumed to be chi-square distributed for \\((r-1)\\) times \\((c-1)\\) degrees of freedom. 3.13.1 Chi-Square Goodness of Fit Test Let’s work with the data presented above at the beginning of this section: angle range 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) number of animals 28 32 17 23 As stated above, we expect 25 counts in each bin. Given this expectation, are these data plausible, at level \\(\\alpha = 0.05\\)? We first compute the test statistic: \\[ W = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} = \\frac{(28-25)^2}{25} + \\frac{(32-25)^2}{25} + \\frac{(17-25)^2}{25} + \\frac{(23-25)^2}{25} = \\frac{9}{25} + \\frac{49}{25} + \\frac{64}{25} + \\frac{4}{25} = \\frac{126}{25} = 5.04 \\,. \\] The number of degrees of freedom is \\(m-1 = 3\\), so the rejection region boundary is \\(F_W(1-\\alpha) = F_W(0.95) =\\) qchisq(0.95,3) = 7.815. Since 5.04 \\(&lt;\\) 7.815, we fail to reject the null hypothesis that the animals are distributed uniformly as a function of azimuthal angle. (The \\(p\\)-value is 1-pchisq(5.04,3) = 0.169.) See Figure 3.13. Figure 3.13: An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, so we fail to reject the null hypothesis. 3.13.2 Simulating an Exact Multinomial Test Let’s assume the same data as in the last example. One reason\\(-\\)in fact, the reason\\(-\\)why we utilize the chi-square GoF test when analyzing these data is that “it has always been done this way.” Stated differently, we have historically not worked with the multinomial distribution directly because we couldn’t…at least, not until computers came along. But now we can estimate the \\(p\\)-value for the hypothesis in the last example via simulation. Will we achieve a result very different from that above ($p = 0.169)? set.seed(101) O &lt;- c(28,32,17,23) p &lt;- rep(1/4,4) num.sim &lt;- 100000 k &lt;- sum(O) m &lt;- length(O) pmf.obs &lt;- dmultinom(O,prob=p) # the observed multinomial pmf X &lt;- rmultinom(num.sim,k,p) # generates an m x num.sim matrix # (m determined as the length of p) pmf &lt;- apply(X,2,function(x){dmultinom(x,prob=p)}) # simulated pmf&#39;s sum(pmf&lt;pmf.obs)/num.sim ## [1] 0.15974 We observe \\(p = 0.1597\\). This is close to, but at the same time still substantially less than, 0.169. In case the reader is to say “but, the computation time must be much longer than for the chi-square GoF test,” the above computation takes \\(\\sim\\) 1 CPU second. The chi-square test is definitely important to know, in part because “it has always been done this way”…but we would argue that when a simulation of the exact test is possible, one should code that simulation! (And run as many simulations as possible, to reduce uncertainty in the final result. Here, we can take our estimated \\(p\\)-value of 0.1597 and state that our one standard error uncertainty is approximately \\(\\sqrt{kp(1-p)}/k = 0.0011\\), i.e., we expect the true \\(p\\)-value to be in the range \\(0.1597 \\pm 3 \\cdot 0.0011\\) or \\([0.1564,0.1630]\\). 3.13.3 Chi-Square Test of Independence Let’s go back to our animal data. When we observe the animals in each quadrant, we record their color: black or red. So now are data look like this: 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) black 20 18 5 14 57 red 8 14 12 9 43 28 32 17 23 When we record two attributes for each subject (here, azimuthal angle and color), we can perform a chi-square test of independence to answer the question of whether the attributes are independent random variables. In other words, here, does the coloration depend on angle? The null hypothesis is no. We will test this hypothesis assuming \\(\\alpha = 0.05\\). We first determine the expected number of counts in each bin, \\(\\widehat{E[X_{ij}]} = r_i c_j / k\\): 0\\(^\\circ\\)-90\\(^\\circ\\) 90\\(^\\circ\\)-180\\(^\\circ\\) 180\\(^\\circ\\)-270\\(^\\circ\\) 270\\(^\\circ\\)-360\\(^\\circ\\) black 57 \\(\\cdot\\) 28/100 = 15.96 57 \\(\\cdot\\) 32/100 = 18.24 57 \\(\\cdot\\) 17/100 = 9.69 57 \\(\\cdot\\) 23/100 = 13.11 57 red 43 \\(\\cdot\\) 28/100 = 12.04 43 \\(\\cdot\\) 32/100 = 13.76 43 \\(\\cdot\\) 17/100 = 7.31 43 \\(\\cdot\\) 23/100 = 9.89 43 28 32 17 23 We can already see that working with the numbers directly is tedious. Can we make a matrix of such numbers using R? r &lt;- c(57,43) c &lt;- c(28,32,17,23) E &lt;- (r %*% t(c))/sum(r) # multiply r and the transpose of c print(E) # much better ## [,1] [,2] [,3] [,4] ## [1,] 15.96 18.24 9.69 13.11 ## [2,] 12.04 13.76 7.31 9.89 If we wish to continue using R, we need to define a matrix of observed data values: O &lt;- matrix(c(20,8,18,14,5,12,14,9),nrow=2) # fills in column-by-column print(O) ## [,1] [,2] [,3] [,4] ## [1,] 20 18 5 14 ## [2,] 8 14 12 9 Now we have what we need. The test statistic is round(sum( (O-E)^2/E ),3) ## [1] 7.805 and the number of degrees of freedom is \\((r-1)(c-1) = 1 \\cdot 3 = 3\\), so the rejection region is round(qchisq(0.95,3),3) ## [1] 7.815 We find that if \\(\\alpha = 0.05\\), we cannot reject the null hypothesis. We might be tempted to do so, as our test statistic very nearly falls into the rejection region, but we cannot. We could, if we were so inclined, remind ourselves that chi-square-based hypothesis tests are approximate, and run a simulation to try to estimate the true distribution of \\(W\\), and see what the rejection region and \\(p\\)-value actually are…that way, we might be able to actually reject the null. "],["the-poisson-and-related-distributions.html", "4 The Poisson (and Related) Distributions 4.1 Motivation 4.2 Probability Mass Function 4.3 Cumulative Distribution Function 4.4 Linear Functions of Poisson Random Variables 4.5 Point Estimation 4.6 Confidence Intervals 4.7 Hypothesis Testing 4.8 The Gamma Distribution 4.9 Poisson Regression 4.10 Chi-Square-Based Hypothesis Testing", " 4 The Poisson (and Related) Distributions 4.1 Motivation One of the challenges of a Prussian soldier’s life in the 19th century was avoiding being kicked by horses. This was no trivial matter: over one 20-year period, 122 soldiers died from injuries sustained while being kicked by horses. The statistician Ladislaus Bortkiewicz compiled the following data, showing the number of soldiers killed by horse kicks in any one Prussian army corps in any one year: \\(x\\) \\(N(x)\\) 0 109 1 65 2 22 3 3 4 1 \\(x\\) is the number of deaths observed in any one Prussian army corp in any one year, and \\(N(x)\\) is the number of corps-years in which \\(x\\) deaths were observed. (\\(N(x)\\) sums to \\(20 \\cdot 10 = 200\\), reflecting that the compiled data represent 10 army corps observed over a 20-year period.) The data presented above are an example of a process, i.e., a sequence of observations, but we can immediately see that unlike the case with coin flips, this process is not a Bernoulli process. That’s because the number of possible outcomes is greater than 2 (0 and 1); in fact, the number of possible outcomes is countably infinite, so this is not even a multinomial process. Well, the reader might say, we could simply discretize the data more finely, so that the number of possible outcomes is at least finite (multinomial) or better yet falls to two (Bernoulli). Let’s get monthly data, or daily data, or hourly data. However, there is no time period \\(\\Delta t\\) for which the number of possible outcomes is limited to some maximum value: in theory, an infinite number of soldiers could die in the same second, or even the same nanosecond, etc. But let’s keep playing with this idea of making the time periods smaller and smaller. Let the number of time periods into which we divide our observation interval, \\(k\\), go to infinity such that the probability of observing a horse-kick death \\(p \\rightarrow 0\\) and such that \\(kp \\rightarrow \\lambda\\), where \\(\\lambda\\) is a constant. Under these conditions, as we will see in the next section, the binomial distribution transforms into the Poisson distribution, which Bortkiewicz dubbed the law of small numbers. Before we go to the section, though, let’s define the Poisson distribution in words: it gives the probability of observing a particular number of events (“counts”) in a fixed interval of space and/or time, assuming there is a constant mean rate of events and the occurrence of any one event is independent of the occurrence of other events. (For those who do not wish to wait until the section on point estimation to know the final answer: \\(\\hat{\\lambda}_{MLE} = 0.61\\), i.e., if the data are plausibly Poisson distributed [which is another question to ask entirely!], the rate of death was 0.61 soldiers per corps per year.) 4.2 Probability Mass Function Recall: a probability mass function is one way to represent a discrete probablity distribution, and it has the properties (a) \\(0 \\leq P(X=x) \\leq 1\\) and (b) \\(\\sum_x P(X=x) = 1\\), where the sum is over all possible values of \\(x\\). \\[\\begin{align*} P(X=x) &amp;= \\binom{k}{x} p^x (1-p)^{k-x} \\\\ &amp;= \\frac{k!}{x!(k-x)!} \\left(\\frac{\\lambda}{k}\\right)^x \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\\\ &amp;= \\frac{k!}{(k-x)! k^x} \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\\\ &amp;= \\left(\\frac{k}{k}\\right) \\left(\\frac{k-1}{k}\\right) \\cdots \\left(\\frac{k-x+1}{k}\\right) \\left(\\frac{\\lambda^x}{x!}\\right) \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} \\rightarrow \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^{k-x} ~\\mbox{as}~~ k \\rightarrow \\infty \\\\ &amp;= \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^k \\left(1-\\frac{\\lambda}{k}\\right)^{-x} \\rightarrow \\frac{\\lambda^x}{x!} \\left(1-\\frac{\\lambda}{k}\\right)^k ~\\mbox{as}~~ k \\rightarrow \\infty \\,. \\end{align*}\\] At this point, we concentrate on the parenthetical term above. Given that \\[ \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k}\\right)^k = e^{-1} \\,, \\] we can state that \\[ \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k/\\lambda}\\right)^{k/\\lambda} = e^{-1} \\implies \\lim_{k \\rightarrow \\infty} \\left(1 - \\frac{1}{k/\\lambda}\\right)^k = e^{-k} \\,. \\] We are now in a position to write down the probability mass function for a Poisson random variable (see Figure 4.1): \\[ P(X=x) = p(x) = \\frac{\\lambda^x}{x!} e^{-\\lambda} ~\\mbox{where}~ \\lambda &gt; 0 ~\\mbox{and}~ x \\in [0,\\infty) \\,. \\] A Poisson random variable converges in distribution to a normal random variable as \\(\\lambda \\rightarrow \\infty\\), a result which affects how the Poisson has historically been implemented in hypothesis testing. (We will elaborate on this point when we return to the chi-square goodness of fit test later in the chapter.) To indicate that we have sampled a datum from a Poisson distribution, we write \\(X \\sim\\) Poisson(\\(\\lambda\\)). The expected value and variance of the Poisson distribution are \\(E[X] = \\lambda\\) and \\(V[X] = \\lambda\\), respectively; the former is derived below in an example. Figure 4.1: Poisson probability mass functions for \\(\\lambda = 1\\) (red), 5 (green), and 10 (blue). Poisson Distribution - R Functions quantity R function call PMF dpois(x,lambda) CDF ppois(x,lambda) Inverse CDF qpois(q,lambda) \\(n\\) iid random samples rpois(n,lambda) 4.2.1 The Expected Value of a Poisson Random Variable Recall: the expected value of a discretely distributed random variable is \\[ E[X] = \\sum_x x P(X=x) = \\sum_x x p_X(x) \\,, \\] where the sum is over all \\(x\\) for which \\(p_X(x) &gt; 0\\). For a Poisson distribution, the expected value is \\[ E[X] = \\sum_{x=0}^\\infty x \\frac{\\lambda^x}{x!} e^{-\\lambda} = \\sum_{x=1}^\\infty x \\frac{\\lambda^x}{x!} e^{-\\lambda} = \\sum_{x=1}^\\infty \\frac{\\lambda^x}{(x-1)!} e^{-\\lambda} \\,. \\] The goal is to move constants into or out of the summation so that the summation becomes one of a probability mass function over the entire domain of a distribution. Here, we move \\(\\lambda\\) out of the summation, and make the substitution \\(y = x-1\\); the summand then takes on the form of a Poisson pmf, summed over its entire domain, so the summation evaluates to 1: \\[ E[X] = \\lambda \\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!} e^{-\\lambda} = \\lambda \\sum_{y=0}^\\infty \\frac{\\lambda^{y}}{y!} e^{-\\lambda} = \\lambda \\,. \\] Note that a similar calculation that starts with the derivation of \\(E[X(X-1)]\\) yields the variance. 4.3 Cumulative Distribution Function Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as \\(F_X(x) = \\sum_{y\\leq x} p_Y(y)\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\), with \\(F_X(-\\infty) = 0\\) and \\(F_X(\\infty) = 1\\). For the Poisson distribution, the cdf is \\[ F_X(x) = \\sum_{y=0}^{\\lfloor x \\rfloor} p_Y(y) = \\sum_{y=0}^{\\lfloor x \\rfloor} \\frac{\\lambda^y}{y!} \\exp(-\\lambda) = \\frac{\\Gamma(\\lfloor x+1 \\rfloor,\\lambda)}{\\lfloor x \\rfloor !} \\,, \\] where \\(\\lfloor x \\rfloor\\) denotes the largest integer that is less than or equal to \\(x\\) (e.g., if \\(x\\) = 8.33, \\(\\lfloor x \\rfloor\\) = 8), and \\(\\Gamma(\\cdot,\\cdot)\\) is the upper incomplete gamma function \\[ \\Gamma(\\lfloor x+1 \\rfloor,\\lambda) = \\int_{\\lambda}^\\infty u^{\\lfloor x \\rfloor} e^{-u} du \\,. \\] (An example of an R function which computes the upper incomplete gamma function is incgam() in the pracma package.) As we are dealing with a probability mass function, the cdf is a step function, as illustrated in the left panel of Figure 4.2. Recall that because of the step-function nature of the cdf, the form of inequalities in a probabilistic statement matter: e.g., \\(P(X &lt; x)\\) and \\(P(X \\leq x)\\) will not be the same if \\(x\\) is zero or a positive integer. Recall: an inverse cdf function \\(F_X^{-1}(\\cdot)\\) takes as input the total probability \\(q \\in [0,1]\\) in the range \\((-\\infty,x]\\) and returns the value of \\(x\\). A discrete distribution has no unique inverse cdf; it is convention to utilize the generalized inverse cdf, \\(x = F_X^{-1}(q) = \\mbox{inf}\\{x : F_X(x) \\geq q\\}\\), where “inf” indicates the return the smallest value of \\(x\\) such that \\(F_X(x) \\geq q\\). In the right panel of Figure 4.2, we display the inverse cdf for the same distribution used to generate the figure in the left panel (\\(\\lambda = 2\\)). Like the cdf, the inverse cdf for a discrete distribution is a step function. Figure 4.2: Illustration of the cumulative distribution function \\(F_X(x)\\) (left) and inverse cumulative distribution function \\(F_X^{-1}(q)\\) (right) for a Poisson distribution with \\(\\lambda=2\\). Note that because the domain of the Poisson distribution is countably infinite, we do not reach \\(F_X(x) = 1\\) in this example. 4.3.1 Computing Probabilities If \\(X \\sim\\) Poisson(5), which is \\(P(4 \\leq X &lt; 6)\\)? We first note that due to the form of the inequality, we do not include \\(X=6\\) in the computation. Thus \\(P(4 \\leq X &lt; 6) = p_X(4) + p_X(5)\\), which equals \\[ \\frac{5^4}{4!}e^{-5} + \\frac{5^5}{5!}e^{-5} = \\frac{5^4}{4!}e^{-5} \\left( 1 + \\frac{5}{5} \\right) = 2\\frac{5^4}{4!}e^{-5} = 0.351\\,. \\] If we call on R: dpois(4,lambda=5) + dpois(5,lambda=5) ## [1] 0.3509347 We can also utilize cdf functions here: \\(P(4 \\leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \\leq 5) - P(X \\leq 3) = F_X(5) - F_X(3)\\), which in R is computed via ppois(5,lambda=5) - ppois(3,lambda=5) ## [1] 0.3509347 \\(X \\sim\\) Poisson(5), what is the value of \\(a\\) such that \\(P(X \\leq a) = 0.9\\)? First, we set up the inverse cdf formula: \\[ P(X \\leq a) = F_X(a) = 0.9 ~~ \\Rightarrow ~~ a = F_X^{-1}(0.9) \\] Note that we didn’t do anything differently here than we would have done in a continuous distribution setting…and we can proceed directly to R because it utilizes the generalized inverse cdf algorithm. qpois(0.9,lambda=5) ## [1] 8 4.4 Linear Functions of Poisson Random Variables Let’s assume we are given \\(n\\) iid Poisson random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Poisson(\\(\\lambda\\)). What is the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(m_X(t) = E[e^{tX}]\\). Also, if \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) as the mgf for a known family of distributions, then we can immediately identify the distribution of \\(Y\\) and the parameters of that distribution. The mgf for a Poisson random variable \\(X\\) is \\[ m_X(t) = \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] \\,. \\] (We derive this in an example below.) Thus the mgf for \\(Y = \\sum_{i=1}^n X_i\\) is \\[ m_Y(t) = \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] \\cdots \\exp\\left[\\lambda\\left(e^t-1\\right)\\right] = \\exp\\left[(\\lambda+\\cdots+\\lambda)(e^t-1)\\right] = \\exp\\left[n\\lambda(e^t-1)\\right] \\,. \\] This mgf retains the form of a Poisson mgf. We thus see that the sum of Poisson-distributed random variables is itself Poisson distributed with parameter \\(n\\lambda\\), i.e., \\(Y = \\sum_{i=1}^n X_i \\sim\\) Poisson(\\(n\\lambda\\)). While we can identify the distribution of the sum, we cannot identify the distribution of the sample mean by name: if \\(\\bar{X} = \\left(\\sum_{i=1}^n X_i\\right)/n\\), then \\[ m_{\\bar{X}}(t) = \\exp\\left[n\\lambda(e^{t/n}-1)\\right] \\,. \\] We know of no family of distributions with this mgf. We are left in the exact same situation that we faced in Chapter 3 when working with the binomial distribution, and as was the case in that chapter, we can pursue two alternatives to learn more about the distribution for \\(\\bar{X}\\): If \\(n \\gtrsim 30\\), we can utilize the Central Limit Theorem to state that \\[ \\bar{X} \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(\\lambda,\\frac{\\lambda}{n}\\right) \\,, \\] i.e., that the random variable \\(\\bar{X}\\) converges in distribution to a normal random variable with mean \\(E[\\bar{X}] = \\mu = \\lambda\\) and variance \\(V[\\bar{X}] = \\sigma^2/n = \\lambda/n\\). We can note that when we divide by \\(n\\), we are simply transforming the domain of the pmf for \\(Y = \\sum_{i=1}^n X_i\\) without changing the values of the probability masses. Since we know \\(Y \\sim\\) Poisson(\\(n\\lambda\\)), we can write \\[ p_{\\bar{X}}(\\bar{x}) = \\frac{(n\\lambda)^{n\\bar{x}}}{(n\\bar{x})!} e^{-n\\lambda} ~~~~ \\bar{x} \\in [0,1/n,2/n,\\ldots,\\infty) \\,. \\] This has the form of a Poisson pmf but not the domain, and thus the sampling distribution is unnamed. However, as usual we can apply the “general rule” for \\(\\bar{X}\\) and state immediately that \\(E[\\bar{X}] = \\mu = \\lambda\\) and \\(V[\\bar{X}] = \\sigma^2/n = \\lambda/n\\). 4.4.1 The Moment-Generating Function of a Poisson Random Variable The moment-generating function for a random variable \\(X\\) is found by utilizing the Law of the Unconscious Statistician and computing \\(E[e^{tX}]\\). If \\(X\\) is a Poisson random variable, then \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\sum_{x=0}^\\infty e^{tx} p_X(x) \\\\ &amp;= \\sum_{x=0}^\\infty e^{tx} \\frac{\\lambda^x}{x!} e^{-\\lambda} \\\\ &amp;= e^{-\\lambda} \\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} e^{tx} \\\\ &amp;= e^{-\\lambda} \\left[ 1 + \\lambda e^t + \\frac{\\lambda^2}{2!}e^{2t} + \\ldots \\right] \\\\ &amp;= e^{-\\lambda} \\left[ 1 + y + \\frac{y^2}{2!} + \\ldots \\right] \\\\ &amp;= e^{-\\lambda} e^y = \\exp(-\\lambda) \\exp(\\lambda e^t) = \\exp[\\lambda(e^t-1)] \\,. \\end{align*}\\] 4.4.2 The Distribution of the Difference of Two Poisson Random Variables Assume we point a camera at an object, such as a star. A star gives off photons at a particular rate \\(\\alpha_S\\) (with units, e.g., photons per second) and thus if we open the shutter for a length of time \\(t\\), the number of photons we observe from the star is a Poisson random variable \\(S \\sim\\) Poisson(\\(\\alpha_St\\)). But the star is not the only object in the field of view; there may be other objects in the background that give off photons at a rate \\(\\alpha_B\\), and the number of photons we observe from the background will be \\(B \\sim\\) Poisson(\\(\\alpha_Bt\\)). Thus what we record is not \\(S\\), but \\(T = S+B\\)…so how can we statistical inferences about \\(S\\) itself? One possibility is to point the camera to an “empty” field near the star, and record some number of photons \\(B\\). Then we can estimate \\(S\\) using \\(S = T - B\\). What is the distribution of \\(S\\)? We utilize the method of moment-generating functions and write \\[\\begin{align*} m_S(t) = m_T(t) m_B(-t) &amp;= \\exp[\\lambda_T(e^t-1)] \\exp[\\lambda_B(e^{-t}-1)] \\\\ &amp;= \\exp[\\lambda_T(e^t-1) + \\lambda_B(e^{-t}-1)] \\\\ &amp;= \\exp[-(\\lambda_T+\\lambda_B) + \\lambda_Te^t + \\lambda_Be^{-t}] \\,, \\end{align*}\\] where \\(\\lambda_T = \\lambda_S+\\lambda_B = (\\alpha_S+\\alpha_B)t\\) and \\(\\lambda_B = \\alpha_Bt\\). At first, utilizing the method of mgfs appears to be a fool’s errand: this is not an mgf we know. But it turns out that the family of distributions associated with this mgf does have a name: \\(S = T-B\\) is a Skellam-distributed random variable, with mean \\(\\lambda_T-\\lambda_B\\) and variance \\(\\lambda_T+\\lambda_B\\). We can work with this distribution to, e.g., construct confidence intervals for \\(\\mu_S\\), etc. 4.4.3 The Probability Mass Function for the Sample Mean The derivation of the pmf for the sample mean proceeds entirely analogously with the derivation given in the previous chapter for the sample mean of \\(n\\) iid binomial random variables: we can use the method of moment generating functions to find that the sum of \\(n\\) iid Poisson random variables is itself a Poisson random variable with mean and variance \\(n\\lambda\\), and then use the general transformation framework to find the pmf for \\(\\bar{X} = (1/n)\\sum_{i=1}^n X_i\\). However, as we did in the last chapter, we can view the problem as “derive the pmf for the sum and then tranform the domain.” By doing this, we find that we can write that \\[ p_{\\bar{X}}(\\bar{x}) = \\frac{(n\\lambda)^{n\\bar{x}}}{(n\\bar{x})!} e^{-n\\lambda} ~~ \\bar{x} \\in [0,1/n,2/n,\\ldots,\\infty) \\,. \\] This pmf has the functional form of a Poisson pmf but not the domain of one, thus it has no “name” and no tabulated properties. See the example in Figure 4.3. lambda &lt;- 10 n &lt;- 10 x.bar &lt;- seq(lambda-5*sqrt(lambda/n),lambda+5*sqrt(lambda/n),by=1/n) p.x.bar &lt;- dpois(n*x.bar,n*lambda) df &lt;- data.frame(x.bar=x.bar,p.x.bar=p.x.bar) ggplot(data=df,aes(x=x.bar,y=p.x.bar)) + geom_point(col=&quot;blue&quot;,size=4) + labs(x=&quot;x.bar&quot;,y=&quot;p(x.bar)&quot;) + theme(axis.title=element_text(size = rel(1.25))) Figure 4.3: Probability mass function for the sample mean of \\(n = 10\\) iid Poisson random variables, for \\(\\lambda = 10\\). 4.5 Point Estimation In previous chapters, we describe two commonly used approaches for defining good estimators (as opposed to simply defining one ourselves and hoping for the best!): maximum likelihood estimation and finding the minimum variance unbiased estimator. We review both below, in the context of estimating the Poisson \\(\\lambda\\) parameter, and then for completeness introduce one last, less-commonly used approach, the so-called method of moments. Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). First, let’s take the logarithm of the likelihood function written out above: \\[ \\ell(\\lambda \\vert \\mathbf{x}) = \\left(\\sum_{i=1}^n x_i\\right) \\log \\lambda - n \\lambda - \\log\\left(\\prod_{i=1}^n x_i!\\right) \\,. \\] The derivative of \\(\\ell(\\lambda \\vert \\mathbf{x})\\) with respect to \\(\\lambda\\) is \\[ \\frac{d\\ell}{d\\lambda} = \\left(\\frac{1}{\\lambda}\\sum_{i=1}^n x_i \\right) - n \\,. \\] Setting the derivative to zero and rearranging terms, we find that \\[ \\hat{\\lambda}_{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} \\] is the MLE for \\(\\lambda\\). By the general rule introduced in Chapter 1, \\(E[\\hat{\\lambda}_{MLE}] = E[\\bar{X}] = \\lambda\\) (so \\(\\hat{\\lambda}_{MLE}\\) is an unbiased estimator), and \\(V[\\hat{\\lambda}_{MLE}] = \\lambda/n\\) (so \\(\\hat{\\lambda}_{MLE}\\) is a consistent estimator, since \\(\\hat{\\lambda}_{MLE} \\rightarrow \\lambda\\) as \\(n \\rightarrow \\infty\\). (There is no guarantee that the MLE will produce an unbiased estimator; it just happens to do so here. It will produce at least an asymptotically unbiased estimator, and it will always produce a consistent estimator.) Recall that if we wish to find the MLE for a function of the parameter, e.g., \\(\\lambda^2\\), we simply apply that function to \\(\\hat{\\theta}_{MLE}\\). Hence \\(\\hat{\\lambda^2}_{MLE}\\) is \\(\\bar{X}^2\\). This is the invariance property of the MLE. Also, recall that the MLE converges in distribution to a normal random variable with mean \\(\\theta\\) and variance \\(1/I_n(\\theta)\\), where \\(I_n(\\theta)\\) is the Fisher information content of the data sample. Recall: deriving the minimum variance unbiased estimator involves two steps: factorizing the likelihood function to uncover a sufficient statistic \\(U\\) (that we assume is both minimal and complete); and finding a function \\(h(U)\\) such that \\(E[h(U)] = \\lambda\\). The likelihood function is \\[ \\mathcal{L}(\\lambda \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda} = \\left(\\prod_{i=1}^n \\frac{1}{x_i!}\\right) \\left(\\prod_{i=1}^n \\lambda^{x_i}e^{-\\lambda}\\right) = \\underbrace{\\left(\\prod_{i=1}^n \\frac{1}{x_i!}\\right)}_{h(\\mathbf{x})} \\underbrace{\\lambda^{\\sum_{i=1}^n x_i}e^{-n\\lambda}}_{g\\left(\\sum_{i=1}^n x_i,\\lambda\\right)} \\,. \\] We see that the sufficient statistic is \\(U = \\sum_{i=1}^n X_i\\). Let’s determine the expected value for \\(U\\): \\[ E[U] = E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i] = \\sum_{i=1}^n \\lambda = n\\lambda \\,. \\] Thus \\(h(U) = U/n = \\bar{X}\\) is the MVUE for \\(\\lambda\\). As this matches the MLE, we know already that the MVUE is an unbiased (by definition here!) and consistent estimator. The next question is whether the variance of the MVUE achieves the CRLB. We show that it does in an example below. Note: the MVUE does not possess the invariance property, and it may be the case that it does not achieve the Cramer-Rao Lower Bound! Its primary advantage over the MLE is that the MVUE is the best estimator of those that are always unbiased, for all sample sizes. The method of moments is a classic (read: old) means by which to define estimators that can be useful when, for instance, working with the likelihood function itself is difficult. (As such, it is an alternative to working with likelihood functions numerically.) Recall that by definition, \\(\\mu_k&#39; = E[X^k]\\) is the \\(k^{\\rm th}\\) moment of the distribution of the random variable \\(X\\). For instance, \\[ \\mu_1&#39; = E[X] ~~ \\mbox{and} ~~ \\mu_2&#39; = E[X^2] = V[X] + (E[X])^2 \\,. \\] If we shift from the population to the data sample, we can define analagous sample moments, e.g., \\[ m_1&#39; = \\frac{1}{n} \\sum_{i=1}^n X_i = \\bar{X} ~~ \\mbox{and} ~~ m_2&#39; = \\frac{1}{n} \\sum_{i=1}^n X_i^2 = \\overline{X^2} \\,. \\] Let’s suppose that we have \\(p\\) parameters that we are trying to estimate. In method of moments estimation, we generally set the first \\(p\\) population moments equal to the first \\(p\\) sample moments and solve the system of equations to determine parameter estimates. These estimates are generally consistent, but also may be biased. (Situations may exist where higher-order moments may be preferable to use, such as when the one parameter of a distribution is \\(\\sigma^2\\) and thus we might derive a better estimator using the second moments, but typically we will use the first \\(p\\) moments.) For the Poisson distribution, there is one parameter to estimate and thus we set \\(\\mu_1&#39; = E[X] = \\lambda = m_1&#39; = \\bar{X}\\). We thus find that \\(\\hat{\\lambda}_{MoM} = \\bar{X}\\). For a more relevant example of method of moments usage, see below. 4.5.1 Revisiting the Death-by-Horse-Kick Example We begin this chapter by displaying the number of deaths per Prussian army corps per year resulting from horse kicks. Leaving aside the question of whether the data are truly Poisson distributed (a question we will try to answer later in this chapter), what is the estimated rate of death per corps per year? The total number of events observed are \\[ 0 \\times 109 + 1 \\times 65 + 2 \\times 22 + 3 \\times 3 + 4 \\times 1 = 65 + 44 + 9 + 4 = 122 \\,, \\] and the total sample size is \\(n = 200\\), so \\[ \\hat{\\lambda} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{122}{200} = 0.61 \\,. \\] This is the MLE, the MVUE, and the MoM estimate for \\(\\lambda\\). In the next section, we will use these data to estimate a 95% confidence interval for \\(\\lambda\\). 4.5.2 The Cramer-Rao Lower Bound on the Variance of \\(\\lambda\\) Estimators Recall: the Cramer-Rao Lower Bound (or CRLB) is the lower bound on the variance of any unbiased estimator. If an unbiased estimator achieves the CRLB, it is the MVUE…but it can be the case that the MVUE does not achieve the CRLB. For a discrete distribution, the CRLB is given by \\[ V[\\hat{\\theta}] \\geq -\\frac{1}{nE\\left[\\frac{d^2}{d\\theta^2} \\log p_X(X \\vert p) \\right]} = \\frac{1}{nI(\\theta)} \\] where \\(I(\\theta)\\) is the Fisher information. For the Poisson distribution, \\[\\begin{align*} p(x \\vert \\lambda) &amp;= \\frac{\\lambda^x}{x!}e^{-\\lambda} \\\\ \\log p(x \\vert \\lambda) &amp;= x \\log \\lambda - \\lambda - \\log x! \\\\ \\frac{d}{d\\lambda} \\log p(x \\vert \\lambda) &amp;= \\frac{x}{\\lambda} - 1 \\\\ \\frac{d^2}{d\\lambda^2} \\log p(x \\vert \\lambda) &amp;= -\\frac{x}{\\lambda^2} \\\\ E \\left[ \\frac{d^2}{d\\lambda^2} \\log p(X \\vert \\lambda) \\right] &amp;= -\\frac{1}{\\lambda^2} E[X] \\\\ &amp;= -\\frac{1}{\\lambda^2} \\lambda = -\\frac{1}{\\lambda} \\end{align*}\\] and \\[ V[\\hat{\\lambda}] \\geq -\\frac{1}{-n/\\lambda} = \\frac{\\lambda}{n} \\,. \\] Thus \\(\\hat{\\lambda}_{MLE}\\), \\(\\hat{\\lambda}_{MVUE}\\), and \\(\\hat{\\lambda}_{MoM}\\) all achieve the CRLB. 4.5.3 Minimum Variance Unbiased Estimation and the Invariance Property As stated above, the MVUE does not possess the property of invariance. (This severely limits the general applicability of the algorithm!) To demonstrate the lack of invariance, we will define the MVUE for \\(\\lambda^2\\). The first thing to notice is that we cannot fall back on factorization to determine an appropriate sufficient statistic, since \\(\\lambda^2\\) does not appear directly in the likelihood function. So we iterate: we make an initial guess and see where that guess takes us, and we guess again if our initial guess is wrong, etc. An appropriate “guess” for \\(\\lambda^2\\) is \\(\\bar{X}^2\\): \\[ E[\\bar{X}^2] = V[\\bar{X}] + (E[\\bar{X}])^2 = \\frac{\\lambda}{n} + \\lambda^2 \\] We do get the term \\(\\lambda^2\\) here…but we also get \\(\\lambda/n\\). Hmm…so let’s try \\(\\bar{X}^2 - \\bar{X}/n\\) instead: \\[ E\\left[\\bar{X}^2 - \\frac{\\bar{X}}{n}\\right] = E[\\bar{X}^2] - \\frac{1}{n}E[\\bar{X}] = \\frac{\\lambda}{n} + \\lambda^2 - \\frac{\\lambda}{n} = \\lambda^2 \\,. \\] Done! The MVUE for \\(\\lambda^2\\) is thus \\(\\hat{\\lambda^2}_{MVUE} = \\bar{X}^2-\\bar{X}/n\\), which is not equal to \\(\\hat{\\lambda^2}_{MLE} = \\bar{X}^2\\) (except asymptotically, in the limit \\(n \\rightarrow \\infty\\)). 4.5.4 Method of Moments Estimation for the Gamma Distribution We will not officially introduce the gamma distribution until later in this chapter, but it is a good one to use when exploring method of moments estimation. The probability density function for a gamma random variable \\(X\\) is \\[ f_X(x) = \\frac{x^{\\alpha-1}}{\\beta^{\\alpha}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} \\,, \\] for \\(x \\geq 0\\) and \\(\\alpha,\\beta &gt; 0\\). The expected value is \\(E[X] = \\alpha \\beta\\) while the variance is \\(V[X] = \\alpha \\beta^2\\) (and thus \\(E[X^2] = \\alpha \\beta^2 + \\alpha^2 \\beta^2\\)). To compute a maximum likelihood estimate here, we would need to be able to differentiate the gamma function \\(\\Gamma(\\alpha)\\)…and to find the MVUE, we would need to determine how to utilize joint sufficient statistics in the MVUE algorithm, which we do not know how to do. Thus we fall back on the method of moments. (Well, in real life, we would probably actually fall back on numerical optimization of the likelihood function, but if we seek equations to write down…) Let’s assume we have \\(n\\) iid gamma-distributed random variables. Because there are two parameters, we match the first two moments: \\[\\begin{align*} \\mu_1&#39; = E[X] = \\alpha \\beta &amp;= m_1&#39; = \\bar{X} \\\\ \\mu_2&#39; = E[X^2] = \\alpha \\beta^2 + \\alpha^2 \\beta^2 &amp;= m_2&#39; = \\frac{1}{n}\\sum_{i=1}^n X_i^2 = \\overline{X^2} \\,. \\end{align*}\\] Let \\(\\beta = \\bar{X}/\\alpha\\). Then \\[\\begin{align*} \\alpha \\left( \\frac{\\bar{X}}{\\alpha} \\right)^2 + \\alpha^2 \\left( \\frac{\\bar{X}}{\\alpha} \\right)^2 &amp;= \\overline{X^2} \\\\ \\frac{(\\bar{X})^2}{\\alpha} &amp;= \\overline{X^2} - (\\bar{X})^2 \\\\ \\Rightarrow ~~ \\hat{\\alpha}_{MoM} &amp;= \\frac{(\\bar{X})^2}{\\overline{X^2} - (\\bar{X})^2} \\,, \\end{align*}\\] and thus \\[ \\hat{\\beta}_{MoM} = \\frac{\\bar{X}}{\\hat{\\alpha}_{MoM}} = \\frac{\\overline{X^2} - (\\bar{X})^2}{\\bar{X}} \\,. \\] 4.6 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by, e.g., solving for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The construction of confidence intervals thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) to \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 14 of Chapter 1.) As far as the construction of confidence intervals given a discrete sampling distribution goes, nothing changes algorithmically from Chapter 3. Below, in an example, we review how to construct such an interval for the Poisson parameter \\(\\lambda\\). What we will do here is answer the question, “what do we do if we neither know nor are willing to assume the distribution from which our data are sampled?” After all, our root-finding algorithm relies upon knowing the sampling distribution of an observed statistic, and that in turn relies on knowing the distribution from which we draw each of our \\(n\\) iid data. What we can do, in some situations, is fall back upon bootstrapping. The bootstrap, invented by Bradley Efron in 1979, uses the observed data themselves to build up empirical sampling distributions for statistics. Let’s suppose we are handed the following data: \\[ \\mathbf{X} = \\{X_1,X_2,\\ldots,X_n\\} \\overset{iid}{\\sim} P \\,, \\] where the distribution \\(P\\) is unknown. Now, let’s suppose further that from these data we compute a statistic: a single number. How can we build up an empirical sampling distribution from a single number? The answer is to repeatedly resample the data we observe, with replacement. For instance, if we have as data the numbers \\(\\{1,2,3\\}\\), a bootstrap sample might be \\(\\{1,1,3\\}\\) or \\(\\{2,3,3\\}\\), etc. Every time we resample the data, we compute the statistic we are interested in and record its value. Voila: we have an empirical sampling distribution. And if we can link the elements of that sampling distribution to a population parameter, we can immediately write down a confidence interval. For instance, if we have the \\(n_{\\rm boot}\\) statistics \\(\\{\\bar{X}_1,\\ldots,\\bar{X}_k\\}\\), we can put bounds on the population mean \\(\\mu\\): \\[\\begin{align*} \\hat{\\mu}_L &amp;= \\bar{X}_{\\alpha/2} \\\\ \\hat{\\mu}_H &amp;= \\bar{X}_{1-\\alpha/2} \\,, \\end{align*}\\] where \\(\\alpha/2\\) and \\(1-\\alpha/2\\) represent sample percentiles, e.g., the 2.5\\(^{\\rm th}\\) and 97.5\\(^{\\rm th}\\) percentiles. 4.6.1 Confidence Interval for the Poisson Parameter \\(\\lambda\\) As we did in Chapters 2 and 3, below we will adapt the general-purpose R code for constructing confidence intervals that we provide in Appendix B to a specific problem: here, putting a confidence interval on the Poisson parameter \\(\\lambda\\). Assume that we sample \\(n\\) iid data. Then, as shown above, \\(Y = \\sum_{i=1}^n X_i \\sim\\) Poisson(\\(n\\lambda\\)); our observed test statistic is \\(y_{\\rm obs} = \\sum_{i=1}^n x_i\\). For this statistic, \\(E[Y] = n\\lambda\\) increases with \\(\\lambda\\), so \\(\\lambda_{1-\\alpha/2}\\) maps to the lower bound, while \\(\\lambda_{\\alpha/2}\\) maps to the upper bound. confint &lt;- function(y.obs,n,alpha=0.05) { f &lt;- function(lambda,y.obs,n,q) { ppois(y.obs,lambda=n*lambda)-q } lo &lt;- uniroot(f,interval=c(0,10000),y.obs,n,1-alpha/2)$root hi &lt;- uniroot(f,interval=c(0,10000),y.obs,n,alpha/2)$root return(c(lo,hi)) } # Let&#39;s assume we observe ten years of data in a Poisson process set.seed(101) n &lt;- 10 lambda &lt;- 8 X &lt;- rpois(n,lambda=lambda) confint(sum(X),n) ## [1] 5.810583 9.178655 We find that the interval is \\([\\hat{\\lambda}_L,\\hat{\\lambda}_H] = [5.81,9.18]\\), which overlaps the true value of 8. (See Figure 4.4.) Note that the interval over which we search for the root is [0,10000], which is (effectively) the range of possible values for \\(\\lambda\\). Figure 4.4: Probability mass functions for Poisson distributions with \\(n=10\\) (left) \\(\\lambda=5.81\\) and (right) \\(\\lambda=9.18\\). We assume that we observe \\(y_{\\rm obs} = \\sum_{i=1}^n x_i = 73\\) events in total and that we want to construct a 95% confidence interval. \\(\\lambda=5.81\\) is the smallest value of \\(\\lambda\\) such that \\(F_Y^{-1}(0.975) = 73\\), while \\(\\lambda=9.18\\) is the largest value of \\(\\lambda\\) such that \\(F_Y^{-1}(0.025) = 73\\). In Chapter 3, we find that when we are constructing exact intervals with discrete sampling distributions (“exact” meaning that the sampling distribution is the correct one, not an approximation), the actual coverage can differ, either positively or negatively, from what we expect. This is a “discreteness effect” that goes away as the sample size \\(n\\) increases (i.e., as the discrete sampling distribution tends more and more to having a continuous appearance). Thus when we deal with a sufficiently small data sample, it is good practice to run a simulation to try to estimate the actual coverage, as we do below. set.seed(101) num.sim &lt;- 10000 n &lt;- 10 lambda &lt;- 8 lower &lt;- rep(NA,num.sim) upper &lt;- rep(NA,num.sim) for ( ii in 1:num.sim ) { X &lt;- rpois(n,lambda=lambda) b &lt;- confint(sum(X),n) lower[ii] &lt;- b[1] upper[ii] &lt;- b[2] } truth &lt;- lambda in.bound &lt;- (lower &lt;= truth) &amp; (upper &gt;= truth) cat(&quot;The estimated coverage is &quot;,sum(in.bound)/num.sim,&quot;\\n&quot;) ## The estimated coverage is 0.9547 Our estimated coverage is 0.9547: we observe 47 more simulated confidence intervals that overlap the true value than we expect. Is this consistent with expectation? We have \\(k = 10000\\) trials, where the expected success proportion is \\(p = 0.95\\). Let \\(X\\) be the number of simulations in which the confidence intervals overlap the true parameter value. Then \\(E[X] = kp\\) (here, 9500), \\(V[X] = kp(1-p)\\) (here, 475), and \\(\\sigma_X = \\sqrt{V[X]}\\) (here, 21.79). Our observed value is \\(X = 9547\\), which is \\(47/21.79 = 2.16\\) standard deviations away from what we expect. This is implausible; the probability of sampling a value that deviates from the expectation by 47 (or more), conditional on our expectation being correct, is \\(\\approx\\) 0.03 (as the reader can confirm using appropriate calls to pbinom(). We can conclude that in this particular case, the actual coverage differs from what we expect. We note that we can reduce the level of uncertainty (here, 21.79) by running more simulations. For instance, if we run one million simulations insted of 10,000, the level of uncertainty will be reduced by a factor of 10, to 2.179. As a general rule, we should always try to run as many simulations as time will allow! 4.6.2 Revisiting the Death-by-Horse-Kick Example In the last section above, we determined that the rate of death from horse kicks per Prussian army corps per year was \\(\\hat{\\lambda} = 0.61\\). Here, we determine a 95% interval estimate for \\(\\lambda\\). X &lt;- c(rep(0,109),rep(1,65),rep(2,22),rep(3,3),rep(4,1)) n &lt;- length(X) confint(sum(X),n) ## [1] 0.5111476 0.7283388 The 95% confidence interval is \\([0.511,0.728]\\). 4.6.3 Determining a Confidence Interval Using the Bootstrap Let’s assume we have the same data as in the first example above. set.seed(101) n &lt;- 10 lambda &lt;- 8 X &lt;- rpois(n,lambda=lambda) print(X) ## [1] 7 4 9 9 6 6 8 7 9 8 The confidence interval that we construct for \\(\\lambda\\), which is the mean of the distribution, is \\([5.81,9.18]\\). How does the bootstrap estimate of the mean compare? n.boot &lt;- 10000 x.bar &lt;- rep(NA,n.boot) for ( ii in 1:n.boot ) { s &lt;- sample(length(X),length(X),replace=TRUE) x.bar[ii] &lt;- mean(X[s]) } q &lt;- quantile(x.bar,probs=c(0.025,0.975)) cat(&quot;The estimated interval is [&quot;,round(q[1],2),&quot;,&quot;,round(q[2],2),&quot;].\\n&quot;,sep=&quot;&quot;) ## The estimated interval is [6.3,8.2]. The estimated interval is \\([6.3,8.2]\\). This is substantially smaller than what we found above, and makes sense: for instance, the largest observed datum is 9, so the largest possible value of the bootstrap sample mean is 9…which is smaller than the upper bound of 9.18. What we are seeing is the effect of a small sample size: in the limit of small \\(n\\), the length of bootstrap confidence intervals is on average smaller than that of exact ones, with greater variability in lengths. As \\(n\\) increases, the mean lengths converge, but the variability in lengths remains larger for bootstrap intervals than for exact ones. 4.6.4 The Proportion of Observed Data in a Bootstrap Sample Let’s assume that we sample \\(n\\) iid data from some distribution \\(P\\). When we create a bootstrap sample of these data, some of the observed data appear multiple times, while other data do not appear at all. What is the average proportion of observed data in any given bootstrap sample? Let \\(i\\) be the index of an arbitrary datum, where the indices are \\(\\{1,2,\\ldots,n-1,n\\}\\). Let \\(X\\) be the number of times \\(i\\) is chosen when we construct a bootstrap sample of size \\(n\\): \\(X \\sim\\) Binom(\\(n,1/n\\)). \\(P(X \\geq 1)\\) then represents the average proportion of observed data in a bootstrap sample: \\[ P(X \\geq 1) = 1 - P(X = 0) = 1 - (1-1/n)^n \\,, \\] which, as \\(n \\rightarrow \\infty\\), approaches \\(1-1/e = 0.632\\). Thus, for a sufficiently large sample, 63.2% of the observed data will appear at least once in a bootstrapped dataset. 4.7 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-sided test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bounds given by \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\), which are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps, e.g., \\(y_{\\alpha/2}\\) to either the lower or upper rejection region boundary by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven. In Chapter 3, we build upon the framework outlined above by introducing the Neyman-Pearson lemma. This result allows us to bypass the “guesswork” that goes into defining a hypothesis test statistic, by defining for us the most powerful test of a simple null hypothesis versus a simple specified alternative. Recall: when we test the simple hypotheses \\(H_o: \\theta = \\theta_o\\) versus \\(H_a: \\theta = \\theta_a\\), the Neyman-Pearson lemma allows us to state that the hypothesis test with maximum power has a rejection region of the form \\[ \\frac{\\mathcal{L}(\\theta_o \\vert \\mathbf{x})}{\\mathcal{L}(\\theta_a \\vert \\mathbf{x})} \\leq c(\\alpha) \\,, \\] where \\(c(\\alpha)\\) is a constant whose value depends on the specified Type I error \\(\\alpha\\). In practice, we determine the sufficient statistic \\(U\\), examine the form of the likelihood ratio to determine the form of the rejection region (\\(U \\leq u_\\alpha\\) versus \\(U \\geq u_{1-\\alpha}\\)), and use the sampling distribution of \\(U\\) to derive the rejection region boundary (as \\(F_U^{-1}(\\alpha)\\) or \\(F_U^{-1}(1-\\alpha)\\)). If the rejection region does not depend on \\(\\theta_a\\), then the test is said to be a uniformly most powerful (UMP) test. In an example, we demonstrate how to apply the NP lemma to construct a hypothesis test for the Poisson parameter \\(\\lambda\\), given a sample of \\(n\\) iid data. However, here we describe a more general hypothesis test framework, dubbed the likelihood ratio test (or LRT). “Wait…the NP lemma had a likelihood ratio. How is the LRT different?” That is a good question. It differs in how the null and alternative hypotheses are specified: \\[ H_o: \\theta \\in \\Theta_o ~~\\mbox{vs.}~~ H_a: \\theta \\in \\Theta_o^c \\,, \\] where \\(\\Theta_o\\) (“capital theta naught”) represents a set of possible null values for \\(\\theta\\), while \\(\\Theta_o^c\\) is the complement of that set. For instance, for tests involving the Poisson parameter \\(\\lambda\\), \\(\\Theta_o\\) could be \\(\\lambda \\in [5,10]\\), so that \\(\\Theta_o^c\\) is \\(\\lambda &lt; 5\\) or \\(\\lambda &gt; 10\\). (The null hypothesis in this example is a composite hypothesis, although it can be specified as a simple one, and usually is.) Let \\(\\Theta = \\Theta_o \\cup \\Theta_o^c\\), i.e., the union of the null and alternative sets. The rejection region for the LRT is \\[ \\lambda_{LR} = \\frac{\\mbox{sup}_{\\theta \\in \\Theta_o} \\mathcal{L}(\\theta \\vert \\mathbf{x})}{\\mbox{sup}_{\\theta \\in \\Theta} \\mathcal{L}(\\theta \\vert \\mathbf{x})} \\leq c(\\alpha) \\,, \\] where, like it is in the context of the NP lemma, \\(c(\\alpha)\\) is a constant that depends on the specified Type I error \\(\\alpha\\). “Since the LRT is more general, why did we ever utilize the NP lemma?” That is another good question. There are three primary points to make about why we would use the NP lemma: The NP lemma tests two simple hypotheses. For the LRT, the hypotheses can be composite hypotheses. The NP lemma allows us to define the most powerful test for disambiguating two simple hypotheses. The LRT is generally a powerful test, but given the composite nature of one (or both) of the hypotheses, it comes with no guarantee of being the most powerful test. (For instance, perhaps the score test or the Wald test, which can define different statistics from the LRT, yields the most powerful test.) The NP lemma framework ultimately allows us to define exact rejection regions, assuming we know the functional form of the sampling distribution of the sufficient statistic \\(U\\). Sometimes we know the sampling distribution in the context of the LRT, but often we do not, which potentially leads us to fall back on Wilks’ theorem (which we discuss below). When we can specify the sampling distribution for \\(U\\), LRT problems proceed like NP lemma problems: we use the ratios to determine the form of the rejection regions and then find the boundaries of those regions. See the second example below. But how would we proceed if we do not know the sampling distribution? We might perform simulations, but as stated above we might also fall back on Wilks’ theorem. Let \\(r_o\\) denote the number of free parameters in \\(H_o: \\theta \\in \\Theta_o\\) and let \\(r\\) denote the number of free parameters in \\(\\theta \\in \\Theta = \\Theta_o \\cup \\Theta_a\\). (Note that \\(\\Theta\\) must include all possible values of the parameters. For instance, if \\(H_o\\) is \\(\\theta = \\theta_o\\), then \\(H_a\\) must be \\(\\theta \\neq \\theta_o\\) and not \\(\\theta &lt; \\theta_o\\) or \\(\\theta &gt; \\theta_o\\).) Then, for large \\(n\\), \\[ -2\\log \\lambda_{LR} \\stackrel{d}{\\rightarrow} W \\sim \\chi_{r-r_o}^2 \\,. \\] Since this result is related to the central limit theorem, large \\(n\\) would be, by rule-of-thumb, 30 or more. In the language of R, if w.obs = -2*log(lambda.LR), then we would reject the null if 1-pchisq(w.obs,r-r.o) is less than \\(\\alpha\\). Note that when we apply Wilks’ theorem, the results of all tests are contingent upon whether or not \\(W \\geq w_{1-\\alpha}\\). Note that we say we “might…fall back on Wilks’ theorem.” That’s because it has limited use in the context of problems that we are dealing with in this book. First of all, as noted above, \\(\\Theta\\) has to encompass the full parameter space, thus Wilks’ theorem cannot be used to carry out upper- or lower-tail tests when the null is \\(\\theta = \\theta_o\\). Also, it is most useful when data are sampled from distributions with two or more freely varying parameters (e.g., the normal distribution, with \\(\\mu\\) and \\(\\sigma^2\\) both unknown), and where the null hypothesis fixes more parameter values than the alternative hypothesis (so that \\(r-r_o &gt; 0\\)). When there are \\(p\\) freely varying parameters, then there are \\(p\\) joint sufficient statistics, and thus we would have a \\(p\\)-dimensional sampling distribution that would be difficult to work with. We can use Wilks’ theorem when there is only one parameter, but again, we only consider doing so in cases where the sampling distribution for the one sufficient statistic \\(U\\) is unknown. 4.7.1 The Uniformly Most Powerful Test of Poisson \\(\\lambda\\) Let’s say that we are counting the number of students that enter a classroom each minute. We assume that the entry of students is a homogeneous Poisson process (i.e., \\(\\lambda\\), the expected number of students, does not change from minute to minute). We think that five students, on average, will pass through the door each minute, while someone else thinks the number will be three. We collect data during five independent one-minute intervals: 4, 4, 3, 2, 3. Can we reject our null hypothesis at the level \\(\\alpha = 0.05\\)? What is the \\(p\\)-value? And what is the power of the test for \\(\\lambda_a = 3\\)? We test the simple hypotheses \\(H_o: \\lambda_o = 5\\) and \\(H_a: \\lambda_a = 3\\). The factorized likelihood for our data sample is \\[ \\mathcal{L}(\\lambda \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda} = \\underbrace{\\lambda^{\\sum_{i=1}^n x_i} e^{-n\\lambda}}_{g(\\sum x_i,\\lambda}) \\cdot \\underbrace{\\frac{1}{\\prod_{i=1}^n x_i!}}_{h(\\mathbf{x})} \\,. \\] Our sufficient statistic is thus \\(U = \\sum_{i=1}^n X_i = n\\bar{X}\\), and the ratio of likelihoods is \\[ \\frac{\\mathcal{L}(\\lambda_o \\vert \\mathbf{x})}{\\mathcal{L}(\\lambda_a \\vert \\mathbf{x})} = \\frac{(1/\\prod_{i=1}^n x_i!) \\lambda_o^{u_{\\rm obs}} e^{-\\lambda_o}}{(1/\\prod_{i=1}^n x_i!) \\lambda_a^{u_{\\rm obs}} e^{-\\lambda_a}} \\propto \\left(\\frac{\\lambda_o}{\\lambda_a}\\right)^{u_{\\rm obs}} \\,. \\] If \\(\\lambda_a &lt; \\lambda_o\\), the ratio goes towards zero as \\(u_{\\rm obs} \\rightarrow 0\\). Thus the rejection region is \\(U \\leq u_\\alpha = F_U^{-1}(\\alpha)\\). Using the method of moment-generating functions, we find that \\(U = \\sum_{i=1}^n X_i\\) is Poisson-distributed with parameter \\(n\\lambda\\). Hence we can adapt a result from Chapter 3 and determine \\(u_\\alpha\\) in R via the function call qpois(alpha,lambda=n*lambda.o)-1. Since \\(u_{\\rm obs} = 16\\) and \\(u_\\alpha = 16\\), so we reject the null hypothesis that \\(\\lambda = 5\\). We further adapt results from Chapter 3 to write down that the \\(p\\)-value is ppois(u.obs,lambda=n*lambda.o), which is 0.038. Thus far, the only way that we’ve utilized the alternative hypothesis \\(H_a: \\lambda_a = 3\\) is when determining the orientation of the rejection region. Now we will use this value to determine the power of the test. The power is the probability of rejecting the null hypothesis given a specific value of \\(\\lambda\\), i.e., \\(P(U \\leq u_\\alpha \\vert \\lambda)\\). Here, that value is given by ppois(u.lo,n*lambda), which for \\(\\lambda = \\lambda_a = 3\\) is 0.664: if \\(\\lambda\\) is truly equal to 3, we would reject the null hypothesis that \\(\\lambda_o = 5\\) after collecting five data about two-thirds of the time. For completeness, we write down results for Poisson distributed data that are analogous to what we write down in Chapter 3 for binomially distributed data. Alternative Rejection Region(s) R Code for Poisson Distribution \\(p_a &lt; p_o\\) \\(u_{\\rm obs} \\leq u_{\\alpha}\\) u.lo &lt;- qpois(alpha,lambda=n*lambda.o) - 1 \\(p_a &gt; p_o\\) \\(u_{\\rm obs} \\geq u_{1-\\alpha}\\) u.hi &lt;- qpois(1-alpha,lambda=n*lambda.o) + 1 Alternative Formula R Code for Poisson Distribution \\(p_a &lt; p_o\\) \\(F_U(u_{\\rm obs} \\vert p_o)\\) p &lt;- ppois(u.obs,lambda=n*lambda.o) \\(p_a &gt; p_o\\) \\(1-F_U(u_{\\rm obs}-1 \\vert p_o)\\) p &lt;- 1-ppois(u.obs-1,lambda=n*lambda.o) Alternative Formula R Code for Poisson Distribution \\(p_a &lt; p_o\\) \\(F_U(u_\\alpha \\vert p_a)\\) power &lt;- ppois(u.lo,lambda=n*lambda.a) \\(p_a &gt; p_o\\) \\(1-F_U(u_{1-\\alpha}-1 \\vert p_a)\\) power &lt;- 1-ppois(u.hi-1,lambda=n*lambda.a) Note that since the rejection region does not depend on the value \\(\\lambda_a\\), we have defined the uniformly most powerful test of \\(\\lambda_o\\) versus \\(\\lambda_a\\). 4.7.2 Likelihood Ratio Test of the Poisson Parameter \\(\\lambda\\) Let’s assume the same setting as for the last example, but here, let’s say that we will test \\(H_o: \\lambda = \\lambda_o = 5\\) versus \\(H_a: \\lambda \\neq \\lambda_o\\). The alternative hypothesis is a composite hypothesis, because it does not uniquely specify the shape of the probability mass function. As we will see, because we know the sampling distribution for the sufficient statistic \\(U = \\sum_{i=1}^n X_i\\), we can adopt the NP lemma algorithm within the LRT to derive the two rejection regions…but because the alternative hypothesis is composite, there is no longer a guarantee that the test that we define\\(-\\) \\(H_o: \\lambda = \\lambda_o = 5\\) versus \\(H_a: \\lambda \\neq \\lambda_o\\) \\(-\\)is the most powerful test of these hypotheses. It may be, it may not be. Let the set of possible values of \\(\\lambda\\) be denoted \\(\\Lambda\\) (“capital lambda”). \\(\\Lambda = \\Lambda_o \\cup \\Lambda_a \\in (0,\\infty)\\), so \\(\\mbox{sup}_{\\lambda \\in \\Lambda} \\mathcal{L}(\\lambda \\vert \\mathbf{x})\\) is the value of the likelihood for \\(\\hat{\\lambda}_{MLE} = \\bar{X}\\). Thus the denominator of the likelihood ratio is \\[ \\prod_{i=1}^n \\frac{\\hat{\\lambda}^{x_i}}{x_i!}e^{-\\hat{\\lambda}} = \\frac{1}{\\prod_{i=1}^n x_i!} \\hat{\\lambda}^{\\sum_{i=1}^n x_i} e^{-n\\hat{\\lambda}} \\,. \\] The form of the numerator is the same, with \\(\\lambda_o\\) replacing \\(\\hat{\\lambda}_{MLE}\\). Thus \\[ \\lambda_{LR} = \\left(\\frac{\\lambda_o}{\\hat{\\lambda}}\\right)^{\\sum_{i=1}^n x_i} e^{-n(\\lambda_o - \\hat{\\lambda})} = \\left(\\frac{\\lambda_o}{\\hat{\\lambda}}\\right)^{u_{\\rm obs}} e^{-n(\\lambda_o - \\hat{\\lambda})}\\,, \\] where \\({u_{\\rm obs}}\\) is the sufficient statistic. We note that if \\(\\hat{\\lambda} &lt; \\lambda_o\\), then the ratio goes towards zero as \\(u_{\\rm obs} \\rightarrow 0\\), while if \\(\\hat{\\lambda} &gt; \\lambda_o\\), the ratio goes towards zero as \\(u_{\\rm obs} \\rightarrow \\infty\\). Thus there are two rejection regions: \\(U \\leq u_{\\alpha/2} = F_U^{-1}(\\alpha/2)\\), and \\(U \\geq u_{1-\\alpha/2} = F_U^{-1}(\\alpha/2)\\). The sampling distribution for \\(U\\) is Poisson(\\(n\\lambda\\)), and so we know how to determine the boundaries: \\(u_{\\alpha/2}\\) is given by qpois(alpha/2,n*lambda.o)-1, or 15, while \\(u_{1-\\alpha/2}\\) is given by qpois(1-alpha/2,n*lambda.o)+1, or 36. Our observed statistic is \\(U = 16\\), thus we fail to reject the null hypothesis. As a reminder, because the alternative hypothesis is a composite hypothesis, the NP lemma does not apply here, and thus we cannot guarantee that the likelihood ratio test we have just constructed is the most powerful of all possible tests of \\(H_o: \\lambda = \\lambda_o\\) versus \\(H_a: \\lambda \\neq \\lambda_o\\). 4.7.3 Using Wilks’ Theorem to Test Hypotheses About the Normal Mean We have collected \\(n\\) iid data from a normal distribution and we wish to test the hypothesis \\(H_o: \\mu = \\mu_o\\) versus the hypothesis \\(H_a: \\mu \\neq \\mu_o\\) using the likelihood ratio test. (We assume the variance is unknown.) For this problem, \\[\\begin{align*} \\Theta_o &amp;= \\{ \\mu,\\sigma^2 : \\mu = \\mu_o, \\sigma^2 &gt; 0 \\} \\\\ \\Theta_a &amp;= \\{ \\mu,\\sigma^2 : \\mu \\neq \\mu_o, \\sigma^2 &gt; 0 \\} \\,, \\end{align*}\\] and so \\(\\Theta = \\Theta_o \\cup \\Theta_a = \\{ \\mu,\\sigma^2 : \\mu \\in (-\\infty,\\infty), \\sigma^2 &gt; 0 \\}\\), with \\(r_o = 1\\) (\\(\\sigma^2\\)) and \\(r = 2\\) (\\(\\mu,\\sigma^2\\)). The likelihood for the normal pdf is \\[ \\mathcal{L}(\\mu,\\sigma \\vert \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp\\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right) \\] and the log-likelihood is \\[ \\ell(\\mu,\\sigma \\vert \\mathbf{x}) = -\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2 \\,. \\] The test statistic for Wilks’ theorem is \\[ W = -2 \\left[ \\ell(\\mu_o,\\widehat{\\sigma^2}_{MLE} \\vert \\mathbf{x}) - \\ell(\\hat{\\mu}_{MLE},\\widehat{\\sigma^2}_{MLE} \\vert \\mathbf{x}) \\right] \\,, \\] where \\(\\hat{\\mu}_{MLE}\\) and \\(\\widehat{\\sigma^2}_{MLE}\\) are the MLEs for \\(\\mu\\) and \\(\\sigma\\). We know these results from previous derivations: \\[\\begin{align*} \\hat{\\mu}_{MLE} &amp;= \\bar{X} \\\\ \\widehat{\\sigma^2}_{MLE} &amp;= \\frac{n-1}{n}S^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2 \\,. \\end{align*}\\] (“Wait a second,” the reader says. “Shouldn’t we use \\(\\mu_o\\) instead of \\(\\bar{X}\\) for \\(\\widehat{\\sigma^2}_{MLE}\\) when optimizing the likelihood in the numerator above? In other words, shouldn’t we use \\[ \\widehat{\\sigma^2}_{MLE,o} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\mu_o)^2 \\] instead of \\(\\widehat{\\sigma^2}_{MLE}\\)?” No, for the simple reason that \\(\\widehat{\\sigma^2}_{MLE} \\geq \\widehat{\\sigma^2}_{MLE,o}\\): we achieve a “more likely” value of the distribution width when we center on where the data actually are, rather than where we hypothesize they are.) Ultimately, we compare the value of \\(W\\) against the chi-square distribution for \\(r-r_o = 1\\) degree of freedom, and thus we reject the null (at level \\(\\alpha = 0.05\\)) if \\(W &gt; 3.841\\) (= qchisq(0.95,1)). 4.7.4 Simulating the Likelihood Ratio Test Wilks’ theorem generates an approximate result\\(-\\)it assumes that the test statistic is chi-square-distributed\\(-\\)and a major issue is that we do not know how good the approximation is. For instance, let’s say \\(n = 20\\)…this might be an insufficient sample size for the Wilks’ theorem machinery to yield an accurate and precise result (at least in terms of a rejection-region boundary). To get a sense as to how well Wilks’ theorem works for us, we can run simulations. We simulate sets of data under the null (\\(\\mu = 5\\)), and for each, we compute \\(W\\), and we determine the \\(100(1-\\alpha)^{\\rm th}\\) percentile value. This is our estimate of the rejection-region boundary. set.seed(101) alpha &lt;- 0.05 num.sim &lt;- 10000 n &lt;- 20 mu.o &lt;- 5 sigma2 &lt;- 9 # an arbitrary value alpha &lt;- 0.05 W &lt;- rep(NA,num.sim) f &lt;- function(X,n,mu.o) { hat.mu.mle &lt;- mean(X) hat.sigma2.mle &lt;- (n-1)*var(X)/n logL.o &lt;- -(n/2)*log(2*pi*hat.sigma2.mle)-(1/2/hat.sigma2.mle)*sum((X-mu.o)^2) logL &lt;- -(n/2)*log(2*pi*hat.sigma2.mle)-(1/2/hat.sigma2.mle)*sum((X-hat.mu.mle)^2) return(-2*(logL.o - logL)) } for ( ii in 1:num.sim ) { X &lt;- rnorm(n,mean=mu.o,sd=sqrt(sigma2)) W[ii] &lt;- f(X,n,mu.o) } df &lt;- data.frame(w=W[W&lt;8]) x &lt;- seq(0.01,8,by=0.01) f.x &lt;- dchisq(x,1) df.chi2 &lt;- data.frame(x=x,f.x=f.x) ggplot(data=df,aes(x=w,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0,8,by=0.5)) + geom_vline(xintercept=quantile(W,probs=c(1-alpha)),lwd=1,col=&quot;green&quot;) + geom_vline(xintercept=qchisq(1-alpha,1),lty=2,lwd=1,col=&quot;green&quot;) + geom_line(data=df.chi2,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + coord_cartesian(ylim=c(0,1)) + theme(axis.title=element_text(size = rel(1.25))) Figure 4.5: The empirical distribution of the statistic \\(-2(\\log\\mathcal{L}_o - \\log\\mathcal{L}_a)\\), with the chi-square distribution for \\(\\Delta r = 1\\) degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks’ theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks’ theorem at best provides approximate results and that simulations can provide more accurate and precise results. # TBD alternative - green lines over red curve ggplot(data=df,aes(x=w,y=after_stat(density))) + geom_histogram(fill=&quot;blue&quot;,col=&quot;black&quot;,breaks=seq(0,8,by=0.5)) + geom_line(data=df.chi2,aes(x=x,y=f.x),col=&quot;red&quot;,lwd=1) + geom_vline(xintercept=quantile(W,probs=c(1-alpha)),lwd=1,col=&quot;green&quot;) + geom_vline(xintercept=qchisq(1-alpha,1),lty=2,lwd=1,col=&quot;green&quot;) + coord_cartesian(ylim=c(0,1)) + theme(axis.title=element_text(size = rel(1.25))) Figure 4.6: The empirical distribution of the statistic \\(-2(\\log\\mathcal{L}_o - \\log\\mathcal{L}_a)\\), with the chi-square distribution for \\(\\Delta r = 1\\) degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks’ theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks’ theorem at best provides approximate results and that simulations can provide more accurate and precise results. cat(&quot;The empirical rejection region boundary is&quot;,round(quantile(W,probs=c(1-alpha)),3),&quot;\\n&quot;) ## The empirical rejection region boundary is 4.584 cat(&quot;The proportion of simulated statistic values in the Wilks&#39; theorem rejection region&quot;,round(sum(W&gt;=qchisq(1-alpha,1))/num.sim,3),&quot;\\n&quot;) ## The proportion of simulated statistic values in the Wilks&#39; theorem rejection region 0.071 The boundary value via simulation is 4.584, which is sufficiently far from the Wilks’ theorem value of 3.841 to be concerning. The upshot: if \\(n\\) is small, it is best not to assume that \\(W\\) is chi-square-distributed; run simulations to determine rejection region boundaries and \\(p\\)-values instead. 4.8 The Gamma Distribution The gamma distribution is a continuous distribution that is commonly used to, e.g., model the waiting times between discrete events. Its probability density function is given by \\[ f_X(x) = \\frac{x^{\\alpha-1}}{\\beta^\\alpha} \\frac{\\exp\\left(-x/\\beta\\right)}{\\Gamma(\\alpha)} \\,, \\] where \\(x \\in [0,\\infty)\\), \\(\\alpha\\) and \\(\\beta\\) are both \\(&gt;\\) 0, and \\(\\Gamma(\\alpha)\\) is the gamma function: \\[ \\Gamma(\\alpha) = \\int_0^\\infty u^{\\alpha-1} e^{-u} du \\,. \\] (See Figure 4.7.) \\(\\alpha\\) and \\(\\beta\\) are referred to as “shape” and “scale” parameters, respectively. The gamma family of distributions exhibits a wide variety of functional shapes and it is the parent family to a number of other distributions, some of which we have met before. One in particular is the exponential distribution, \\[ f_X(x) = \\frac{1}{\\beta} \\exp\\left(-\\frac{x}{\\beta}\\right) \\,, \\] which is a gamma distribution with \\(\\alpha = 1\\). Note how we lead off above by saying that the gamma distribution is commonly used to model the waiting times between discrete events. The exponential distribution specifically models the waiting time between one event and the next in a Poisson process. The number of strong earthquakes that occur in California in one year? That can be modeled as a Poisson random variable. The time that elapses between two consecutive strong earthquakes in California? That can be modeled using the exponential distribution. (For completeness: the Erlang distribution is a generalization of the exponential distribution, in the sense that we can use it to model the waiting time between the \\(i^{\\rm th}\\) and \\((i+\\alpha)^{\\rm th}\\) events, where \\(\\alpha\\) is a positive integer, in a Poisson process.) Distributions Within the Gamma Family of Distributions distribution \\(\\alpha\\) \\(\\beta\\) exponential 1 \\((0,\\infty)\\) Erlang \\(\\{1,2,3,\\ldots\\}\\) \\((0,\\infty)\\) chi-square \\(\\{1/2,1,3/2,\\ldots\\}\\) 2 Let’s conclude this section by repeating the exercise we did in the last chapter while discussing the beta distribution, the one in which we examined the functional form of the likelihood function \\(\\mathcal{L}(p \\vert k,x)\\). Here, we write down the Poisson likelihood function \\[ \\mathcal{L}(\\lambda \\vert x) = \\frac{\\lambda^x}{x!} e^{-\\lambda} \\] and compare it with the gamma pdf. We can match the gamma pdf if we map the Poisson \\(\\lambda\\) to the gamma \\(x\\), and the Poisson \\(x\\) to the gamma \\(\\alpha-1\\) (and if we set \\(\\beta\\) to 1). But because the Poisson \\(x\\) is an integer with values \\(\\{0,1,2,\\ldots\\}\\), we find that the integrand specifically matches the Erlang pdf, for which \\(\\alpha = \\{1,2,3,\\ldots\\}\\). So, if we observe a random variable \\(X \\sim\\) Poisson(\\(\\lambda\\)), then the likelihood function \\(\\mathcal{L}(\\lambda \\vert x)\\) has the shape (and normalization!) of a Gamma(\\(x+1,1\\)) (or Erlang(\\(x+1\\))) distribution. (About the normalization: if we integrate the likelihood function over its domain, we find that \\[ \\frac{1}{x!} \\int_0^\\infty \\lambda^x e^{-\\lambda} d\\lambda = \\frac{1}{x!} \\Gamma(x+1) = \\frac{x!}{x!} = 1 \\,. \\] The mathematics works out because \\(x\\) is integer valued and thus \\(\\Gamma(x+1) = x!\\).) Figure 4.7: Three examples of gamma probability density functions: Gamma(2,2) (red), Gamma(4,2) (blue), and Gamma(2,3) (green). Gamma Distribution - R Functions quantity R function call PDF dgamma(x,shape,scale) CDF pgamma(x,shape,scale) Inverse CDF qgamma(p,shape,scale) \\(n\\) iid random samples rgamma(n,shape,scale) 4.8.1 The Expected Value of a Gamma Random Variable The expected value of a gamma random variable is found by introducing constants into the expected value integral so that a gamma pdf integrand is formed. Specifically \\[\\begin{align*} E[X] = \\int_0^\\infty x f_X(x) dx &amp;= \\int_0^\\infty x \\frac{x^{\\alpha-1}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^\\alpha} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha+1)} \\frac{\\beta^{\\alpha+1}}{\\beta^{\\alpha+1}} dx \\\\ &amp;= \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^{\\alpha+1}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha+1)} \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)} \\frac{\\beta^{\\alpha+1}}{\\beta^\\alpha} dx \\\\ &amp;= \\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)} \\frac{\\beta^{\\alpha+1}}{\\beta^\\alpha} \\int_0^\\infty \\frac{x^{\\alpha}}{\\beta^{\\alpha+1}} \\frac{\\exp(-x/\\beta)}{\\Gamma(\\alpha+1)} dx \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{\\Gamma(\\alpha)} \\beta \\times 1 \\\\ &amp;= \\alpha \\beta \\,. \\end{align*}\\] By introducing the constants, we are able to transform the integrand to that of a Gamma(\\(\\alpha+1,\\beta\\)) distribution, and because the integral is over the entire domain of a gamma distribution, the integral evaluates to 1. A similar calculation involving the derivation of \\(E[X^2]\\) allows us to determine that the variance of a gamma random variable is \\(V[X] = \\alpha \\beta^2\\). 4.8.2 The Distribution of the Sum of Exponential Random Variables As stated above, the exponential distribution, i.e., the gamma distribution with \\(\\alpha = 1\\), is used to model the waiting time between two successive events in a Poisson process. Let’s assume that we have recorded \\(n\\) separate times between \\(n\\) separate pairs of events. What is the distribution of \\(T = T_1 + \\cdots + T_n\\)? As we typically do when faced with a linear function of \\(n\\) iid random variables, we utilize the method of moment-generating functions: \\[ m_T(t) = \\prod_{i=1}^n m_{T_i}(t) \\,, \\] where \\(m_{T_i}(t) = (1-\\beta t)^{-1}\\) is the mgf for the exponential distribution. Thus \\[ m_T(t) = \\prod_{i=1}^n (1-\\beta t)^{-1} = (1-\\beta t)^{-n} \\,. \\] This has the form of the mgf for a Gamma(\\(n,\\beta\\)) distribution, or, equivalently, an Erlang(\\(n,\\beta\\)) distribution. In other words the sum of \\(n\\) iid waiting times has the same distribution as the waiting time between the \\(i^{\\rm th}\\) and \\((i+n)^{\\rm th}\\) events of a Poisson process. 4.8.3 Memorylessness and the Exponential Distribution An important feature of the exponential distribution is that when we use it to model, e.g., the lifetimes of components in a system, it exhibits memorylessness. In other words, if \\(T\\) is the random variable representing a component’s lifetime, where \\(T \\sim\\) Exponential(\\(\\beta\\)) \\(E[T] = \\beta\\), it doesn’t matter how old the component is when you first examine it: from that point onward, the average lifetime will be \\(\\beta\\). Let’s demonstrate how this works via a word problem. We examine a component “born” at time \\(t_0=0\\) at a later time \\(t_1\\), and we wish to determine the probability that it will live beyond an even later time \\(t_2\\). In other words, we wish to compute \\[ P(T \\geq t_2-t_0 \\vert T \\geq t_1-t_0) \\,. \\] (We know the component lived to time \\(t_1\\), hence the added condition.) Let \\(T \\sim\\) Exponential(\\(\\beta\\)). Then \\[\\begin{align*} P(T \\geq t_2-t_0 \\vert T \\geq t_1-t_0) &amp;= \\frac{P(T \\geq t_2-t_0 \\cap T \\geq t_1-t_0)}{P(T \\geq t_1-t_0)} \\\\ &amp;= \\frac{P(T \\geq t_2-t_0)}{P(T \\geq t_1-t_0)} \\\\ &amp;= \\frac{\\int_{t_2-t_0}^\\infty (1/\\beta) \\exp(-t/\\beta) dt}{\\int_{t_1-t_0}^\\infty (1/\\beta) \\exp(-t/\\beta) dt} \\\\ &amp;= \\frac{\\left. -\\exp(-t/\\beta) \\right|_{t_2-t_0}^\\infty}{\\left. -\\exp(-t/\\beta) \\right|_{t_1-t_0}^\\infty} \\\\ &amp;= \\frac{0 + \\exp(-(t_2-t_0)/\\beta)}{0 + \\exp(-(t_1-t_0)/\\beta)} \\\\ &amp;= \\exp[-(t_2-t_1)/\\beta] = P(T \\geq t_2-t_1) \\,. \\end{align*}\\] Note that \\(t_0\\) drops out of the final result: no matter how long ago \\(t_0\\) might have been, the probability that the component will live \\(t_2-t_1\\) units longer is the same, and the average additional lifetime is still \\(\\beta\\). 4.9 Poisson Regression Suppose that for a given measurement \\(x\\), we record a random variable \\(Y\\) that is a number of counts that we observe. For instance, \\(x\\) might be the time of day, and \\(Y\\) might be the observed number of cars parked in a lot at that time. Because \\(Y\\) is (a) integer valued, and (b) non-negative, an appropriate distribution for the random variable \\(Y \\vert x\\) might be the Poisson distribution, and thus to model these data, we may want to pursue Poisson regression. Recall: To implement a generalized linear model (or GLM), we need to do two things: examine the response values and select an appropriate distribution for them (are they discretely or continuously valued? what is the functional domain?); and define a link function \\(g(\\theta \\vert x)\\) that maps the line \\(\\beta_0 + \\beta_1 x_i\\), which has infinite range, into a more limited range (e.g., \\([0,\\infty)\\)). Because \\(\\lambda &gt; 0\\), in Poisson regression we adopt a link function that maps \\(\\beta_0 + \\beta_1 x\\) from the range \\((-\\infty,\\infty)\\) to \\((0,\\infty)\\). There is no unique choice of link function, but the conventionally applied one is the logarithm: \\[ g(\\lambda \\vert x) = \\log (\\lambda \\vert x) = \\beta_0 + \\beta_1 x ~~\\implies~~ \\lambda \\vert x = e^{\\beta_0 + \\beta_1 x} \\,. \\] Similar to logistic regression, our goal is to estimate \\(\\beta_0\\) and \\(\\beta_1\\), which is done via numerical optimization of the likelihood function \\[ \\mathcal{L}(\\beta_0,\\beta_1 \\vert \\mathbf{y}) = \\prod_{i=1}^n p_{Y \\vert \\beta_0,\\beta_1}(y_i \\vert \\beta_0,\\beta_1) \\,. \\] For the Poisson distribution, \\(E[X] = V[X] = \\lambda\\), so the expectation is that for any given value of \\(x\\), \\(E[Y \\vert x] = V[Y \\vert x]\\). However, it is commonly seen in real-life data that the sample variance of \\(Y \\vert x\\) exceeds the sample mean. This is dubbed overdispersion, and it can arise when, e.g., the observed Poisson process is inhomogeneous…or differently stated, when \\(\\lambda\\) varies as a function of space and/or time. A standard way to deal with overdispersion is to move from Poisson regression to negative binomial regression. Before we say more, however, we note that while the name is technically correct (in the sense that the model assumes that the response data are negative binomially distributed), it can be very confusing for those new to regression, who might view the negative binomial as a distribution that is only useful when, e.g., modeling failures in Bernoulli trials. How could that distribution possibly apply here? The answer is that the negative binomial probability mass function is “just” a function (and as such, it is “allowed” to have more general use than just modeling failures), but more to the point, it is a function that arises naturally when we apply the Law of Total Probability to the Poisson pmf. Let’s suppose that \\(Y \\vert x \\sim\\) Poisson(\\(\\lambda\\)), but that \\(\\lambda\\) itself is a random variable. There is no unique way to model the distribution of \\(\\lambda\\), but the gamma distribution provides a flexible means by which to do so (since the family encompasses a wide variety of shapes, unlike, say, the normal distribution, which can never be skew). Let’s assume that \\(\\lambda \\sim\\) Gamma(\\(\\theta,p/\\theta\\)). The distribution of \\(Y\\) is found with the LoTP: \\[\\begin{align*} p_{Y \\vert \\theta,p}(y \\vert \\theta,p) &amp;= \\int_0^\\infty p_{Y \\vert \\lambda}(y \\vert \\lambda) f_{\\lambda}(\\lambda \\vert \\theta,p) d\\lambda \\\\ &amp;= \\int_0^\\infty \\frac{\\lambda^y}{y!} e^{-\\lambda} \\frac{\\lambda^{\\theta-1}}{(p/\\theta)^\\theta} \\frac{e^{-\\lambda/(p/\\theta)}}{\\Gamma(\\theta)} d\\lambda \\\\ &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\int_0^\\infty \\lambda^{y+\\theta+1} e^{-\\lambda(1+\\theta/p)} d\\lambda \\,. \\end{align*}\\] The integrand looks suspiciously like the integrand of a gamma function integral, but we have to change \\(\\lambda(1+\\theta/p)\\) in the exponential to just \\(\\lambda&#39;\\): \\[ \\lambda&#39; = \\lambda(1+\\theta/p) ~~ \\mbox{and} ~~ d\\lambda&#39; = d\\lambda (1+\\theta/p) \\,. \\] The bounds of the integral do not change. The integral now becomes \\[\\begin{align*} p_{Y \\vert \\theta,p}(y \\vert \\theta,p) &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\frac{1}{(1+\\theta/p)^{y+\\theta}} \\int_0^\\infty (\\lambda&#39;)^{y+\\theta-1} e^{-\\lambda&#39;} d\\lambda&#39; \\\\ &amp;= \\frac{1}{y!}\\left(\\frac{\\theta}{p}\\right)^\\theta \\frac{1}{\\Gamma(\\theta)} \\frac{1}{(1+\\theta/p)^{y+\\theta}} \\Gamma(y+\\theta) \\\\ &amp;= \\frac{(y+\\theta-1)!}{y! (\\theta-1)!} \\left(\\frac{\\theta}{p}\\right)^\\theta \\left(\\frac{p}{p+\\theta}\\right)^{y+\\theta} \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{p}{p+\\theta}\\right)^y \\left(\\frac{\\theta}{p}\\right)^\\theta \\left(\\frac{p}{p+\\theta}\\right)^\\theta \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{p}{p+\\theta}\\right)^y \\left(\\frac{\\theta}{p+\\theta}\\right)^\\theta \\\\ &amp;= \\binom{y+\\theta-1}{y} \\left(\\frac{\\theta}{p+\\theta}\\right)^\\theta \\left(1 - \\frac{\\theta}{p+\\theta}\\right)^y \\,. \\end{align*}\\] This has the functional form of a negative binomial pmf in which \\(y\\) represents the number of failures \\(\\theta\\) is the number of successes, and \\(\\theta/(p+\\theta)\\) is the probability of success. Again, to reiterate: \\(Y\\) might be distributed as a negative binomial random variable, but what we are really modeling is the \\(\\lambda\\) parameter of the Poisson distribution. Now, why do we choose this form of the negative binomial distribution? We do so because it so happens that \\[\\begin{align*} E[Y] &amp;= p \\\\ V[Y] &amp;= p + \\frac{p^2}{\\theta} \\,. \\end{align*}\\] (One can derive these quantities starting with, e.g., \\(E[Y] = \\theta(1-p&#39;)/p&#39;\\) and \\(V[Y] = \\theta(1-p&#39;)/(p&#39;)^2\\) and plugging in \\(p&#39; = \\theta/(p+\\theta)\\).) Varying \\(\\theta\\) allows us to model the overdispersion with a single variable. (We assume that \\(p\\) takes the place of \\(\\lambda\\), in the sense that now \\(p \\vert x = \\exp(\\beta_0 + \\beta_1x\\)).) In the limit that \\(\\theta \\rightarrow \\infty\\), negative binomial regression becomes Poisson regression. Because the overdispersion is represented in a single variable, we can examine the results of learning both Poisson and negative binomial regression models to determine whether or not the quality of fit improves enough to justify the extra model complexity. 4.9.1 Revisiting the Death-by-Horse-Kick Example Modeling Bortkiewicz’s horse-kick dataset provides a simple example of the use of Poisson regression. x &lt;- 0:4 Y &lt;- c(109,65,22,3,1) poi.out &lt;- glm(Y~x,family=poisson) summary(poi.out) ## ## Call: ## glm(formula = Y ~ x, family = poisson) ## ## Deviance Residuals: ## 1 2 3 4 5 ## -1.1700 2.2681 0.6145 -1.9199 -1.3639 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.80136 0.08490 56.55 &lt;2e-16 *** ## x -0.92213 0.07704 -11.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 232.430 on 4 degrees of freedom ## Residual deviance: 12.437 on 3 degrees of freedom ## AIC: 38.911 ## ## Number of Fisher Scoring iterations: 5 The summary output from the Poisson regression model is, in its structure, identical to that of logistic regression. But there are some differences in how values are defined. For instance, the deviance residual is \\[ d_i = \\mbox{sign}(Y_i - \\hat{Y}_i) \\sqrt{2[Y_i \\log (Y_i/\\hat{Y}_i) - (Y_i - \\hat{Y}_i)]} \\,, \\] where \\[ \\hat{Y}_i = \\hat{\\lambda}_i = \\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_i) \\,. \\] (Note that when \\(Y_i = 0\\), \\(Y_i \\log (Y_i/\\hat{Y}_i)\\) is assumed to be zero.) Because there are only five data points in the fit, all the deviance residual values are displayed, rather than a five-number summary. Also, the residual deviance (here, 12.437) is not \\(-2 \\log \\mathcal{L}_{max}\\), as it was for logistic regression. There are two ways to determine \\(\\mathcal{L}_{max}\\); one is to take the AIC value (38.911), subtract 2 times the number of model terms (2 here, thus yielding 34.911), and then dividing by \\(-2\\). The more straightforward way, however, is to utilize the logLik() function: logLik(poi.out) ## &#39;log Lik.&#39; -17.45551 (df=2) As a final note, unlike the case of logistic regression where determining the quality of fit of the learned model is not particularly straightforward, for Poisson regression we can simply assume that the residual deviance is chi-square-distributed for the given number of degrees of freedom under the null. Here, the \\(p\\)-value is 1 - pchisq(12.437,3) ## [1] 0.006026712 or 0.0060. Because this value is less than, e.g., \\(\\alpha = 0.05\\), we would (in this instance) reject the null hypothesis that the observed data are truly Poisson distributed. 4.9.2 Negative Binomial Regression Example In the code chunk below, we simulate 100 data at each of four different values of \\(x\\): 1, 2, 3, and 4. The data are simulated with a Poisson overdispersion factor of 2. set.seed(236) n &lt;- 100 # 100 data per x value x &lt;- rep(c(1,2,3,4),n) Y &lt;- rep(NA,length(x)) for ( ii in 1:length(x) ) { Y[ii] &lt;- rpois(1,rgamma(1,2,scale=x[ii]/2)) } For these data, \\(E[Y \\vert x] = x\\) and \\(V[Y \\vert x] = x + x^2/2\\), meaning that the overdispersion factor is, again, \\(\\theta = 2\\). Let’s see how overdispersion affects the learning of a Poisson regression model. First, the Poisson regression model itself: summary(glm(Y~x,family=poisson)) ## ## Call: ## glm(formula = Y ~ x, family = poisson) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9013 -1.6404 -0.3122 0.6825 5.6697 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.08336 0.09240 -0.902 0.367 ## x 0.38014 0.02944 12.913 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1107 on 399 degrees of freedom ## Residual deviance: 930 on 398 degrees of freedom ## AIC: 1805 ## ## Number of Fisher Scoring iterations: 5 …and second, the negative binomial regression model, as learned using the glm.nb() function of the MASS package. (Note that MASS does not represent the state of Massachusetts, but rather stands for “Modern Applied Statistics with S”…with S being the precursor software to R.) library(MASS) summary(glm.nb(Y~x)) ## ## Call: ## glm.nb(formula = Y ~ x, init.theta = 1.84882707, link = log) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1019 -1.2027 -0.2296 0.4198 2.8666 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.10769 0.13101 -0.822 0.411 ## x 0.38908 0.04483 8.679 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(1.8488) family taken to be 1) ## ## Null deviance: 530.52 on 399 degrees of freedom ## Residual deviance: 454.45 on 398 degrees of freedom ## AIC: 1633.9 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 1.849 ## Std. Err.: 0.258 ## ## 2 x log-likelihood: -1627.868 The negative binomial model is displayed in Figure 4.8. When we compare the output, we first look for the lines beginning with (Dispersion parameter...): (Dispersion parameter for poisson family taken to be 1) (Dispersion parameter for Negative Binomial(1.8488) family taken to be 1) The second line is perhaps sub-optimally worded, as the estimate of the dispersion parameter \\(\\theta\\) is not 1, but the value in parentheses: \\(\\hat{\\theta} = 1.8488\\). This is close to the true value \\(\\theta = 2\\) and is definitely different from \\(\\theta = \\infty\\) (the value for truly Poisson-distributed data), but is it statistically significantly different, such that we know we should adopt the negative binomial model? To answer that question, we need to utilize the standard error for \\(\\hat{\\theta}\\); here, thet is 0.258. By the empirical rule, we would expect the true value of \\(\\theta\\) to lie within three standard errors of \\(\\hat{\\theta}\\)…and \\(1.849 + 3 \\times 0.258 = 2.623\\), so \\(\\theta\\) is definitely smaller than infinity. So it is clear that we would adopt the negative binomial model here. And there’s another bit of evidence that supports our adoption of that model: AIC: 1805 # Poisson AIC: 1633.9 # NB The Akaike Information Criterion, or AIC, as you will recall, is a quality-of-fit metric that penalizes model complexity. If we learn a suite of models, we would generally adopt that associated with the lowest AIC value. Here, the negative binomial model has a much lower AIC value than the Poisson model, so we would definitely adopt it! (Note that when the data are not overdispersed, \\(\\hat{\\theta}\\) will usually be a large number, but not actually infinity. If in doubt about whether “large” is “large enough,” we can always fall back on the outputted AIC values so as to make a decision.) Figure 4.8: The negative binomial regression line superimposed on the simulated data. The jitter() function is applied to the \\(x\\) values to allow us to more easily see the number of counts as a function of \\(x\\) and \\(Y\\). 4.10 Chi-Square-Based Hypothesis Testing In the previous chapter, we introduced the chi-square goodness-of-fit test as a way of conducting hypothesis tests given multinomial data. Given \\(k\\) data recorded in \\(m\\) separate bins, we can compute the test statistic \\[ W = \\sum_{i=1}^m \\frac{(X_i - kp_i)^2}{kp_i} \\,, \\] where \\(X_i\\) is the number of data observed in bin \\(i\\), and where \\(p_i\\) is the probability of any one datum being observed in that bin under the null. If \\(k\\) is sufficiently large, then \\(W\\) converges in distribution to a chi-square random variable for \\(m-1\\) degrees of freedom. (Hence the name of the test!) But: what if the overall observed number of data \\(X\\) is a random variable instead of being a set constant \\(X=k\\)? Imagine a very simple digital camera that has four light collecting bins, each of the same size, and we point it towards a light source that emits an average of \\(\\lambda\\) photons in a particular time period. If we open the shutter for that time period, what we would observe is \\(X \\sim\\) Poisson(\\(\\lambda\\)) photons, with the numbers in each bin being \\(\\{X_1,X_2,X_3,X_4\\}\\). Now let’s say we want to test the hypothesis that the probability of a photon falling into each bin is the same, i.e., \\(H_o: p_1 = p_2 = p_3 = p_4 = 1/4\\). If we were to use the chi-square GoF test directly, we would compute \\[ w_{obs} = \\sum_{i=1}^4 \\frac{(X_i - \\lambda p_i)^2}{\\lambda p_i} \\] and then compute the \\(p\\)-value \\(1-F_W(w_{obs})\\) assume 3 degrees of freedom. (In R, this would be computed via 1 - pchisq(w.obs,3).) We can do this\\(-\\)the computer will not stop us from doing so\\(-\\)but is this valid? The answer is that this is valid so long as each \\(X_i\\) converges in distribution to a normal random variable. And Poisson random variables do converge in distribution to normal random variables. Thus, in short, yes, use of the chi-square GoF test is valid if \\(\\lambda p_i\\) is large. The question is, how large? The Poisson probability mass function is \\[ p_X(x \\vert \\lambda) = \\frac{\\lambda^x}{x!} e^{-\\lambda} \\,. \\] We note that the normal probability density function does not have a factorial in it, so we will start by using Stirling’s approximation: \\[ x! \\approx \\sqrt{2 \\pi x} x^x e^{-x} \\,. \\] This approximation has an error of 1.65% for \\(x = 5\\) and 0.83% for \\(x = 10\\), with the percentage error continuing to shrink as \\(x \\rightarrow \\infty\\). With this approximation, we can write that \\[ p_X(x \\vert \\lambda) \\approx \\frac{\\lambda^x}{\\sqrt{2 \\pi x}} x^{-x} e^{x-\\lambda} \\,. \\] This still does not quite look like a normal pdf. So there is more work to do. We compute the logarithm of this quantity: \\[ \\log p_X(x \\vert \\lambda) \\approx x \\log \\lambda - \\frac{1}{2} \\log (2 \\pi x) - x \\log x + x - \\lambda = -x ( \\log x - \\log \\lambda ) - \\frac{1}{2} \\log (2 \\pi x) + x - \\lambda \\,, \\] and then look at \\(\\log x - \\log \\lambda\\): \\[ \\log x - \\log \\lambda = \\log \\frac{x}{\\lambda} = \\log \\left( 1 - \\frac{\\lambda-x}{\\lambda}\\right) \\approx -\\frac{\\delta}{\\sqrt{\\lambda}} - \\frac{\\delta^2}{2 \\lambda} - \\cdots \\,. \\] Here, \\(\\delta = (\\lambda - x)/\\sqrt{\\lambda}\\). Plugging this result into the expression for \\(\\log p_X(x \\vert \\lambda)\\), we find that \\[ \\log p_X(x \\vert \\lambda) \\approx -\\frac{1}{2} \\log (2 \\pi x) + x \\left( \\frac{\\delta}{\\sqrt{\\lambda}} + \\frac{\\delta^2}{2\\lambda} \\right) - \\delta \\sqrt{\\lambda} \\,. \\] The next step is plug in \\(x = \\lambda - \\sqrt{\\lambda}\\delta\\): \\[\\begin{align*} \\log p_X(x \\vert \\lambda) &amp;\\approx -\\frac{1}{2} \\log (2 \\pi x) + (\\lambda - \\sqrt{\\lambda}\\delta)\\left( \\frac{\\delta}{\\sqrt{\\lambda}} + \\frac{\\delta^2}{2\\lambda} \\right) - \\delta \\sqrt{\\lambda} \\\\ &amp;= -\\frac{1}{2} \\log (2 \\pi x) + \\sqrt{\\lambda}\\delta - \\delta^2 + \\frac{\\delta^2}{2} - \\frac{\\delta^3}{2 \\lambda^{3/2}} - \\sqrt{\\lambda}\\delta \\\\ &amp;\\approx -\\frac{1}{2} \\log (2 \\pi x) - \\frac{\\delta^2}{2} \\,, \\end{align*}\\] where we drop the \\(\\mathcal{O}(\\delta^3)\\) term. When we exponentiate both sides, the final result is \\[ p_X(x \\vert \\lambda) \\approx \\frac{1}{\\sqrt{2 \\pi x}} \\exp\\left( -\\frac{\\delta^2}{2} \\right) = \\frac{1}{\\sqrt{2 \\pi x}} \\exp\\left( -\\frac{(x-\\lambda)^2}{2\\sqrt{\\lambda}} \\right) \\,. \\] This has the (approximate) form of a normal pdf, at least for values \\(x \\approx \\lambda\\). So, in the end, the Poisson probability mass function \\(p_X(x \\vert \\lambda)\\) has approximately the same shape as the normal probability density function \\(f_X(x \\vert \\mu=\\lambda,\\sigma^2=\\lambda)\\) if \\(x \\gg 1\\) and \\(x \\approx \\lambda\\). The conventional rule-of-thumb is that one can utilize the chi-square GoF test with Poisson data so long as \\(\\lambda p_i \\geq 5\\) counts in each bin. 4.10.1 Revisiting the Death-by-Horse-Kick Example In previous examples, we have shown that over the course of 20 years, in 10 separate Prussian army corps, soldiers died as a result of horse kicks at a rate of \\(\\hat{\\lambda} = \\bar{X} = 0.61\\) deaths per corps per year, and that a 95% confidence interval for \\(\\lambda\\) is [0.51,0.73]. However, we never examined what is perhaps the most important question of all: is it plausible that the data are Poisson-distributed in the first place? We last looked at the idea of performing hypothesis tests regarding distributions in Chapter 2, where we introduce the Kolmogorov-Smirnov test for use with arbitrary distributions and the Shapiro-Wilk test to assess the plausibility that our data are normally distributed. So it would seem that here we should work with the KS test, as the Poisson distribution is not the normal distribution…but we can only use the KS test in the context of continuous distributions. So we need a new method! We can utilize the chi-square goodness-of-fit test. Let’s assume \\(\\lambda = 0.61\\). Then the probabilities \\(P(X=x)\\) are as follows: x P(X=x) \\(E_x\\) \\(O_x\\) 0 0.543 108.67 109 1 0.331 66.29 65 2 0.101 20.29 22 3 0.021 4.11 3 4 0.003 0.63 1 The conventional rule-of-thumb is that the expected number of counts in each bin must be \\(\\geq 5\\). Here, we will break that rule slightly by combining bins 3 and 4 into one bin with expected counts 4.11 + 0.63 = 4.74. The chi-square GOF test statistic is thus \\[ W = \\frac{(109-108.67)^2}{108.67} + \\frac{(65-66.29)^2}{66.29} + \\frac{(22-20.29)^2}{20.29} + \\frac{(4-4.74)^2}{4.74} = 0.298 \\,. \\] This figure can be found using R: e &lt;- 200*dpois(0:4,0.61) e[4] &lt;- e[4]+e[5] e &lt;- e[-5] o &lt;- c(109,65,22,4) (W = sum((o-e)^2/e)) ## [1] 0.2980418 When using the chi-square GOF test to assess the viability of a distribution whose parameters are estimated, we lose additional degrees of freedom, so the number of degrees of freedom here is 4 - 2 = 2. The \\(p\\)-value is 1 - pchisq(W,2) ## [1] 0.8615511 We find that we fail to reject the null hypothesis and thus that it is plausible that Bortkiewicz’s horse-kick data are indeed Poisson-distributed. "],["the-uniform-distribution.html", "5 The Uniform Distribution 5.1 Properties 5.2 Linear Functions of Uniform Random Variables 5.3 Sufficient Statistics and the Minimum Variance Unbiased Estimator 5.4 Maximum Likelihood Estimation 5.5 Confidence Intervals 5.6 Hypothesis Testing", " 5 The Uniform Distribution 5.1 Properties The uniform distribution is often used within the realm of probability, in part because of its utility and in part because of its simplicity. We briefly touched upon this distribution at times earlier in this book, such as, for instance, when we talked about hypothesis test \\(p\\)-values (which are distributed uniformly between 0 and 1 when the null hypothesis is correct). Why do we return to the uniform distribution now? Because it is slightly different from other distributions: its two parameters, often denoted \\(a\\) and \\(b\\) (where \\(b &gt; a\\)), do not dictate the shape of its probability density function, but rather its domain. This affects aspects of estimation, such as determining sufficient statistics and deriving maximum likelihood estimates, etc. We highlight these “quirks” of the uniform pdf throughout this chapter. Recall: a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) \\(f_X(x) \\geq 0\\) and (b) \\(\\int_x f_X(x) dx = 1\\), where the integral is over the full domain of \\(f_X(x)\\). The uniform pdf is defined as \\[ f_X(x) = \\frac{1}{b-a} ~~\\mbox{where}~~ x \\in [a,b] \\,. \\] \\(f_X(x)\\) is thus constant between \\(a\\) and \\(b\\). (See Figure 5.1.) This means that we can think of the uniform distribution “geometrically,” as the following is true: \\[ \\underbrace{(b-a)}_{\\mbox{domain}} \\cdot \\underbrace{\\frac{1}{b-a}}_{f_X(x)} = 1 \\] If we know the domain of the pdf, we immediately know \\(f_X(x)\\); conversely, if we know \\(f_X(x)\\), we immediately know the width of the domain (but not \\(a\\) and \\(b\\) themselves). Figure 5.1: Three examples of uniform probability mass functions: Uniform(0,1) (red), Uniform(0.5,2) (blue), and Uniform(-1.5,1.5) (green). Recall: the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as \\(F_X(x) = \\int_{y\\leq x} f_Y(y)\\), and it is defined for all values \\(x \\in (-\\infty,\\infty)\\). (Thus \\(F_X(\\infty) = 0\\) and \\(F_X(\\infty) = 1\\).) The cdf for a uniformly distributed random variable is \\[ F_X(x) = \\int_a^x f_Y(y) dy = \\int_a^x \\frac{1}{b-a} dy = \\frac{x-a}{b-a} ~~ x \\in [a,b] \\,, \\] with a value of 0 for \\(x &lt; a\\) and 1 for \\(x &gt; b\\). (We can quickly confirm that the derivative of the cdf yields the pdf. Recall that for continuous distributions, \\(f_X(x) = dF_X(x)/dx\\).) Recall: an inverse cdf function \\(F_X^{-1}(q)\\) takes as input the total probability \\(q \\in [0,1]\\) in the range \\((-\\infty,x]\\) and returns the value of \\(x\\). A continuous distribution has a unique inverse cdf over the domain of \\(f_X(x)\\). The inverse cdf is exceptionally simple to compute: \\[ q = \\frac{x-a}{b-a} ~~ \\Rightarrow ~~ x = (b-a)q + a \\,. \\] Technically the inverse cdf has no unique solution when \\(q = 0\\) or \\(q = 1\\). However, it is convention (for instance, within R) that the inverse cdf output for continuous distributions be the largest value for which \\(q = 0\\) and the smallest value for which \\(q = 1\\). Thus, for a Uniform(\\(a,b\\)) distribution, when \\(q = 0\\), then \\(x = a\\), and when \\(q = 1\\), \\(x = b\\). Uniform Distribution - R Functions quantity R function call PDF dunif(x,a,b) CDF punif(x,a,b) Inverse CDF qunif(q,a,b) \\(n\\) iid random samples runif(n,a,b) A discrete analogue to the uniform distribution is the discrete uniform distribution, which is defined over a range of integers \\([a,b]\\). (The rolls of a fair, six-sided die would, for instance, be governed by the discrete uniform distribution.) The pmf for the discrete uniform is \\[ p_X(x) = \\frac{1}{n} ~~ x \\in [a,b] \\,, \\] where \\(n = b - a + 1\\) is the number of possible experimental outcomes. The cdf is \\[ F_X(x) = \\frac{\\lfloor x \\rfloor - a + 1}{n} ~~ x \\in [a,b] \\,, \\] where \\(\\lfloor x \\rfloor\\) is the largest integer that is smaller than or equal to \\(x\\), while the inverse cdf is given by the generalized inverse cdf formalism that we’ve previously seen for discrete distributions. Note that there are no standard R functions of the form xdiscunif() for computing the pmf or cdf of the discrete uniform distribution, or for sampling from it. We will show how one can create such functions in an example below. 5.1.1 The Expected Value and Variance of a Uniform Random Variable Recall: the expected value of a continuously distributed random variable is \\[ E[X] = \\int_x x f_X(x) dx\\,, \\] where the integral is over the full domain of \\(f_X(x)\\). The expected value is equivalent to a weighted average, with the weight for each possible value of \\(x\\) given by \\(f_X(x)\\). The expected value of a random variable drawn from a Uniform(\\(a,b\\)) distribution is \\[ E[X] = \\int_a^b x f_X(x) dx = \\int_a^b \\frac{x}{b-a} dx = \\frac{1}{b-a} \\left. \\frac{x^2}{2} \\right|_a^b = \\frac{1}{b-a} \\frac{b^2-a^2}{2} = \\frac{1}{b-a} \\frac{(b-a)(b+a)}{2} = \\frac{a+b}{2} \\,. \\] To find the variance, we work with the shortcut formula: \\(V[X] = E[X^2] - (E[X])^2\\). We know \\(E[X]\\) already; as for \\(E[X^2]\\), we utilize the Law of the Unconscious Statistician: \\[\\begin{align*} E[X^2] = \\int_a^b x^2 f_X(x) dx &amp;= \\int_a^b \\frac{x^2}{b-a} dx \\\\ &amp;= \\frac{1}{b-a} \\left. \\frac{x^3}{3} \\right|_a^b \\\\ &amp;= \\frac{b^3-a^3}{3(b-a)} \\\\ &amp;= \\frac{(b-a)(a^2+ab+b^2)}{3(b-a)} = \\frac{1}{3}\\left(a^2 + ab + b^2\\right) \\,. \\end{align*}\\] Thus \\[\\begin{align*} V[X] &amp;= \\frac{1}{3}\\left(a^2 + ab + b^2\\right) - \\left(\\frac{a+b}{2}\\right)^2 \\\\ &amp;= \\frac{1}{3}\\left(a^2 + ab + b^2\\right) - \\frac{1}{4}\\left(a^2+2ab+b^2\\right) \\\\ &amp;= \\frac{1}{12}\\left(4a^2 + 4ab + 4b^2 - 3a^2 - 6ab - 3b^2\\right) \\\\ &amp;= \\frac{1}{12}\\left(a^2 - 2ab + b^2 \\right) \\\\ &amp;= \\frac{(a-b)^2}{12} \\,. \\end{align*}\\] 5.1.2 Coding R-Style Functions for the Discrete Uniform Distribution There are four standard functions associated with any distribution: the one prefaced by d that returns the output of the probability mass function or probability density function, given a coordinate \\(x\\); the one prefaced by p that returns the output of the cumulative distribution function, given \\(x\\); the one prefaced q that returns the output of the inverse cdf, given a quantile \\(q \\in [0,1]\\); and the random sampler, a function prefaced by r. For the discrete uniform distribution, one can code the probability mass function as follows: ddiscunif &lt;- function(x,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) if ( x %in% y ) return(1/length(y)) return(0) } ddiscunif(4,min=1,max=6) # assume a fair six-sided die ## [1] 0.1666667 As for the cumulative distribution function: pdiscunif &lt;- function(x,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) w &lt;- which(y&lt;=x) if ( length(w) == 0 ) return(0) return(length(w)/length(y)) } pdiscunif(4,min=1,max=6) ## [1] 0.6666667 The inverse cdf implements the generalized inverse algorithm: qdiscunif &lt;- function(q,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) if ( q == 0 ) return(min(y)) if ( q == 1 ) return(max(y)) cdf &lt;- (1:length(y))/length(y) w &lt;- which(cdf&gt;=q) if ( length(w) == 0 ) return(max(y)) return(y[min(w)]) } qdiscunif(0.55,min=1,max=6) ## [1] 4 And last, the random data generator: rdiscunif &lt;- function(n,min=0,max=1,step=1) { y &lt;- seq(min,max,by=step) s &lt;- sample(length(y),n,replace=TRUE) return(y[s]) } set.seed(235) # set to ensure consistent output rdiscunif(10,min=1,max=6) ## [1] 6 5 5 6 2 1 5 1 3 6 5.2 Linear Functions of Uniform Random Variables Let’s assume that we are given \\(n\\) iid Uniform random variables: \\(X_1,X_2,\\ldots,X_n \\sim\\) Uniform(\\(a,b\\)). What is the distribution of the sum \\(Y = \\sum_{i=1}^n X_i\\)? Recall: the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by \\(E[e^{tX}]\\). If \\(Y = \\sum_{i=1}^n a_iX_i\\), then \\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \\cdots m_{X_n}(a_nt)\\); if we can identify \\(m_Y(t)\\) os the mgf for a known family of distributions, then we can immediately identify the distribution of \\(Y\\) and the parameters of that distribution. The mgf for the uniform distribution is \\[\\begin{align*} m_X(t) = E[e^{tX}] &amp;= \\int_a^b \\frac{e^{tx}}{b-a} dx \\\\ &amp;= \\frac{1}{b-a} \\left. \\frac{1}{t}e^{tx} \\right|_a^b \\\\ &amp;= \\frac{e^{tb}-e^{ta}}{t(b-a)} \\,. \\end{align*}\\] The mgf for the sum \\(Y = \\sum_{i=1}^n X_i\\) is thus \\[ m_Y(t) = \\prod_{i=1}^n m_{X_i}(t) = \\left( \\frac{e^{tb}-e^{ta}}{t(b-a)} \\right)^n \\,. \\] This expression does not simplify such that we recognize the distribution of \\(Y\\). If \\(a = 0\\) and \\(b = 1\\), it turns out that the mgf does take on the form of that for an Irwin-Hall distribution. An Irwin-Hall random variable converges in distribution to a normal random variable as \\(n \\rightarrow \\infty\\). We are placed in a similar situation if we look at the sample mean \\(\\bar{X} = Y/n\\): \\[ m_{\\bar{X}}(t) = \\prod_{i=1}^n m_{X_i}\\left(\\frac{t}{n}\\right) = \\left( \\frac{n(e^{tb/n}-e^{ta/n})}{t(b-a)} \\right)^n \\,. \\] If \\(a = 0\\) and \\(b = 1\\), \\(\\bar{X}\\) is sampled from the Bates distribution. A Bates random variable converges in distribution to a normal random variable as \\(n \\rightarrow \\infty\\). For all other combinations of \\(a\\) and \\(b\\), we cannot write down a specific functional form for the sampling distribution of \\(\\bar{X}\\) and thus we would have to perform simulations to test hypotheses, etc. (However, we note that because statistical inference for a uniform distribution involves determining the lower and/or upper bounds, we can utilize order statistics for inference instead of \\(\\bar{X}\\). See the next section below.) 5.2.1 The Moment-Generating Function for a Discrete Uniform Distribution The mgf for a discrete uniform random variable is \\[ E[e^{tX}] = \\sum_{x=a}^b e^{tx} p_X(x) = \\frac{1}{n} \\sum_{x=a}^b e^{tx} \\,. \\] We cannot say anything further without making an assumption. If we say that \\(x \\in [a,a+1,\\ldots,b-1,b]\\), i.e., that there are integer steps between the probability masses, then \\[ E[e^{tX}] = \\frac{1}{n} \\sum_{x=a}^b e^{tx} = \\frac{1}{n}e^{ta} \\left( 1 + e^{t(a+1)} + \\cdots + e^{t(b-a)} \\right) \\,. \\] If \\(t\\) is negative, then we can make use of a geometric sum: \\[ 1 + e^t + \\cdots = \\frac{1}{1-e^t} = \\underbrace{1 + \\cdots + e^{t(b-a)}}_{} + \\underbrace{e^{t(b-a+1)} + \\cdots}_{} \\,, \\] where the first underbraced quantity is what appears above in the expected value. Thus we can rearrange terms and write \\[\\begin{align*} 1 + e^{t(a+1)} + \\cdots + e^{t(b-a)} &amp;= \\frac{1}{1-e^t} - \\left( e^{t(b-a+1)} + \\cdots \\right) \\\\ &amp;= \\frac{1}{1-e^t} - e^{t(b-a+1)}\\left(1 + e^t + \\cdots\\right) \\\\ &amp;= \\frac{1}{1-e^t} - \\frac{e^{t(b-a+1)}}{1-e^t} = \\frac{1-e^{t(b-a+1)}}{1-e^t} \\,. \\end{align*}\\] Putting everything together, we find that \\[ m_X(t) = \\frac{1}{n}e^{ta} \\frac{1-e^{t(b-a+1)}}{1-e^t} = \\frac{e^{ta}-e^t(b+1)}{n(1-e^t)} \\,. \\] This is the usual form of the mgf presented for the discrete uniform distribution, but again, this is only valid if the masses are separated by one unit: \\(x \\in [a,a+1,\\ldots,b-1,b]\\). 5.3 Sufficient Statistics and the Minimum Variance Unbiased Estimator Recall: a sufficient statistic for a population parameter \\(\\theta\\) captures all information about \\(\\theta\\) contained in a data sample; no additional statistic will provide more information about \\(\\theta\\). Sufficient statistics are not unique: functions of sufficient statistics are themselves sufficient statistics. Before we discuss sufficient statistics in the context of the uniform distribution, it is useful to (re-)introduce the indicator function. This function, mentioned briefly in Chapter 1, takes on the value 1 if a specified condition is met and 0 otherwise. For instance, \\[ \\mathbb{I}_{x_i \\in [0,1]} = \\left\\{ \\begin{array}{cl} 1 &amp; x_i \\in [0,1] \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\,. \\] One use for the indicator function is to, well, indicate the domain of a pmf or pdf. For instance, we can write \\[ f_X(x) = \\left\\{ \\begin{array}{ll} e^{-x} &amp; x \\geq 0 \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right. \\] to express that the exponential distribution with rate \\(\\beta = 1\\) is defined within the domain \\(x \\in [0,\\infty)\\), or, equivalently, we can write \\[ f_X(x) = e^{-x} \\mathbb{I}_{x \\in [0,\\infty)} \\,. \\] The latter form expresses the same information in a more condensed fashion. So…what do indicator functions have to do with uniform distributions? Let’s suppose we sample \\(n\\) iid data \\(\\{X_1,\\ldots,X_n\\}\\) from a uniform distribution with lower bound 0 and upper bound \\(\\theta\\), and our goal is to define a sufficient statistic for \\(\\theta\\). Let’s work with the factorization criterion: \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = g(\\mathbf{x},\\theta) \\cdot h(\\mathbf{x}) \\,. \\] The likelihood is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\prod_{i=1}^n \\frac{1}{\\theta} = \\frac{1}{\\theta^n} \\,. \\] OK…no…wait, there are no data in this expression, so we cannot define a sufficient statistic. The way around this is to re-express the pdf as \\[ f_X(x) = \\frac{1}{\\theta} \\mathbb{I}_{x \\in [0,\\theta]} \\] and to re-write the likelihood as \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i \\vert \\theta) = \\frac{1}{\\theta^n} \\prod_{i=1}^n \\mathbb{I}_{x \\in [0,\\theta]} \\,. \\] A product of indicator functions will equal 1 if and only if all data lie in the domain \\(x \\in [0,\\theta]\\). This is equivalent to saying that \\(\\theta \\geq X_{(n)}\\), the order statistic representing the maximum observed datum. Thus \\(X_{(n)}\\) is a sufficient statistic for \\(\\theta\\): we know \\(\\theta\\) is greater than this statistic’s value, and none of the data aside from \\(X_{(n)}\\) provide any additional information about \\(\\theta\\). The upshot: when the parameter \\(\\theta\\) dictates (at least in part) the domain of a distribution, the sufficient statistics for \\(\\theta\\) will be functions of an order statistic. When we first introduced the factorization criterion and sufficient statistics back in Chapter 3, we did it so that ultimately we could write down the minimum variance unbiased estimator (or MVUE). Recall: deriving the minimum variance unbiased estimator involves two steps: factorizing the likelihood function to uncover a sufficient statistic \\(U\\) (that we assume is both minimal and complete); and finding a function \\(h(U)\\) such that \\(E[h(U)] = \\theta\\). When \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim}\\) Uniform(\\(0,\\theta\\)), can we define an MVUE for \\(\\theta\\)? The answer is yes…but we have to recall how we define the pdf of \\(X_{(n)}\\) first. Recall: the maximum of \\(n\\) iid random variables sampled from a pdf \\(f_X(x)\\) has a sampling distribution given by \\[ f_{(n)}(x) = n f_X(x) [ F_X(x) ]^{n-1} \\,, \\] where \\(F_X(x)\\) is the associated cdf. For the Uniform(\\(0,\\theta\\)) distribution, \\[ f_X(x) = \\frac{1}{\\theta} ~~\\mbox{and}~~ F_X(x) = \\int_0^x f_Y(y) dy = \\int_0^x \\frac{1}{\\theta} dy = \\frac{x}{\\theta} \\,, \\] so \\[ f_{(n)}(x) = n \\frac{1}{\\theta} \\left[ \\frac{x}{\\theta} \\right]^{n-1} = n \\frac{x^{n-1}}{\\theta^n} \\,. \\] To find the MVUE, we first compute the expected value of the sufficient statistic \\(X_{(n)}\\): \\[ E[X_{(n)}] = \\int_0^\\theta x n \\frac{x^{n-1}}{\\theta^n} dx = \\left. \\frac{n}{(n+1)\\theta^n} x^{n+1} \\right|_0^\\theta = \\frac{n}{n+1} \\theta \\,, \\] and then rearrange terms: \\[ E\\left[\\frac{n+1}{n}X_{(n)}\\right] = \\theta \\,. \\] Thus \\[ \\hat{\\theta}_{MVUE} = \\frac{n+1}{n}X_{(n)} \\] is the MVUE for \\(\\theta\\). (Note that a similar calculation to this one can be used to determine, e.g., the MVUE for the \\(\\theta\\) when the data are sampled from a Uniform(\\(\\theta,b\\)) distribution.) 5.3.1 The Sufficient Statistic for the Domain Parameter of the Pareto Distribution The Pareto [puh-RAY-toh] distribution, also known in some quarters as the power-law, is \\[ f_X(x) = \\frac{\\alpha \\beta^\\alpha}{x^{\\alpha+1}} \\,, \\] where \\(\\alpha &gt; 0\\) is the shape parameter and \\(x \\in [\\beta,\\infty)\\), where \\(\\beta\\) is the scale (or location) parameter. Let’s assume \\(\\alpha\\) is known. A sufficient statistic for \\(\\beta\\), found via likelihood factorization, is \\[ \\mathcal{L}(\\beta \\vert \\mathbf{x}) = \\prod_{i=1}^n f_X(x_i) = \\underbrace{\\beta^{n\\alpha}}_{g(\\mathbf{x},\\beta)} \\cdot \\underbrace{\\frac{\\alpha^n}{(\\prod_{i=1}^n x_i)^{\\alpha+1}}}_{h(\\mathbf{x})} \\,. \\] Wait…again, as is the case for the uniform distribution, no data appear in the expression \\(g(\\cdot)\\). So we would go back and introduce an indicator function into the pdf; it should be clear that when we do so, \\(g(\\mathbf{x},\\beta)\\) changes to \\[ g(\\mathbf{x},\\beta) = \\beta^{n\\alpha} \\prod_{i=1}^n \\mathbb{I}_{x_i \\in [\\beta,\\infty)} \\] and thus that because all data have to be larger than \\(\\beta\\), the sufficient statistic is the minimum observed datum, \\(X_{(1)}\\). 5.3.2 MVUE Properties for Uniform Distribution Bounds The properties of estimators that we have examined thus far include the bias (are our estimates offset from the truth, on average?), the variance (over how large a range do our estimates vary?), etc. Let’s look at some of these properties here, assuming we sample \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution. Bias: the MVUE is by definition unbiased, since \\(E[\\frac{n+1}{n}X_{(n)}] = \\theta\\). Variance: the variance of \\(\\hat{\\theta}_{MVUE}\\) is \\[\\begin{align*} V[\\hat{\\theta}_{MVUE}] &amp;= E\\left[\\left(\\hat{\\theta}_{MVUE}\\right)^2\\right] - \\left( E\\left[ \\hat{\\theta}_{MVUE} \\right] \\right)^2 \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( E[X_{(n)}^2] - (E[X_{(n)}])^2 \\right) \\,, \\end{align*}\\] where \\[ E[X_{(n)}^2] = \\int_0^\\theta x^2 n \\frac{x^{n-1}}{\\theta^n} dx = \\left. \\frac{n}{(n+2)\\theta^n} x^{n+2} \\right|_0^\\theta = \\frac{n}{n+2} \\theta^2 \\,. \\] Thus \\[\\begin{align*} V[\\hat{\\theta}_{MVUE}] &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n}{n+2} \\theta^2 - \\frac{n^2}{(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n(n+1)^2 - n^2(n+2)}{(n+2)(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{(n+1)^2}{n^2} \\left( \\frac{n}{(n+2)(n+1)^2} \\theta^2 \\right) \\\\ &amp;= \\frac{1}{n(n+2)} \\theta^2 \\rightarrow \\frac{\\theta^2}{n^2} ~~\\mbox{as}~~ n \\rightarrow \\infty \\,. \\end{align*}\\] We observe that since the variance goes to zero as \\(n \\rightarrow \\infty\\), the MVUE is a consistent estimator… …but does the MVUE achieve the Cramer-Rao Lower Bound (CRLB), the theoretical lower bound on the variance of unbiased estimators? It turns out that not only does it achieve the lower bound (which one can show equals \\(\\theta^2/n\\)), but it even surpasses that bound! This seemingly worrisome result is actually fine, however, because the CRLB theorem is not applicable in situations where the domain of a pmf or pdf depends on \\(\\theta\\). 5.4 Maximum Likelihood Estimation Recall: the value of \\(\\theta\\) that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for \\(\\theta\\). The maximum is, thus far, found by taking the (partial) derivative of the (log-)likelihood function with respect to \\(\\theta\\), setting the result to zero, and solving for \\(\\theta\\). That solution is the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\). Also recall the invariance property of the MLE: if \\(\\hat{\\theta}_{MLE}\\) is the MLE for \\(\\theta\\), then \\(g(\\hat{\\theta}_{MLE})\\) is the MLE for \\(g(\\theta)\\). Now that we have recalled how maximum likelihood estimation works, we can state that this is not how the MLE is found for a domain-affecting parameter! (Hence the “thus far” in the recall statement above.) Let’s assume, for instance, that we sample \\(n\\) iid random variables from a Uniform(\\(0,\\theta\\)) distribution. As stated above (without the indicator function), the likelihood is \\[ \\mathcal{L}(\\theta \\vert \\mathbf{x}) = \\frac{1}{\\theta^n} \\,. \\] This means that the smaller \\(\\theta\\) is, the larger the likelihood will be. So how small can \\(\\theta\\) be? We can answer this intuitively: the domain \\([0,\\theta]\\) has to just encompass all the observed data, i.e., \\[ \\hat{\\theta}_{MLE} = X_{(n)} \\,. \\] If \\(\\theta\\) were smaller, \\(X_{(n)}\\) would lie outside the domain. It is fine for \\(\\theta\\) to be larger, since then all the data lie in the domain \\([0,\\theta]\\)…but the larger \\(\\theta\\) is, the smaller the likelihood. We plot an example likelihood function in Figure 5.2. We observe immediately that the usual MLE algorithm will not work here, as the likelihood function is discontinuous at \\(\\theta = X_{(n)}\\) and thus we cannot compute a first derivative. All we can do is, e.g., plot the likelihood and identify the MLE as that value for which the likelihood is maximized (or identify the value intuitively as we do above). Figure 5.2: The likelihood function given \\(n=5\\) data drawn from a Uniform(0,\\(\\theta\\)) distribution, with \\(\\theta = 1\\). As \\(\\theta\\) cannot be smaller than the maximum observed value, the likelihood is zero for \\(\\theta \\leq X_{(n)}\\); it is \\(1/\\theta^n\\) for \\(\\theta &gt; X_{(n)}\\). The maximum likelihood estimate is thus \\(X_{(n)}\\) itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters. 5.4.1 The MLE for the Domain Parameter of the Pareto Distribution In the last section above, we introduce the Pareto distribution, \\[ f_X(x) = \\frac{\\alpha\\beta^\\alpha}{x^{\\alpha+1}} \\,, \\] where \\(\\alpha &gt; 0\\) and \\(x \\in [\\beta,\\infty)\\), and we show that the sufficient statistic for \\(\\beta\\) (with \\(\\alpha\\) fixed) is the smallest observed datum, \\(X_{(1)}\\). Because \\(\\beta\\) is a parameter that dictates the domain, we find the MLE not via differentiation but rather by identifying that the likelihood is maximized when \\(\\beta\\) is exactly equal to \\(X_{(1)}\\), i.e., \\(\\hat{\\beta}_{MLE} = X_{(1)}\\). See Figure 5.3. Figure 5.3: The likelihood function given \\(n=5\\) data drawn from a Pareto(1,\\(\\beta\\)) distribution, with \\(\\beta = 1\\). As \\(\\beta\\) cannot be larger than the minimum observed value, the likelihood is zero for \\(\\beta \\geq X_{(1)}\\); it is \\(\\theta^n(1/\\prod_{i=1}^n x_i)^2\\) for \\(\\beta &lt; X_{(n)}\\). The maximum likelihood estimate is thus \\(X_{(1)}\\) itself; as the likelihood function is discontinuous at this point, the MLE cannot be found via the usual algorithm applied in previous chapters. 5.4.2 MLE Properties for Uniform Distribution Bounds In this example, we will mimic what we do above when discussing the properties of the MVUE, and look at estimator bias and variance, etc., assuming that \\(\\{X_1,\\ldots,X_n\\} \\stackrel{iid}{\\sim}\\) Uniform(\\(0,\\theta\\)). Bias: we know, from our derivation of the MVUE, that \\[ E[\\hat{\\theta}_{MLE}] = E[X_{(n)}] = \\frac{n}{n+1}\\theta \\,, \\] and thus the estimator bias is \\[ B[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}] - \\theta = \\frac{n}{n+1}\\theta - \\theta = \\frac{1}{n+1}\\theta \\,. \\] As we expect for the MLE, the estimator is at least asymptotically unbiased, as the bias goes to zero as the sample size \\(n \\rightarrow \\infty\\). Variance: the variance of the MLE is \\[ V[\\hat{\\theta}_{MLE}] = E[\\hat{\\theta}_{MLE}^2] - \\left(E[\\hat{\\theta}_{MLE}\\right)^2 = E[X_{(n)}^2] - (E[X_{(n)}])^2 \\,. \\] We derived both \\(E[X_{(n)}]\\) and \\(E[X_{(n)}^2]\\) above when discussing the MVUE, so we can write down immediately that \\[ V[\\hat{\\theta}_{MLE}] = \\frac{n}{n+2}\\theta^2 - \\left( \\frac{n}{n+1}\\theta\\right)^2 = \\frac{n}{(n+2)(n+1)^2}\\theta^2 \\rightarrow \\frac{\\theta^2}{n^2} ~~\\mbox{as}~~ n \\rightarrow \\infty\\,. \\] We observe that because the variance goes to zero as \\(n \\rightarrow \\infty\\), the MLE is a consistent estimator. The variance of the MLE is similar to, but not exactly the same as, the variance for the MVUE, although the variances converge to the same value in asymtopia. 5.5 Confidence Intervals Recall: a confidence interval is a random interval \\([\\hat{\\theta}_L,\\hat{\\theta}_U]\\) that overlaps (or covers) the true value \\(\\theta\\) with probability \\[ P\\left( \\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_U \\right) = 1 - \\alpha \\,, \\] where \\(1 - \\alpha\\) is the confidence coefficient. We determine \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by, e.g., solving for the root \\(\\theta_q\\) in each of the following equations: \\[\\begin{align*} F_Y(y_{\\rm obs} \\vert \\theta_{\\alpha/2}) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{\\rm obs} \\vert \\theta_{1-\\alpha/2}) - \\left(1-\\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The construction of confidence intervals thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) to \\(\\hat{\\theta}_L\\) and \\(\\hat{\\theta}_H\\) by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 14 of Chapter 1.) Here, we will recall something else about confidence interval construction: we can choose the statistic that we use. This is important, because as we have seen, if we are given \\(n\\) iid data drawn from, e.g., a Uniform(\\(0,\\theta\\)) distribution, we do not know the distribution of \\(\\bar{X}\\) (unless \\(\\theta=1\\))…while we do know the distribution of the order statistic \\(Y = X_{(n)}\\). We derive it above: \\[ f_{(n)}(x) = n \\frac{x^{n-1}}{\\theta^n} \\,. \\] The cdf is thus \\(F_Y(y) = F_{(n)}(x) = (x/\\theta)^n\\). We work with this expression in an example below, noting that \\(E[Y] = (n-1)\\theta/n\\), i.e., that \\(E[Y]\\) increases with \\(\\theta\\). We conclude our coverage (so to speak) of confidence intervals by going back to the notion of the confidence coefficient \\(1 - \\alpha\\). In a footnote in Chapter 1, we make the point that technically, the confidence coefficient is the infimum, or minimum value, of the probability \\(P(\\hat{\\theta}_L \\leq \\theta \\leq \\hat{\\theta}_H)\\). What does this actually mean? Let’s suppose that we have sampled \\(n\\) iid data from a normal distribution, and that we are going to construct a confidence interval of the form \\[ P(S^2 - a \\leq \\sigma^2 \\leq S^2 + a) \\] for the population variance \\(\\sigma^2\\). We can do this, right? Let’s see… \\[\\begin{align*} P(S^2 - a \\leq \\sigma^2 \\leq S^2 + a) &amp;= P(-a \\leq S^2-\\sigma^2 \\leq a) \\\\ &amp;= P\\left(1 - \\frac{a}{\\sigma^2} \\leq \\frac{S^2}{\\sigma^2} \\leq 1 + \\frac{a}{\\sigma^2}\\right) \\\\ &amp;= P\\left( (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\leq \\frac{(n-1)S^2}{\\sigma^2} \\leq (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right)\\\\ &amp;= F_{W(n-1)}\\left( (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right) - F_{W(n-1)}\\left( (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\right) \\,. \\end{align*}\\] The key to interpreting the last line above is that \\(\\sigma^2\\) is unknown (otherwise, why would we be constructing a confidence interval for it in the first place?), and thus can take on any positive value. What if \\(\\sigma^2\\) is very large? \\[ \\lim_{\\sigma^2 \\to \\infty} F_{W(n-1)}\\left[ (n-1)\\left(1 + \\frac{a}{\\sigma^2}\\right) \\right] - F_{W(n-1)}\\left[ (n-1)\\left(1 - \\frac{a}{\\sigma^2}\\right) \\right] = F_{W(n-1)}(n-1) - F_{W(n-1)}(n-1) = 0 \\,. \\] Thus the confidence coefficient for the interval \\(S^2 - a \\leq \\sigma^2 \\leq S^2 + a\\) is \\(1 - \\alpha = 0\\) (or, we have that \\(\\alpha = 1\\)). The upshot: one cannot write down just any interval and assume that it is a valid one! 5.5.1 Interval Estimation Given Order Statistics We assume that we are given \\(n\\) iid data drawn from a Uniform(\\(0,\\theta\\)) distribution. Above, we note that the cdf for the maximum observed datum, \\(X_{(n)}\\), is \\(F_{(n)}(x) = (x/\\theta)^n\\). To find the lower and upper bounds on \\(\\theta\\), respectively, we solve for \\(\\theta\\) in the expressions \\[\\begin{align*} \\left(\\frac{X_{(n)}}{\\theta_{1-\\alpha/2}}\\right)^n &amp;= 1 - \\frac{\\alpha}{2} ~~ \\mbox{(lower)} \\\\ \\left(\\frac{X_{(n)}}{\\theta_{\\alpha/2}}\\right)^n &amp;= \\frac{\\alpha}{2} ~~ \\mbox{(upper)} \\,. \\end{align*}\\] (We assume that we are constructing a two-sided interval. Similar expressions would yield the lower or upper bound.) We thus find that \\[ \\hat{\\theta}_L = \\theta_{1-\\alpha/2} = \\frac{X_{(n)}}{(1-\\alpha/2)^{1/n}} ~~\\mbox{and}~~ \\hat{\\theta}_H = \\theta_{\\alpha/2} = \\frac{X_{(n)}}{(\\alpha/2)^{1/n}} \\,. \\] 5.5.2 Confidence Coefficient for a Uniform-Based Interval Estimator In the example above, we show that the interval estimate with confidence coefficient \\(1-\\alpha\\) for the uniform upper bound \\(\\theta\\) has the form \\([aX_{(n)},bX_{(n)}]\\). Can we also define an appropriate interval estimator if, for instance, it has the form \\([X_{(n)} + a,X_{(n)} + b]\\)? The short answer is no…because the confidence coefficient will be zero! To see why, let’s expand out and solve: \\[\\begin{align*} P(X_{(n)} + a \\leq \\theta \\leq X_{(n)} + b) &amp;= P(\\theta - b \\leq X_{(n)} \\leq \\theta - a)\\\\ &amp;= P(X_{(n)} \\leq \\theta - a) - P(X_{(n)} \\leq \\theta - b)\\\\ &amp;= F_{(n)}(\\theta-a) - F_{(n)}(\\theta-b)\\\\ &amp;= \\left(\\frac{\\theta-a}{\\theta}\\right)^2 - \\left(\\frac{\\theta-b}{\\theta}\\right)^2\\\\ &amp;= \\left(1-\\frac{a}{\\theta}\\right)^2 - \\left(1-\\frac{b}{\\theta}\\right)^2 \\,. \\end{align*}\\] The confidence coefficient is the infimum (or minimum value) that this expression can take on. For an interval of the form \\([aX_{(n)},bX_{(n)}]\\), \\(\\theta\\) does not appear, and thus the infimum is a constant. Here, however, \\[ \\lim_{\\theta \\to \\infty} P(X_{(n)} + a \\leq \\theta \\leq X_{(n)} + b) = 0 \\,, \\] and thus the confidence coefficient is (i.e., the proportion of computed intervals that overlap the true value \\(\\theta\\) goes to zero). Thus an interval estimator of the form \\([aX_{(n)},bX_{(n)}]\\) is a better one than one of the form \\([X_{(n)} + a,X_{(n)} + b]\\). 5.6 Hypothesis Testing Recall: a hypothesis test is a framework to make an inference about the value of a population parameter \\(\\theta\\). The null hypothesis \\(H_o\\) is that \\(\\theta = \\theta_o\\), while possible alternatives \\(H_a\\) are \\(\\theta \\neq \\theta_o\\) (two-sided test), \\(\\theta &gt; \\theta_o\\) (upper-tail test), and \\(\\theta &lt; \\theta_o\\) (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic \\(y_{\\rm obs}\\) falls outside the bounds given by \\(y_{\\alpha/2}\\) and \\(y_{1-\\alpha/2}\\), which are solutions to the equations \\[\\begin{align*} F_Y(y_{\\alpha/2} \\vert \\theta_o) - \\frac{\\alpha}{2} &amp;= 0 \\\\ F_Y(y_{1-\\alpha/2} \\vert \\theta_o) - \\left(1 - \\frac{\\alpha}{2}\\right) &amp;= 0 \\,. \\end{align*}\\] The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic \\(Y\\). One maps, e.g., \\(y_{\\alpha/2}\\) to either the lower or upper rejection region boundary by taking into account how the expected value \\(E[Y]\\) varies with the parameter \\(\\theta\\). (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven. One aspect of hypothesis testing that we reiterate here is that the hypotheses are always to be established, along with the level of the test, before we collect data. This should be obvious\\(-\\)looking at the data prior to establishing hypotheses and test levels can (and often will) lead to bias\\(-\\)so why are we reiterating this now? We are making this point because when we perform tests involving domain-specifying parameters, there are some quirks that we observe when we establish rejection regions. Let’s look at an example: we sample \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution, and we use these data to test the hypothesis \\(H_o : \\theta = \\theta_o\\) versus the hypothesis \\(H_a : \\theta \\neq \\theta_o\\) at level \\(\\alpha\\). We know that the sufficient statistic upon which we will build our test is \\(X_{(n)}\\). Given this, what can we say about the rejection region right away? We can say that we will reject the null if \\(X_{(n)} &gt; \\theta_o\\). This is a “trivial” statement, as no mathematics is involved. Initially, this might seem off-putting: we would, of course, never set \\(\\theta_o\\) to be less than the maximum datum, would we? (That would be silly.) But…that’s an incorrect way of looking at this situation, since that implies that we looked at the data first and only established the hypotheses afterwards. If we do things in the proper order, then it can be very much the case that the maximum datum will exceed \\(\\theta_o\\). If we observe this, then life is easy: we simply reject the null and move on. But…how do we establish the part of the rejection region involving values of \\(X_{(n)}\\) that are less than \\(\\theta_o\\)? That seems simple enough: we are performing a two-tail test, so we set the cdf for \\(X_{(n)}\\) to \\(\\alpha/2\\) and invert and…oh, but there’s a problem. If the null is actually correct, then the power of the test for \\(\\theta = \\theta_o\\) would be \\(\\alpha/2\\) and not \\(\\alpha\\). (If the null is correct, it is impossible to observe \\(X_{(n)} &gt; \\theta_o\\), so all rejections would happen “to the left” of \\(\\theta_o\\)!) So this is quirk number two: we have to be careful about whether we use, e.g., \\(\\alpha/2\\) or \\(\\alpha\\) we finding the rejection region boundary. If in doubt, think about the test power and how we can reject the null if \\(\\theta = \\theta_o\\), and make sure the power is actually \\(\\alpha\\). The last hypothesis-test-related topic that we will touch upon is the concept of multiple comparisons. This is a somewhat opaque term that denotes the situation in which we perform many hypothesis tests simultaneously and need to correct for the fact that if the null is correct in all cases, it becomes more and more likely that we will observe (multiple) instances in which we decide to reject the null. We can illustrate this using the binomial distribution: if we collect \\(k\\) sets of data (e.g., \\(k\\) separate sets of \\(n\\) iid data sampled from a Uniform(0,\\(\\theta\\)) distribution), and perform level-\\(\\alpha\\) hypothesis tests for each, then the number of tests results in which we reject the null is \\[ X \\sim \\mbox{Binom}(k,\\alpha) \\,, \\] The expected value of \\(X\\) is \\(E[X] = k\\alpha\\), which increases with \\(k\\). The family-wise error rate, or FWER, is the probability that at least one test will result in a rejection when the null is correct: \\[ FWER = P(X &gt; 0) = 1 - P(X = 0) = 1 - \\binom{k}{x} \\alpha^0 (1-\\alpha)^k = 1 - (1-\\alpha)^k \\,. \\] For instance, if \\(k = 10\\) and \\(\\alpha = 0.05\\), the family-wise error rate is 0.401: for every 10 tests we perform, the probability of erroneously rejecting one or more null hypotheses (i.e., detecting one or more false positives) is about 40 percent. This increase in the FWER with \\(k\\) is not good, and is well-known to be associated with a commonly seen data analysis issue dubbed data dredging or p-hacking. \\(p\\)-hacking greatly increases the probability that researchers will make incorrect claims about what their data say, and worse yet, that they will publish papers purporting these claims. To mitigate this issue, we can attempt to change the test level for individual tests such that the overall FWER is reduced to \\(\\alpha\\). There are many procedures for how we might go about changing the test level for individual tests, but the most commonly used one is the Bonferroni correction: \\[ \\alpha \\rightarrow \\frac{\\alpha}{k} \\,. \\] What is the FWER given this correction? Let’s assume the null is correct for all \\(k\\) tests. Then \\[ FWER = P\\left( p_1 \\leq \\frac{\\alpha}{k} \\cup \\cdots \\cup p_k \\leq \\frac{\\alpha}{k} \\right) = \\sum_{i=1}^k P\\left( p_i \\leq \\frac{\\alpha}{k}\\right) = \\sum_{i=1}^k \\frac{\\alpha}{k} = \\alpha \\,. \\] This works! Except…what happens if actually only \\(k&#39;\\) out of the \\(k\\) are actually true? The FWER becomes \\(k&#39; \\alpha / k \\leq \\alpha\\). Thus when there are incorrect nulls sprinkled into the mix, the FWER is too low…which means that the Bonferroni correction is unduly conservative. Using it will lead to us possibly not detecting false nulls that we should have detected! We illustrate this issue in an example below. An alternative to the Bonferroni correction and related procedures is to not focus upon the FWER, but to attempt to limit the false discovery rate, or FDR, instead. The simplest and most often used FDR-based procedure is the one of Benjamini and Hochberg (1995): compute all \\(k\\) \\(p\\)-values; sort the \\(p\\)-values into ascending order: \\(p_{(1)},\\ldots,p_{(k)}\\); determine the largest value \\(k&#39;\\) such that \\(p_{(k&#39;)} \\leq k&#39; \\alpha / k\\); and reject the null for all tests that map to \\(p_{(1)},\\ldots,p_{(k&#39;)}\\). In an example below, we illustrate the use of the BH procedure. To be clear: \\(\\alpha\\) here represents the proportion of rejected null hypotheses that are actually correct. (“We reject the null 20 times. Assuming \\(\\alpha = 0.05\\), then we expect that we were right to reject the null 19 times, and that we’d be mistaken once.”) This is different from the FWER setting, where \\(\\alpha\\) represents the probability of erroneously rejecting one or more null hypotheses. (“We perform 100 independent tests. Assuming \\(\\alpha = 0.05\\), we expect to erroneously reject the null five percent of the time when the null is correct…but we can say nothing about how often we correctly reject the null.”) We can further illustrate this point with the following table: Null Correct Null False Total Null Rejected V S R Fail to Reject U T k-R Total k’ k-k’ k The only observable random variable here is \\(R\\), the total number of rejected null hypotheses. In the FDR procedure, we focus on the first row. We know that \\[ E[V] = \\alpha k&#39; \\leq \\alpha k \\] and we know that \\(V+S \\leq k\\), so \\[ E\\left[\\frac{V}{V+S}\\right] = E\\left[\\frac{V}{R}\\right] \\leq \\frac{\\alpha k&#39;}{k} \\leq \\alpha \\,. \\] The FWER procedure, on the other hand, focuses on the first column, with \\[ E\\left[\\frac{V}{V+U}\\right] = \\frac{\\alpha k&#39;}{k&#39;} = \\alpha \\,. \\] 5.6.1 The Power Curve for Testing the Uniform Distribution Upper Bound Assume, as we do above, that we have sampled \\(n\\) iid data from a Uniform(\\(0,\\theta\\)) distribution, and that we will use these data to test the hypotheses \\[ H_o: \\theta = \\theta_o ~~\\mbox{versus}~~ H_a: \\theta \\neq \\theta_o \\,. \\] The sufficient statistic is the maximum datum \\(X_{(n)}\\). As stated above, we know that we will reject the null when \\(X_{(n)} &gt; \\theta_o\\); that’s a “trivial” statement. As for the rejection region boundary when \\(X_{(n)} \\leq \\theta_o\\): we know that the cdf for \\(X_{(n)}\\) is \\(F_{(n)}(x) = (x/\\theta)^n\\), so the lower boundary is \\[ \\left(\\frac{x_{\\alpha/2}}{\\theta_o}\\right)^n = \\frac{\\alpha}{2} ~~~ \\Rightarrow ~~~ \\ldots \\,. \\] Except, this is wrong: what would be the power if \\(\\theta = \\theta_o\\)? It would be \\(\\alpha/2\\) and not \\(\\alpha\\). So despite the fact that we are carrying out a two-tail test, all the \\(\\alpha\\) “goes to the left” of \\(\\theta_o\\) (because it is impossible to reject “to the right”: if the null is correct, \\(X_{(n)} &gt; \\theta_o\\) is impossible. So we have that \\[ \\left(\\frac{x_{\\alpha}}{\\theta_o}\\right)^n = \\alpha \\] and thus that \\[ x_{\\alpha} = \\theta_o \\alpha^{1/n} \\,. \\] If \\(X_{(n)} \\leq x_{\\alpha}\\), we reject the null. Full stop. The power of this test is \\[\\begin{align*} P(\\mbox{reject}~\\mbox{null} \\vert \\theta) &amp;= P(X_{(n)} \\leq x_\\alpha \\cup X_{(n)} &gt; \\theta_o \\vert \\theta) \\\\ &amp;= F_{(n)}(x_\\alpha \\vert \\theta) + [1 - F_{(n)}(\\theta_o \\vert \\theta)] \\\\ &amp;= \\left\\{ \\begin{array}{rl} 1 &amp; \\theta \\leq x_\\alpha \\\\ \\left(\\frac{x_\\alpha}{\\theta}\\right)^n &amp; x_\\alpha &lt; \\theta \\leq \\theta_o \\\\ 1 + \\left(\\frac{x_\\alpha}{\\theta}\\right)^n - \\left(\\frac{\\theta_o}{\\theta}\\right)^n &amp; \\theta &gt; \\theta_o \\end{array} \\right. \\,. \\end{align*}\\] For the first condition above: if \\(\\theta &lt; x_\\alpha\\), then \\(X_{(n)} &lt; x_\\alpha\\), so every test will result in a rejection, and the power is thus 1. We plot out the power curve for \\(\\theta_o = 1\\) and \\(n = 10\\) in Figure 5.4. Figure 5.4: The power curve for the test of \\(H_o : \\theta = \\theta_o = 1\\) versus \\(H_a : \\theta \\neq \\theta_o\\). The curve displays three discrete segments whose functional forms are given in the body of the text, and it achieves its minimum value, \\(\\alpha\\), at \\(\\theta = 1\\). 5.6.2 An Illustration of Multiple Comparisons When Testing for the Normal Mean In the code chunk below, we generate \\(k = 100\\) independent datasets of size \\(n = 40\\); for \\(k&#39; = 80\\) datasets, \\(\\mu = 0\\), and for the remainder, \\(\\mu = 0.5\\). For simplicity, we assume \\(\\sigma^2\\) is known and is equal to one. For each dataset, we test the hypotheses \\(H_o : \\mu = 0\\) versus \\(H_a : \\mu &gt; 0\\). set.seed(101) n &lt;- 40 k &lt;- 100 k.p &lt;- 80 mu &lt;- c(rep(0,k.p),rep(0.5,k-k.p)) mu.o &lt;- 0 sigma2 &lt;- 1 p &lt;- rep(NA,k) for ( ii in 1:k ) { X &lt;- rnorm(n,mean=mu[ii],sd=sigma2) p[ii] &lt;- 1 - pnorm(mean(X),mean=mu.o,sd=sqrt(sigma2/n)) } Below, we try two separate corrections for multiple comparisons: the Bonferroni correction (controlling FWER), and the Benjamini-Hochberg procedure (controlling FDR). alpha &lt;- 0.05 cat(&quot;The number of rejected null hypotheses for Bonferroni: &quot;, sum(p &lt; alpha/k),&quot;\\n&quot;) ## The number of rejected null hypotheses for Bonferroni: 9 w &lt;- which(p &lt; alpha/k) cat(&quot;The number of falsely rejected null hypotheses is: &quot;,sum(w&lt;=k.p),&quot;\\n&quot;) ## The number of falsely rejected null hypotheses is: 0 p.sort &lt;- sort(p) cat(&quot;The number of rejected null hypotheses for FDR: &quot;, sum(p.sort &lt; (1:k)*alpha/k),&quot;\\n&quot;) ## The number of rejected null hypotheses for FDR: 17 p.rej &lt;- p.sort[p.sort&lt;(1:k)*alpha/k] w &lt;- p %in% p.rej w &lt;- which(w==TRUE) cat(&quot;The number of falsely rejected null hypotheses is: &quot;,sum(w&lt;=k.p),&quot;\\n&quot;) ## The number of falsely rejected null hypotheses is: 0 With the Bonferroni correction, we reject nine null hypotheses (with the guarantee that there is, on average, a five percent chance that we erroneously reject one or more of the true nulls…here, we reject no correct null hypotheses. See Figure 5.5. With the BH procedure, we reject 17 null hypotheses (with the guarantee that on average, five percent of these 17 [meaning, effectively, 1] is an erroneous rejection…here, we reject no correct null hypotheses). Figure 5.5: An illustration of the difference between the Bonferroni correction and the Benjamini-Hochberg procedure. The blue dots represent sorted \\(p\\)-values resulting from a simulation in which 80 of 100 null hypotheses are correct (so that a perfect disambiguation between null and non-null hypotheses would result in 20 rejected nulls, with none falsely rejected. The Bonferroni correction shifts \\(\\alpha = 0.05\\) downwards to the green short-dashed line; 9 \\(p\\)-values lie below the line, so 9 (true) null hypotheses are rejected in all. The BH procedure looks for the number of \\(p\\)-values lying below the red dashed line; that number is 17 (with no false rejections). "],["multivariate-distributions.html", "6 Multivariate Distributions 6.1 Independence of Random Variables 6.2 Properties of Multivariate Distributions 6.3 Covariance and Correlation 6.4 Marginal and Conditional Distributions 6.5 Conditional Expected Value and Variance 6.6 The Multivariate Normal Distribution", " 6 Multivariate Distributions Thus far we have looked at univariate probability distributions, i.e., we have assumed that there is a single random variable \\(X\\) that maps the events in a sample space to the real-number line. The quantity represented by \\(X\\) might be, for instance, height or weight. In this chapter, we shift to the multivariate case, wherein we might have \\(p\\) random variables \\(\\mathbf{X} = \\{X_1,\\ldots,X_p\\}\\) representing, e.g., height and weight (if \\(p = 2\\)). When there are a set of \\(p\\) random variables, they are sampled from a joint \\(p\\)-dimensional probability mass or density function that encapsulates how the random variables are jointly distributed. Both joint pmfs \\(p_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p)\\) and joint pdfs \\(f_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p)\\) have similar properties as their univariate counterparts: they are non-negative (with \\(p_{X_1,\\ldots,X_p}(x_1,\\ldots,x_p) \\leq 1\\)) and they either sum or integrate to 1. They is nothing “special” about these functions; they are simply more mathematically complex and thus often less easy to work with. However, there are new concepts for us to cover that only arise in a multivariate context. Independence of Random Variables: do the sampled values for one random variable (e.g., heights) depend on the sampled values for the others (e.g., weights)? If not, the random variables are independent. (In this simple example, we expect that heights and weights to be very much dependent: on average, taller people are heavier.) Covariance and Correlation: these metrics build upon the concept of variance and indicate the amount of linear dependence between two random variables. Marginal Distributions: these show how a subset of the random variables is (jointly) distributed, without regard to the values taken on by the other random variables. For instance, if \\(f_{X_h,X_W}(x_h,x_w)\\) represents the joint distribution of heights and weights in a population, then the marginal distribution \\(f_{X_h}(x_h)\\) represents how heights are distributed, without regard to weight. Conditional Distributions: these show how a subset of the random variables is (jointly) distributed, conditional on the other random variables taking on specific values. For instance, the conditional distribution \\(f_{X_h \\vert X_w}(x_h \\vert x_w)\\) indicates the distribution of heights given a specific weight. Conditional Expectation and Variance: these metrics represent the mean and “width” of conditional distributions. Note that throughout this chapter, we will illustrate concepts using bivariate distributions, as adding mathematical complexity by increasing dimensionality provides no additional benefit in terms of conceptual understanding. The one exception to this is in the last section, where we discuss the multivariate normal distribution. 6.1 Independence of Random Variables In Chapter 1, we describe at a high level the concept of two or more random variables being independent of each other. In that chapter, we give the example of a bivariate probability density function, i.e., a function which outputs the probability density given two inputs, \\(x_1\\) and \\(x_2\\): \\(f_{X_1,X_2}(x_1,x_2)\\). To test for independence, we need only inspect the functional form of \\(f_{X_1,X_2}(x_1,x_2)\\) and its domain; \\(X_1\\) and \\(X_2\\) are independent if and only if the boundaries of the domain depend on either \\(x_1\\) or \\(x_2\\) but not both at the same time (i.e., the domain is “rectangular”); and \\(f_{X_1,X_2}(x_1,x_2)\\) can be factored into the product of functions that only depend on \\(x_1\\) and on \\(x_2\\), respectively: \\(f_{X_1}(x_1) \\times f_{X_2}(x_2)\\). (Furthermore, we state that if \\(f_{X_1}(x_1) = f_{X_2}(x_2)\\), then \\(X_1\\) and \\(X_2\\) are iid random variables.) If either condition given above does not hold, then we conclude that \\(X_1\\) and \\(X_2\\) are dependent random variables. 6.1.1 Determining Whether Two Random Variables are Independent Let \\(X_1\\) and \\(X_2\\) have the following joint pdf: \\[ f_{X_1,X_2}(x_1,x_2) = c(1-x_2) \\] for \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). For now, we are not concerned with the value of the constant \\(c\\). Are \\(X_1\\) and \\(X_2\\) independent random variables? The first thing to do is inspect the joint domain. This can be tricky, and it is often best to break the statement of the domain up into multiple “pieces.” Here, we can say that first, we know that \\(x_1 \\in [0,1]\\) and that \\(x_2 \\in [0,1]\\). This limits the domain to a box on the \\(x_1\\)-\\(x_2\\) plane. (See the axes and the red dashed lines in Figure 6.1.) Furthermore, we know that \\(x_1 \\leq x_2\\), or, equivalently, that \\(x_2 \\geq x_1 + 0\\), i.e., that \\(x_2\\) lies above a line with slope one and intercept zero. (See the orange short-dashed line in Figure 6.1.) Given that the domain is triangular, we can state unequivocally here that \\(X_1\\) and \\(X_2\\) are not independent random variables. In particular, we do not need to check whether \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) \\times f_{X_2}(x_2)\\). Figure 6.1: The domain of \\(f_{X_1,X_2}(x_1,x_2)\\), expressed mathematically as \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). The red dashed lines indicate that \\(0 \\leq x_1 \\leq 1\\) and \\(0 \\leq x_2 \\leq 1\\), and the orange short-dashed line indicates that \\(x_2 \\geq x_1\\). The blue triangle is the domain of the function. As a second example, let \\(X_1\\) and \\(X_2\\) have the following joint pdf: \\[ f_{X_1,X_2}(x_1,x_2) = ce^{-x_1x_2} \\,, \\] where \\(0 \\leq x_1 \\leq 1\\) and \\(0 \\leq x_2 \\leq 1\\). Again, we are not concerned about the value of \\(c\\), at least for now. Are \\(X_1\\) and \\(X_2\\) independent random variables? We can see by inspection that the domain is “rectangular,” specifically a square with vertices (0,0), (0,1), (1,0), and (1,1). So far, so good. Can we factor \\(f_{X_1,X_2}(x_1,x_2)\\) into two separate functions that depend only on \\(x_1\\) or only on \\(x_2\\)? The answer is no. (As a reminder, \\(e^{ab} \\neq e^a e^b\\)!) Hence we can state that \\(X_1\\) and \\(X_2\\) are not independent random variables. 6.2 Properties of Multivariate Distributions As stated in Chapter 1, a probability distribution is a mapping \\(P : \\Omega \\rightarrow \\mathbb{R}^n\\), where \\(\\Omega\\) is the sample space for an experiment. This mapping describes how probabilities are distributed across the values of a random variable. In the first five chapters, \\(n = 1\\), meaning that we focus on univariate distributions. In this chapter, however, \\(n &gt; 1\\), with the bulk of our discussion focusing upon the case \\(n = 2\\). Nothing changes, fundamentally, when we work with multivariate distributions: (a) they are non-negative; and (b) they sum or integrate to 1. Assuming \\(n = 2\\), we can write that joint pmf joint pdf \\(p_{X_1,X_2}(x_1,x_2) \\in [0,1]\\) \\(f_{X_1,X_2}(x_1,x_2) \\in [0,\\infty)\\) \\(\\sum_{x_1} \\sum_{x_2} p_{X_1,X_2}(x_1,x_2) = 1\\) \\(\\int_{x_1} \\int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = 1\\) We will reiterate that the joint pdf \\(f_{X_1,X_2}(x_1,x_2)\\) is not the probability of sampling the tuple \\((x_1,x_2)\\) but rather is a probability density; to determine probabilities, we would invoke multi-dimensional integration: \\[ P(a_1 \\leq X_1 \\leq b_1,a_2 \\leq X_2 \\leq b_2) = \\int_{a_1}^{b_1} \\int_{a_2}^{b_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\,. \\] As for the joint cumulative distribution function, or joint cdf, the convention is to treat each axis separately from the others, i.e., to define it as \\[ F_{X_1,X_2}(x_1,x_2) = P(X_1 \\leq x_1 \\cap X_2 \\leq x_2) \\,. \\] The joint cdf satisfies the following properties: if either \\(X_i = -\\infty\\), the joint cdf is zero; if both \\(X_i = \\infty\\), the joint cdf is one; it increases monotonically along each coordinate axis; and if \\(X_1\\) and \\(X_2\\) are independent random variables, then \\(F_{X_1,X_2}(x_1,x_2) = F_{X_1}(x_1) F_{X_2}(x_2)\\). To characterize bivariate distributions (and multivariate ones in general), we compute distribution moments by utilizing the Law of the Unconscious Statistician: \\[\\begin{align*} E[g(X_1,X_2)] &amp;= \\sum_{x_1} \\sum_{x_2} g(x_1,x_2) p_{X_1,X_2}(x_1,x_2) \\qquad \\mbox{discrete}\\\\ &amp;= \\int_{x_1} \\int_{x_2} g(x_1,x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\qquad \\mbox{continuous} \\,, \\end{align*}\\] with the shortcut formula continuing to hold in the multivariate case: \\[ V[g(X_1,X_2)] = E\\left[ g(X_1,X_2)^2 \\right] - (E[g(X_1,X_2)])^2 \\,. \\] We note that if we write \\(g(x_1,x_2)\\) as \\(g_1(x_1)g_2(x_2)\\) and if \\(X_1\\) and \\(X_2\\) are independent random variables, then \\[\\begin{align*} E[g_1(X_1)g_2(X_2)] &amp;= \\int_{x_1} \\int_{x_2} g_1(x_1) g_2(x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\\\ &amp;= \\int_{x_1} \\int_{x_2} g_1(x_1) g_2(x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_2 dx_1 \\\\ &amp;= \\left[ \\int_{x_1} g_1(x_1) f_{X_1}(x_1) \\right] \\cdot \\left[ \\int_{x_2} g_2(x_2) f_{X_2}(x_2) \\right] \\\\ &amp;= E[g_1(X_1)] E[g_2(X_2)] \\,. \\end{align*}\\] So, for instance, \\(E[X_1X_2] = E[X_1]E[X_2]\\) if \\(X_1\\) and \\(X_2\\) are independent, but not generally. 6.2.1 Characterizing a Discrete Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is \\(E[X_1X_2]\\)? We utilize the Law of the Unconscious Statistician: \\[\\begin{align*} E[X_1X_2] &amp;= \\sum_{x_1} \\sum_{x_2} x_1 x_2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0 \\times 0.20 + 2 \\times 0 \\times 0.40 + 1 \\times 1 \\times 0.30 + 2 \\times 2 \\times 0.10 \\\\ &amp;= 0.30 + 0.40 = 0.70 \\,. \\end{align*}\\] What is \\(F_{X_1,X_2}(3/2,3/2)\\)? The joint cdf is \\[\\begin{align*} P(X_1 \\leq 3/2 \\cap X_2 \\leq 3/2) &amp;= P(X_1 = 1 \\cap X_2 = 0) + P(X_1 = 1 \\cap X_2 = 1) \\\\ &amp;= 0.20 + 0.30 = 0.50 \\,. \\end{align*}\\] 6.2.2 Characterizing a Continuous Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = c(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). In an example above, we determined that \\(X_1\\) and \\(X_2\\) are dependent random variables due to the domain of the pdf not being “rectangular” on the \\(x_1\\)-\\(x_2\\) plane (see Figures 6.1 and 6.2). Here, we will first determine the value of \\(c\\) that makes this function a valid joint pdf. Carrying out this calculation will involve double integration; for a short review of double integration, see Chapter 8. We note here that we can integrate in either order (i.e., along the \\(x_1\\) axis first, or the \\(x_2\\) axis first), as the final result will be the same. Here, we will integrate along \\(x_1\\) first, since the lower bound along this axis is zero (a choice which often simplifies the overall calculation) and because \\(x_1\\) does not appear in the integrand: \\[\\begin{align*} \\int_0^1 \\left[ \\int_0^{x_1 = x_2} c (1-x_2) dx_1 \\right] dx_2 &amp;= c \\int_0^1 (1-x_2) \\left[ \\int_0^{x_1 = x_2} dx_1 \\right] dx_2 \\\\ &amp;= c \\int_0^1 (1-x_2) \\left[ \\left. x_1 \\right|_0^{x^2} \\right] dx_2 \\\\ &amp;= c \\int_0^1 x_2 (1-x_2) dx_2 \\\\ &amp;= c B(2,2) = c \\Gamma(2) \\Gamma(2) / \\Gamma(4) = c \\times 1! \\times 1! / 3! = c/6 = 1 \\,. \\end{align*}\\] Thus \\(c = 6\\). Note that we take advantage of the fact that \\(x_2 (1-x_2)\\) integrated from 0 to 1 is a Beta(2,2) distribution. If we had not made that association, we would still have observed the same final solution after integrating the polynomial in the integrand. Figure 6.2: The probability density function \\(6(1-x_2)\\) as a function of \\(x_1\\) (axis pointing to upper right) and \\(x_2\\) (axis pointing to upper left). The pdf peaks at \\((x_1,x_2) = (0,0)\\) and falls off towards \\(x_2 = 1\\) as a plane, with value greater than zero only within the domain \\(0 \\leq x_1 \\leq x_2 \\leq 1\\); Figure 6.1 shows that domain. Now we will ask, what are \\(E[X_2]\\) and \\(V[X_2]\\)? \\[\\begin{align*} E[X_2] = \\int_0^1 \\left[ \\int_0^{x_1 = x_2} 6 x_2 (1-x_2) dx_1 \\right] dx_2 &amp;= 6 \\int_0^1 x_2 (1-x_2) \\left[ \\int_0^{x_1 = x_2} dx_1 \\right] dx_2 \\\\ &amp;= 6 \\int_0^1 x_2 (1-x_2) \\left[ \\left. x_1 \\right|_0^{x^2} \\right] dx_2 \\\\ &amp;= 6 \\int_0^1 x_2^2 (1-x_2) dx_2 \\\\ &amp;= 6 B(3,2) = 6 \\Gamma(3) \\Gamma(2) / \\Gamma(5) = 6 \\times 2! \\times 1! / 4! = 1/2 \\,. \\end{align*}\\] In a similar manner, we can determine that \\(E[X_2^2] = 6 B(4,2) = 6 \\times 3! \\times 1! / 5! = 36/120 = 3/10\\) and that \\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 3/10 - 1/4 = 1/20\\). 6.2.3 The Bivariate Uniform Distribution The bivariate uniform distribution is defined as \\[ f_{X_1,X_2}(x_1,x_2) = \\frac{1}{A} \\,, \\] where \\(A\\) is the area of the domain. (Thus the integral of the bivariate function is \\(A \\times 1/A = 1\\).) Let \\(f_{X_1,X_2}(x_1,x_2) = 1\\) over the square defined by the vertices (0,0), (0,1), (1,0), and (1,1). What is \\(P(X_1 &gt; 2X_2)\\)? A nice feature of the bivariate uniform is that we can work with it “geometrically”: if we can determine the fraction of the domain that abides by the stated condition, then we have our answer. (This is because the joint pdf is flat, so integration is unnecessary.) We can rewrite \\(x_1 &gt; 2x_2\\) as \\(x_2 &lt; x_1/2\\), i.e., the region of interest in the domain is the region below the line with intercept zero and slope 1/2. (See Figure 6.3.) This region is a triangle with vertices (0,0), (1,1/2), and (1,0), which has the area (recall: one-half times base times height) \\(1/2 \\times 1 \\times 1/2 = 1/4\\). Done! \\(P(X_1 &gt; 2X_2) = 1/4\\). Figure 6.3: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region \\(x_1 &gt; 2x_2\\) is indicated by the blue triangle. 6.3 Covariance and Correlation The covariance between two random variables \\(X_1\\) and \\(X_2\\) is a metric that quantifies the amount of linear dependence between them. It is defined as \\[ {\\rm Cov}(X_1,X_2) = E[(X_1-\\mu_1)(X_2-\\mu_2)] \\,, \\] but one rarely uses this expression, as there is a shortcut formula: \\[\\begin{align*} {\\rm Cov}(X_1,X_2) &amp;= E[X_1X_2 - X_1\\mu_2 - X_2\\mu_1 + \\mu_1\\mu_2] \\\\ &amp;= E[X_1X_2] - \\mu_2E[X_1] - \\mu_1E[X_2] + \\mu_1\\mu_2 \\\\ &amp;= E[X_1X_2] - E[X_2]E[X_1] - E[X_1]E[X_2] + E[X_1]E[X_2] \\\\ &amp;= E[X_1X_2] - E[X_1]E[X_2] \\,. \\end{align*}\\] In the previous section, we saw that \\(E[X_1X_2] = E[X_1]E[X_2]\\) if \\(X_1\\) and \\(X_2\\) are independent random variables. Thus independent random variables have no covariance, which makes sense: independent random variables would have by definition no linear dependence. However, it is not true that \\(E[X_1X_2] = E[X_1]E[X_2]\\) implies that \\(X_1\\) and \\(X_2\\) are independent random variables…it just implies there is no linear dependence between the two variables. See Figure 6.4. Figure 6.4: Examples of data with negative covariance (left), no covariance (center), and positive covariance (right). Covariance is not necessarily an optimal metric for expressing linear dependence, as its value is not readily interpretable. To see this, assume we have the expression \\((tX_1 - X_2)\\) for some constant \\(t\\). Then \\((tX_1-X_2)^2 \\geq 0\\) and \\(E[(tX_1-X_2)^2] \\geq 0\\). Thus \\[\\begin{align*} E[(tX_1-X_2)^2] &amp;= E[t^2X_1^2 - 2tX_1X_2 + X_2^2] \\\\ &amp;= E[X_1^2] t^2 - 2E[X_1X_2] t + E[X_2^2] \\\\ &amp;= a t^2 + b t + c \\geq 0 \\,. \\end{align*}\\] (Recall that expected values are constants, and not themselves random variables.) The key here is that if \\(a t^2 + b t + c = 0\\), there is one real root to this quadratic equation, while if \\(a t^2 + b t + c &gt; 0\\), there are no real roots. Thus the discriminant, \\(b^2 - 4ac\\), must be \\(\\leq 0\\), and so \\[\\begin{align*} b^2 - 4ac &amp;= 4 (E[X_1X_2])^2 - 4 E[X_1^2] E[X_2^2] \\leq 0 \\\\ \\Rightarrow&amp; (E[X_1X_2])^2 \\leq E[X_1^2] E[X_2^2] \\,. \\end{align*}\\] At this point, the reader might ask “well, what about this?” The expression to the left above, \\((E[X_1X_2])^2\\), is \\([{\\rm Cov}(X_1,X_2)]^2\\) when \\(\\mu_1 = \\mu_2 = 0\\), while the expression to the right, \\(E[X_1^2] E[X_2^2]\\), is \\(V[X_1]V[X_2]\\) when \\(\\mu_1 = \\mu_2 = 0\\). Thus \\[ \\vert {\\rm Cov}(X_1,X_2) \\vert \\leq \\sqrt{V[X_1]V[X_2]} = \\sigma_1 \\sigma_2 \\,, \\] or \\(- \\sigma_1 \\sigma_2 \\leq {\\rm Cov}(X_1,X_2) \\leq \\sigma_1 \\sigma_2\\). The fact that we may not know immediately the value of \\(\\sigma_1 \\sigma_2\\) is what makes any numerical value of \\({\\rm Cov}(X_1,X_2)\\) hard to interpret. Thus we conventionally turn to an alternate expression of linear dependence, the correlation coefficient: \\[ \\rho_{X_1,X_2} = \\frac{{\\rm Cov}(X_1,X_2)}{\\sigma_1\\sigma_2} \\Rightarrow -1 \\leq \\rho \\leq 1 \\,. \\] If \\(\\rho_{X_1,X_2} &lt; 0\\), then an increase in the sampled value of \\(X_1\\) is associated with, on average, a smaller sampled value of \\(X_2\\), i.e., \\(X_1\\) and \\(X_2\\) are negatively correlated…whereas if \\(\\rho_{X_1,X_2} &gt; 0\\), larger sampled values for \\(X_1\\) are associated on average with larger sampled values of \\(X_2\\), i.e., \\(X_1\\) and \\(X_2\\) are positively correlated. (We note that we have actually seen \\(\\rho_{X_1,X_2}\\) before, in Chapter 2, when we introduced correlation as a metric of the strength of linear association as measured in simple linear regression. Recall that we estimate \\(\\rho_{X_1,X_2}\\) with, e.g., the Pearson correlation coefficient \\(R\\), and that we use coefficient of determination \\(R^2\\) to quantify the usefulness of the linear regression model. See Figure 6.5.) Figure 6.5: The same data as shown in Figure 6.4, with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right. Let’s suppose we are given \\(n\\) correlated random variables \\(\\{X_1,\\ldots,X_n\\}\\), and we define the sum \\(Y = \\sum_{i=1}^n a_i X_i\\). Furthermore, let’s assume we know the pdf/pmf, expected value, and variance of each of the \\(X_i\\)’s. What do we know about the distribution of \\(Y\\)? With work, we might be able to determine its pdf or pmf, but we would have to go beyond the method of moment-generating functions because that method requires the random variables to be independent of each other. We will not do that here. However, we can determine the expected value of \\(Y\\): \\[ E[Y] = E\\left[ \\sum_{i=1}^n a_i X_i \\right] = \\sum_{i=1}^n a_i E[X_i] \\,. \\] Dependencies between the \\(X_i\\)’s does not affect this equality. As for the variance of \\(Y\\)… We start by encapsulating the information about the covariances between each variable pair into a covariance matrix. (See Chapter 08 for a short introduction to matrices and their basic use.) For \\(n\\) random variables, the \\(n \\times n\\) covariance matrix \\(\\boldsymbol{\\Sigma}\\) is \\[ \\boldsymbol{\\Sigma} = \\left[ \\begin{array}{ccc} {\\rm Cov}(X_1,X_1) &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; {\\rm Cov}(X_n,X_n) \\end{array} \\right] = \\left[ \\begin{array}{ccc} V[X_1] &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; V[X_n] \\end{array} \\right] \\,. \\] We note that the diagonal of this matrix (the elements from the upper left corner to the lower right corner) contains the individual variances, since Cov(\\(X_i,X_i\\)) = \\(V[X_i]\\), and that because Cov(\\(X_i,X_j\\)) = Cov(\\(X_j,X_i\\)), the matrix is symmetric about the diagonal. Now, let \\(a^T = [ a_1 ~ a_2 ~ \\ldots a_n ]\\) be the \\(1 \\times n\\) matrix (or transposed vector) of coefficients. Then: \\[ V[Y] = a^T \\boldsymbol{\\Sigma} a \\,. \\] Nice and compact! Let’s compare this to the equivalent expression that does not utilize matrices: \\[\\begin{align*} V[Y] &amp;= \\left[ a_1 ~ \\ldots ~ a_n \\right] \\left[ \\begin{array}{ccc} V[X_1] &amp; \\cdots &amp; {\\rm Cov}(X_1,X_n) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\rm Cov}(X_n,X_1) &amp; \\cdots &amp; V[X_n] \\end{array} \\right] \\left[ \\begin{array}{c} a_1 \\\\ \\vdots \\\\ a_n \\end{array} \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j {\\rm Cov}(X_i,X_j) \\,. \\end{align*}\\] Note that we can derive this expression directly using the Law of the Unconscious Statistician: \\[\\begin{align*} V[Y] = E[Y^2] - (E[Y])^2 = E[(Y - E[Y])^2] &amp;= E\\left[ \\left( \\sum_{i=1}^n a_i X_i - \\sum_{i=1}^n a_i \\mu_i \\right)^2 \\right] \\\\ &amp;= E\\left[ \\left(\\sum_{i=1}^n a_i(X_i - \\mu_i) \\right)^2 \\right] \\,. \\end{align*}\\] Let’s look at this for a moment. If we have, e.g., \\(Y = a_1X_1+a_2X_2\\), then \\[\\begin{align*} (a_1X_1+a_2X_2)^2 &amp;= a_1^2X_1^2 + a_2^2X_2^2 + a_1a_2X_1X_2 + a_2a_1X_2X_1 \\\\ &amp;= \\sum_{i=1}^2 a_i^2 X_i^2 + 2a_1a_2X_1X_2 \\\\ &amp;= \\sum_{i=1}^2 a_i^2 X_i^2 + 2 \\sum_{i=1}^{2-1} \\sum_{j=i+1}^2 a_i a_j X_i X_j \\,. \\end{align*}\\] Recognizing that the 2’s in the summation bounds would be the sample size \\(n\\) in general, we can write that \\[ \\left(\\sum_{i=1}^n a_i X_i\\right)^2 = \\sum_{i=1}^n a_i^2 X_i^2 + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j X_i X_j \\,. \\] So, picking up where we left off… \\[\\begin{align*} V[Y] &amp;= E\\left[ \\left(\\sum_{i=1}^n a_i(X_i - \\mu_i) \\right)^2 \\right] \\\\ &amp;= E\\left[ \\sum_{i=1}^n a_i^2(X_i-\\mu_i)^2 + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j (X_i - \\mu_i) (X_j - \\mu_2) \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 E\\left[(X_i-\\mu_i)^2\\right] + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j E\\left[(X_i - \\mu_i) (X_j - \\mu_2) \\right] \\\\ &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j \\mbox{Cov}(X_i,X_j) \\,. \\end{align*}\\] 6.3.1 Correlation of Two Discrete Random Variables Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is the correlation between \\(X_1\\) and \\(X_2\\)? In an example above, we determined that \\(E[X_1X_2] = 0.7\\). This is but one piece of the correlation puzzle: we also need to determine \\(E[X_1]\\) and \\(E[X_2]\\) along with \\(V[X_1]\\) and \\(V[X_2]\\). \\[\\begin{align*} E[X_1] &amp;= \\sum_{x_1} \\sum_{x_2} x_1 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0.20 + 1 \\times 0.30 + 2 \\times 0.40 + 2 \\times 0.10 = 1.50 \\\\ \\\\ E[X_1^2] &amp;= \\sum_{x_1} \\sum_{x_2} x_1^2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1^2 \\times 0.20 + 1^2 \\times 0.30 + 2^2 \\times 0.40 + 2^2 \\times 0.10 = 2.50 \\\\ \\\\ E[X_2] &amp;= \\sum_{x_2} \\sum_{x_2} x_2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1 \\times 0.30 + 2 \\times 0.10 = 0.50 \\\\ \\\\ E[X_2^2] &amp;= \\sum_{x_2} \\sum_{x_2} x_2^2 p_{X_1,X_2}(x_1,x_2) \\\\ &amp;= 1^2 \\times 0.30 + 2^2 \\times 0.10 = 0.70 \\,. \\end{align*}\\] Hence Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2] = 0.70 - 0.75 = -0.05\\), while \\(V[X_1] = E[X_1^2] - (E[X_1])^2 = 2.50 - 2.25 = 0.25\\) and \\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 0.70 - 0.25 = 0.45\\). Thus the correlation between \\(X_1\\) and \\(X_2\\) is \\[ \\rho_{X_1,X_2} = \\frac{\\mbox{Cov}(X_1,X_2)}{\\sqrt{V[X_1]V[X_2]}} = \\frac{-0.05}{\\sqrt{0.25 \\cdot 0.45}} = -0.149 \\,. \\] \\(X_1\\) and \\(X_2\\) are (relatively weakly) negatively correlated: increasing \\(X_1\\) leads to slightly decreased values of \\(X_2\\), on average. 6.3.2 Correlation of the Sum of Two Discrete Random Variables Let \\(X_1\\) and \\(X_2\\) be the random variables defined in the previous example, and let \\(Y = X_1 - X_2\\). What is \\(V[Y]\\)? Because only two random variables are involved, we will first do the calculation using the summation form given at the end of the section: \\[\\begin{align*} V[Y] &amp;= \\sum_{i=1}^n a_i^2 V[X_i] + 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i a_j {\\rm Cov}(X_i,X_j) \\\\ &amp;= 1^2 V[X_1] + (-1)^2 V[X_2] + 2 (1) (-1) \\mbox{Cov}(X_1,X_2) \\\\ &amp;= V[X_1] + V[X_2] - 2 \\mbox{Cov}(X_1,X_2) \\\\ &amp;= 0.25 + 0.45 - 2(-0.05) = 0.80 \\,. \\end{align*}\\] Here is the same calculation using R. a &lt;- c(1,-1) Sigma &lt;- matrix(c(0.25,-0.05,-0.05,0.45),nrow=2) # fill column-by-column t(a) %*% Sigma %*% a ## [,1] ## [1,] 0.8 Note that a is by default a column vector, and thus must be transposed via the t() function, and that %*% is the matrix multiplication operator. 6.3.3 Uncorrelated is Not the Same as Independent: a Demonstration Let the region shown in Figure 6.6 be the domain of a bivariate uniform distribution. (The area of the domain is 1, hence the amplitude of the bivariate pdf is 1/1 = 1.) Let \\(X_1\\) and \\(X_2\\) be random variables drawn from this distribution. Because the domain is not “rectangular,” we can state that \\(X_1\\) and \\(X_2\\) are dependent random variables. What is the correlation between them? The definition of covariance is Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2] - E[X_1]E[X_2]\\). If we examine the figure, we can convince ourselves that \\(E[X_1] = 0\\) due to the uniform nature of the pdf and the symmetry of the domain about \\(x_1 = 0\\). Hence Cov(\\(X_1,X_2\\)) = \\(E[X_1X_2]\\), which is \\[\\begin{align*} E[X_1X_2] &amp;= \\int_0^1 x_2 \\left[ \\int_{x_2-1}^{1-x_2} x_1 dx_1 \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\left. \\frac{x_1^2}{2} \\right|_{x_2-1}^{1-x_2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\frac{(x_2-1)^2}{2} - \\frac{(1-x_2)^2}{2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ \\frac{(x_2-1)^2}{2} - \\frac{(x_2-1)^2}{2} \\right] dx_2 \\\\ &amp;= \\int_0^1 x_2 \\left[ 0 \\right] dx_2 = 0 \\,. \\end{align*}\\] Hence Cov(\\(X_1,X_2\\)) = 0 (and the correlation between \\(X_1\\) and \\(X_2\\) is zero). This result demonstrates that uncorrelated random variables are not necessarily independent random variables: they are simply random variables that exhibit no linear dependence. (One way to view this intuitively is to imagine that we sample \\(n\\) data from this distribution and then regress \\(X_2\\) (as \\(Y\\)) against \\(X_1\\) (as \\(x\\)) using linear regression. The linear regression line would, on average, pass flat through the points, i.e., would, on average, have a slope of zero, indicating no linear association between \\(X_1\\) and \\(X_2\\). Figure 6.6: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region \\(x_1 &gt; 2x_2\\) is indicated by the blue triangle. 6.3.4 Covariance of Multinomial Random Variables In Chapter 3, we introduce the multinomial distribution, which governs experiments in which we sample data that can \\(m\\) different discrete values. The probability mass function is \\[ p_{X_1,\\ldots,X_m}(x_1,\\ldots,x_m \\vert p_1,\\ldots,p_m) = \\frac{k!}{x_1! \\cdots x_m!}p_1^{x_1}\\cdots p_m^{x_m} \\,, \\] where \\(x_i\\) represents the number of times outcome \\(i\\) is observed in \\(k\\) trials, and where \\(p_i\\) is the probability of observing outcome \\(i\\). As stated in Chapter 3, the distribution for each random variable \\(X_i\\) is Binomial(\\(k,p_i\\)), but the \\(X_i\\)’s are not independent; because \\(\\sum_{i=1}^m X_i = k\\), increase the value of one of the random variables would lead the values of the others to decrease on average. Here we will derive the result that was quoted in Chapter 3, namely that Cov(\\(X_i\\),\\(X_j\\)) = \\(-kp_ip_j\\) for \\(i \\neq j\\). Because \\(X_i\\) and \\(X_j\\) are each binomially distributed random variables, we can view them as sums of Bernoulli distributed random variables, i.e., we can write \\[ X_i = \\sum_{l=1}^k U_l ~~\\mbox{and}~~ X_j = \\sum_{l=1}^k V_l \\,, \\] where \\(U_l \\sim\\) Bernoulli(\\(p_i\\)) and \\(V_l \\sim\\) Bernoulli(\\(p_j\\)), and where \\(l\\) is an index representing the \\(l^{\\rm th}\\) trial. The covariance of \\(X_i\\) and \\(X_j\\) is thus \\[\\begin{align*} \\mbox{Cov}(X_i,X_j) &amp;= E[X_iX_j] - E[X_i]E[X_j] \\\\ &amp;= E[X_iX_j] - (k p_i)(k p_j) \\\\ &amp;= E[X_iX_j] - k^2p_ip_j \\,, \\end{align*}\\] where \\[\\begin{align*} E[X_iX_j] &amp;= E[U_1V_1 + U_1V_2 + \\cdots + U_1V_k + U_2V_1 + U_2V_2 + \\cdots + U_2V_k + \\cdots + U_kV_k] \\\\ &amp;= E[U_1V_1] + E[U_1V_2] + \\cdots + E[U_kV_k] \\,. \\end{align*}\\] There are \\(k^2\\) terms in this summation overall. Because we cannot observe two different outcomes in the same trial, \\(E[U_iV_i] = 0\\) for all indices \\(i \\in [1,k]\\). As for the other \\(k^2 - k\\) terms…let’s starting by looking at one of them: \\[ E[U_1V_2] = E[U_1]E[V_2] = p_ip_j \\,. \\] This result holds because the results of any two trials in a multinomial experiment are independent random variables. Thus the sum of the remaining \\(k^2-k\\) terms is \\((k^2-k)p_ip_j\\), and thus \\[ \\mbox{Cov}(X_i,X_j) = (k^2-k)p_ip_j - k^2p_ip_j = -kp_ip_j \\,. \\] 6.3.5 Tying Covariance Back to Simple Linear Regression Above, we point out how \\(R^2\\) is related to the correlation coefficient between the \\(x_i\\)’s and \\(Y_i\\)’s. Here, we extend that discussion to the determinination of the correlation coefficient between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in simple linear regression. When discussing simple linear regression in Chapter 2, we give formulae for the variances of both \\(V[\\hat{\\beta}_0]\\) and \\(V[\\hat{\\beta}_1]\\). However, now that we know about the concept of covariance, we can ask a question that we did not ask then: are \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) independent random variables? The answer is: of course they are not; if we have a set of data that we are trying to draw a line through, it should be intuitively obvious that changing the intercept of the line will lead to its slope having to change as well in order to (re-)optimize the sum of squared errors. So: the covariance between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is non-zero…but how do we determine it? The standard method of determining covariance involves the calculation of the so-called variance-covariance matrix. (To be clear, this is nomenclature that is specific to regression contexts.) Stated without derivation or proof, that matrix is given by the following: \\[ \\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\,, \\] where \\(\\sigma^2\\) is the true variance (which, as you will recall, we assume does not vary as a function of \\(x\\)), \\(\\mathbf{X}\\) is the design matrix \\[ \\mathbf{X} = \\left( \\begin{array}{cc} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{array} \\right) \\,, \\] the superscript \\(T\\) denotes the matrix transpose, and where the superscript \\(-1\\) denotes matrix inversion. The column of 1’s in the design matrix represents what we multiply \\(\\beta_0\\) by in the model, while the column of \\(x_i\\)’s represents what we multiply \\(\\beta_1\\) by. As far as \\(\\sigma^2\\) is concerned…we generally never know this quantity, so we plug in \\(\\widehat{\\sigma^2}\\) in place of \\(\\sigma^2\\) above. Let’s demonstrate how this works using the same data as we used to talk through the output from the R function lm() in Chapter 2. set.seed(202) x &lt;- runif(40,min=0,max=10) Y &lt;- 4 + 0.5*x + rnorm(40) lm.out &lt;- lm(Y~x) summary(lm.out) ## ## Call: ## lm(formula = Y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00437 -0.53068 0.04523 0.40338 2.47660 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.5231 0.3216 14.063 &lt; 2e-16 *** ## x 0.4605 0.0586 7.859 1.75e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9792 on 38 degrees of freedom ## Multiple R-squared: 0.6191, Adjusted R-squared: 0.609 ## F-statistic: 61.76 on 1 and 38 DF, p-value: 1.749e-09 Recall that the estimate of \\(\\sigma\\) is given by the Residual standard error, which here is 0.9792, so \\(\\widehat{\\sigma^2} = 0.9587\\). We can extract this from the object lm.out: hat.sigma &lt;- summary(lm.out)$sigma hat.sigma2 &lt;- hat.sigma^2 (To determine what quantities are available in an R list output by a function such as summary(lm), we can type names(summary(lm)). Here, names() indicates that sigma is an accessible list element.) We can set up the design matrix as follows: X &lt;- cbind(rep(1,40),x) “cbind” means “column bind”: the first argument is a vector of 1’s that is bound to the second argument, which is the vector of \\(x_i\\)’s, thus creating a matrix with 40 rows and 2 columns. We can now compute the variance-covariance matrix: hat.sigma2 * solve(t(X) %*% X) ## x ## 0.10344444 -0.01652116 ## x -0.01652116 0.00343436 The function t() returns the transpose of \\(X\\) (a matrix with 2 rows and 40 columns), while solve() is the matrix inversion function. What do we see above? First, if we take the square roots of the matrix elements at upper left and lower right (i.e., along the matrix diagonal), we get \\[ se(\\hat{\\beta}_0) = \\sqrt{0.1034} = 0.3216 ~~~ \\mbox{and} ~~~ se(\\hat{\\beta}_0) = \\sqrt{0.0034} = 0.0586 \\,. \\] These quantities match the values in the Std. Error column of the coefficient table output by summary(). Good! In addition, however, we see the off-diagonal element \\(-0.0165\\): this is Cov(\\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)). The negative sign makes sense: if we increase the intercept, we have to make the slope smaller (i.e., more negative) to optimize that coefficient. The correlation is given by \\[ \\frac{\\mbox{Cov}(\\hat{\\beta}_0,\\hat{\\beta}_1)}{se(\\hat{\\beta}_0) \\cdot se(\\hat{\\beta}_1)} = \\frac{-0.0165}{0.3216 \\cdot 0.0586} = -0.8755 \\,. \\] We see immediately that the intercept and slope are strongly (and negatively) correlated. In “real-life” situations, do we need to carry out the chain of computations we carry out above? No…because R provides wrapper functions to help us: (Sigma &lt;- vcov(lm.out)) # compute the variance-covariance matrix ## (Intercept) x ## (Intercept) 0.10344444 -0.01652116 ## x -0.01652116 0.00343436 cov2cor(Sigma) # convert the covariance to correlation ## (Intercept) x ## (Intercept) 1.0000000 -0.8765245 ## x -0.8765245 1.0000000 (We note that the difference between the correlation coefficient computed above and that output by cov2cor() is entirely due to round-off error.) 6.3.6 Tying Coviarance to Simple Logistic Regression In the last example, we state that the variance-covariance matrix for simple linear regression is given by \\[ \\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1} \\,. \\] A similar expression exists for logistic regression: \\[ \\boldsymbol{\\Sigma} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\,, \\] where the diagonal weight matrix \\(\\mathbf{W}\\) is given by \\[ \\mathbf{W} = \\left( \\begin{array}{cccc} \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_1)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_1))^2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_2)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_2))^2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_n)}{(1+\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1x_n))^2} \\end{array} \\right) \\,. \\] Note that if we were to replace every non-zero matrix entry above with \\(1/\\sigma^2\\), we would recover the expression \\(\\boldsymbol{\\Sigma} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\). Below, we show how to populate the \\(\\mathbf{W}\\) matrix and use it to determine the variance-covariance matrix. # run simple logistic regression on training dataset log.out &lt;- glm(class~col.iz,data=df.train,family=binomial) summary(log.out) ## ## Call: ## glm(formula = class ~ col.iz, family = binomial, data = df.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7460 -1.1661 0.3405 1.1280 2.0606 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.16841 0.09265 1.818 0.06911 . ## col.iz -0.96729 0.31051 -3.115 0.00184 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 970.41 on 699 degrees of freedom ## Residual deviance: 958.16 on 698 degrees of freedom ## AIC: 962.16 ## ## Number of Fisher Scoring iterations: 4 hat.beta0 &lt;- log.out$coefficients[1] hat.beta1 &lt;- log.out$coefficients[2] e &lt;- exp(hat.beta0 + hat.beta1*df.train$col.iz) W &lt;- diag(e/(1+e)^2) X &lt;- cbind(rep(1,nrow(df.train)),df.train$col.iz) (Sigma &lt;- solve(t(X) %*% W %*% X)) ## [,1] [,2] ## [1,] 0.008584293 -0.01635715 ## [2,] -0.016357147 0.09641920 sqrt(Sigma[1,1]) ## [1] 0.09265146 sqrt(Sigma[2,2]) ## [1] 0.3105144 We see that the diagonal elements match the standard errors output by the summary() function. We can find the correlation between \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) by using the function cov2cor(): cov2cor(Sigma) ## [,1] [,2] ## [1,] 1.0000000 -0.5685564 ## [2,] -0.5685564 1.0000000 The correlation is not as strong as we observed above in our simple linear regression example, but in absolute terms we would still say that \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are strongly negatively correlated. 6.4 Marginal and Conditional Distributions Let’s say that we perform an experiment where we sample a pair of random variables \\((X_1,X_2)\\) from a bivariate distribution, where \\(X_1\\) and \\(X_2\\) are dependent random variables. Despite doing this, we might ultimately find that we are only interested in how \\(X_1\\) is distributed, regardless of the sampled value of \\(X_2\\) (or vice-versa). The distribution that we’d like to derive is dubbed a marginal distribution. To compute the marginal distribution for \\(X_1\\) when given a bivariate pdf \\(f_{X_1,X_2}(x_1,x_2)\\), we integrate over all values of \\(x_2\\): \\[ f_{X_1}(x_1) = \\int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 \\,, \\] while for a bivariate pmf, we simply replace integration with summation, e.g., \\[ p_{X_1}(x_1) = \\sum_{x_2} p_{X_1,X_2}(x_1,x_2) \\,. \\] (If we are interested in computing the marginal distribution for \\(X_2\\), we would simply swap the indices 1 and 2 above.) An important thing to keep in mind is that a marginal pdf (or pmf) is a pdf (or pmf), meaning that it is like any other pdf (or pmf): it is non-negative and it integrates (or sums) to one, it has an expected value and a standard deviation, etc. A related, albeit narrower question to the one motivating the computation of marginals is: what is the distribution of \\(X_1\\) when \\(X_2 = x_2\\)? (It is narrower in the sense that for a marginal, we do not care about the sampled value of \\(X_2\\), while here, we do.) This distribution is dubbed a conditional distribution, and it is defined as follows: \\[ f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} \\,. \\] For a bivariate pmf, the expression is similar. As was the case with a marginal distribution, a conditional pdf (or pmf) is a pdf (or pmf). One might notice immediately the similarity between this expression and the one defining conditional probability in Chapter 1: \\(p(A \\vert B) = p(A \\cap B)/p(B)\\). This is not coincidental. For marginals, the analogous probability expression is \\(p(A) = \\sum_i p(A \\vert B_i) p(B_i)\\), i.e., the law of total probability! If this is not immediately evident, note that we can replace the \\(f_{X_1,X_2}(x_1,x_2)\\) in the marginal integral with \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) f_{X_2}(x_2)\\). Then we can see how \\(x_1\\) takes the place of \\(A\\) and \\(x_2\\) takes the places of \\(B_i\\).) Why do we divide by a marginal distribution when deriving a conditional distribution? The answer is simple: if we do not, then there is no guarantee that the conditional pdf or pmf will integrate or sum to one. Finding the conditional distribution is akin to chopping through the bivariate pdf at a particular value of \\(x_1\\) or \\(x_2\\), and tracing the pdf along the chopped edge. This traced-out function will be non-negative, but it will not necessarily integrate to one. The division by the marginal acts to “normalize” the traced-out function, i.e., the division raises or lowers it such that it subsequently will integrate to one. As a final note, we will mention that if \\(X_1\\) and \\(X_2\\) are independent random variables, then, e.g., \\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\\); and \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2) = f_{X_1}(x_1)\\) and \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1) = f_{X_2}(x_2)\\) The first bullet point above shows the result that we look for when establishing the independence of two random variables mathematically: we compute the marginals for each variable, take the product of marginals, and see if it matches the original bivariate function. However, this conventional textbook approach to establishing independence is unnecessary, as it is sufficient to examine the domain of the bivariate function and to determine if we can factorize it into separate functions of \\(x_1\\) and \\(x_2\\). 6.4.1 Marginal and Conditional Distributions for a Bivariate PMF Let \\(X_1\\) and \\(X_2\\) have the following joint probability mass function \\(p_{X_1,X_2}(x_1,x_2)\\): \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20 0.30 0.00 \\(x_1 = 2\\) 0.40 0.00 0.10 What is the marginal distribution \\(p_{X_2}(x_2)\\) and the conditional distribution \\(p_{X_1 \\vert X_2}(x_1 \\vert x_2)\\)? When probability masses are involved, the marginal distribution is derived by summing over an axis (here, \\(x_1\\)): \\[ p_{X_2}(x_2) = \\sum_{x_1} p_{X_1,X_2}(x_1,x_2) \\,. \\] For this problem, the summation can be done by inspection, yielding the following marginal distribution: \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) 0.60 0.30 0.10 As stated in the text above, a marginal pmf is a pmf, meaning that for instance the values sum to 1, and we can compute quantities like \\(E[X_2] = 0 \\times 0.6 + 1 \\times 0.3 + 2 \\times 0.1 = 0.5\\). As for the conditional distribution, we have the following expression \\[ p_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)} \\,, \\] which in practice means we take the original bivariate table and divide each row by the marginal for \\(x_2\\), if we are conditioning on \\(x_2\\), or divide each column by the marginal for \\(x_1\\), if we are conditioning on \\(x_1\\). Here, we condition on \\(x_2\\), so our result is \\(x_2 = 0\\) \\(x_2 = 1\\) \\(x_2 = 2\\) \\(x_1 = 1\\) 0.20/0.60 = 0.33 0.30/0.30 = 1.00 0.00/0.10 = 0.00 \\(x_1 = 2\\) 0.40/0.60 = 0.67 0.00/0.30 = 0.00 0.10/0.10 = 1.00 In such a table, the values in the individual columns all need to sum to one: given some value of \\(x_2\\), we will with probability 1 sample a value of \\(x_1\\). (As should be clear, if we condition on \\(x_1\\) instead, we’d generate a table in which the values in each row would sum to 1.) 6.4.2 Marginal and Conditional Distributions for a Bivariate PDF Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). What is the marginal distribution \\(f_{X_1}(x_1)\\)? What is the conditional distribution \\(f_{X_2 \\vert X_l}(x_2 \\vert x_1)\\)? Refer back to Figure 6.1. The marginal distribution is defined as a function of \\(x_1\\), so we integrate over \\(x_2\\)…which means that for any given value of \\(x_1\\), the bounds of integration are \\(x_1 = x_2\\) to 1: \\[ f_{X_1}(x_1) = \\int_{x_1}^1 6(1-x_2) dx_2 \\,. \\] Why is the lower bound \\(x_1\\) instead of \\(x_2\\)? They are interchangable, but if we use \\(x_2\\), then our final result would not be a function of \\(x_1\\)! In general, if we integrate along one axis, the integral bounds should be expressed in terms of the other axis. To continue: \\[\\begin{align*} f_{X_1}(x_1) &amp;= \\int_{x_1}^1 6(1-x_2) dx_2 \\\\ &amp;= 6 \\left[ \\int_{x_1}^1 dx_2 - \\int_{x_1}^1 x_2 dx_2 \\right] \\\\ &amp;= 6 \\left[ \\left. x_2 \\right|_{x_1}^1 - \\left. \\frac{x_2^2}{2} \\right|_{x_1}^1 \\right] \\\\ &amp;= 6 \\left[ 1 - x_1 - \\left(\\frac{1}{2}-\\frac{x_1^2}{2} \\right) \\right] \\\\ &amp;= 3 - 6x_1 + 3x_1^2 = 3(1-x_1)^2 \\,, \\end{align*}\\] for \\(0 \\leq x_1 \\leq 1\\). See Figure 6.7. Given the functional form and domain, we can recognize this as a Beta(1,3) distribution. (Such recognition is helpful if, for instance, we were to ask for the expected value of \\(X_1\\). Rather than doing yet another integral, we’d write that \\(E[X_1] = \\alpha/(\\alpha+\\beta) = 1/4\\).) As for the conditional distribution…once we have the marginal, this is easy to write down: \\[ f_{X_2 \\vert X_1}(x_2 \\vert x_1) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \\frac{6(1-x_2)}{3(1-x_1)^2} = \\frac{2(1-x_2)}{(1-x_1)^2} \\,. \\] Refering back to Figure (fig:mulfig1), we note that this conditional expression can only be non-zero along the line from \\(x_2 = x_1\\) to 1. So the domain of this conditional distribution is \\(x_2 \\vert x_1 \\in [x_1,1]\\). See Figure 6.7. Figure 6.7: The marginal distribution \\(f_{X_1}(x_1)\\) and conditional distribution \\(f_{X_2 \\vert X_1}(x_2 \\vert x_1=0.3)\\) for the bivariate function \\(f_{X_1,X_2}(x_1,x_2) = 6(1-x_2)\\) with domain \\(0 \\leq x_1 \\leq x_2 \\leq 1\\). 6.5 Conditional Expected Value and Variance As was stated above, a conditional pdf or pmf is a pdf or pmf…meaning that like a pdf or pmf, it has an expected value and a variance. For instance, the expected value of a conditional pdf \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\), or the conditional expected value, is \\[ E[X_1 \\vert X_2] = \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\,. \\] As you might expect, an analogous expression exists for bivariate pmfs: \\(E[X_1 \\vert X_2] = \\sum_{x_1} p_{X_1 \\vert X_2}(x_1 \\vert x_2)\\). Also, there are two important points to make here: While, e.g., \\(E[X_1]\\) is a constant, \\(E[X_1 \\vert X_2]\\) is a random variable due to the randomness of \\(X_2\\)…that is, unless one specifies \\(E[X_1 \\vert X_2=x_2]\\), which is constant because \\(X_2\\) is no longer random. Beware notation! One can generalize \\(E[X_1 \\vert X_2]\\) in the manner of the Law of the Unconscious Statistician: \\(E[g(X_1) \\vert X_2] = \\int_{x_1} g(x_1) f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1\\). The definition of conditional variance builds off of that of the conditional expected value: \\[\\begin{align*} V[X_1 \\vert X_2] &amp;= E[(X_1-\\mu_1)^2 \\vert X_2] \\\\ &amp;= E[X_1^2 \\vert X_2] - (E[X_1 \\vert X_2])^2 \\\\ &amp;= \\int_{x_1} x_1^2 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 - \\left[ \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\right]^2 \\,. \\end{align*}\\] As we can see in the second line above, the fact that we are dealing with a condition does not fundamentally change how variance is computed: the form of the shortcut formula is the same as before, just with the condition added. Can one go from a conditional expected value to an unconditional expected value \\(E[X_1]\\)? Yes: intuitively, this involves averaging the values found for \\(E[X_1 \\vert X_2]\\) over all possible values of \\(x_2\\), weighting each of these possible values by how relatively likely it is in the first place (i.e., in the case of a bivariate pdf, weighting each value of \\(E[X_1 \\vert X_2]\\) by \\(f_{X_2}(x_2)\\)): \\[\\begin{align*} E[X_1] = E[E[X_1 \\vert X_2]] &amp;= \\int_{x_2} f_{X_2}(x_2) E[X_1 \\vert X_2] dx_2 \\\\ &amp;= \\int_{x_2} f_{X_2}(x_2) \\int_{x_1} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 dx_2 \\\\ &amp;= \\int_{x_2} \\int_{x_1} x_1 f_{X_2}(x_2) f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 dx_2 \\\\ &amp;= \\int_{x_1} \\int_{x_2} x_1 f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = E[X_1] \\,. \\end{align*}\\] As you might expect, given the conditional variance \\(V[X_1 \\vert X_2]\\), we can compute the unconditional variance \\(V[X_1]\\)…and for this, it is simplest to begin by writing down that \\[ V[X_1] = V[E[X_1 \\vert X_2]] + E[V[X_1 \\vert X_2]] \\,. \\] The intuitive interpretation of this equation is that it reflects that \\(X_1\\) can vary because the mean of \\(X_1 \\vert X_2\\) can shift as a function of \\(X_2\\) (so we want to quantify how much \\(E[X_1 \\vert X_2]\\) varies…giving the first term on the right above), but it can also vary due to being randomly distributed about the mean (so we want to quantify how much, on average, is that random variation…which gives the second term above). As one might guess, writing out the above expression in terms of integrals or summations over bivariate functions is going to be messy and thus we forego doing this here. (This might lead one to ask “how will we solve unconditional variance problems if expressions with integrals or summations are not provided?” It turns out there is an entire class of problems that we can solve without integration or summation, and we provide an example of a problem from that class below.) Note that if \\(X_1\\) and \\(X_2\\) are independent, it follows that, e.g., \\(E[X_1 \\vert X_2] = E[X_1]\\) and \\(V[X_1 \\vert X_2] = V[X_1]\\). Let’s look at the latter expression. Above, we wrote that \\(V[X_1] = V[E[X_1 \\vert X_2]] + E[V[X_1 \\vert X_2]]\\). If \\(X_1\\) and \\(X_2\\) are independent, then \\(E[X_1 \\vert X_2]\\) is a constant, and thus \\(V[E[X_1 \\vert X_2]] = 0\\) (because a constant does not vary). In addition, \\(V[X_1 \\vert X_2]\\) is a constant (it doesn’t vary as \\(X_2\\) changes) and thus \\(E[V[X_1 \\vert X_2]] = V[X_1 \\vert X_2]\\). So in the end, \\(V[X_1] = 0 + V[X_1 \\vert X_2] = V[X_1 \\vert X_2]\\). 6.5.1 Conditional and Unconditional Expected Value Given a Bivariate Distribution Let \\(X_1\\) and \\(X_2\\) have the following joint probability density function: \\[ f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \\,, \\] with \\(0 \\leq x_1 \\leq x_2 \\leq 1\\) (see Figure 6.1). For this distribution, the marginal \\(f_{X_2}(x_2)\\) is \\(6x_2(1-x_2)\\) for \\(x_2 \\in [0,1]\\), or a Beta(2,2) distribution, while the conditional distribution \\(f_{X_1 \\vert X_2}(x_1 \\vert x_2)\\) is \\[ f_{X_1 \\vert X_2}(x_1 \\vert x_2) = \\frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \\frac{6(1-x_2)}{6(1-x_2)x_2} = \\frac{1}{x_2} \\,, \\] for \\(x_1 \\in [0,x_2]\\). (The interested reader can verify these results!) What is \\(E[X_1 \\vert X_2]\\) and what is \\(E[X_1]\\)? For the conditional expected value, we have that \\[\\begin{align*} E[X_1 \\vert X_2] &amp;= \\int_0^{x_2} x_1 f_{X_1 \\vert X_2}(x_1 \\vert x_2) dx_1 \\\\ &amp;= \\int_0^{x_2} \\frac{x_1}{x_2} dx_1 \\\\ &amp;= \\frac{1}{x_2} \\int_0^{x_2} x_1 dx_1 \\\\ &amp;= \\frac{1}{x_2} \\left. \\frac{x_1^2}{2} \\right|_0^{x_2} = \\frac{1}{x_2} \\frac{x_2^2}{2} = \\frac{x_2}{2} \\,. \\end{align*}\\] It turns out that we did not necessarily have to do this integral, however. Note that the conditional expression is \\(1/x_2\\) for \\(x_1 \\in [0,x_2]\\)…this is a uniform distribution! So we’d see, by inspection, that the conditional expected value is \\(x_2/2\\). To determine the unconditional expected value \\(E[X_1]\\), we weight every possible value of \\(E[X_1 \\vert X_2]\\) by the probability (density) that we would even observe \\(X_2\\) in the first place (which is the marginal distribution for \\(X_2\\)): \\[\\begin{align*} E[X_1] &amp;= \\int_0^1 f_{X_2}(x_2) E[X_1 \\vert X_2] dx_2 \\\\ &amp;= \\int_0^1 6 x_2 (1 - x_2) \\frac{x_2}{2} dx_2 \\\\ &amp;= 3 \\int_0^1 x_2^2 (1 - x_2) dx_2 \\\\ &amp;= 3 B(3,2) = 3 \\times 2! \\times 1! / 4! = 1/4 \\,. \\end{align*}\\] 6.5.2 Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions Let’s look at the following problem: the number of homework assignments due in a given week, \\(N\\), varies from week to week and its value is sampled from a Poisson distribution with mean \\(\\lambda\\). The time to complete any one homework assignment, \\(X\\), also varies, and its value is sampled from a Gamma distribution with parameter \\(\\alpha\\) and \\(\\beta\\) (and is the form of the Gamma with expected value \\(\\alpha\\beta\\) and variance \\(\\alpha\\beta^2\\)). Let \\(T \\vert N = \\sum_{i=1}^N X_i\\) be the total time spent completing \\(N\\) homework assignments. (Assume all the \\(X_i\\)’s are independent random variables.) What are \\(E[T]\\) and \\(V[T]\\)? We first note that because we are working with two univariate distributions for which the expected value and variance are known, there is no need to, e.g., integrate. We simply need to identify \\(E[T \\vert N]\\) and \\(V[T \\vert N]\\) and work with those expressions directly. We’ll start with the expected value: \\[ E[T] = E[E[T \\vert N]] = E\\left[E\\left[\\sum_{i=1}^N X_i\\right]\\right] = E\\left[\\sum_{i=1}^N E[X_i]\\right] = E[N\\alpha\\beta] = \\alpha \\beta E[N] = \\alpha\\beta\\lambda \\,. \\] Somewhat more complicated is the computation of the variance: \\[\\begin{align*} V[T] &amp;= V[E[T \\vert N]] + E[V[T \\vert N]] \\\\ &amp;= V[N \\alpha \\beta] + E\\left[ V \\left[ \\sum_{i=1}^N X_i \\right] \\right] \\\\ &amp;= \\alpha^2 \\beta^2 V[N] + E \\left[ \\sum_{i=1}^N V[X_i] \\right] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + E \\left[ \\sum_{i=1}^N \\alpha \\beta^2 \\right] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + E[N\\alpha \\beta^2 ] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + \\alpha \\beta^2 E[N] \\\\ &amp;= \\alpha^2 \\beta^2 \\lambda + \\alpha \\beta^2 \\lambda = \\alpha (\\alpha + 1) \\beta^2 \\lambda \\,. \\end{align*}\\] Thus in any randomly chosen week, the time needed to complete homework is \\(\\alpha\\beta\\lambda\\) on average, with standard deviation \\(\\sqrt{\\alpha (\\alpha + 1) \\beta^2 \\lambda}\\). 6.6 The Multivariate Normal Distribution The multivariate normal distribution is the most important one in multivariate settings, due in part to its role in the multivariate analogue of the central limit theorem: if we have a collection of \\(n\\) iid random vectors, where \\(n\\) is sufficiently large then the sample mean vector is going to be approximately multivariate normally distributed. The joint probability density function for the multivariate normal is given by \\[ f_{\\mathbf{X}}(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^p \\vert \\boldsymbol{\\Sigma} \\vert}} \\exp\\left(-\\frac12 (\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\right) \\,. \\] Here, \\(\\mathbf{x} = \\{x_1,\\ldots,x_p\\}\\), and \\(\\boldsymbol{\\mu} = \\{\\mu_1,\\ldots,\\mu_p\\}\\) are the centroids of the normal along each of the \\(p\\) coordinate axes. \\(\\boldsymbol{\\Sigma}^{-1}\\) is the inverse of the covariance matrix \\(\\boldsymbol{\\Sigma}\\), while \\(\\vert \\boldsymbol{\\Sigma} \\vert\\) is the determinant of \\(\\boldsymbol{\\Sigma}\\). (If the reader is unfamiliar with these terms, see the short description of matrices in Chapter 8.) We denote sampling from this distribution with the notation \\[ \\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\,, \\] where \\(\\mathbf{X}\\) is a \\(p\\)-dimensional vector. To be clear: the multivariate normal is not a distribution that we work with by hand! We would conventionally use, e.g., R to do any and all calculations. That said, there are two qualitative facts about the multivariate normal that are useful to know. A marginal distribution of the multivariate normal is itself a univariate or multivariate normal (depending on \\(p\\) and how many axes are being integrated over). To derive a marginal distribution, one simply needs to remove elements from the mean vector and from the covariance matrix. For instance, if \\(p = 2\\) and we wish to derive the marginal distribution for \\(X_1\\), then \\[\\begin{align*} \\boldsymbol{\\mu} &amp;= [ \\mu_1 ~ \\mu_2 ] ~~\\rightarrow \\mu_{\\rm marg} = \\mu_1 \\\\ \\boldsymbol{\\Sigma} &amp;= \\left( \\begin{array}{cc} V[X_1] &amp; \\mbox{Cov}(X_1,X_2) \\\\ \\mbox{Cov}(X_2,X_1) &amp; V[X_2] \\end{array} \\right) ~~\\rightarrow~~ \\Sigma_{\\rm marg} = V[X_1] \\,. \\end{align*}\\] Here, we removed the second element of \\(\\boldsymbol{\\mu}\\) and the second row and second column of \\(\\boldsymbol{\\Sigma}\\). Note that \\(\\Sigma_{\\rm marg}^{-1}\\) is trivially \\(1/V[X_1] = 1/\\sigma_1^2\\) and that the determinant of \\(\\Sigma_{\\rm marg}\\) is trivially \\(V[X_1] = \\sigma_1^2\\). Plugging in these values, we can see that the marginal distribution for \\(X_1\\) has the form of a univariate normal pdf. A conditional distribution of the multivariate normal is itself a univariate or multivariate normal (depending on \\(p\\) and how many axes are being conditioned upon). To derive a conditional distribution, we first identify which axes are being conditioned upon. Call that set of axes \\(v\\); all others we dub \\(u\\). (For instance, perhaps we want to determine the joint pdf of \\(X_1\\) and \\(X_3\\) given that we have set \\(X_2 = x_2\\) and \\(X_4 = x_4\\). Here, \\(u = \\{1,3\\}\\) and \\(v = \\{2,4\\}\\).) We split the mean vector and the covariance matrix into pieces: \\[\\begin{align*} \\boldsymbol{\\mu} &amp;\\rightarrow [ \\boldsymbol{\\mu}_u ~ \\boldsymbol{\\mu}_v ] \\\\ \\boldsymbol{\\Sigma} &amp;\\rightarrow \\left( \\begin{array}{cc} \\boldsymbol{\\Sigma}_{uu} &amp; \\boldsymbol{\\Sigma}_{uv} \\\\ \\boldsymbol{\\Sigma}_{vu} &amp; \\boldsymbol{\\Sigma}_{vv} \\end{array} \\right) \\,. \\end{align*}\\] (So, for instance, \\(\\boldsymbol{\\mu}_v = [ \\mu_2 ~ \\mu_4 ]\\), and \\[ \\boldsymbol{\\Sigma}_{uu} = \\left( \\begin{array}{cc} V[X_1] &amp; \\mbox{Cov}(X_1,X_3) \\\\ \\mbox{Cov}(X_3,X_1) &amp; V[X_3] \\end{array} \\right) \\,, \\] etc.) Given these pieces, we can define the conditional distribution \\(\\mathbf{X}_u \\vert \\mathbf{X}_v = \\mathbf{x}_v\\) as \\[ \\mathbf{X}_u \\vert \\mathbf{X}_v = \\mathbf{x}_v \\sim \\mathcal{N}(\\boldsymbol{\\mu}_c,\\boldsymbol{\\Sigma}_c) \\,, \\] where \\[\\begin{align*} \\boldsymbol{\\mu}_c &amp;= \\boldsymbol{\\mu}_u + \\boldsymbol{\\Sigma}_{uv} \\boldsymbol{\\Sigma}_{vv}^{-1} (\\mathbf{x}_v - \\boldsymbol{\\mu}_v) \\\\ \\boldsymbol{\\Sigma}_c &amp;= \\boldsymbol{\\Sigma}_{uu} - \\boldsymbol{\\Sigma}_{uv} \\boldsymbol{\\Sigma}_{vv}^{-1} \\boldsymbol{\\Sigma}_{vu} \\,. \\end{align*}\\] Multivariate Normal - R Functions (mvtnorm Package) quantity R function call PDF dmvnorm(x,mu,Sigma) CDF pmvnorm(x,mu,Sigma) Inverse CDF qmvnorm(q,mu,Sigma) \\(n\\) iid random samples rmvnorm(n,mu,Sigma) 6.6.1 The Marginal Distribution of a Multivariate Normal Distribution Let’s suppose that we have a three-dimensional normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,4,6\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{ccc} 2 &amp; 0.5 &amp; 1.2 \\\\ 0.5 &amp; 2 &amp; 1 \\\\ 1.2 &amp; 1 &amp; 2 \\end{array} \\right) \\,. \\] What is the marginal distribution for \\((X_1,X_2)\\)? To determine the marginal distribution, we simply remove the third element of \\(\\boldsymbol{\\mu}\\) and the third row and column of \\(\\boldsymbol{\\Sigma}\\); the marginal distribution is a bivariate normal with means \\(\\boldsymbol{\\mu} = \\{2,4\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{cc} 2 &amp; 0.5 \\\\ 0.5 &amp; 2 \\end{array} \\right) \\,. \\] In R, we might visualize the result using the following code: mu &lt;- c(2,4,6) (Sigma &lt;- matrix(c(2,0.5,1.2,0.5,2,1,1.2,1,2),nrow=3)) ## [,1] [,2] [,3] ## [1,] 2.0 0.5 1.2 ## [2,] 0.5 2.0 1.0 ## [3,] 1.2 1.0 2.0 keep &lt;- c(1,2) mu.marg &lt;- mu[keep] Sigma.marg &lt;- Sigma[keep,keep] # TBD - metR library for contour labels x1.plot &lt;- seq(-1.5,5.5,by=0.05) x2.plot &lt;- seq(0.5,7.5,by=0.05) x.plot &lt;- expand.grid(x1=x1.plot,x2=x2.plot) library(mvtnorm) x.plot$fx &lt;- matrix(dmvnorm(x.plot,mu.marg,Sigma.marg),ncol=1) ggplot(data=x.plot,aes(x=x1,y=x2)) + geom_contour(aes(z=fx),col=&quot;turquoise&quot;,breaks=seq(0.01,0.08,by=0.01)) + metR::geom_text_contour(aes(z=fx),skip=0.75,col=&quot;turquoise&quot;,stroke=0.2,stroke.color=&quot;azure2&quot;) + labs(x=expression(x[1]),y=expression(x[2])) + base_theme Figure 6.8: Contour plot indicating the location and orientation of a bivariate normal distribution with mean \\(\\boldsymbol{\\mu} = \\{2,4\\}\\) and covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center. 6.6.2 The Conditional Distribution of a Multivariate Normal Distribution Let’s assume the same setting as in the previous example. What is the conditional distribution of \\(X_1\\) and \\(X_3\\) given that \\(X_2 = 5\\)? Because of the complexity of the calculation, we will answer this question using R code only, following the mathematical prescription given in the text above. mu &lt;- c(2,4,6) Sigma &lt;- matrix(c(2,0.5,1.2,0.5,2,1,1.2,1,2),nrow=3) u &lt;- c(1,3) v &lt;- c(2) v.coord &lt;- c(5) mu.u &lt;- mu[u] mu.v &lt;- mu[v] Sigma.uu &lt;- Sigma[u,u] Sigma.uv &lt;- Sigma[u,v] Sigma.vu &lt;- Sigma[v,u] Sigma.vv &lt;- Sigma[v,v] mu.c &lt;- mu.u + Sigma.uv %*% solve(Sigma.vv) %*% (v.coord - mu.v) (Sigma.c &lt;- Sigma.uu - Sigma.uv %*% solve(Sigma.vv) %*% Sigma.vu) ## [,1] [,2] ## [1,] 1.875 0.95 ## [2,] 0.950 1.50 x1.plot &lt;- seq(-1.5,5.5,by=0.05) x2.plot &lt;- seq(3,10,by=0.05) x.plot &lt;- expand.grid(x1=x1.plot,x2=x2.plot) library(mvtnorm) x.plot$fx &lt;- matrix(dmvnorm(x.plot,mu.c,Sigma.c),ncol=1) ggplot(data=x.plot,aes(x=x1,y=x2)) + geom_contour(aes(z=fx),col=&quot;turquoise&quot;,breaks=seq(0.01,0.11,by=0.01)) + metR::geom_text_contour(aes(z=fx),skip=0.8,col=&quot;turquoise&quot;,stroke=0.2,stroke.color=&quot;azure2&quot;) + labs(x=expression(x[1]),y=expression(x[3])) + base_theme Figure 6.9: Contour plot indicating the location and orientation of a bivariate normal distribution with conditional mean \\(\\boldsymbol{\\mu} = \\{2.25,6.50\\}\\) and conditional covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center. 6.6.3 The Calculation of Sample Covariance Let’s assume that we observe \\(n = 40\\) iid data drawn from a bivariate normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,1\\}\\) and covariance matrix \\[ \\boldsymbol{\\Sigma} = \\left( \\begin{array}{cc} 1 &amp; 0.5 \\\\ 0.5 &amp; 2 \\end{array} \\right) \\,. \\] Furthermore, we assume that the covariance matrix is unknown to us. How would we estimate this matrix? set.seed(101) n &lt;- 40 mu &lt;- c(2,1) Sigma &lt;- matrix(c(1,0.5,0.5,2),nrow=2) X &lt;- rmvnorm(n,mu,Sigma) head(round(X,3),3) # x1: column 1 | x2: column2 ## [,1] [,2] ## [1,] 1.798 1.704 ## [2,] 1.385 1.158 ## [3,] 2.551 2.707 df &lt;- data.frame(x=X[,1],y=X[,2]) ggplot(data=df,aes(x=x,y=y)) + geom_point(col=&quot;blue&quot;,size=3) + labs(x=expression(x[1]),y=expression(x[2])) + theme(axis.title=element_text(size = rel(1.25))) Figure 6.10: Sample of \\(n = 40\\) iid data drawn from a bivariate normal distribution with means \\(\\boldsymbol{\\mu} = \\{2,1\\}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) given in the main body of the text. First, let’s sample some data. See Figure 6.10. The sample covariance \\(C_{jk}\\) between variables \\(j\\) and \\(k\\) is given by \\[ C_{jk} = \\frac{1}{n-1} \\sum_{i=1}^n (X_{ji} - \\bar{X}_j)(X_{ki} - \\bar{X}_k) \\,. \\] So, for instance, for our data we would determine \\(C_{12}\\) in R as follows: (1/(n-1))*sum((X[,1]-mean(X[,1]))*(X[,2]-mean(X[,2]))) ## [1] 0.389421 This calculation matches that done by the R function cov(): cov(X) ## [,1] [,2] ## [1,] 0.8057418 0.389421 ## [2,] 0.3894210 1.653003 The sample covariance matrix is an unbiased estimator of the true covariance. We mention for completeness that just as the (scaled) sample variance \\((n-1)S^2/\\sigma^2\\) is chi-square-distributed for \\(n-1\\) degrees of freedom, the sample covariance matrix is sampled from a Wishart distribution, a multivariate generalization of the chi-square distribution. "],["further-conceptual-details-optional.html", "7 Further Conceptual Details (Optional) 7.1 Types of Convergence 7.2 The Central Limit Theorem 7.3 Asymptotic Normality of Maximum Likelihood Estimates 7.4 Point Estimation: (Relative) Efficiency 7.5 Sufficient Statistics", " 7 Further Conceptual Details (Optional) 7.1 Types of Convergence There are two primary types of convergence which interest us in this book: convergence in probability and convergence in distribution. Let \\(X_1,X_2,\\ldots\\) be a sequence of random variables, and let \\(Y\\) be some other random variable. (We note that the concept of a sequence can be initially confusing for students: here, \\(X_n\\) is a statistic formed from a set of data with sample size \\(n\\). A classic example of a sequence is \\(\\bar{X}_1,\\bar{X}_2,...\\); the first element is the mean for a sample of size 1 (the datum itself!), the second element is the mean that we compute after we independently sample a second datum from the same underlying distribution to go along with the first, etc.) In addition, let \\(F_{X_n}\\) denote the cumulative distribution function for \\(X_n\\) and \\(F_Y\\) denote the cdf for \\(Y\\). We say that \\(X_n\\) converges in probability to \\(Y\\) if for every \\(\\epsilon &gt; 0\\), \\[ P(\\vert X_n - Y \\vert &gt; \\epsilon) \\rightarrow 0 \\] as \\(n \\rightarrow \\infty\\); i.e., \\(X_n \\stackrel{p}{\\rightarrow} Y\\). \\(X_n\\) converges in distribution to \\(Y\\) if \\[ \\lim_{n \\rightarrow \\infty} F_{X_n}(u) = F_Y(u) \\] for all values of \\(u\\) for which \\(F_Y(u)\\) is continuous; i.e., \\(X_n \\stackrel{d}{\\rightarrow} Y\\). An example of convergence in probability is the weak law of large numbers, which states that \\(\\bar{X}_n \\stackrel{p}{\\rightarrow} \\mu\\). (This is a “weak” statement because it does not invoke “almost sure” convergence, a concept beyond the scope of this book.) What is “weak” about this statement? The statement \\(P(\\vert \\bar{X}_n - Y \\vert &gt; \\epsilon) \\rightarrow 0\\), where \\(Y\\) has mean \\(\\mu\\), is not as restrictive a statement as others that one can make; for any chosen value of \\(\\epsilon\\), \\(\\vert \\bar{X}_n - Y \\vert\\) can stray so as to be greater than \\(\\epsilon\\) an infinite amount of times as \\(n \\rightarrow \\infty\\). In other words, \\(\\bar{X}\\) asymptotically approaches \\(\\mu\\), but is still “allowed” to substantially deviate from that value for any finite \\(n\\). 7.2 The Central Limit Theorem We introduce the Central Limit Theorem in Chapter 2. It states that if we have \\(n\\) iid random variables \\(\\{X_1,\\ldots,X_n\\}\\) with mean \\(E[X_i] = \\mu\\) and finite variance \\(V[X_i] = \\sigma^2 &lt; \\infty\\), and if \\(n\\) is sufficiently large, then \\(\\bar{X}\\) converges in distribution to a standard normal random variable: \\[ \\lim_{n \\rightarrow \\infty} P\\left(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\leq z \\right) = \\Phi(z) \\,. \\] We can prove this using moment-generating functions. Let \\(X_i \\stackrel{iid}{\\sim} P(\\mu,\\sigma^2)\\), where \\(P\\) is some distribution with finite variance, and let \\[ Y = \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} = \\frac{1}{\\sqrt{n}} \\left(\\frac{n\\bar{X} - n\\mu}{\\sigma}\\right) = \\frac{1}{\\sqrt{n}} \\left(\\frac{\\sum_{i=1}^n X_i - n\\mu}{\\sigma}\\right) = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} = \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n Z_i \\,. \\] Since the \\(Z_i\\)’s are standardized random variables, by definition \\(E[Z_i] = 0\\) and \\(V[Z_i]\\) = 1. Here’s where things “break down” if we do not know \\(\\sigma\\), but instead plug \\(S\\) in in CLT-related problems: if we use \\(S\\), then \\(V[X_i] \\neq 1\\). However, a theoretical result known as Slutsky’s theorem saves us here. As we are determining here and below, \\[ \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\stackrel{d}{\\rightarrow} Z \\sim \\mathcal{N}(0,1) \\,, \\] and hence \\[ \\sqrt{n}(\\bar{X} - \\mu) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}(0,\\sigma^2) \\,. \\] Furthermore, \\(S^2 \\stackrel{p}{\\rightarrow} \\sigma^2\\) (by the weak law of large numbers), or equivalently \\(S \\stackrel{p}{\\rightarrow} \\sigma\\) (by the continuous mapping theorem). Given these pieces of information, Slutsky’s theorem tells us that \\(\\sqrt{n}(\\bar{X} - \\mu)/S \\stackrel{d}{\\rightarrow} Y/\\sigma\\), a normally distributed random variable with mean 0 and variance 1. Hence eventually the CLT is valid, even when we plug in \\(S\\)! To determine the distribution of \\(Y\\), we use the method of moment-generating functions: \\[ m_Y(t) = m_{Z_1}\\left(\\frac{t}{\\sqrt{n}}\\right) \\cdots m_{Z_n}\\left(\\frac{t}{\\sqrt{n}}\\right) = \\left[m_{Z_i}\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^n \\,. \\] “Wait,” one might say. “We don’t know the mgf for the quantity \\(Z_i\\), so how can this possibly be helpful?” It is because we can work with the expected value and variance to get at the final result. First, \\[\\begin{align*} m_{Z_i}\\left(\\frac{t}{\\sqrt{n}}\\right) &amp;= m_{Z_i}\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} + m_{Z_i}&#39;\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} \\frac{t}{\\sqrt{n}} + m_{Z_i}&#39;&#39;\\left.\\left(\\frac{t}{\\sqrt{n}}\\right)\\right|_{t=0} \\frac{t^2}{2n} + \\cdots \\\\ &amp;\\approx m_{Z_i}(0) + E[Z_i] \\frac{t}{\\sqrt{n}} + E[Z_i^2] \\frac{t^2}{2n} \\\\ &amp;= E[\\exp(0Z_i)] + 0 + (V[Z_i] + (E[Z_i])^2) \\frac{t^2}{2n} \\\\ &amp;= 1 + V[Z_i] \\frac{t^2}{2n} = 1 + \\frac{t^2}{2n} \\,. \\end{align*}\\] Thus \\[ m_Y(t) \\approx \\left[ 1 + \\frac{t^2}{2n} \\right]^n = \\left[ 1 + \\frac{t^2/2}{n} \\right]^n \\,, \\] and, as \\(n \\rightarrow \\infty\\), \\[ \\lim_{n \\rightarrow \\infty} \\left[ 1 + \\frac{t^2/2}{n} \\right]^n = \\exp\\left(\\frac{t^2}{2}\\right) \\,. \\] This is the moment-generating function for a standard normal…hence \\(Y\\) converges in distribution to a standard normal random variable and \\(\\bar{X}\\) converges in distribution to normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). 7.3 Asymptotic Normality of Maximum Likelihood Estimates When discussing point estimation in Chapter 2, we declared that as \\(n \\rightarrow \\infty\\), the maximum likelihood estimate for a parameter \\(\\theta\\) converges in distribution to a normal random variable: \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) ~\\mbox{or}~ (\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y&#39; \\sim \\mathcal{N}\\left(0,\\frac{1}{nI(\\theta)}\\right) \\,. \\] Here, we sketch out why this is true. As we will see, this result rests upon fundamental concepts covered earlier in this book and chapter: the weak law of large numbers (aka, convergence in probability), and the central limit theorem. We start with the log-likelihood function \\(\\ell(\\theta \\vert \\mathbf{x})\\). (However, to simplify the notation in what follows, we will drop the \\(\\mathbf{x}\\) for now.) By definition, the MLE for \\(\\theta\\) is that value for which \\(\\ell&#39;(\\theta) = \\ell&#39;(\\hat{\\theta}_{MLE}) = 0\\). The Taylor expansion of \\(\\ell&#39;(\\theta)\\) around \\(\\hat{\\theta}_{MLE}\\) is \\[ \\ell&#39;(\\theta) \\approx \\ell&#39;(\\hat{\\theta}_{MLE}) + (\\theta - \\hat{\\theta}_{MLE})\\ell&#39;&#39;(\\theta) + \\cdots = (\\theta - \\hat{\\theta}_{MLE})\\ell&#39;&#39;(\\theta) + \\cdots \\,. \\] We rearrange terms (and introduce the factor \\(\\sqrt{n}\\)): \\[\\begin{align*} (\\hat{\\theta}_{MLE}-\\theta)\\ell&#39;&#39;(\\theta) &amp;\\approx -\\ell&#39;(\\theta) \\\\ \\sqrt{n} (\\hat{\\theta}_{MLE}-\\theta)\\ell&#39;&#39;(\\theta) &amp;\\approx -\\sqrt{n}\\ell&#39;(\\theta) \\\\ \\sqrt{n} (\\hat{\\theta}_{MLE}-\\theta) &amp;\\approx \\frac{-\\sqrt{n}\\ell&#39;(\\theta)}{\\ell&#39;&#39;(\\theta)} = \\frac{\\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta)}{-\\frac{1}{n}\\ell&#39;&#39;(\\theta)} \\,. \\end{align*}\\] (Note that we reversed \\(\\theta\\) and \\(\\hat{\\theta}_{MLE}\\) in the parentheses!) Let’s look at the numerator and the denominator separately. For the numerator: \\[\\begin{align*} \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) &amp;= \\sqrt{n}\\left(\\frac{\\ell&#39;(\\theta \\vert \\mathbf{x})}{n} - 0\\right) \\\\ &amp;= \\sqrt{n}\\left(\\frac{\\ell&#39;(\\theta \\vert \\mathbf{x})}{n} - E[\\ell&#39;(\\theta \\vert x_i)] \\right) \\\\ &amp;= \\sqrt{n}\\left(\\frac{\\sum_{i=1}^n \\ell&#39;(\\theta \\vert x_i)}{n} - E[\\ell&#39;(\\theta \\vert x_i)] \\right) \\,. \\end{align*}\\] Here we utilize the results that, e.g., \\(\\ell&#39;(\\theta \\vert \\mathbf{x})\\) equals the sum of the \\(\\ell&#39;\\)’s for each datum, and that the average value of the slope for the likelihood function is zero. What we have written is equivalent in form to \\[ \\sqrt{n} (\\bar{X} - \\mu) \\] and given the CLT, we know that as \\(n \\rightarrow \\infty\\), this quantity converges in distribution to normal random variable \\(Y \\sim \\mathcal{N}(0,\\sigma^2)\\). Thus \\[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,V[\\ell&#39;(\\theta \\vert x_i)]\\right) \\,, \\] and, since \\(V[\\ell&#39;(\\theta \\vert x_i)] = E[(\\ell&#39;(\\theta \\vert x_i))^2] - (E[\\ell&#39;(\\theta \\vert x_i)])^2 = E[(\\ell&#39;(\\theta \\vert x_i))^2] = I(\\theta)\\), \\[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,I(\\theta)\\right) \\,. \\] For the denominator, we can utilize the weak law of large numbers: \\[ -\\frac{1}{n} \\ell&#39;&#39;(\\theta \\vert \\mathbf{x}) = -\\frac{1}{n} \\left( \\sum_{i=1}^n \\ell&#39;&#39;(\\theta \\vert x_i) \\right) \\stackrel{p}{\\rightarrow} -E[\\ell&#39;&#39;(\\theta \\vert x_i)] = I(\\theta) \\,. \\] At last, we can determine the variance of the ratio: \\[ \\lim_{n \\rightarrow \\infty} V[\\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta)] = V\\left[ \\frac{\\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta)}{I(\\theta)} \\right] = \\frac{1}{I(\\theta)^2} V\\left[ \\frac{1}{\\sqrt{n}}\\ell&#39;(\\theta) \\right] = \\frac{1}{I(\\theta)^2} I(\\theta) = \\frac{1}{I(\\theta)} \\,. \\] We combine this information with the finding above that the numerator in the ratio is, asymptotically, a normally distributed random variable with mean 0 and variance \\(I(\\theta)\\) to write \\[ \\sqrt{n}(\\hat{\\theta}_{MLE}-\\theta) \\stackrel{d}{\\rightarrow} Y \\sim \\mathcal{N}\\left(0,\\frac{1}{I(\\theta)}\\right) \\,. \\] 7.4 Point Estimation: (Relative) Efficiency The efficiency of an unbiased estimator of a parameter \\(\\theta\\) is \\[ e(\\hat{\\theta}) = \\frac{1/I(\\theta)}{V[\\hat{\\theta}]} \\,, \\] i.e., the ratio of the minimum possible variance for an unbiased estimator of \\(\\theta\\) (the Cramer-Rao lower bound) to the variance for the estimator in question. If an unbiased estimator attains \\(e(\\hat{\\theta})\\) for all possible values of \\(\\theta\\), the estimator is efficient. (It is also the MVUE!) The relative efficiency is a metric that we can use to compare two unbiased estimators: \\[ e(\\hat{\\theta}_1,\\hat{\\theta}_2) = \\frac{V[\\hat{\\theta}_2]}{V[\\hat{\\theta}_1]} \\,. \\] If \\(e(\\hat{\\theta}_1,\\hat{\\theta}_2) &gt; 1\\), we would opt to use \\(\\hat{\\theta}_1\\); otherwise, if \\(e(\\hat{\\theta}_1,\\hat{\\theta}_2) &lt; 1\\), we would opt to use \\(\\hat{\\theta}_2\\). (We could use either if the relative efficiency is exactly one.) As an example, let \\(X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)\\), and let \\(\\hat{\\mu}_1 = \\bar{X}\\) and \\(\\hat{\\mu}_2 = (X_1+X_2)/2\\). What is the relative efficiency of these two estimators? We know the general result that \\(V[\\hat{\\mu}_1] = V[\\bar{X}] = \\sigma^2/n\\), and we know that \\(\\hat{\\mu}_2\\) is simply the sample mean of the first two data, so \\(V[\\hat{\\mu}_2] = V[(X_1+X_2)/2] = \\sigma^2/2\\). The relative efficiency is thus \\[ e(\\hat{\\mu}_1,\\hat{\\mu}_2) = \\frac{V[\\hat{\\mu}_2]}{V[\\hat{\\mu}_1]} = \\frac{\\sigma^2/2}{\\sigma^2/n} = \\frac{n}{2} \\,. \\] This value is \\(&gt; 1\\) for all \\(n &gt; 2\\), and it is never less than 1, so we would choose to use \\(\\hat{\\mu}_1 = \\bar{X}\\) as our estimator of the population mean. 7.5 Sufficient Statistics Let us assume that we are given \\(n\\) independent and identically distributed (iid) data sampled from some distribution \\(P\\) whose shape is (without loss of generality) governed by a single parameter \\(\\theta\\): \\[ \\{X_1,\\ldots,X_n\\} \\overset{iid}{\\sim} P(\\theta) \\] A sufficient statistic \\(U\\) is a function of \\(\\mathbf{X}\\) that encapsulates all the information needed to estimate \\(\\theta\\), i.e., if \\(U\\) is sufficient, no other computed statistic would provide any additional information that would help us estimate \\(\\theta\\). Sufficient statistics are not unique: if \\(U\\) is a sufficient statistic, then every one-to-one function \\(f(U)\\) is as well, so long as \\(f(U)\\) does not depend on \\(\\theta\\). In Chapter 3 we show how one finds a sufficient statistic by factorizing the likelihood function, and uses it to define the minimum variance unbiased estimator (the MVUE). In that chapter, we sweep a number of things under the metaphorical rug, such as the fact that to be used to determine the MVUE, a sufficient statistic has to be both minimal and complete. We elaborate on those points below. However, we start by providing an alternate means by which to determine a sufficient statistic. 7.5.1 A Formal Definition of Sufficiency A statistic \\(U(\\mathbf{X})\\) is a sufficient statistic for the parameter \\(\\theta\\) if the conditional distribution of \\(\\mathbf{X}\\) given \\(U(\\mathbf{X})\\) does not depend on \\(\\theta\\), i.e., if \\(P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}) = k)\\) does not depend on \\(\\theta\\). As an example, let \\(\\{X_1,\\ldots,X_n\\} \\overset{iid}{\\sim}\\) Bernoulli(\\(p\\)), and let us propose \\(U(\\mathbf{X}) = \\sum_{i=1}^n X_i\\) as a sufficient ststistic. To determine if it is, we need to determine if \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{P(X_1=x_1,\\ldots,X_n=x_n,U(\\mathbf{X})=k}{P(U(\\mathbf{X}=k)} \\] does not depend on \\(p\\). The first thing to note is that for the numerator to be non-zero, \\(k = x_1+\\cdots+x_n = \\sum_{i=1}^n x_i\\), and thus \\(U(\\mathbf{Y})=k\\) is, from an information standpoint, completely redundant with respect to \\(X_1=x_1,\\ldots,X_n=x_n\\)…so we can ignore it: \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{P(X_1=x_1,\\ldots,X_n=x_n)}{P(U(\\mathbf{X}=k)} \\,. \\] The next thing to note is that because the data are iid, we can rewrite the numerator as a product of probabilities: \\[ P(X_1=x_1,\\ldots,X_n=x_n) = P(X_1=x_1) \\cdots P(X_n=x_n) = \\prod_{i=1}^n P(X_i=x_i) \\,, \\] which means, in the context of Bernoulli random variables, that the numerator is \\[ \\prod_{i=1}^n P(X_i=x_i) = p^k (1-p)^{n-k} \\,. \\] (Why \\(p^k\\), etc.? We are given that \\(U(\\mathbf{X}) = k\\), i.e., that there are \\(k\\) observed successes (and \\(n-k\\) observed failures) in the sample. That takes care of the numerator. Now for the denominator: \\[ P(U(\\mathbf{X} = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\,, \\] because the sum of \\(n\\) Bernoulli-distributed random variables is binomially distributed (as you can show yourself using the method of moment-generating functions). Thus \\[ P(X_1=x_1,\\ldots,X_n=x_n \\vert U(\\mathbf{X}=k) = \\frac{p^k(1-p)^{n-k}}{\\binom{n}{k} p^k(1-p)^{n-k}} = \\frac{1}{\\binom{n}{k}} \\,, \\] which does not depend on the parameter \\(p\\). Thus \\(U(\\mathbf{X}) = \\sum_{i=1}^n X_i\\) is indeed a sufficient statistic for \\(p\\). (The foregoing clearly illustrates why factorization is a preferred means by which to determine sufficient statistics!) 7.5.2 Minimal Sufficiency A question that naturally arises when dealing with sufficient statistics is: if we have many sufficient statistics, which one is the best one? It would be the one that “reduces” the data the most. In a given context, \\(U(\\mathbf{X})\\) is minimal sufficient if, given any another sufficient statistic \\(T(\\mathbf{X})\\), \\(U(\\mathbf{X}) = f(T(\\mathbf{X}))\\). To generate a minimal sufficient statistic, one can make use of the Lehmann-Scheffe theorem. If we have two iid samples of data of the same size, \\(\\{X_1,\\ldots,X_n\\}\\) and \\(\\{Y_1,\\ldots,Y_n\\}\\), then, if the ratio of likelihoods \\[ \\frac{\\mathcal{L}(x_1,\\ldots,x_n \\vert \\theta)}{\\mathcal{L}(y_1,\\ldots,y_n \\vert \\theta)} \\] is constant as a function of \\(\\theta\\) if and only if \\(g(x_1,\\ldots,x_n) = g(y_1,\\ldots,y_n)\\) for a function \\(g(\\cdot)\\), then \\(g(X_1,\\ldots,X_n)\\) is a minimal sufficient statistic for \\(\\theta\\). As an example, let \\(\\{X_1,\\ldots,X_n\\}\\) and \\(\\{Y_1,\\ldots,Y_n\\} \\overset{iid}{\\sim}\\) Poisson(\\(\\lambda\\)). The ratio of likelihoods is \\[ \\frac{\\mathcal{L}(x_1,\\ldots,x_n \\vert \\theta)}{\\mathcal{L}(y_1,\\ldots,y_n \\vert \\theta)} = \\frac{\\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} e^{-\\lambda}}{\\prod_{i=1}^n \\frac{\\lambda^{y_i}}{y_i!} e^{-\\lambda}} = \\frac{\\prod_{i=1}^n \\frac{1}{x_i!}}{\\prod_{i=1}^n \\frac{1}{y_i!}} \\frac{\\lambda^{\\sum_{i=1}^n x_i}}{\\lambda^{\\sum_{i=1}^n y_i}} = \\frac{\\prod_{i=1}^n \\frac{1}{x_i!}}{\\prod_{i=1}^n \\frac{1}{y_i!}} \\lambda^{\\left(\\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i\\right)} \\,. \\] This is only constant as a function of \\(\\lambda\\) if we can “get rid of” \\(\\lambda\\), by setting its exponent to 0…i.e., by setting \\(\\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i\\). Thus \\(g(\\mathbf{x}) = \\sum_{i=1}^n x_i\\) is a minimal sufficient statistic for \\(\\lambda\\). Note that in the context of the present course, any and all sufficient statistics that we define via, e.g., factorization are minimal sufficient. One might ask “what is an example of a sufficient statistic that is not minimally sufficient?” The answer is simple but may not initially be intuitive, in that we naturally think of a statistic as being a single number (the output from applying a given function to a full dataset). However, e.g., a full dataset is a statistic, and it is a sufficient statistic: there is sufficient information in a full dataset so as to compute the likelihood of a population parameter \\(\\theta\\). However, a full dataset is not minimally sufficient! If \\(T(\\mathbf{X}) = \\bar{X}\\), and \\(U(\\mathbf{X}) = \\{X_1,\\ldots,X_n\\}\\), then we cannot define a function \\(f\\) such that \\(\\{X_1,\\ldots,X_n\\} = f(\\bar{X})\\). 7.5.3 Completeness The concept of the completeness of a statistic is a general concept, i.e., it is not limited to sufficient statistics. One can think of it as the idea that each value of \\(\\theta\\) in a distribution maps to a distinct pmf or pdf. We bring up completeness here because the concept appears in the context of determining an MVUE: the sufficient statistic that we find via, e.g., factorization technically needs to be both minimal and complete. Let \\(f_U(u \\vert \\theta)\\) be the family of pmfs or pdfs for the statistic \\(U(\\mathbf{X})\\). \\(U\\) is dubbed a complete statistic if \\(E[g(U)] = 0\\) for all \\(\\theta\\) implies \\(P(g(U) = 0) = 1\\) for all \\(\\theta\\). As an example, let \\(U \\sim\\) Binomial(\\(n,p\\)), with \\(0 &lt; p &lt; 1\\), and let \\(g(\\cdot)\\) be a function such that \\(E[g(U)] = 0\\). Then \\[ 0 = E[g(U)] = \\sum_{u=0}^n g(u) \\binom{n}{u} p^u (1-p)^{n-u} = (1-p)^n \\sum_{u=0}^n g(u) \\binom{n}{u} \\left( \\frac{p}{1-p}\\right)^u \\,. \\] For any choice of \\(u\\) and \\(n\\), \\(\\binom{n}{u} [p/(1-p)]^u &gt; 0\\). So, in order to have \\(E[g(U)] = 0\\) for all \\(p\\), \\(g(u) = 0\\) for all \\(u\\), i.e., \\(P(g(U) = 0) = 1\\). Therefore \\(U\\) is a complete statistic. Note that a complete statistic is also minimal sufficient, but the converse is not necessarily true: a minimal sufficient statistic may not be complete. 7.5.4 The Rao-Blackwell Theorem The Rao-Blackwell theorem implies that an unbiased estimator for \\(\\theta\\) with a small variance is, or can be made to be, a function of a sufficient statistic: if we have an unbiased estimator for \\(\\theta\\), we might be able to improve it using the prescription of the theorem. In the end, the theorem provides a mathematically more challenging route to defining a minimum variance unbiased estimator than what we show in the main text\\(-\\)likelihood factorization followed by the “de-biasing” of a sufficient statistic\\(-\\)and hence we relegate it here, to the chapter of optional material. Let \\(\\hat\\theta\\) be an unbiased estimator for \\(\\theta\\) such that \\(V[\\hat\\theta] &lt; \\infty\\). If \\(U\\) is a sufficient statistic for \\(\\theta\\), define \\(\\hat\\theta^* = E[\\hat\\theta \\vert U]\\). Then, for all \\(\\theta\\), \\[ E[\\hat\\theta^*] = \\theta ~~~ \\text{and} ~~~ V[\\hat\\theta^*] \\leq V[\\hat\\theta] \\,. \\] Let’s suppose we have sampled \\(n\\) iid data from a Binomial distribution with number of trials \\(k\\) and proportion \\(p\\). We propose an estimator for \\(p\\): \\(X_1/k\\). First, is \\(\\hat{p} = X_1/k\\) unbiased? We have that \\[ E[\\hat{p}-p] = E\\left[\\frac{X_1}{k}\\right] - p = \\frac{1}{k}E[X_1] - p = \\frac{1}{k}kp - p = 0 \\,. \\] So…yes. Second, is \\(V[\\hat{p}] &lt; \\infty\\)? \\(V[X_1/k] = V[X_1]/k^2 = p(1-p)/k\\)…so, also yes. We are thus free to use the Rao-Blackwell theorem to improve upon \\(\\hat{p} = X_1/k\\): \\[ \\hat\\theta^* = E[\\hat\\theta \\vert U] = E\\left[\\frac{X_1}{k} \\left| ~ U = \\sum_{i=1}^n X_i \\right. \\right] \\,. \\] Since we are given the sum of the data, and since the data are iid, we would expect, on average, that \\(X_1\\) has the value \\(U/n\\), and thus that \\(X_1/k\\) has the value \\(U/(nk)\\). Thus \\[ \\hat\\theta^* = \\frac{\\bar{X}}{k} \\] is the MVUE for \\(p\\). 7.5.5 Exponential Family of Distributions The exponential family is a set of distributions whose probability mass or density functions can be expressed as either \\[ f_X(x \\vert \\theta) = a(\\theta) b(x) \\exp[ c(\\theta) t(x) ] \\] or \\[ f_X(x \\vert \\theta) = b(x) \\exp[ c(\\theta) t(x) - d(\\theta)] \\,. \\] These two forms are equivalent, with \\(a(\\theta) = \\exp[-d(\\theta)]\\). Note that the domain of \\(f_X(x \\vert \\theta)\\) cannot depend on \\(\\theta\\), meaning that distributions like the uniform and Pareto cannot be members of the exponential family even if their density functions could be expressed in the form above. There are many concepts related to the exponential family that are beyond the scope of this book. The one factoid that we will note here is that sufficient statistics for the parameter \\(\\theta\\) can be read directly off a distribution’s exponential family form. Assuming we have \\(n\\) iid random variables, we can write the likelihood and factorize it, and isolate the sufficient statistic: \\[\\begin{align*} \\mathcal{L}(\\theta \\vert \\mathbf{x}) &amp;= \\prod_{i=1}^n f_X(x_i \\vert \\theta) \\\\ &amp;= \\underbrace{[a(\\theta)]^n \\exp\\left[ \\sum_{i=1}^n c(\\theta) t(x_i) \\right]}_{g(\\theta,u)} \\underbrace{\\left( \\prod_{i=1}^n b(x_i) \\right)}_{h(\\mathbf{x})} \\,. \\end{align*}\\] The sufficient statistic is thus \\[ U = \\sum_{i=1}^n t(X_i) \\,. \\] As an example, what is the sufficient statistic for the mean of an exponential distribution? The exponential pdf is \\((1/\\beta)\\exp(-x/\\beta)\\) for \\(\\beta &gt; 0\\) and \\(x \\geq 0\\). If we use the first exponential family form above, we can read off that \\[ a(\\beta) = \\frac{1}{\\beta} ~~ b(x) = 1 ~~ c(\\beta) = -\\frac{1}{\\beta} ~~ t(x) = x \\,. \\] Thus, assuming we have an iid sample of size \\(n\\), the sufficient statistic would be \\(U = \\sum_{i=1}^n t(X_i) = \\sum_{i=1}^n X_i\\). "],["appendix-a-table-of-symbols.html", "Appendix A: Table of Symbols", " Appendix A: Table of Symbols Symbol What the Symbol Represents \\(\\theta\\) parameter(s) that dictate the shape/location of a distribution \\(\\hat{\\theta}\\) parameter estimate(s) \\(\\alpha\\) the confidence coefficient or hypothesis test Type I error \\(\\beta_i\\) the \\(i^{\\rm th}\\) coefficient in a regression model \\(\\Gamma(x)\\) the gamma function \\((= (x-1)!\\) if \\(x\\) is an integer) \\(\\epsilon\\) the error term in a linear regression model \\(\\mu\\) the distribution mean (or population mean) \\((= E[X])\\) \\(\\mu_k&#39;\\) the \\(k^{\\rm th}\\) distribution moment \\((= E[X^k])\\) \\(\\nu\\) the number of degrees of freedom \\(\\sigma\\) the distribution standard deviation (or population standard deviation) \\(\\sigma^2\\) the distribution variance (or population variance) \\((= V[X])\\) \\(\\boldsymbol{\\Sigma}\\) the covariance matrix \\(\\Omega\\) the sample space corresponding to an experiment \\(A\\),\\(B\\) generic symbols for events in a sample space \\(B(\\alpha,\\beta)\\) the beta function \\(B[\\hat{\\theta}]\\) the bias of the estimator \\(\\hat{\\theta}\\) \\(E[X]\\) the expected value of the random variable \\(X\\) \\((= \\mu)\\) erf(\\(x\\)) the error function \\(f_X(x)\\) the probability density function for the continuous random variable \\(X\\) \\(F_X(x)\\) the cumulative distribution function for the random variable \\(X\\) iid independent and identically distributed random variables MSE mean-squared error (\\(V[\\hat{\\theta}] + B[\\hat{\\theta}]^2\\)) \\(P(a \\leq X \\leq b)\\) the probability that \\(X \\in [a,b]\\) \\(p_X(x)\\) the probability mass function for the discrete random variable \\(X\\) \\(S\\) the standard deviation of \\(n\\) sampled data \\(S^2\\) the variance of \\(n\\) sampled data \\(T\\) a datum sampled from a \\(t\\) distribution \\(U\\) a sufficient statistic, found via likelihood factorization \\(V[X]\\) the variance of the random variable \\(X\\) \\((= \\sigma^2)\\) \\(W\\) a datum sampled from a chi-square distribution \\(X\\) a single datum (or random variable), sampled from a pmf or pdf \\(X_i\\) the \\(i^{\\rm th}\\) datum of \\(n\\) sampled data \\(X_{(i)}\\) the \\(i^{\\rm th}\\) smallest datum of \\(n\\) sampled data \\(\\bar{X}\\) the mean of \\(n\\) sample \\(Y\\) a statistic, a function of the data in a data sample \\(\\mathcal{L}(\\theta \\vert \\mathbf{x}\\) the likelihood of \\(\\theta\\) given data coordinates \\(\\mathbf{x}\\) \\(\\ell(\\theta \\vert \\mathbf{x}\\) the log-likelihood \\(\\log \\mathcal{L}(\\theta \\vert \\mathbf{x})\\) \\(\\cup\\) “or” in a probability statment \\(\\cap\\) “and” in a probability statment \\(|\\) a condition, stated to the right of \\(|\\), in a probability statement \\(\\in\\) “in,” as in \\(x \\in [a,b]\\) or “\\(x\\) is in the range \\(a\\) to \\(b\\)” \\(\\prod_{i=1}^n x_i\\) the product \\(x_1 \\cdot x_2 \\cdot \\cdots \\cdot x_n\\) \\(\\sum_{i=1}^n x_i\\) the summation \\(x_1 \\cdot x_2 \\cdot \\cdots \\cdot x_n\\) "],["appendix-b-root-finding-algorithm-for-confidence-intervals.html", "Appendix B: Root-Finding Algorithm for Confidence Intervals", " Appendix B: Root-Finding Algorithm for Confidence Intervals confint &lt;- function(FUN,param,stat,lpb=-1.e+4,upb=1.e+4, bound=&quot;two-sided&quot;,alpha=0.05,tolb=1.e-6,tol=1.e-6,...) { f &lt;- function(x,FUN,param,stat,c,...) { pars &lt;- function(...) { list(...) } args &lt;- pars(...) args &lt;- append(args,stat) names(args)[length(args)] &lt;- &quot;q&quot; if ( is.null(param) == FALSE ) { args &lt;- append(args,x) names(args)[length(args)] &lt;- param } do.call(FUN,args) - c } findRoot &lt;- function(f,lpb,upb,tolb,FUN,param,stat,q,tol,...) { tryCatch( {uniroot(f,interval=c(lpb+tolb,upb-tolb),tol=tol,FUN,param,stat,q,...)$root}, warning = function(w) { cat(&quot;The interval bounds are invalid.\\n&quot;); return(NULL); }, error = function(e) { cat(&quot;Root not computable.\\n&quot;); return(NULL) } ) } if ( bound == &quot;two-sided&quot; ) { lo &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha/2,tol,...) if ( is.null(lo) ) { return(NULL) } hi &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha/2,tol,...) if ( hi &lt; lo ) { tmp = hi; hi = lo; lo = tmp; } return(c(lo,hi)) } else if ( bound == &quot;lower&quot; ) { lo.inc &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha,tol,...) if ( is.null(lo.inc) ) { return(NULL) } lo.dec &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha,tol,...) lo &lt;- min(c(lo.inc,lo.dec)) return(lo) } else if ( bound == &quot;upper&quot; ) { hi.inc &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,alpha,tol,...) if ( is.null(hi.inc) ) { return(NULL) } hi.dec &lt;- findRoot(f,lpb,upb,tolb,FUN,param,stat,1-alpha,tol,...) hi &lt;- max(c(hi.inc,hi.dec)) return(hi) } else { stop(&quot;invalid choice for bound&quot;) } return(NULL) } "],["bibliography.html", "Bibliography", " Bibliography Benjamini, Y., and Hochberg, Y. (1995). Controlling the False Discovery Rate: a Practical and Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society, Series B 57 289-300. Casella, G., and Berger, R. L. (2002). Statistical Inference, 2nd edition. Belmont, CA: Duxbury. Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics 7 1-26. Kolmogorov, A. (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung (in German; translated as Foundations of the Theory of Probability in 1950). Berlin: Julius Springer. Ross, S. (1988). A First Course in Probability, 3rd edition. New York, NY: Macmillan. Sharpiro, S. S., and Wilk, M. B. (1965). An Analysis of Variance Test for Normality (Complete Samples). Biometrika 52 591-611. Thulin, M. (2014). The Cost of Using Exact Confidence Intervals for a Binomial Proportion. Electronic Journal of Statistics 8 817-840. Wasserman, L. (2004). All of Statistics, 1st edition. New York, NY: Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

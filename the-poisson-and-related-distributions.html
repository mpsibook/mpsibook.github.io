<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-binomial-and-related-distributions.html"/>
<link rel="next" href="the-uniform-distribution.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.19" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#exercises"><i class="fa fa-check"></i><b>1.19</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
<li class="chapter" data-level="2.19" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-exponential-family-of-distributions"><i class="fa fa-check"></i><b>2.19</b> The Exponential Family of Distributions</a>
<ul>
<li class="chapter" data-level="2.19.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exponential-family-with-a-vector-of-parameters"><i class="fa fa-check"></i><b>2.19.1</b> Exponential Family with a Vector of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="2.20" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exercises-1"><i class="fa fa-check"></i><b>2.20</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.2.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-binomial-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>3.2.5</b> The Binomial Distribution as Part of the Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sufficiency-principle-examples-of-when-we-cannot-reduce-data"><i class="fa fa-check"></i><b>3.6.3</b> The Sufficiency Principle: Examples of When We Cannot Reduce Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.5</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#utilizing-simulations-when-data-reduction-is-not-possible"><i class="fa fa-check"></i><b>3.8.6</b> Utilizing Simulations When Data Reduction is Not Possible</a></li>
<li class="chapter" data-level="3.8.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.7</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises-2"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-poisson-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>4.2.1</b> The Poisson Distribution as Part of the Exponential Family</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.2</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
<li class="chapter" data-level="4.6.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation"><i class="fa fa-check"></i><b>4.6.5</b> Confidence Interval Estimation via Simulation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#exercises-3"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.3.1</b> Sufficient Statistics for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-using-an-order-statistic"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Using an Order Statistic</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-test-for-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> Hypothesis Test for the Uniform Distribution Upper Bound</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#using-numerical-integration-to-characterize-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Using Numerical Integration to Characterize a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.4</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
<li class="chapter" data-level="6.4.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#mutual-information"><i class="fa fa-check"></i><b>6.4.3</b> Mutual Information</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#multiple-comparisons"><i class="fa fa-check"></i><b>7.6</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>7.6.1</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><a href="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><i class="fa fa-check"></i>Appendix B: Confidence Interval and Hypothesis Test Reference Tables</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-1"><i class="fa fa-check"></i>Chapter 1</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-2"><i class="fa fa-check"></i>Chapter 2</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-6"><i class="fa fa-check"></i>Chapter 6</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-poisson-and-related-distributions" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> The Poisson (and Related) Distributions<a href="the-poisson-and-related-distributions.html#the-poisson-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation-2" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Motivation<a href="the-poisson-and-related-distributions.html#motivation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the challenges of a Prussian soldiers life in the
19th century was avoiding being kicked by horses. This was
no trivial matter: over one 20-year period, 122 soldiers
died from horse-kick-related injuries. The data below are from
the 1925 R. A. Fisher work <em>Statistical Methods for Research Workers</em>;
they are a subset of the original data that were compiled earlier by
the statistician Ladislaus Bortkiewicz.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(N(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">109</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">65</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">22</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(x\)</span> is the number of deaths observed in any one Prussian
army corp in any one year, and <span class="math inline">\(N(x)\)</span> is the number of corps-years
in which <span class="math inline">\(x\)</span> deaths were observed. (<span class="math inline">\(N(x)\)</span>
sums to <span class="math inline">\(20 \cdot 10 = 200\)</span>, reflecting that the compiled data represent
10 army corps observed over a 20-year period.)</p>
<p>The data presented above are an example of a process, i.e., a sequence of
observations, but we can immediately see that unlike the case with
coin flips, this process is <em>not</em> a Bernoulli process.
Thats because the number of possible outcomes is greater than two (0 and 1);
in fact, the number of possible outcomes is countably
infinite, so we could not even call this a multinomial process.
Well, the reader might say, we <em>could</em> simply
discretize the data more finely, so that the number of
possible outcomes is at least finite (multinomial) or better yet
falls to two (Bernoulli). Lets get monthly data, or daily data, or hourly data.
However, there is no time period <span class="math inline">\(\Delta t\)</span> for which the number of possible
outcomes is limited to some maximum value: in theory, an infinite number of
soldiers could die in the same second, or even the same nanosecond, etc.</p>
<p>But lets keep playing with this idea of making the time periods smaller
and smaller. Let the number of time periods into which we divide our
observation interval, <span class="math inline">\(k\)</span>, go to infinity such that the probability of
observing a horse-kick death <span class="math inline">\(p \rightarrow 0\)</span> and such that
<span class="math inline">\(kp \rightarrow \lambda\)</span>, where <span class="math inline">\(\lambda\)</span> is a constant.
Under these conditions, as we will see in the next section,
the binomial distribution transforms
into the Poisson distribution, which Bortkiewicz dubbed
the <em>law of small numbers</em>. Before we go to the section, though, lets
define the Poisson distribution in words: it gives the probability
of observing a particular number of events (counts) in a fixed
interval of space and/or time, assuming there is a constant mean rate
of events and the occurrence of any one event is independent of the
occurrence of other events.</p>
<p>(For those who do not wish to wait until the section on point estimation
to know the final answer: <span class="math inline">\(\hat{\lambda}_{MLE} = 0.61\)</span>, i.e., if the
data are plausibly Poisson distributed [which is another question to ask
entirely!], the true rate of death is estimated
to be 0.61 soldiers per corps per year.)</p>
</div>
<div id="probability-mass-function-1" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Probability Mass Function<a href="the-poisson-and-related-distributions.html#probability-mass-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) <span class="math inline">\(0 \leq p_X(x) \leq 1\)</span> and (b) <span class="math inline">\(\sum_x p_X(x) = 1\)</span>, where the sum is over all values of <span class="math inline">\(x\)</span> in the distributions domain.</em></p>
<p>To derive the Poisson probability mass function, we start by writing
down the binomial distribution, setting <span class="math inline">\(p\)</span> to <span class="math inline">\(\lambda/k\)</span>, where <span class="math inline">\(\lambda\)</span>
is an arbitrarily valued positive constant, and letting <span class="math inline">\(k\)</span> go to infinity:
<span class="math display">\[\begin{align*}
P(X=x) &amp;= \binom{k}{x} p^x (1-p)^{k-x} \\
&amp;= \frac{k!}{x!(k-x)!} \left(\frac{\lambda}{k}\right)^x \left(1-\frac{\lambda}{k}\right)^{k-x} \\
&amp;= \frac{k!}{(k-x)! k^x} \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^{k-x} \\
&amp;= \left(\frac{k}{k}\right) \left(\frac{k-1}{k}\right) \cdots \left(\frac{k-x+1}{k}\right) \left(\frac{\lambda^x}{x!}\right) \left(1-\frac{\lambda}{k}\right)^{k-x} \rightarrow \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^{k-x} ~\mbox{as}~~ k \rightarrow \infty \\
&amp;= \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^k \left(1-\frac{\lambda}{k}\right)^{-x} \rightarrow \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^k ~\mbox{as}~~ k \rightarrow \infty \,.
\end{align*}\]</span>
At this point, we concentrate on the parenthetical term above. Given that
<span class="math display">\[
\lim_{k \rightarrow \infty} \left(1 - \frac{1}{k}\right)^k = e^{-1} \,,
\]</span>
we can state that
<span class="math display">\[
\lim_{k \rightarrow \infty} \left(1 - \frac{1}{k/\lambda}\right)^{k/\lambda} = e^{-1} \implies \lim_{k \rightarrow \infty} \left(1 - \frac{1}{k/\lambda}\right)^k = e^{-k} \,.
\]</span>
We are now in a position to write down the probability mass function
for a Poisson random variable (see Figure <a href="the-poisson-and-related-distributions.html#fig:ppmf">4.1</a>):
<span class="math display">\[
P(X=x) = p_X(x) = \frac{\lambda^x}{x!} e^{-\lambda} ~\mbox{where}~ \lambda &gt; 0 ~\mbox{and}~ x \in [0,\infty) \,.
\]</span>
A Poisson random variable converges in distribution to a normal random variable
as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, a result which affects how Poisson-distributed
data have historically been treated in, e.g., hypothesis test settings.
(We will elaborate on this point when we
return to the chi-square goodness of fit test later in the chapter.)
To indicate that we have sampled a datum from a Poisson
distribution, we write <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>).
The expected value and variance of the Poisson distribution are
<span class="math inline">\(E[X] = \lambda\)</span> and <span class="math inline">\(V[X] = \lambda\)</span>, respectively; the former is
derived below in an example.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ppmf"></span>
<img src="_main_files/figure-html/ppmf-1.png" alt="\label{fig:ppmf}Poisson probability mass functions for $\lambda = 1$ (red), 5 (green), and 10 (blue)." width="50%" />
<p class="caption">
Figure 4.1: Poisson probability mass functions for <span class="math inline">\(\lambda = 1\)</span> (red), 5 (green), and 10 (blue).
</p>
</div>
<hr />
<div id="the-poisson-distribution-as-part-of-the-exponential-family" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> The Poisson Distribution as Part of the Exponential Family<a href="the-poisson-and-related-distributions.html#the-poisson-distribution-as-part-of-the-exponential-family" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that the exponential family of distributions, introduced in
Chapter 2, comprises distributions whose probability mass or density
functions can be written in the form
<span class="math display">\[\begin{align*}
h(x) \exp\left( \eta(\theta)T(x) - A(\theta) \right) \,.
\end{align*}\]</span>
Is the Poisson distribution a member of the larger exponential family
of distributions? We will start our answer to this question by noting that
<span class="math display">\[\begin{align*}
\lambda^x = \exp\left(\log \lambda^x\right) = \exp\left(x \log \lambda\right) \,.
\end{align*}\]</span>
Thus
<span class="math display">\[\begin{align*}
p_X(x \vert \lambda) = \frac{1}{x!} \exp\left(x \log \lambda - \lambda\right) \,.
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
h(x) &amp;= \frac{1}{x!} \\
\eta(\lambda) &amp;= \log \lambda \\
T(x) &amp;= x \\
A(\lambda) &amp;= \lambda \,.
\end{align*}\]</span>
The Poisson distribution is indeed a member of the exponential family.</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-of-a-poisson-random-variable" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> The Expected Value of a Poisson Random Variable<a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the expected value of a discretely distributed random variable is</em>
<span class="math display">\[
E[X] = \sum_x x p_X(x) \,,
\]</span>
<em>where the sum is over all values of <span class="math inline">\(x\)</span> within the domain of the pmf p_X(x). The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(p_X(x)\)</span>.</em></p>
<blockquote>
<p>For a Poisson distribution, the expected value is
<span class="math display">\[
E[X] = \sum_{x=0}^\infty x \frac{\lambda^x}{x!} e^{-\lambda} = \sum_{x=1}^\infty x \frac{\lambda^x}{x!} e^{-\lambda} = \sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!} e^{-\lambda} \,.
\]</span>
The goal is to move constants into or out of
the summation so that the summation becomes one of a
probability mass function over its entire domain.
Here, we move <span class="math inline">\(\lambda\)</span> out of the summation,
and make the substitution <span class="math inline">\(y = x-1\)</span>; the summand then takes
on the form of a Poisson pmf, summed over its entire domain:
<span class="math display">\[
E[X] = \lambda \sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!} e^{-\lambda} = \lambda \sum_{y=0}^\infty \frac{\lambda^{y}}{y!} e^{-\lambda} = \lambda \,.
\]</span>
Note that a similar calculation that starts with the
derivation of <span class="math inline">\(E[X(X-1)]\)</span> yields the Poisson variance.</p>
</blockquote>
</div>
</div>
<div id="cumulative-distribution-function-2" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Cumulative Distribution Function<a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as <span class="math inline">\(F_X(x) = \sum_{y\leq x} p_Y(y)\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>For the Poisson distribution, the cdf is
<span class="math display">\[
F_X(x) = \sum_{y=0}^{\lfloor x \rfloor} p_Y(y) = \sum_{y=0}^{\lfloor x \rfloor} \frac{\lambda^y}{y!} \exp(-\lambda) = \frac{\Gamma(\lfloor x+1 \rfloor,\lambda)}{\lfloor x \rfloor !} \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the
floor function, which returns the
largest integer that is less than or
equal to <span class="math inline">\(x\)</span> (e.g., if <span class="math inline">\(x\)</span> = 8.33, <span class="math inline">\(\lfloor x \rfloor\)</span> = 8),
and where <span class="math inline">\(\Gamma(\cdot,\cdot)\)</span> is the upper incomplete gamma function
<span class="math display">\[
\Gamma(\lfloor x+1 \rfloor,\lambda) = \int_{\lambda}^\infty u^{\lfloor x \rfloor} e^{-u} du \,.
\]</span>
(An example of an <code>R</code> function which computes the upper incomplete
gamma function is <code>incgam()</code> in the <code>pracma</code> package.)
As we are dealing with a probability mass function, the
cdf is a step function, as illustrated in the left panel of
Figure <a href="the-poisson-and-related-distributions.html#fig:poicdf">4.2</a>.
Recall that because of the step-function nature of the cdf,
the form of inequalities in a probabilistic statement matter: e.g.,
<span class="math inline">\(P(X &lt; x)\)</span> and <span class="math inline">\(P(X \leq x)\)</span> will not be the same
if <span class="math inline">\(x\)</span> is zero or a positive integer.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(x = F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span>.
A discrete distribution has no unique inverse cdf; it is convention to
utilize the generalized inverse cdf,</em>
<span class="math inline">\(x = \mbox{inf}\{x : F_X(x) \geq q\}\)</span>,
<em>where inf indicates that the function is to return
the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q\)</span>.</em></p>
<p>In the right panel of Figure <a href="the-poisson-and-related-distributions.html#fig:poicdf">4.2</a>, we display the inverse cdf
for the same distribution used to generate the figure in the left panel
(<span class="math inline">\(\lambda = 2\)</span>). Like the cdf, the inverse cdf for a discrete distribution
is a step function.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poicdf"></span>
<img src="_main_files/figure-html/poicdf-1.png" alt="\label{fig:poicdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a Poisson distribution with $\lambda=2$. Note that because the domain of the Poisson distribution is countably infinite, we do observe any values of $x$ for which $F_X(x) = 1$." width="45%" /><img src="_main_files/figure-html/poicdf-2.png" alt="\label{fig:poicdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a Poisson distribution with $\lambda=2$. Note that because the domain of the Poisson distribution is countably infinite, we do observe any values of $x$ for which $F_X(x) = 1$." width="45%" />
<p class="caption">
Figure 4.2: Illustration of the cumulative distribution function <span class="math inline">\(F_X(x)\)</span> (left) and inverse cumulative distribution function <span class="math inline">\(F_X^{-1}(q)\)</span> (right) for a Poisson distribution with <span class="math inline">\(\lambda=2\)</span>. Note that because the domain of the Poisson distribution is countably infinite, we do observe any values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(F_X(x) = 1\)</span>.
</p>
</div>
<hr />
<div id="computing-probabilities-9" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Computing Probabilities<a href="the-poisson-and-related-distributions.html#computing-probabilities-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Poisson(5), which is <span class="math inline">\(P(4 \leq X &lt; 6)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We first note that due to the form of the inequality, we do <em>not</em> include <span class="math inline">\(X=6\)</span>
in the computation. Thus <span class="math inline">\(P(4 \leq X &lt; 6) = p_X(4) + p_X(5)\)</span>, which equals
<span class="math display">\[
\frac{5^4}{4!}e^{-5} + \frac{5^5}{5!}e^{-5} = \frac{5^4}{4!}e^{-5} \left( 1 + \frac{5}{5} \right) = 2\frac{5^4}{4!}e^{-5} = 0.351\,.
\]</span>
If we utilize <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="the-poisson-and-related-distributions.html#cb253-1" tabindex="-1"></a><span class="fu">dpois</span>(<span class="dv">4</span>,<span class="at">lambda=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">dpois</span>(<span class="dv">5</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.3509347</code></pre>
<blockquote>
<p>Note that we can take advantage of <code>R</code>s vectorization capabilities
by writing</p>
</blockquote>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="the-poisson-and-related-distributions.html#cb255-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dpois</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,<span class="at">lambda=</span><span class="dv">5</span>))</span></code></pre></div>
<pre><code>## [1] 0.3509347</code></pre>
<blockquote>
<p>This approach is far more convenient than writing out a string of
calls to <code>dpois()</code>, particularly when the number of values of
<span class="math inline">\(x\)</span> to sum over becomes large.
We can also utilize cdf functions here: <span class="math inline">\(P(4 \leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4)
= P(X \leq 5) - P(X \leq 3) = F_X(5) - F_X(3)\)</span>, which in <code>R</code> is computed as
follows:</p>
</blockquote>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="the-poisson-and-related-distributions.html#cb257-1" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">5</span>,<span class="at">lambda=</span><span class="dv">5</span>) <span class="sc">-</span> <span class="fu">ppois</span>(<span class="dv">3</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.3509347</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Poisson(5), what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(X \leq a) = 0.9\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we set up the inverse cdf formula:
<span class="math display">\[
P(X \leq a) = F_X(a) = 0.9 ~~ \Rightarrow ~~ a = F_X^{-1}(0.9)
\]</span>
Note that we didnt do anything differently here than we would have done
in a continuous distribution settingand we can proceed directly to
<code>R</code> because it utilizes the generalized inverse cdf algorithm.</p>
</blockquote>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="the-poisson-and-related-distributions.html#cb259-1" tabindex="-1"></a><span class="fu">qpois</span>(<span class="fl">0.9</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
</div>
</div>
<div id="linear-functions-of-random-variables-1" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Linear Functions of Random Variables<a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid Poisson random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>).
What is the distribution of the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>?</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(m_X(t) = E[e^{tX}]\)</span>. Also, if <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> as the mgf for a known family of distributions, then we can immediately identify the distribution for <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for a Poisson random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[
m_X(t) = \exp\left[\lambda\left(e^t-1\right)\right] \,.
\]</span>
(We derive this in an example below.)
Thus the mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
<span class="math display">\[
m_Y(t) = \exp\left[\lambda\left(e^t-1\right)\right] \cdots \exp\left[\lambda\left(e^t-1\right)\right] = \exp\left[(\lambda+\cdots+\lambda)(e^t-1)\right] = \exp\left[n\lambda(e^t-1)\right] \,.
\]</span>
This mgf retains the form of a Poisson mgf. We thus
see that the sum of Poisson-distributed random variables is
itself Poisson distributed with parameter <span class="math inline">\(n\lambda\)</span>, i.e.,
<span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Poisson(<span class="math inline">\(n\lambda\)</span>).</p>
<p>While we can identify the distribution of the sum by name, we cannot
say the same about the sample mean. If
<span class="math inline">\(\bar{X} = \left(\sum_{i=1}^n X_i\right)/n\)</span>, then
<span class="math display">\[
m_{\bar{X}}(t) = \exp\left[n\lambda(e^{t/n}-1)\right] \,.
\]</span>
We cannot identify a named family of distributions with this specific
mgf. However, we <em>do</em> know the distribution: it has a pmf that is identical
in form to that of the Poisson distribution, but has the domain
<span class="math inline">\(\{0,1/n,2/n,...\}\)</span>. (We can derive this result mathematically by making
the transformation <span class="math inline">\(\sum_{i=1}^n X_i \rightarrow (\sum_{i=1}^n X_i)/n\)</span>.)
We could define the pmf ourselves
using our own <code>R</code> function, but there is no real need to: if we wish to
construct a confidence interval for <span class="math inline">\(\lambda\)</span>,
we can simply construct one for <span class="math inline">\(n\lambda\)</span> using the
sum of the data as a statistic, and then divide the bounds by <span class="math inline">\(n\)</span>.
(We could also, in theory, utilize the Central Limit Theorem if
<span class="math inline">\(n \gtrsim 30\)</span>, but there is absolutely no reason to do that to make
inferences about <span class="math inline">\(\lambda\)</span>: we know the distribution of the sum of the data
exactly, and thus there is no need to fall back upon approximations.)</p>
<hr />
<div id="the-moment-generating-function-of-a-poisson-random-variable" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> The Moment-Generating Function of a Poisson Random Variable<a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The moment-generating function for a random variable <span class="math inline">\(X\)</span> is found by
utilizing the Law of the Unconscious Statistician and computing
<span class="math inline">\(E[e^{tX}]\)</span>. If <span class="math inline">\(X\)</span> is a Poisson random variable, then
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^\infty e^{tx} p_X(x) = \sum_{x=0}^\infty e^{tx} \frac{\lambda^x}{x!} e^{-\lambda} = e^{-\lambda} \sum_{x=0}^\infty \frac{\lambda^x}{x!} e^{tx} \\
&amp;= e^{-\lambda} \left[ 1 + \lambda e^t + \frac{\lambda^2}{2!}e^{2t} + \ldots \right] = e^{-\lambda} \left[ 1 + y + \frac{y^2}{2!} + \ldots \right] \\
&amp;= e^{-\lambda} e^y = \exp(-\lambda) \exp(\lambda e^t) = \exp[\lambda(e^t-1)] \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-distribution-of-the-difference-of-two-poisson-random-variables" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> The Distribution of the Difference of Two Poisson Random Variables<a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we are pointing a camera at an object, such as a star.
A star gives off
photons at a particular average
rate <span class="math inline">\(\alpha_S\)</span> (with units of, e.g., photons per second).
Thus if we open the shutter for a length of time <span class="math inline">\(t\)</span>, the number
of photons we observe from the star is a Poisson random variable <span class="math inline">\(S \sim\)</span>
Poisson(<span class="math inline">\(\lambda_S=\alpha_St\)</span>).
But the star is not the only object in the field
of view; there may be other objects in the background that give off photons
at a rate <span class="math inline">\(\alpha_B\)</span>, and the number of photons we observe from the
background will be <span class="math inline">\(B \sim\)</span> Poisson(<span class="math inline">\(\lambda_B=\alpha_Bt\)</span>).
Thus what we record
is not <span class="math inline">\(S\)</span>, but <span class="math inline">\(T = S+B\)</span>so how can we make statistical inferences about
<span class="math inline">\(S\)</span> itself?</p>
</blockquote>
<blockquote>
<p>One possibility is to point the camera to an empty field near the star,
and record some number of photons <span class="math inline">\(B\)</span> over the same amount of time.
Then we can estimate <span class="math inline">\(S\)</span> using
<span class="math inline">\(S = T - B\)</span>. What is the distribution of <span class="math inline">\(S\)</span>?</p>
</blockquote>
<blockquote>
<p>We can utilize the method of moment-generating functions and write that
<span class="math display">\[\begin{align*}
m_S(t) = m_T(t) m_B(-t) &amp;= \exp[\lambda_T(e^t-1)] \exp[\lambda_B(e^{-t}-1)] \\
&amp;= \exp[\lambda_T(e^t-1) + \lambda_B(e^{-t}-1)] \\
&amp;= \exp[-(\lambda_T+\lambda_B) + \lambda_Te^t + \lambda_Be^{-t}] \,,
\end{align*}\]</span>
where <span class="math inline">\(\lambda_T = \lambda_S+\lambda_B = (\alpha_S+\alpha_B)t\)</span>.
At first, utilizing the method of mgfs appears
to be a fools errand: this is not an mgf we know. But it turns out that
the family of distributions associated with this mgf <em>does</em> have a name:
<span class="math inline">\(S = T-B\)</span> is a Skellam-distributed random variable, with mean
<span class="math inline">\(\lambda_T-\lambda_B\)</span> and variance <span class="math inline">\(\lambda_T+\lambda_B\)</span>. We can work
with this distribution to, e.g., construct confidence intervals for
<span class="math inline">\(\mu_S\)</span>, etc., if we so choose.</p>
</blockquote>
</div>
</div>
<div id="point-estimation-3" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Point Estimation<a href="the-poisson-and-related-distributions.html#point-estimation-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous chapters, we describe two commonly used point
estimators:
the maximum likelihood estimator
and the minimum variance unbiased estimator. We review both
below, in the context of estimating the Poisson <span class="math inline">\(\lambda\)</span> parameter, and
then for completeness introduce one last, less-commonly used approach,
the so-called <em>method of moments</em>.</p>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>First, lets take the logarithm of the likelihood function written out above:
<span class="math display">\[
\ell(\lambda \vert \mathbf{x}) = \left(\sum_{i=1}^n x_i\right) \log \lambda - n \lambda - \log\left(\prod_{i=1}^n x_i!\right) \,.
\]</span>
(Note that we technically need not write down any terms
that do not include <span class="math inline">\(\lambda\)</span>, like the last term, as such terms
will disappear entirely during differentiation.)
The derivative of <span class="math inline">\(\ell(\lambda \vert \mathbf{x})\)</span> with
respect to <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\frac{d\ell}{d\lambda} = \left(\frac{1}{\lambda}\sum_{i=1}^n x_i \right) - n \,.
\]</span>
Setting the derivative to zero and rearranging terms, we find that
<span class="math display">\[
\hat{\lambda}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
\]</span>
is the MLE for <span class="math inline">\(\lambda\)</span>. By the general rule introduced in Chapter 1,
<span class="math inline">\(E[\hat{\lambda}_{MLE}] = E[\bar{X}] = \lambda\)</span> (so <span class="math inline">\(\hat{\lambda}_{MLE}\)</span>
is an unbiased estimator), and <span class="math inline">\(V[\hat{\lambda}_{MLE}] = \lambda/n\)</span> (so
<span class="math inline">\(\hat{\lambda}_{MLE}\)</span> is a consistent estimator, since
<span class="math inline">\(\hat{\lambda}_{MLE} \rightarrow \lambda\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.
(There is no guarantee that the MLE is an unbiased estimator; it
just happens to be so here. Recall that the MLE will
always be at least asymptotically
unbiased, and it will always be a consistent estimator.)</p>
<p>If we wish to find the MLE for a function of the parameter,
e.g., <span class="math inline">\(\lambda^2\)</span>, we simply apply that function to <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.
Hence <span class="math inline">\(\hat{\lambda^2}_{MLE}\)</span> is <span class="math inline">\(\bar{X}^2\)</span>. This is the invariance
property of the MLE.</p>
<p>Also, recall that the MLE converges in distribution to a normal random
variable with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(1/I_n(\theta)\)</span>, where
<span class="math inline">\(I_n(\theta)\)</span> is the Fisher information content of the data sample. Here,
that means that
<span class="math display">\[
\hat{\lambda} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(\lambda,\frac{\lambda}{n}\right) \,.
\]</span></p>
<p><strong>Recall</strong>: <em>deriving the minimum variance unbiased estimator involves two steps:</em></p>
<ol style="list-style-type: decimal">
<li><em>factorizing the likelihood function to uncover a sufficient statistic <span class="math inline">\(U\)</span> (that we assume is both minimal and complete); and</em></li>
<li><em>finding a function <span class="math inline">\(h(U)\)</span> such that <span class="math inline">\(E[h(U)] = \lambda\)</span>.</em></li>
</ol>
<p><em>A sufficient statistic for a parameter (or parameters) <span class="math inline">\(\theta\)</span> captures all information about <span class="math inline">\(\theta\)</span> contained in the sample.</em></p>
<p>The likelihood function is
<span class="math display">\[
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda} = \left(\prod_{i=1}^n \frac{1}{x_i!}\right) \left(\prod_{i=1}^n \lambda^{x_i}e^{-\lambda}\right) = \underbrace{\left(\prod_{i=1}^n \frac{1}{x_i!}\right)}_{h(\mathbf{x})} \underbrace{\lambda^{\sum_{i=1}^n x_i}e^{-n\lambda}}_{g\left(\sum_{i=1}^n x_i,\lambda\right)} \,.
\]</span>
We see that a sufficient statistic is <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>. Lets determine the expected value for <span class="math inline">\(U\)</span>:
<span class="math display">\[
E[U] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \lambda = n\lambda \,.
\]</span>
Thus <span class="math inline">\(h(U) = U/n = \bar{X}\)</span> is the MVUE for <span class="math inline">\(\lambda\)</span>. As this matches the
MLE, we know already that the MVUE is an unbiased (by definition)
and consistent estimator. The next question is whether the variance
of the MVUE achieves the Cramer-Rao Lower Bound on the variance of
an unbiased estimator. We show that it does in an example below.</p>
<p>Note: the MVUE does not possess the invariance property, and it may be
the case that it does not achieve the CRLB. Its primary
advantage over the MLE is that the MVUE is the best estimator
among those that are always unbiased, regardless of sample size.</p>
<hr />
<p>The <em>method of moments</em> is a classic means by which to define
estimators that has been historically useful when we are
faced with the following situation: (a) the likelihood function is not
easily differentiated, because it contains terms like <span class="math inline">\(\Gamma(\theta)\)</span>; and
(b) the distribution has two or more free parameters, making it hard to
define the MVUE (since we would have joint sufficient statistics).
For instance, we face this situation when we work with the beta
distribution from Chapter 3. But why do we say historically useful?
Because in the age of computers, we can always determine MLEs numerically,
using an optimization function. We will show an example of such
a numerical computation below.</p>
<p>Recall that by definition, <span class="math inline">\(E[X^k]\)</span> is the <span class="math inline">\(k^{\rm th}\)</span> moment
of the distribution of the random variable <span class="math inline">\(X\)</span>. We can define
analagous <em>sample moments</em>, e.g.,
<span class="math display">\[
m_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} ~~ \mbox{and} ~~ m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = \overline{X^2} \,.
\]</span>
(For <span class="math inline">\(m_2\)</span>: note that the average of <span class="math inline">\(X_i^2\)</span> is <em>not</em> the same as
the average of <span class="math inline">\(X_i\)</span>, squared.)
Lets suppose that we have <span class="math inline">\(p\)</span> parameters that we are trying to estimate.
In method of moments estimation, we generally set the first <span class="math inline">\(p\)</span> population
moments equal to the first <span class="math inline">\(p\)</span> sample moments and solve the system of equations
to determine parameter estimates. These estimates are generally consistent,
but also may be biased. (Situations may exist where higher-order moments
may be preferable to use, such as when the one parameter of a distribution
is <span class="math inline">\(\sigma^2\)</span> and thus we might derive a better estimator using second
moments, but typically we will use the first <span class="math inline">\(p\)</span> moments.)</p>
<p>For the Poisson distribution, there is one parameter to estimate and
thus we set <span class="math inline">\(\mu_1&#39; = E[X] = \lambda = m_1&#39; = \bar{X}\)</span>. We thus find that
<span class="math inline">\(\hat{\lambda}_{MoM} = \bar{X}\)</span>. For a more relevant example of method
of moments usage, see below.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We begin this chapter by displaying the number of deaths per
Prussian army corps per year resulting from horse kicks. Leaving
aside the question of whether the data are truly Poisson distributed
(a question we will try to answer later in this chapter), what is
the estimated rate of death per corps per year?</p>
</blockquote>
<blockquote>
<p>The total number of events observed are
<span class="math display">\[
0 \times 109 + 1 \times 65 + 2 \times 22 + 3 \times 3 + 4 \times 1 = 65 + 44 + 9 + 4 = 122 \,,
\]</span>
and the total sample size is <span class="math inline">\(n = 200\)</span>, so
<span class="math display">\[
\hat{\lambda} = \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i = \frac{122}{200} = 0.61
\,.
\]</span>
This is the MLE, the MVUE, and the MoM estimate for <span class="math inline">\(\lambda\)</span>.
In the next section, we will use these data to estimate a 95%
confidence interval for <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-crlb-on-the-variance-of-lambda-estimators" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators<a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall</strong>: <em>the Cramer-Rao Lower Bound (or CRLB) is the lower bound on the variance of any unbiased estimator. If an unbiased estimator achieves the CRLB, it is the MVUEbut it can be the case that the MVUE does not achieve the CRLB. For a discrete distribution, the CRLB is given by</em>
<span class="math display">\[
V[\hat{\theta}] \geq -\left(nE\left[\frac{d^2}{d\theta^2} \log p_X(X \vert p) \right]\right)^{-1} = \frac{1}{nI(\theta)}
\]</span>
<em>where <span class="math inline">\(I(\theta)\)</span> is the Fisher information.</em></p>
<blockquote>
<p>For the Poisson distribution,
<span class="math display">\[\begin{align*}
p_X(x \vert \lambda) &amp;= \frac{\lambda^x}{x!}e^{-\lambda} \\
\log p_X(x \vert \lambda) &amp;= x \log \lambda - \lambda - \log x! \\
\frac{d}{d\lambda} \log p_X(x \vert \lambda) &amp;= \frac{x}{\lambda} - 1 \\
\frac{d^2}{d\lambda^2} \log p_X(x \vert \lambda) &amp;= -\frac{x}{\lambda^2} \\
E \left[ \frac{d^2}{d\lambda^2} \log p_X(X \vert \lambda) \right] &amp;= -\frac{1}{\lambda^2} E[X] \\
&amp;= -\frac{1}{\lambda^2} \lambda = -\frac{1}{\lambda}
\end{align*}\]</span>
and thus
<span class="math display">\[
V[\hat{\lambda}] \geq -\frac{1}{-n/\lambda} = \frac{\lambda}{n} \,.
\]</span>
Thus <span class="math inline">\(\hat{\lambda}_{MLE}\)</span>, <span class="math inline">\(\hat{\lambda}_{MVUE}\)</span>, <em>and</em>
<span class="math inline">\(\hat{\lambda}_{MoM}\)</span> all achieve the CRLB.</p>
</blockquote>
<hr />
</div>
<div id="minimum-variance-unbiased-estimation-and-the-invariance-property" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Minimum Variance Unbiased Estimation and the Invariance Property<a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As stated above, the MVUE does not possess the property of invariance.
To demonstrate the lack of invariance, we will define the MVUE for
<span class="math inline">\(\lambda^2\)</span>.</p>
</blockquote>
<blockquote>
<p>The first thing to notice is that we cannot fall back on factorization
to determine an appropriate sufficient statistic, since <span class="math inline">\(\lambda^2\)</span>
does not appear directly in the likelihood function. So we iterate:
we make an initial guess and see where that guess takes us, and we
guess again if our initial guess is wrong, etc.</p>
</blockquote>
<blockquote>
<p>An appropriate guess for <span class="math inline">\(\lambda^2\)</span> is <span class="math inline">\(\bar{X}^2\)</span>:
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{\lambda}{n} + \lambda^2
\]</span>
We do get the term <span class="math inline">\(\lambda^2\)</span> herebut we also get <span class="math inline">\(\lambda/n\)</span>.
Hmmso lets try <span class="math inline">\(\bar{X}^2 - \bar{X}/n\)</span> instead:
<span class="math display">\[
E\left[\bar{X}^2 - \frac{\bar{X}}{n}\right] = E[\bar{X}^2] - \frac{1}{n}E[\bar{X}] = \frac{\lambda}{n} + \lambda^2 - \frac{\lambda}{n} = \lambda^2 \,.
\]</span>
Done! The MVUE for <span class="math inline">\(\lambda^2\)</span> is thus <span class="math inline">\(\hat{\lambda^2}_{MVUE} = \bar{X}^2-\bar{X}/n\)</span>,
which is <em>not</em> equal to <span class="math inline">\(\hat{\lambda^2}_{MLE} = \bar{X}^2\)</span> (except
in the limit <span class="math inline">\(n \rightarrow \infty\)</span>).</p>
</blockquote>
<hr />
</div>
<div id="method-of-moments-estimation-for-the-gamma-distribution" class="section level3 hasAnchor" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> Method of Moments Estimation for the Gamma Distribution<a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We will not officially introduce the gamma distribution until later in
this chapter, but it is a good one to look at when exploring method of
moments estimation. The probability density function for a gamma
random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}}{\beta^{\alpha}} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\alpha,\beta &gt; 0\)</span>.
The expected value is <span class="math inline">\(E[X] = \alpha \beta\)</span> while the variance
is <span class="math inline">\(V[X] = \alpha \beta^2\)</span> (and thus <span class="math inline">\(E[X^2] = \alpha \beta^2 +
\alpha^2 \beta^2\)</span>).</p>
</blockquote>
<blockquote>
<p>Lets assume we have <span class="math inline">\(n\)</span> iid gamma-distributed random variables.
Because there are two parameters, we match the first two moments:
<span class="math display">\[\begin{align*}
\mu_1&#39; = E[X] = \alpha \beta &amp;= m_1&#39; = \bar{X} \\
\mu_2&#39; = E[X^2] = \alpha \beta^2 + \alpha^2 \beta^2 &amp;= m_2&#39; = \frac{1}{n}\sum_{i=1}^n X_i^2 = \overline{X^2} \,.
\end{align*}\]</span>
Let <span class="math inline">\(\beta = \bar{X}/\alpha\)</span>. Then
<span class="math display">\[\begin{align*}
\alpha \left( \frac{\bar{X}}{\alpha} \right)^2 + \alpha^2 \left( \frac{\bar{X}}{\alpha} \right)^2 &amp;= \overline{X^2} \\
\frac{(\bar{X})^2}{\alpha} &amp;= \overline{X^2} - (\bar{X})^2 \\
\Rightarrow ~~ \hat{\alpha}_{MoM} &amp;= \frac{(\bar{X})^2}{\overline{X^2} - (\bar{X})^2} \,,
\end{align*}\]</span>
and thus
<span class="math display">\[
\hat{\beta}_{MoM} = \frac{\bar{X}}{\hat{\alpha}_{MoM}} = \frac{\overline{X^2} - (\bar{X})^2}{\bar{X}} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="maximum-likelihood-estimation-via-numerical-optimization" class="section level3 hasAnchor" number="4.5.5">
<h3><span class="header-section-number">4.5.5</span> Maximum Likelihood Estimation via Numerical Optimization<a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As noted above, a historical reason for using the method of moments to
make point estimates is that it will work in those situations
where we cannot derive the MVUE or the MLE. For instance, lets suppose
that we draw <span class="math inline">\(n\)</span> iid data from a beta distribution, which has the following
probability density function:
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha,\beta)} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \in [0,1]\)</span> and <span class="math inline">\(\alpha,\beta &gt; 0\)</span>. If <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are
both freely varying, then we are in a situation in which we have joint
sufficient statisticsand thus we are not able to compute the MVUEs.
We then fall back on trying to compute the MLEsbut what is the
partial derivative of <span class="math inline">\(n \log B(\alpha,\beta)\)</span> with respect to, e.g.,
<span class="math inline">\(\alpha\)</span>?</p>
</blockquote>
<blockquote>
<p>With computers, we can circumvent this last issue by attempting to maximize
the value of the likelihood, as a function of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, using
a numerical optimizer.</p>
</blockquote>
<blockquote>
<p>The subject of numerical optimization is far too vast for us to be able
to cover all the important details here. It suffices to say that our goal
is to (a) define an <em>objective function</em> (here, the log-likelihood), and
(b) pass that function into an optimizer that explores the space of
free parameters (here, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) and returns the parameter
values that optimize the objective functions value (here, the ones that
maximize the log-likelihood).</p>
</blockquote>
<blockquote>
<p>Lets start with the objective function. <em>If</em> the pdf or pmf of our
random variables is already coded in <code>R</code>, we can use the coded function
rather than write out the function mathematically. Recall
that when we have sampled iid (continuously valued) data,
<span class="math display">\[\begin{align*}
\ell(\theta \vert \mathbf{x}) = \sum_{i=1}^n \log f_X(x_i \vert \theta)
\end{align*}\]</span>
For the specific case of the beta distribution, we can convert this
statement into code:</p>
</blockquote>
<pre><code>sum(log(dbeta(x,shape1=alpha,shape2=beta)))</code></pre>
<blockquote>
<p>where <code>dbeta()</code> is the beta pdf function.
This <em>is</em> the objective function, except for one tweak that we will make:
because <code>R</code>s
<code>optim()</code> function <em>minimizes</em> the objective function value by default,
and we want to <em>maximize</em> the log-likelihood value,
we will use</p>
</blockquote>
<pre><code>-sum(log(dbeta(x,shape1=alpha,shape2=beta)))</code></pre>
<blockquote>
<p>instead. So lets now write down the full function:</p>
</blockquote>
<pre><code>f &lt;- function(par,x)
{
  -sum(log(dbeta(x,shape1=par[1],shape2=par[2])))
}</code></pre>
<blockquote>
<p>Note how <code>R</code> expects the parameter values to be contained within a single
vector, which here we call <code>par</code>.</p>
</blockquote>
<blockquote>
<p>The rest of the coding is straightforward: (a) we initialize the vector
<code>par</code> with initial (good!) guesses for the values of the parameters,
(b) pass <code>par</code> and the observed data into <code>optim()</code>, and (c) access
the <code>par</code> element of the list output by <code>optim()</code>.</p>
</blockquote>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="the-poisson-and-related-distributions.html#cb264-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">236</span>)</span>
<span id="cb264-2"><a href="the-poisson-and-related-distributions.html#cb264-2" tabindex="-1"></a></span>
<span id="cb264-3"><a href="the-poisson-and-related-distributions.html#cb264-3" tabindex="-1"></a><span class="co"># generate observed data</span></span>
<span id="cb264-4"><a href="the-poisson-and-related-distributions.html#cb264-4" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb264-5"><a href="the-poisson-and-related-distributions.html#cb264-5" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">2.44</span>  <span class="co"># arbitrarily chosen true values</span></span>
<span id="cb264-6"><a href="the-poisson-and-related-distributions.html#cb264-6" tabindex="-1"></a>beta  <span class="ot">&lt;-</span> <span class="fl">5.39</span></span>
<span id="cb264-7"><a href="the-poisson-and-related-distributions.html#cb264-7" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n,<span class="at">shape1=</span>alpha,<span class="at">shape2=</span>beta)</span>
<span id="cb264-8"><a href="the-poisson-and-related-distributions.html#cb264-8" tabindex="-1"></a></span>
<span id="cb264-9"><a href="the-poisson-and-related-distributions.html#cb264-9" tabindex="-1"></a><span class="co"># compute MLEs via optimization</span></span>
<span id="cb264-10"><a href="the-poisson-and-related-distributions.html#cb264-10" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(par,x)</span>
<span id="cb264-11"><a href="the-poisson-and-related-distributions.html#cb264-11" tabindex="-1"></a>{</span>
<span id="cb264-12"><a href="the-poisson-and-related-distributions.html#cb264-12" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">dbeta</span>(x,<span class="at">shape1=</span>par[<span class="dv">1</span>],<span class="at">shape2=</span>par[<span class="dv">2</span>])))</span>
<span id="cb264-13"><a href="the-poisson-and-related-distributions.html#cb264-13" tabindex="-1"></a>}</span>
<span id="cb264-14"><a href="the-poisson-and-related-distributions.html#cb264-14" tabindex="-1"></a>par   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>)</span>
<span id="cb264-15"><a href="the-poisson-and-related-distributions.html#cb264-15" tabindex="-1"></a>opt   <span class="ot">&lt;-</span> <span class="fu">optim</span>(par,f,<span class="at">x=</span>X) <span class="co"># need to specify x via an extra argument</span></span>
<span id="cb264-16"><a href="the-poisson-and-related-distributions.html#cb264-16" tabindex="-1"></a>opt<span class="sc">$</span>par</span></code></pre></div>
<pre><code>## [1] 2.510399 5.168113</code></pre>
<blockquote>
<p>We thus find that <span class="math inline">\(\hat{\alpha}_{\rm MLE} = 2.510\)</span> and
<span class="math inline">\(\hat{\beta}_{\rm MLE} = 5.168\)</span>. These values are close to, but not equal
to, the true values; because these are MLEs, we know that our estimates
will get closer and closer to the true values as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-3" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Confidence Intervals<a href="the-poisson-and-related-distributions.html#confidence-intervals-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}\)</span> by solving the following equation:</em>
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span>, <span class="math inline">\(y_{\rm obs}\)</span> is the observed value of the statistic,
and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined using
the confidence interval reference table introduced in
section 16 of Chapter 1.</em></p>
<p>As far as the construction of confidence intervals given a discrete
sampling distribution goes, nothing changes algorithmically from
where we were in Chapter 3; in an example below,
we review how to construct such an interval for the
Poisson parameter <span class="math inline">\(\lambda\)</span>.</p>
<hr />
<p>In the context of statistical inference, there is one more important
question to answer. What do we do if we
neither know nor are willing to assume the distribution from which our
data are sampled?
Our root-finding algorithm relies upon knowing
the sampling distribution of an observed statistic, and that in turn relies on
knowing the distribution from which we draw each of our <span class="math inline">\(n\)</span> iid data.
One thing we can do is fall back upon <em>bootstrapping</em>. We demonstrate
bootstrap confidence interval estimation in an example below.</p>
<hr />
<div id="confidence-interval-for-the-poisson-parameter-lambda" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume that we sample <span class="math inline">\(n\)</span> iid data. We know that the sum
<span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is a sufficient statistic for <span class="math inline">\(\lambda\)</span>
and that <span class="math inline">\(Y \sim\)</span> Poisson(<span class="math inline">\(n\lambda\)</span>).
For this statistic, <span class="math inline">\(E[Y] = n\lambda\)</span>, which increases with <span class="math inline">\(\lambda\)</span>, so
we know that we will utilize the yes lines of the confidence
interval reference table.
Our observed test statistic is <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="the-poisson-and-related-distributions.html#cb266-1" tabindex="-1"></a><span class="co"># Let&#39;s assume we observe ten years of data in a Poisson process</span></span>
<span id="cb266-2"><a href="the-poisson-and-related-distributions.html#cb266-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb266-3"><a href="the-poisson-and-related-distributions.html#cb266-3" tabindex="-1"></a>alpha  <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb266-4"><a href="the-poisson-and-related-distributions.html#cb266-4" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb266-5"><a href="the-poisson-and-related-distributions.html#cb266-5" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb266-6"><a href="the-poisson-and-related-distributions.html#cb266-6" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n,<span class="at">lambda=</span>lambda)</span>
<span id="cb266-7"><a href="the-poisson-and-related-distributions.html#cb266-7" tabindex="-1"></a></span>
<span id="cb266-8"><a href="the-poisson-and-related-distributions.html#cb266-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(nlambda,y.obs,q)</span>
<span id="cb266-9"><a href="the-poisson-and-related-distributions.html#cb266-9" tabindex="-1"></a>{</span>
<span id="cb266-10"><a href="the-poisson-and-related-distributions.html#cb266-10" tabindex="-1"></a>  <span class="fu">ppois</span>(y.obs,<span class="at">lambda=</span>nlambda)<span class="sc">-</span>q</span>
<span id="cb266-11"><a href="the-poisson-and-related-distributions.html#cb266-11" tabindex="-1"></a>}</span>
<span id="cb266-12"><a href="the-poisson-and-related-distributions.html#cb266-12" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.001</span>,<span class="dv">10000</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X)<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root<span class="sc">/</span>n</span></code></pre></div>
<pre><code>## [1] 5.722035</code></pre>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="the-poisson-and-related-distributions.html#cb268-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.001</span>,<span class="dv">10000</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root<span class="sc">/</span>n</span></code></pre></div>
<pre><code>## [1] 9.178654</code></pre>
<blockquote>
<p>Note the discreteness correction that we apply when deriving the
lower bound, and
note the division by <span class="math inline">\(n\)</span> after the calls to <code>uniroot()</code>: this
converts the interval bounds from being bounds on <span class="math inline">\(n\lambda\)</span> to
being bounds on <span class="math inline">\(\lambda\)</span> itself.
The interval is <span class="math inline">\([\hat{\lambda}_L,\hat{\lambda}_U] =
[5.72,9.18]\)</span>, which overlaps the true value of 8. (See
Figure <a href="the-poisson-and-related-distributions.html#fig:poici">4.3</a>.)
Note that the interval over which we
search for the root is [0.001,10000], which is
(effectively) the range of possible values for <span class="math inline">\(\lambda\)</span>.
(Recall that <span class="math inline">\(\lambda &gt; 0\)</span>, hence the small but non-zero lower bound.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poici"></span>
<img src="_main_files/figure-html/poici-1.png" alt="\label{fig:poici}Probability mass functions for Poisson distributions for which (left) $n\lambda=57.2$, and (right) $n\lambda=91.8$. We assume that we observe $y_{\rm obs} = \sum_{i=1}^n x_i = 73$ events in total and that we want to construct a 95\% confidence interval for $\lambda$. $n\lambda=57.2$ is the smallest value of $n\lambda$ such that $F_Y^{-1}(0.975) = 73$, while $n\lambda=91.8$ is the largest value of $n\lambda$ such that $F_Y^{-1}(0.025) = 73$." width="45%" /><img src="_main_files/figure-html/poici-2.png" alt="\label{fig:poici}Probability mass functions for Poisson distributions for which (left) $n\lambda=57.2$, and (right) $n\lambda=91.8$. We assume that we observe $y_{\rm obs} = \sum_{i=1}^n x_i = 73$ events in total and that we want to construct a 95\% confidence interval for $\lambda$. $n\lambda=57.2$ is the smallest value of $n\lambda$ such that $F_Y^{-1}(0.975) = 73$, while $n\lambda=91.8$ is the largest value of $n\lambda$ such that $F_Y^{-1}(0.025) = 73$." width="45%" />
<p class="caption">
Figure 4.3: Probability mass functions for Poisson distributions for which (left) <span class="math inline">\(n\lambda=57.2\)</span>, and (right) <span class="math inline">\(n\lambda=91.8\)</span>. We assume that we observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 73\)</span> events in total and that we want to construct a 95% confidence interval for <span class="math inline">\(\lambda\)</span>. <span class="math inline">\(n\lambda=57.2\)</span> is the smallest value of <span class="math inline">\(n\lambda\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 73\)</span>, while <span class="math inline">\(n\lambda=91.8\)</span> is the largest value of <span class="math inline">\(n\lambda\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 73\)</span>.
</p>
</div>
<blockquote>
<p>In Chapter 3, we discussed how when we work with discrete sampling
distributions, the coverage of the confidence intervals that we construct,
given a value <span class="math inline">\(\theta = \theta_o\)</span>, will be equal to the probability of
sampling a datum in the acceptance region given a null hypothesis
<span class="math inline">\(H_o : \theta = \theta_o\)</span>, and thus the coverage will be, in general,
<span class="math inline">\(&gt; 1-\alpha\)</span>. Lets verify that that is the case here.</p>
</blockquote>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="the-poisson-and-related-distributions.html#cb270-1" tabindex="-1"></a>y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qpois</span>(alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">lambda=</span>lambda)</span>
<span id="cb270-2"><a href="the-poisson-and-related-distributions.html#cb270-2" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qpois</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">lambda=</span>lambda)</span>
<span id="cb270-3"><a href="the-poisson-and-related-distributions.html#cb270-3" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dpois</span>(y.rr.lo<span class="sc">:</span>y.rr.hi,<span class="at">lambda=</span>lambda))</span></code></pre></div>
<pre><code>## [1] 0.968989</code></pre>
<blockquote>
<p>In this particular situation, the coverage is 96.9%.</p>
</blockquote>
<hr />
</div>
<div id="revisiting-the-death-by-horse-kick-example-1" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the last section above, we determined that the rate of death from
horse kicks per Prussian army corps per year was <span class="math inline">\(\hat{\lambda} = 0.61\)</span>.
Here, we determine a 95% interval estimate for <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="the-poisson-and-related-distributions.html#cb272-1" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">109</span>),<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">65</span>),<span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">22</span>),<span class="fu">rep</span>(<span class="dv">3</span>,<span class="dv">3</span>),<span class="fu">rep</span>(<span class="dv">4</span>,<span class="dv">1</span>))</span>
<span id="cb272-2"><a href="the-poisson-and-related-distributions.html#cb272-2" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="fu">length</span>(X)</span>
<span id="cb272-3"><a href="the-poisson-and-related-distributions.html#cb272-3" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb272-4"><a href="the-poisson-and-related-distributions.html#cb272-4" tabindex="-1"></a></span>
<span id="cb272-5"><a href="the-poisson-and-related-distributions.html#cb272-5" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(nlambda,y.obs,q)</span>
<span id="cb272-6"><a href="the-poisson-and-related-distributions.html#cb272-6" tabindex="-1"></a>{</span>
<span id="cb272-7"><a href="the-poisson-and-related-distributions.html#cb272-7" tabindex="-1"></a>  <span class="fu">ppois</span>(y.obs,<span class="at">lambda=</span>nlambda)<span class="sc">-</span>q</span>
<span id="cb272-8"><a href="the-poisson-and-related-distributions.html#cb272-8" tabindex="-1"></a>}</span>
<span id="cb272-9"><a href="the-poisson-and-related-distributions.html#cb272-9" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.001</span>,<span class="dv">10000</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X)<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root<span class="sc">/</span>n</span></code></pre></div>
<pre><code>## [1] 0.5065682</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="the-poisson-and-related-distributions.html#cb274-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.001</span>,<span class="dv">10000</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root<span class="sc">/</span>n</span></code></pre></div>
<pre><code>## [1] 0.7283408</code></pre>
<blockquote>
<p>The 95% confidence interval is <span class="math inline">\([0.507,0.728]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="determining-a-confidence-interval-using-the-bootstrap" class="section level3 hasAnchor" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Determining a Confidence Interval Using the Bootstrap<a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The bootstrap, invented by Bradley Efron
in 1979, uses the observed data themselves to build up empirical
sampling distributions for statistics.
Lets suppose we are handed the following data:
<span class="math display">\[
\mathbf{X} = \{X_1,X_2,\ldots,X_n\} \overset{iid}{\sim} P \,,
\]</span>
where the distribution <span class="math inline">\(P\)</span> is unknown. Now, lets suppose further that
from these data we compute a statistic: a single number.
How can we build up an empirical sampling distribution from a single
number? The answer is to repeatedly <em>resample</em> the data we observe,
<em>with replacement</em>. For instance, if we have as data the numbers <span class="math inline">\(\{1,2,3\}\)</span>,
a bootstrap sample might be <span class="math inline">\(\{1,1,3\}\)</span> or <span class="math inline">\(\{2,3,3\}\)</span>, etc. Every time
we resample the data, we compute the statistic we are interested in and
record its value. Voila: we have an empirical sampling distribution.
And if we can link the elements of that sampling distribution to a population
parameter, we can immediately write down a confidence interval. For instance,
if we have the <span class="math inline">\(n_{\rm boot}\)</span> statistics <span class="math inline">\(\{\bar{X}_1,\ldots,\bar{X}_k\}\)</span>, we
can put bounds on the population mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\begin{align*}
\hat{\mu}_L &amp;= \bar{X}_{\alpha/2} \\
\hat{\mu}_U &amp;= \bar{X}_{1-\alpha/2} \,,
\end{align*}\]</span>
where <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> represent sample percentiles, e.g.,
the 2.5<span class="math inline">\(^{\rm th}\)</span> and 97.5<span class="math inline">\(^{\rm th}\)</span> percentiles.</p>
</blockquote>
<blockquote>
<p>Now, lets assume we have the same data as in the first example above.</p>
</blockquote>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="the-poisson-and-related-distributions.html#cb276-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb276-2"><a href="the-poisson-and-related-distributions.html#cb276-2" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb276-3"><a href="the-poisson-and-related-distributions.html#cb276-3" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb276-4"><a href="the-poisson-and-related-distributions.html#cb276-4" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n,<span class="at">lambda=</span>lambda)</span>
<span id="cb276-5"><a href="the-poisson-and-related-distributions.html#cb276-5" tabindex="-1"></a><span class="fu">print</span>(X)</span></code></pre></div>
<pre><code>##  [1] 7 4 9 9 6 6 8 7 9 8</code></pre>
<blockquote>
<p>The confidence interval that we construct for <span class="math inline">\(\lambda\)</span>, which is
the distribution mean, is <span class="math inline">\([5.72,9.18]\)</span>. How does the bootstrap
estimate of the mean compare?</p>
</blockquote>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="the-poisson-and-related-distributions.html#cb278-1" tabindex="-1"></a>n.boot <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb278-2"><a href="the-poisson-and-related-distributions.html#cb278-2" tabindex="-1"></a>x.bar  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,n.boot)</span>
<span id="cb278-3"><a href="the-poisson-and-related-distributions.html#cb278-3" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.boot ) {</span>
<span id="cb278-4"><a href="the-poisson-and-related-distributions.html#cb278-4" tabindex="-1"></a>  s         <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">length</span>(X),<span class="fu">length</span>(X),<span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb278-5"><a href="the-poisson-and-related-distributions.html#cb278-5" tabindex="-1"></a>  x.bar[ii] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[s])</span>
<span id="cb278-6"><a href="the-poisson-and-related-distributions.html#cb278-6" tabindex="-1"></a>}</span>
<span id="cb278-7"><a href="the-poisson-and-related-distributions.html#cb278-7" tabindex="-1"></a><span class="fu">quantile</span>(x.bar,<span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   6.3   8.2</code></pre>
<blockquote>
<p>The estimated interval is <span class="math inline">\([6.3,8.2]\)</span>. This is substantially smaller than
what we found above, and that makes sense: for instance, the largest observed
datum is 9, so the largest possible value of the bootstrap sample mean
is 9which is smaller than the previously determined
upper bound of 9.18. What we are seeing
is the effect of a small sample size: in the limit of small <span class="math inline">\(n\)</span>,
the length of bootstrap confidence intervals is on average smaller than
that of exact ones, with greater variability in lengths. As <span class="math inline">\(n\)</span>
increases, the mean lengths converge, but the variability in lengths remains
larger for bootstrap intervals than for exact ones. What this means is
that when we can propose a plausible distribution for our data, we should
do so, as we will get more meaningful confidence intervals than if we
were to fall back upon bootstrapping.</p>
</blockquote>
<hr />
</div>
<div id="the-proportion-of-observed-data-in-a-bootstrap-sample" class="section level3 hasAnchor" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> The Proportion of Observed Data in a Bootstrap Sample<a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we sample <span class="math inline">\(n\)</span> iid data from some distribution <span class="math inline">\(P\)</span>.
When we create a bootstrap sample of these data, some of the observed data
appear multiple times, while other data do not appear at all. What is
the average proportion of observed data in any given bootstrap sample?</p>
</blockquote>
<blockquote>
<p>Let <span class="math inline">\(i\)</span> be the index of an arbitrary datum, where the indices are
<span class="math inline">\(\{1,2,\ldots,n-1,n\}\)</span>. Let <span class="math inline">\(X\)</span> be the number of times <span class="math inline">\(i\)</span> is chosen
when we construct a bootstrap sample of size <span class="math inline">\(n\)</span>:
<span class="math inline">\(X \sim\)</span> Binom(<span class="math inline">\(n,1/n\)</span>). <span class="math inline">\(P(X \geq 1)\)</span> then represents
the average proportion of observed data in a bootstrap sample:
<span class="math display">\[
P(X \geq 1) = 1 - P(X = 0) = 1 - (1-1/n)^n \,,
\]</span>
which, as <span class="math inline">\(n \rightarrow \infty\)</span>, approaches <span class="math inline">\(1-1/e = 0.632\)</span>.
Thus, for a sufficiently large sample, 63.2% of the observed data
will appear at least once in a bootstrapped dataset.</p>
</blockquote>
<hr />
</div>
<div id="confidence-interval-estimation-via-simulation" class="section level3 hasAnchor" number="4.6.5">
<h3><span class="header-section-number">4.6.5</span> Confidence Interval Estimation via Simulation<a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we sample <span class="math inline">\(n\)</span> iid data from a Beta<span class="math inline">\((a,2)\)</span>
distribution:
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x^{a-1}(1-x)}{B(a,2)} \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,1]\)</span> and <span class="math inline">\(a &gt; 0\)</span>. A sufficient statistic for
<span class="math inline">\(a\)</span>, found via factorization, is <span class="math inline">\(\prod_{i=1}^n X_i\)</span>; another,
more easy to work with, one is <span class="math inline">\(Y = -\sum_{i=1}^n \log X_i\)</span>. (Why do we
include a minus sign? For no other reason that when we include it,
<span class="math inline">\(Y\)</span> generally increases as <span class="math inline">\(n\)</span> increases.) The expected value of the
random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(E[X] = a/(a+2)\)</span>, which increases
as <span class="math inline">\(a\)</span> increases. However, as <span class="math inline">\(X \rightarrow 1\)</span>,
<span class="math inline">\(-\log X \rightarrow 0\)</span>, and so <span class="math inline">\(E[Y]\)</span> will <em>decrease</em> with <span class="math inline">\(a\)</span>.</p>
</blockquote>
<blockquote>
<p>We are now all set to construct, e.g., 95% confidence intervals for <span class="math inline">\(a\)</span>.
Exceptwe cannot identify the sampling distribution for <span class="math inline">\(Y\)</span>. So we are
stuckbut, in actuality, we are not. We just need to numerically
estimate sampling distributions for <span class="math inline">\(Y \vert a\)</span> via simulation,
and find the values of <span class="math inline">\(a\)</span> that make the estimated
<span class="math inline">\(F_Y(y_{\rm obs} \vert a)\)</span> (approximately) equal to 0.025 and 0.975.</p>
</blockquote>
<blockquote>
<p>Lets first build up the code that we would need to estimate the
sampling distribution and estimate the cdf for the value <span class="math inline">\(y_{\rm obs}\)</span>.</p>
</blockquote>
<pre><code>X &lt;- matrix(rbeta(n*num.sim,shape1=a,shape2=2),nrow=num.sim)
Y &lt;- apply(X,1,function(x){-sum(log(x))})
sum(Y &lt;= y.obs)/num.sim</code></pre>
<blockquote>
<p>On the first line, we create <code>num.sim</code> separate datasets given a
parameter value, and store them row-by-row in a matrix. On the second
line, we use <code>R</code>s <code>apply()</code> function, row-by-row (as indicated
by the argument <code>1</code>), to determine the statistic value for each dataset.
Then, on the third line, we determine the proportion of statistic values
that are less than or equal to <span class="math inline">\(y_{\rm obs}\)</span>: this is the empirical
cumulative distribution function value <span class="math inline">\(\hat{F}_n(y_{\rm obs} \vert a)\)</span>.
We then find the value of <span class="math inline">\(a\)</span> for which
<span class="math inline">\(\hat{F}_n(y_{\rm obs} \vert a) - q = 0\)</span> using <code>uniroot()</code>.</p>
</blockquote>
<blockquote>
<p>Lets put this all together in an example where <span class="math inline">\(n = 5\)</span> and
<span class="math inline">\(y_{\rm obs} = 10\)</span>:</p>
</blockquote>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="the-poisson-and-related-distributions.html#cb281-1" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb281-2"><a href="the-poisson-and-related-distributions.html#cb281-2" tabindex="-1"></a>y.obs <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb281-3"><a href="the-poisson-and-related-distributions.html#cb281-3" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb281-4"><a href="the-poisson-and-related-distributions.html#cb281-4" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(a,n,y.obs,q,<span class="at">num.sim=</span><span class="dv">1000000</span>,<span class="at">seed=</span><span class="dv">236</span>)</span>
<span id="cb281-5"><a href="the-poisson-and-related-distributions.html#cb281-5" tabindex="-1"></a>{</span>
<span id="cb281-6"><a href="the-poisson-and-related-distributions.html#cb281-6" tabindex="-1"></a>  <span class="fu">set.seed</span>(seed)</span>
<span id="cb281-7"><a href="the-poisson-and-related-distributions.html#cb281-7" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rbeta</span>(n<span class="sc">*</span>num.sim,<span class="at">shape1=</span>a,<span class="at">shape2=</span><span class="dv">2</span>),<span class="at">nrow=</span>num.sim)</span>
<span id="cb281-8"><a href="the-poisson-and-related-distributions.html#cb281-8" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,<span class="cf">function</span>(x){<span class="sc">-</span><span class="fu">sum</span>(<span class="fu">log</span>(x))})</span>
<span id="cb281-9"><a href="the-poisson-and-related-distributions.html#cb281-9" tabindex="-1"></a>  <span class="fu">sum</span>(Y <span class="sc">&lt;=</span> y.obs)<span class="sc">/</span>num.sim<span class="sc">-</span>q</span>
<span id="cb281-10"><a href="the-poisson-and-related-distributions.html#cb281-10" tabindex="-1"></a>}</span>
<span id="cb281-11"><a href="the-poisson-and-related-distributions.html#cb281-11" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="fu">c</span>(<span class="fl">0.1</span>,<span class="dv">3</span>),<span class="at">n=</span>n,<span class="at">y.obs=</span>y.obs,<span class="at">q=</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.248806</code></pre>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="the-poisson-and-related-distributions.html#cb283-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="fu">c</span>(<span class="fl">0.1</span>,<span class="dv">3</span>),<span class="at">n=</span>n,<span class="at">y.obs=</span>y.obs,<span class="at">q=</span><span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 1.374306</code></pre>
<blockquote>
<p>Our estimated 95% confidence interval for <span class="math inline">\(a\)</span> is [0.249,1.374].</p>
</blockquote>
<blockquote>
<p>We note that the default number of simulated datasets in the code
above is <span class="math inline">\(10^6\)</span>. This choice is driven a desire to make the result
as precise as possible while keeping the runtime reasonable. (Here,
the runtime is <span class="math inline">\(\lesssim\)</span> 1 CPU minute per <code>uniroot()</code> call on a
standard desktop or laptop computer.) The empirical cdf is an
<em>estimate</em>, i.e., even if we hold <span class="math inline">\(a\)</span> constant, <code>sum(Y &lt;= y.obs)</code>
is a random variable. Lets denote the sum as <span class="math inline">\(S \vert a\)</span>, the
total number of simulated datasets as <span class="math inline">\(k\)</span>, and the true cdf value
(given <span class="math inline">\(y_{\rm obs}\)</span>) as <span class="math inline">\(p\)</span>. Then
<span class="math display">\[\begin{align*}
S \vert a \sim \text{Binomial}(k,p) \,.
\end{align*}\]</span>
The standard error for the empirical cdf is thus
<span class="math display">\[\begin{align*}
\sqrt{V\left[\frac{1}{k}(S \vert a)\right]} = \sqrt{\frac{p(1-p)}{k}} \,.
\end{align*}\]</span>
If <span class="math inline">\(k = 10^6\)</span> and, e.g., <span class="math inline">\(p = 0.025\)</span>, the standard error is
<span class="math inline">\(\sim 10^{-4}\)</span>, which is on par with typical uncertainties of estimates
generated by <code>uniroot()</code>.
We thus expect the uncertainties of our interval bound
estimates to be <span class="math inline">\(\sim 10^{-4}\)</span>.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-2" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Hypothesis Testing<a href="the-poisson-and-related-distributions.html#hypothesis-testing-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>In Chapter 3, we built upon the framework described above by introducing the
Neyman-Pearson lemma. This result allows us to bypass the guesswork that goes
into defining a hypothesis test statistic, by defining for us the most powerful
test of a simple null hypothesis versus a simple specified alternative.</p>
<p><strong>Recall:</strong> <em>when we test the simple hypotheses <span class="math inline">\(H_o: \theta = \theta_o\)</span> versus <span class="math inline">\(H_a: \theta = \theta_a\)</span>, the Neyman-Pearson lemma allows us to state that the hypothesis test with maximum power has a rejection region of the form</em>
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} &lt; c(\alpha) \,,
\]</span>
<em>where <span class="math inline">\(c(\alpha)\)</span> is a constant whose value depends on the specified Type I error <span class="math inline">\(\alpha\)</span>. When we sample data from an exponential-family distribution, we would simply determine a sufficient statistic <span class="math inline">\(Y\)</span>, and develop a hypothesis test as we have done previously using that statistic (or a function of it), assuming we know or can derive its sampling distribution. If the rejection region does not depend on <span class="math inline">\(\theta_a\)</span>, then the test is said to be a uniformly most powerful (UMP) test.</em></p>
<p>In an example, we demonstrate how to apply the NP lemma to construct a
hypothesis test for the Poisson parameter <span class="math inline">\(\lambda\)</span>, given a sample of
<span class="math inline">\(n\)</span> iid data. Here we describe a more general hypothesis
test framework, dubbed the <em>likelihood ratio test</em> (or <em>LRT</em>).</p>
<p>Waitthe NP lemma had a likelihood ratio. How is the LRT different?</p>
<p>That is a good question.
It differs in how we specify the null and alternative hypotheses:
<span class="math display">\[
H_o: \theta \in \Theta_o ~~\mbox{vs.}~~ H_a: \theta \in \Theta_o^c \,,
\]</span>
where <span class="math inline">\(\Theta_o\)</span> (capital theta naught) represents a set of possible null
values for <span class="math inline">\(\theta\)</span>, while <span class="math inline">\(\Theta_o^c\)</span> is the complement of that set. For
instance, for tests involving the Poisson parameter <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\Theta_o\)</span> could
be <span class="math inline">\(\lambda \in [5,10]\)</span>, so that <span class="math inline">\(\Theta_o^c\)</span> is <span class="math inline">\(\lambda &lt; 5\)</span> or
<span class="math inline">\(\lambda &gt; 10\)</span>. (The null hypothesis in this example is a composite hypothesis,
although it can be specified as a simple one, and usually is.)
Let <span class="math inline">\(\Theta = \Theta_o \cup \Theta_o^c\)</span>, i.e., the union of
the null and alternative sets. The rejection region for the LRT is
<span class="math display">\[
\lambda_{LR} = \frac{\mbox{sup}_{\theta \in \Theta_o} \mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta} \mathcal{L}(\theta \vert \mathbf{x})} &lt; c(\alpha) \,,
\]</span>
where, like it is in the context of the NP lemma,
<span class="math inline">\(c(\alpha)\)</span> is a constant that depends on the specified Type I error <span class="math inline">\(\alpha\)</span>.</p>
<p>Since the LRT is more general, why would we ever utilize the NP lemma?</p>
<p>That is another good question.</p>
<p>The primary point to make is that when we construct a test of two simple
hypothesis within the framework of the NP lemma, we <em>know</em> that we are
constructing the most powerful test of those hypotheses.
On the other hand, while the LRT is generally a powerful test,
given the composite nature of one (or both) of the hypotheses
it comes with no guarantee of being the most powerful test.
For instance, perhaps an alternative like the score test (also known as
the Lagrange multiplier test) or the Wald test
would provide the most powerful test in a given analysis situation. Diving
into the details of these alternatives is beyond the scope of this book; those
who are interested in learning more about them should start by looking
at Buse (1982).</p>
<p>How does using the likelihood ratio test play out in practice? Lets
look at some possible use cases. (For simplicity, below we will assume
that we have sampled data from an exponential-family distribution, and
thus that we can reduce the data to, e.g., a single-number sufficient
statistic for <span class="math inline">\(\theta\)</span>. If we were to sample data from
non-exponential-family distributions,
we could, as we did in Chapter 3, fall back upon the use of simulations
to determine the empirical distribution
of the statistic <span class="math inline">\(\lambda_{LR}\)</span> [or <span class="math inline">\(\log\lambda_{LR}\)</span>] under the null and
use that distribution to estimate rejection-region boundaries, <span class="math inline">\(p\)</span>-values,
and test power.)</p>
<ol style="list-style-type: decimal">
<li>We have one freely varying parameter <span class="math inline">\(\theta\)</span> and we wish to
test <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span>.</li>
</ol>
<p>In this situation, we identify a sufficient statistic (or a one-to-one
function of it)
for which we know the sampling distribution and use it to define a
two-tail test as we have done previously, utilizing the hypothesis
test reference tables.</p>
<ol start="2" style="list-style-type: decimal">
<li>We have one freely varying parameter <span class="math inline">\(\theta\)</span> and we wish to define
a one-sided alternative hypothesis, e.g., <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>.</li>
</ol>
<p>Here, the null hypothesis would be the complement of <span class="math inline">\(H_a\)</span>, i.e.,
<span class="math inline">\(H_o : \theta \geq \theta_o\)</span>.
The distribution of the ratio
<span class="math display">\[
\lambda_{LR} = \frac{\mbox{sup}_{\theta \geq \theta_o} \mathcal{L}(\theta \vert \mathbf{x})}{\mathcal{L}(\hat{\theta}_{MLE} \vert \mathbf{x})}
\]</span>
is thus not uniquely specifiable: it is a function of <span class="math inline">\(\theta\)</span>, and our
null does not state a specific value for <span class="math inline">\(\theta\)</span>.
So how would we determine <span class="math inline">\(y_{\rm RR}\)</span>?
It turns out the most conservative rejection-region
boundary we can specify is the one that we would derive assuming that
<span class="math inline">\(\theta = \theta_o\)</span>which means that to define an LRT, we would simply
assume <span class="math inline">\(\theta_o\)</span> as our null value, identify a sufficient statistic (or a
one-to-one function of it)
for which we know the sampling distribution, and define a
one-tail test as we have done previously.</p>
<ol start="3" style="list-style-type: decimal">
<li>We have two (or more) freely varying parameters.</li>
</ol>
<p>In this context, we will have joint sufficient statistics
that are sampled from a multivariate sampling distribution that may be
difficult to work with directly. Thus we fall back upon <em>Wilks theorem</em>.</p>
<p>Let <span class="math inline">\(r_o\)</span> denote the number of free parameters in
<span class="math inline">\(H_o: \theta \in \Theta_o\)</span> and let <span class="math inline">\(r\)</span>
denote the number of free parameters in
<span class="math inline">\(\theta \in \Theta = \Theta_o \cup \Theta_a\)</span>.
(To reiterate, <span class="math inline">\(\Theta\)</span> must include <em>all possible values of the parameters</em>.
For instance, if <span class="math inline">\(H_o\)</span> is <span class="math inline">\(\theta = \theta_o\)</span>, then <span class="math inline">\(H_a\)</span> must be <span class="math inline">\(\theta \neq
\theta_o\)</span> and not <span class="math inline">\(\theta &lt; \theta_o\)</span> or <span class="math inline">\(\theta &gt; \theta_o\)</span>.)
For large <span class="math inline">\(n\)</span>,
<span class="math display">\[
-2\log \lambda_{LR} \stackrel{d}{\rightarrow} W \sim \chi_{r-r_o}^2 \,,
\]</span>
and we would reject the null hypothesis if
<span class="math inline">\(-2\log \lambda_{LR} &gt; w_{\rm RR} = F_{W(r-r_o)}^{-1}(1-\alpha)\)</span>.
Since this result is related to the central limit theorem, large <span class="math inline">\(n\)</span>
would be, by rule-of-thumb, 30 or more.</p>
<hr />
<div id="the-uniformly-most-powerful-test-of-poisson-lambda" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets say that we are counting the number of students that enter
a classroom each minute. We assume that the entry of students is
a homogeneous Poisson process (i.e., <span class="math inline">\(\lambda\)</span>, the expected number
of entering students per time period, is constant). We think that
five students, on average, will pass through the door each minute, while
someone else thinks the number will be three. We collect data during
five independent one-minute intervals: 4, 4, 3, 2, 3. Can we
reject our null hypothesis at the level <span class="math inline">\(\alpha = 0.05\)</span>?
What is the <span class="math inline">\(p\)</span>-value?
And what is the power of the test for <span class="math inline">\(\lambda_a = 3\)</span>?</p>
</blockquote>
<blockquote>
<p>We test the simple hypotheses <span class="math inline">\(H_o: \lambda_o = 5\)</span> and <span class="math inline">\(H_a: \lambda_a = 3\)</span>.
The factorized likelihood for our data sample is
<span class="math display">\[
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda} = \underbrace{\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}}_{g(\sum x_i,\lambda)} \cdot \underbrace{\frac{1}{\prod_{i=1}^n x_i!}}_{h(\mathbf{x})} \,.
\]</span>
A sufficient statistic is thus <span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span>
Poisson(<span class="math inline">\(n\lambda\)</span>). Since <span class="math inline">\(E[Y] = n\lambda\)</span> increases with <span class="math inline">\(\lambda\)</span>,
we find ourselves on the yes line of the hypothesis test
reference tables, and specifically the yes line for a lower-tail test,
and the rejection region is
<span class="math inline">\(Y &lt; y_{\rm RR} = F_Y^{-1}(\alpha \vert \lambda_o)\)</span>.</p>
</blockquote>
<blockquote>
<p>We determine <span class="math inline">\(y_{\rm RR}\)</span>
via the <code>R</code> function call <code>y.rr &lt;- qpois(alpha,lambda=n*lambda.o)</code>.
Since <span class="math inline">\(y_{\rm obs} = 16\)</span>
and <span class="math inline">\(y_{\rm RR} = 17\)</span>, we reject the null hypothesis that <span class="math inline">\(\lambda = 5\)</span>.
We can further write down that the
<span class="math inline">\(p\)</span>-value is <code>ppois(y.obs,lambda=n*lambda.o)</code>, which is 0.038.
(Note that since this is a lower-tail/yes test, we need not make a
discreteness correction when computing the <span class="math inline">\(p\)</span>-value, i.e.,
we do not need to subtract 1 from <code>y.obs</code> in the function call above.)
Since the rejection region does not depend on
<span class="math inline">\(\lambda_a\)</span>, we have defined the uniformly most powerful test of
<span class="math inline">\(\lambda_o\)</span> versus <span class="math inline">\(\lambda_a\)</span>.</p>
</blockquote>
<blockquote>
<p>Thus far, the only way that weve utilized the alternative hypothesis
<span class="math inline">\(H_a: \lambda_a = 3\)</span> is when determining that we are conducting a
lower-tail test.
We will now use this value to determine the power of the test.
The power is the probability of rejecting the null hypothesis given
a specific value of <span class="math inline">\(\lambda\)</span>, i.e., <span class="math inline">\(P(Y &lt; y_{\rm RR} \vert \lambda_a)\)</span>.
Here, that value is given by <code>ppois(y.rr-1,n*lambda.a)</code>; since this
is a lower-tail/yes test, we <em>do</em> need to make a discreteness correction
when computing the test power. The power
is 0.664: if <span class="math inline">\(\lambda\)</span> is truly equal to 3,
we would reject the null hypothesis that <span class="math inline">\(\lambda_o = 5\)</span> after collecting
five data some two-thirds of the time.</p>
</blockquote>
<hr />
</div>
<div id="likelihood-ratio-test-of-the-poisson-parameter-lambda" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume the same setting as for the last example, but here,
lets say that we will test <span class="math inline">\(H_o: \lambda = \lambda_o = 5\)</span> versus
<span class="math inline">\(H_a: \lambda \neq \lambda_o\)</span>. The alternative
hypothesis is a composite hypothesis, because it does not uniquely
specify the shape of the probability mass function.
Because we know the sampling distribution for the sufficient
statistic <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> in the Poisson distribution
with parameter <span class="math inline">\(n\lambda\)</span>, we can directly
derive each of the two rejection region boundaries, which are
<span class="math display">\[\begin{align*}
y_{\rm RR,lo} &amp;= F_Y^{-1}(\alpha/2 \vert n\lambda_o) \\
y_{\rm RR,hi} &amp;= F_Y^{-1}(1-\alpha/2 \vert n\lambda_o) \,.
\end{align*}\]</span>
In <code>R</code>, we determine the boundaries as
<code>y.rr.lo &lt;- qpois(0.025,25)</code>, or 16, and
<code>y.rr.hi &lt;- qpois(0.975,25)</code>, or 35.
Our observed statistic
is <span class="math inline">\(y_{\rm obs} = 16\)</span>, thus we fail to reject the null hypothesis.</p>
<p>As a reminder, because the alternative hypothesis is a composite
hypothesis, the NP lemma does not apply here, and thus <em>we cannot
guarantee that the test we have just constructed is
the most powerful of all possible tests</em> of <span class="math inline">\(H_o: \lambda = \lambda_o\)</span>
versus <span class="math inline">\(H_a: \lambda \neq \lambda_o\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="using-wilks-theorem-to-test-hypotheses-about-the-normal-mean" class="section level3 hasAnchor" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Using Wilks Theorem to Test Hypotheses About the Normal Mean<a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We have collected <span class="math inline">\(n\)</span> iid data from a normal distribution and
we wish to test the hypothesis <span class="math inline">\(H_o: \mu = \mu_o\)</span> versus
the hypothesis <span class="math inline">\(H_a: \mu \neq \mu_o\)</span> using the likelihood ratio test.
(We assume the variance is unknown.)</p>
</blockquote>
<blockquote>
<p>For this problem,
<span class="math display">\[\begin{align*}
\Theta_o &amp;= \{ \mu,\sigma^2 : \mu = \mu_o, \sigma^2 &gt; 0 \} \\
\Theta_a &amp;= \{ \mu,\sigma^2 : \mu \neq \mu_o, \sigma^2 &gt; 0 \} \,,
\end{align*}\]</span>
and so <span class="math inline">\(\Theta = \Theta_o \cup \Theta_a = \{ \mu,\sigma^2 :
\mu \in (-\infty,\infty), \sigma^2 &gt; 0 \}\)</span>, with
<span class="math inline">\(r_o = 1\)</span> (<span class="math inline">\(\sigma^2\)</span>) and <span class="math inline">\(r = 2\)</span> (<span class="math inline">\(\mu,\sigma^2\)</span>).</p>
</blockquote>
<blockquote>
<p>The likelihood for the normal pdf is
<span class="math display">\[
\mathcal{L}(\mu,\sigma \vert \mathbf{x}) = \prod_{i=1}^n \frac{1}{2 \pi \sigma^2} \exp\left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right)
\]</span>
and the log-likelihood is
<span class="math display">\[
\ell(\mu,\sigma \vert \mathbf{x}) = -\frac{n}{2} \log(2 \pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2 \,.
\]</span>
The test statistic for Wilks theorem is
<span class="math display">\[
W = -2 \left[ \ell(\mu_o,\widehat{\sigma^2}_{MLE,o} \vert \mathbf{x}) - \ell(\hat{\mu}_{MLE},\widehat{\sigma^2}_{MLE} \vert \mathbf{x}) \right] \,,
\]</span>
where <span class="math inline">\(\hat{\mu}_{MLE}\)</span> is the MLE for <span class="math inline">\(\mu\)</span> and where
<span class="math inline">\(\widehat{\sigma^2}_{MLE,o}\)</span> and
<span class="math inline">\(\widehat{\sigma^2}_{MLE}\)</span> are the MLEs for <span class="math inline">\(\sigma\)</span> given the
restricted and full parameter spaces, respectively
We know these results from previous derivations:
<span class="math display">\[\begin{align*}
\hat{\mu}_{MLE} &amp;= \bar{X} \\
\widehat{\sigma^2}_{MLE,o} &amp;= \frac{n-1}{n}S^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu_o)^2 \\
\widehat{\sigma^2}_{MLE} &amp;= \frac{n-1}{n}S^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \,.
\end{align*}\]</span>
Ultimately, we compare the value of <span class="math inline">\(W\)</span> against a chi-square
distribution for <span class="math inline">\(r-r_o = 1\)</span> degree of freedom, and thus we reject the
null (at level <span class="math inline">\(\alpha = 0.05\)</span>) if <span class="math inline">\(W &gt; 3.841\)</span> (= <code>qchisq(0.95,1)</code>).</p>
</blockquote>
<hr />
</div>
<div id="simulating-the-likelihood-ratio-test" class="section level3 hasAnchor" number="4.7.4">
<h3><span class="header-section-number">4.7.4</span> Simulating the Likelihood Ratio Test<a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Wilks theorem generates an approximate result<span class="math inline">\(-\)</span>it assumes that the
test statistic is chi-square-distributed<span class="math inline">\(-\)</span>and a major issue is that we
do not know how good the approximation is. For instance, lets say
<span class="math inline">\(n = 20\)</span>this might be an insufficient sample size
for the Wilks theorem machinery to yield an accurate and precise result
(at least in terms of a rejection-region boundary).</p>
</blockquote>
<blockquote>
<p>To get a sense as to how well Wilks theorem works for us, we can
run simulations. We simulate sets of data under the null (<span class="math inline">\(\mu = 5\)</span>),
and for each, we compute <span class="math inline">\(W\)</span>. The empirical rejection region boundary
is then the, e.g., 95<span class="math inline">\(^{\rm th}\)</span> percentile of the values of <span class="math inline">\(W\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="the-poisson-and-related-distributions.html#cb285-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb285-2"><a href="the-poisson-and-related-distributions.html#cb285-2" tabindex="-1"></a></span>
<span id="cb285-3"><a href="the-poisson-and-related-distributions.html#cb285-3" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb285-4"><a href="the-poisson-and-related-distributions.html#cb285-4" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb285-5"><a href="the-poisson-and-related-distributions.html#cb285-5" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb285-6"><a href="the-poisson-and-related-distributions.html#cb285-6" tabindex="-1"></a>mu.o    <span class="ot">&lt;-</span> <span class="dv">5</span>      </span>
<span id="cb285-7"><a href="the-poisson-and-related-distributions.html#cb285-7" tabindex="-1"></a>sigma2  <span class="ot">&lt;-</span> <span class="dv">9</span>      <span class="co"># an arbitrary value</span></span>
<span id="cb285-8"><a href="the-poisson-and-related-distributions.html#cb285-8" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb285-9"><a href="the-poisson-and-related-distributions.html#cb285-9" tabindex="-1"></a>W       <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb285-10"><a href="the-poisson-and-related-distributions.html#cb285-10" tabindex="-1"></a></span>
<span id="cb285-11"><a href="the-poisson-and-related-distributions.html#cb285-11" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(X,n,mu.o)</span>
<span id="cb285-12"><a href="the-poisson-and-related-distributions.html#cb285-12" tabindex="-1"></a>{</span>
<span id="cb285-13"><a href="the-poisson-and-related-distributions.html#cb285-13" tabindex="-1"></a>  hat.mu.mle     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb285-14"><a href="the-poisson-and-related-distributions.html#cb285-14" tabindex="-1"></a>  hat.sigma2.mle <span class="ot">&lt;-</span> (n<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">var</span>(X)<span class="sc">/</span>n</span>
<span id="cb285-15"><a href="the-poisson-and-related-distributions.html#cb285-15" tabindex="-1"></a></span>
<span id="cb285-16"><a href="the-poisson-and-related-distributions.html#cb285-16" tabindex="-1"></a>  logL.o <span class="ot">&lt;-</span> <span class="sc">-</span>(n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>hat.sigma2.mle)<span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>hat.sigma2.mle)<span class="sc">*</span><span class="fu">sum</span>((X<span class="sc">-</span>mu.o)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb285-17"><a href="the-poisson-and-related-distributions.html#cb285-17" tabindex="-1"></a>  logL   <span class="ot">&lt;-</span> <span class="sc">-</span>(n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>hat.sigma2.mle)<span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>hat.sigma2.mle)<span class="sc">*</span><span class="fu">sum</span>((X<span class="sc">-</span>hat.mu.mle)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb285-18"><a href="the-poisson-and-related-distributions.html#cb285-18" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>(logL.o <span class="sc">-</span> logL))</span>
<span id="cb285-19"><a href="the-poisson-and-related-distributions.html#cb285-19" tabindex="-1"></a>}</span>
<span id="cb285-20"><a href="the-poisson-and-related-distributions.html#cb285-20" tabindex="-1"></a></span>
<span id="cb285-21"><a href="the-poisson-and-related-distributions.html#cb285-21" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb285-22"><a href="the-poisson-and-related-distributions.html#cb285-22" tabindex="-1"></a>  X     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb285-23"><a href="the-poisson-and-related-distributions.html#cb285-23" tabindex="-1"></a>  W[ii] <span class="ot">&lt;-</span> <span class="fu">f</span>(X,n,mu.o)</span>
<span id="cb285-24"><a href="the-poisson-and-related-distributions.html#cb285-24" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lrtsim"></span>
<img src="_main_files/figure-html/lrtsim-1.png" alt="\label{fig:lrtsim}The empirical distribution of the statistic $-2(\log\mathcal{L}_o - \log\mathcal{L}_a)$, with the chi-square distribution for $\Delta r = 1$ degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks' theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks' theorem at best provides approximate results and that simulations can provide more accurate and precise results." width="50%" />
<p class="caption">
Figure 4.4: The empirical distribution of the statistic <span class="math inline">\(-2(\log\mathcal{L}_o - \log\mathcal{L}_a)\)</span>, with the chi-square distribution for <span class="math inline">\(\Delta r = 1\)</span> degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks theorem at best provides approximate results and that simulations can provide more accurate and precise results.
</p>
</div>
<blockquote>
<p>In this simulation, we find that the empirical rejection-region boundary
is 4.584, which is sufficiently
far from the value 3.841 derived from Wilks theorem to be concerning.
We also find that if we adopt 3.841 as our boundary value, our Type I
error is actually 0.071 instead of the expected value of 0.05.
The upshot: if <span class="math inline">\(n\)</span> is small, it is best <em>not</em> to assume that <span class="math inline">\(W\)</span> is
chi-square-distributed; run simulations to determine rejection region
boundaries and <span class="math inline">\(p\)</span>-values instead.</p>
</blockquote>
</div>
</div>
<div id="the-gamma-distribution" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> The Gamma Distribution<a href="the-poisson-and-related-distributions.html#the-gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>gamma distribution</em> is a continuous distribution that is commonly
used to, e.g., model the waiting times between discrete events. Its
probability density function is given by
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}}{\beta^\alpha} \frac{\exp\left(-x/\beta\right)}{\Gamma(\alpha)} \,,
\]</span>
where <span class="math inline">\(x \in [0,\infty)\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are both <span class="math inline">\(&gt;\)</span> 0,
and <span class="math inline">\(\Gamma(\alpha)\)</span> is the gamma function:
<span class="math display">\[
\Gamma(\alpha) = \int_0^\infty u^{\alpha-1} e^{-u} du \,.
\]</span>
(See Figure <a href="the-poisson-and-related-distributions.html#fig:gammapdf">4.5</a>.)
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are referred to as shape and scale parameters,
respectively. The gamma family of distributions
exhibits a wide variety of functional shapes
and it is the parent family to a number of other
distributions, some of which we have met before.
One in particular is the <em>exponential distribution</em>,
<span class="math display">\[
f_X(x) = \frac{1}{\beta} \exp\left(-\frac{x}{\beta}\right) \,,
\]</span>
which is a gamma distribution with <span class="math inline">\(\alpha = 1\)</span>. Note how we lead off
above by saying that the gamma distribution is commonly used to
model the waiting times between discrete events. The
exponential distribution specifically models the waiting time between
one event and the next in a Poisson process. The number of strong
earthquakes that occur in California in one year? That can be modeled as
a Poisson random variable. The time that elapses between two consecutive
strong earthquakes in California? That can be modeled using the
exponential distribution. (For completeness: the Erlang distribution
is a generalization of the exponential distribution, in the sense
that we can use it to model the waiting time between the <span class="math inline">\(i^{\rm th}\)</span>
and <span class="math inline">\((i+\alpha)^{\rm th}\)</span> events, where <span class="math inline">\(\alpha\)</span> is a positive integer,
in a Poisson process.)</p>
<table>
<caption>Distributions Within the Gamma Family of Distributions</caption>
<thead>
<tr class="header">
<th align="right">distribution</th>
<th align="center"><span class="math inline">\(\alpha\)</span></th>
<th align="center"><span class="math inline">\(\beta\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">exponential</td>
<td align="center">1</td>
<td align="center"><span class="math inline">\((0,\infty)\)</span></td>
</tr>
<tr class="even">
<td align="right">Erlang</td>
<td align="center"><span class="math inline">\(\{1,2,3,\ldots\}\)</span></td>
<td align="center"><span class="math inline">\((0,\infty)\)</span></td>
</tr>
<tr class="odd">
<td align="right">chi-square</td>
<td align="center"><span class="math inline">\(\{1/2,1,3/2,\ldots\}\)</span></td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>Lets conclude this section by repeating the exercise we did in the
last chapter while discussing the beta distribution, the one in which
we examined the functional form of the likelihood function
<span class="math inline">\(\mathcal{L}(p \vert k,x)\)</span>. Here, we write down the Poisson likelihood
function
<span class="math display">\[
\mathcal{L}(\lambda \vert x) = \frac{\lambda^x}{x!} e^{-\lambda}
\]</span>
and compare it with the gamma pdf. We can match the gamma pdf if
we map the Poisson <span class="math inline">\(\lambda\)</span> to the gamma <span class="math inline">\(x\)</span>,
the Poisson <span class="math inline">\(x\)</span> to the gamma <span class="math inline">\(\alpha-1\)</span>, and we set <span class="math inline">\(\beta\)</span> to 1.
But because the Poisson <span class="math inline">\(x\)</span> is an integer with values <span class="math inline">\(\{0,1,2,\ldots\}\)</span>,
we find that the integrand specifically
matches the Erlang pdf, for which <span class="math inline">\(\alpha = \{1,2,3,\ldots\}\)</span>.
So, if we observe a random variable <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>),
then the likelihood function <span class="math inline">\(\mathcal{L}(\lambda \vert x)\)</span> has
the shape (and normalization!) of a Gamma(<span class="math inline">\(x+1,1\)</span>) (or Erlang(<span class="math inline">\(x+1\)</span>))
distribution.</p>
<p>(About the normalization: if we integrate the likelihood function over
its domain, we find that
<span class="math display">\[
\frac{1}{x!} \int_0^\infty \lambda^x e^{-\lambda} d\lambda = \frac{1}{x!} \Gamma(x+1) = \frac{x!}{x!} = 1 \,.
\]</span>
The mathematics works out because <span class="math inline">\(x\)</span> is integer valued and thus
<span class="math inline">\(\Gamma(x+1) = x!\)</span>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gammapdf"></span>
<img src="_main_files/figure-html/gammapdf-1.png" alt="\label{fig:gammapdf}Three examples of gamma probability density functions: Gamma(2,2) (solid red line), Gamma(4,2) (dashed green line), and Gamma(2,3) (dotted blue line)." width="50%" />
<p class="caption">
Figure 4.5: Three examples of gamma probability density functions: Gamma(2,2) (solid red line), Gamma(4,2) (dashed green line), and Gamma(2,3) (dotted blue line).
</p>
</div>
<hr />
<div id="the-expected-value-of-a-gamma-random-variable" class="section level3 hasAnchor" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> The Expected Value of a Gamma Random Variable<a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a gamma random variable is found by introducing
constants into the expected value integral so that a gamma pdf integrand
is formed. Specifically
<span class="math display">\[\begin{align*}
E[X] = \int_0^\infty x f_X(x) dx &amp;= \int_0^\infty x \frac{x^{\alpha-1}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} \frac{\Gamma(\alpha+1)}{\Gamma(\alpha+1)} \frac{\beta^{\alpha+1}}{\beta^{\alpha+1}} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^{\alpha+1}} \frac{\exp(-x/\beta)}{\Gamma(\alpha+1)} \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \frac{\beta^{\alpha+1}}{\beta^\alpha} dx \\
&amp;= \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \frac{\beta^{\alpha+1}}{\beta^\alpha} \int_0^\infty \frac{x^{\alpha}}{\beta^{\alpha+1}} \frac{\exp(-x/\beta)}{\Gamma(\alpha+1)} dx \\
&amp;= \frac{\alpha \Gamma(\alpha)}{\Gamma(\alpha)} \beta \times 1 \\
&amp;= \alpha \beta \,.
\end{align*}\]</span>
By introducing the constants, we are able to transform the integrand
to that of a Gamma(<span class="math inline">\(\alpha+1,\beta\)</span>) distribution, and because the
integral is over the entire domain of a gamma distribution, the
integral evaluates to 1.</p>
</blockquote>
<blockquote>
<p>A similar calculation involving the derivation of <span class="math inline">\(E[X^2]\)</span> allows us
to determine that the variance of a gamma random variable is
<span class="math inline">\(V[X] = \alpha \beta^2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-distribution-of-the-sum-of-exponential-random-variables" class="section level3 hasAnchor" number="4.8.2">
<h3><span class="header-section-number">4.8.2</span> The Distribution of the Sum of Exponential Random Variables<a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As stated above, the exponential distribution, i.e., the gamma
distribution with <span class="math inline">\(\alpha = 1\)</span>, is used to model the waiting time
between two successive events in a Poisson process. Lets assume
that we have recorded <span class="math inline">\(n\)</span> separate times between <span class="math inline">\(n\)</span> separate pairs
of events. What is the distribution of <span class="math inline">\(T = T_1 + \cdots + T_n\)</span>?</p>
</blockquote>
<blockquote>
<p>As we do when faced with a linear function of <span class="math inline">\(n\)</span> iid
random variables, we utilize the method of moment-generating functions:
<span class="math display">\[
m_T(t) = \prod_{i=1}^n m_{T_i}(t) \,,
\]</span>
where <span class="math inline">\(m_{T_i}(t) = (1-\beta t)^{-1}\)</span> is the mgf for the exponential
distribution. Thus
<span class="math display">\[
m_T(t) = \prod_{i=1}^n (1-\beta t)^{-1} = (1-\beta t)^{-n} \,.
\]</span>
This has the form of the mgf for a Gamma(<span class="math inline">\(n,\beta\)</span>) distribution,
or, equivalently, an Erlang(<span class="math inline">\(n,\beta\)</span>) distribution. In other words
the sum of <span class="math inline">\(n\)</span> iid waiting times has the same distribution as
the waiting time between the <span class="math inline">\(i^{\rm th}\)</span> and <span class="math inline">\((i+n)^{\rm th}\)</span>
events of a Poisson process.</p>
</blockquote>
<hr />
</div>
<div id="memorylessness-and-the-exponential-distribution" class="section level3 hasAnchor" number="4.8.3">
<h3><span class="header-section-number">4.8.3</span> Memorylessness and the Exponential Distribution<a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>An important feature of the exponential distribution is that when we
use it to model, e.g., the lifetimes of components in a system, it
exhibits <em>memorylessness</em>. In other words, if <span class="math inline">\(T\)</span> is the random variable
representing a components lifetime, where <span class="math inline">\(T \sim\)</span> Exponential(<span class="math inline">\(\beta\)</span>)
and where <span class="math inline">\(E[T] = \beta\)</span>, it doesnt matter how old the component is when
we first examine it: <em>from that point onward</em>, the average lifetime
will be <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets demonstrate how this works. We examine a component
born at time <span class="math inline">\(t_0=0\)</span> at a later time <span class="math inline">\(t_1\)</span>, and we wish to determine
the probability that it will live beyond an even later time <span class="math inline">\(t_2\)</span>. In other
words, we wish to compute
<span class="math display">\[
P(T \geq t_2-t_0 \vert T \geq t_1-t_0) \,.
\]</span>
(We know the component lived to time <span class="math inline">\(t_1\)</span>, hence the added condition.)
Let <span class="math inline">\(T \sim\)</span> Exponential(<span class="math inline">\(\beta\)</span>). Then
<span class="math display">\[\begin{align*}
P(T \geq t_2-t_0 \vert T \geq t_1-t_0) &amp;= \frac{P(T \geq t_2-t_0 \cap T \geq t_1-t_0)}{P(T \geq t_1-t_0)} \\
&amp;= \frac{P(T \geq t_2-t_0)}{P(T \geq t_1-t_0)} \\
&amp;= \frac{\int_{t_2-t_0}^\infty (1/\beta) \exp(-t/\beta) dt}{\int_{t_1-t_0}^\infty (1/\beta) \exp(-t/\beta) dt} \\
&amp;= \frac{\left. -\exp(-t/\beta) \right|_{t_2-t_0}^\infty}{\left. -\exp(-t/\beta) \right|_{t_1-t_0}^\infty} \\
&amp;= \frac{0 + \exp(-(t_2-t_0)/\beta)}{0 + \exp(-(t_1-t_0)/\beta)} \\
&amp;= \exp[-(t_2-t_1)/\beta] = P(T \geq t_2-t_1) \,.
\end{align*}\]</span>
Note that <span class="math inline">\(t_0\)</span> drops out of the final result: no matter how
long ago <span class="math inline">\(t_0\)</span> might have been, the probability that the component will
live <span class="math inline">\(t_2-t_1\)</span> units longer is the same, and the average additional lifetime
is still <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
</div>
</div>
<div id="poisson-regression" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Poisson Regression<a href="the-poisson-and-related-distributions.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that for a given measurement <span class="math inline">\(x\)</span>, we record a random
variable <span class="math inline">\(Y\)</span> that is a number of counts. For instance,
<span class="math inline">\(x\)</span> might be the time of day, and <span class="math inline">\(Y\)</span> might be the observed number
of cars parked in a lot at that time. Because <span class="math inline">\(Y\)</span> is (a) integer valued,
and (b) non-negative, an appropriate distribution for the random variable
<span class="math inline">\(Y \vert x\)</span> might be the Poisson distribution, and thus to model these
data, we may want to pursue the use of Poisson regression.</p>
<p><strong>Recall:</strong> <em>To implement a generalized linear model (or GLM),
we need to do two things:</em></p>
<ol style="list-style-type: decimal">
<li><em>examine the <span class="math inline">\(Y_i\)</span> values and select an appropriate distribution for them
(discrete or continuous?
what is the functional domain?); and</em></li>
<li><em>define a link function <span class="math inline">\(g(\theta \vert x)\)</span> that maps the line
<span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, which has infinite range, into a more
limited range (e.g., <span class="math inline">\([0,\infty)\)</span>).</em></li>
</ol>
<p>Because <span class="math inline">\(\lambda &gt; 0\)</span>, in Poisson regression we adopt a link function
that maps <span class="math inline">\(\beta_0 + \beta_1 x\)</span> from the range <span class="math inline">\((-\infty,\infty)\)</span>
to <span class="math inline">\((0,\infty)\)</span>. There is no unique choice of link function, but the
conventionally applied one is the logarithm:
<span class="math display">\[
g(\lambda \vert x) = \log (\lambda \vert x) = \beta_0 + \beta_1 x ~~\implies~~ \lambda \vert x = e^{\beta_0 + \beta_1 x} \,.
\]</span>
Similar to logistic regression, our goal is to estimate
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which is done via numerical optimization of the
likelihood function
<span class="math display">\[
\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y}) = \prod_{i=1}^n p_{Y \vert \beta_0,\beta_1}(y_i \vert \beta_0,\beta_1) = \prod_{i=1}^n \frac{\exp(\beta_0+\beta_1x_i)^{x_i}}{x_i!} e^{-\exp(\beta_0+\beta_1x_i)} \,.
\]</span></p>
<hr />
<p>For the Poisson distribution, <span class="math inline">\(E[X] = V[X] = \lambda\)</span>, so the expectation
is that for any given value <span class="math inline">\(x\)</span>, <span class="math inline">\(E[Y \vert x] = V[Y \vert x]\)</span>. However,
it is commonly seen in real-life data that the sample variance of
<span class="math inline">\(Y \vert x\)</span> exceeds the sample mean. This is dubbed <em>overdispersion</em>, and
it can arise when, e.g., the observed Poisson process is inhomogeneousor
differently stated, when <span class="math inline">\(\lambda\)</span> varies as a function of space and/or time.
A standard way to deal with overdispersion is to move from Poisson regression
to <em>negative binomial regression</em>.</p>
<p>Before we say morewe note that while the name
negative binomial regression is technically
correct (in the sense that the model assumes that the response data are
negative binomially distributed), it can be very confusing for those new
to regression, who might view the negative binomial as a distribution that is
only useful when, e.g., modeling failures in Bernoulli trials.
How could that distribution possibly apply here? The answer is that the
negative binomial probability mass function is just a function (and as
such, it is allowed to have more general use than just modeling failures),
but more to the point, it is a function that arises naturally when we
apply the Law of Total Probability to the Poisson pmf.</p>
<p>Lets suppose that <span class="math inline">\(Y \vert x \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>), but that <span class="math inline">\(\lambda\)</span>
itself is a random variable. There is no unique way to model the
distribution of <span class="math inline">\(\lambda\)</span>, but the gamma distribution provides a flexible
means by which to do so (since the family encompasses a wide variety of
shapes, unlike, say, the normal distribution, which can never be skew).
Lets assume that <span class="math inline">\(\lambda \sim\)</span> Gamma(<span class="math inline">\(\theta,p/\theta\)</span>). The distribution of <span class="math inline">\(Y\)</span>
is found with the LoTP:
<span class="math display">\[\begin{align*}
p_{Y \vert \theta,p}(y \vert \theta,p) &amp;= \int_0^\infty p_{Y \vert \lambda}(y \vert \lambda) f_{\lambda}(\lambda \vert \theta,p) d\lambda \\
&amp;= \int_0^\infty \frac{\lambda^y}{y!} e^{-\lambda} \frac{\lambda^{\theta-1}}{(p/\theta)^\theta} \frac{e^{-\lambda/(p/\theta)}}{\Gamma(\theta)} d\lambda \\
&amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \int_0^\infty \lambda^{y+\theta+1} e^{-\lambda(1+\theta/p)} d\lambda \,.
\end{align*}\]</span>
The integrand looks suspiciously like the integrand of a gamma function
integral, but we have to change <span class="math inline">\(\lambda(1+\theta/p)\)</span> in the exponential to
just <span class="math inline">\(\lambda&#39;\)</span>:
<span class="math display">\[
\lambda&#39; = \lambda(1+\theta/p) ~~ \mbox{and} ~~ d\lambda&#39; = d\lambda (1+\theta/p) \,.
\]</span>
The bounds of the integral do not change. The integral now becomes
<span class="math display">\[\begin{align*}
p_{Y \vert \theta,p}(y \vert \theta,p) &amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \frac{1}{(1+\theta/p)^{y+\theta}} \int_0^\infty (\lambda&#39;)^{y+\theta-1} e^{-\lambda&#39;} d\lambda&#39; \\
&amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \frac{1}{(1+\theta/p)^{y+\theta}} \Gamma(y+\theta) \\
&amp;= \frac{(y+\theta-1)!}{y! (\theta-1)!} \left(\frac{\theta}{p}\right)^\theta \left(\frac{p}{p+\theta}\right)^{y+\theta} \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{p}{p+\theta}\right)^y \left(\frac{\theta}{p}\right)^\theta \left(\frac{p}{p+\theta}\right)^\theta \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{p}{p+\theta}\right)^y \left(\frac{\theta}{p+\theta}\right)^\theta \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{\theta}{p+\theta}\right)^\theta \left(1 - \frac{\theta}{p+\theta}\right)^y \,.
\end{align*}\]</span>
This has the functional form of a negative binomial pmf in which
<span class="math inline">\(y\)</span> represents the number of failures,
<span class="math inline">\(\theta\)</span> is the number of successes, and <span class="math inline">\(\theta/(p+\theta)\)</span> is the
probability of success.</p>
<p>Now, why do we choose this form of the negative binomial distribution?
We do so because it so happens that
<span class="math display">\[\begin{align*}
E[Y] &amp;= p \\
V[Y] &amp;= p + \frac{p^2}{\theta} \,.
\end{align*}\]</span>
(One can derive these quantities starting with, e.g.,
<span class="math inline">\(E[Y] = \theta(1-p&#39;)/p&#39;\)</span> and <span class="math inline">\(V[Y] = \theta(1-p&#39;)/(p&#39;)^2\)</span> and
plugging in <span class="math inline">\(p&#39; = \theta/(p+\theta)\)</span>.)
Varying <span class="math inline">\(\theta\)</span> thus allows us to model the overdispersion with a single
variable. (We assume that <span class="math inline">\(p\)</span> takes the place of <span class="math inline">\(\lambda\)</span>, in the
sense that now <span class="math inline">\(p \vert x = \exp(\beta_0 + \beta_1x\)</span>).)
In the limit that <span class="math inline">\(\theta \rightarrow \infty\)</span>, negative binomial regression
becomes Poisson regression. Because the overdispersion is represented
in a single variable, we can examine the results of learning both Poisson and
negative binomial regression models to determine whether or not
the quality of fit improves enough to justify the extra model complexity.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example-2" class="section level3 hasAnchor" number="4.9.1">
<h3><span class="header-section-number">4.9.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Modeling Bortkiewiczs horse-kick dataset provides a simple example
of the use of Poisson regression.</p>
</blockquote>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="the-poisson-and-related-distributions.html#cb286-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb286-2"><a href="the-poisson-and-related-distributions.html#cb286-2" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">109</span>,<span class="dv">65</span>,<span class="dv">22</span>,<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb286-3"><a href="the-poisson-and-related-distributions.html#cb286-3" tabindex="-1"></a></span>
<span id="cb286-4"><a href="the-poisson-and-related-distributions.html#cb286-4" tabindex="-1"></a>poi.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y<span class="sc">~</span>x,<span class="at">family=</span>poisson)</span>
<span id="cb286-5"><a href="the-poisson-and-related-distributions.html#cb286-5" tabindex="-1"></a><span class="fu">summary</span>(poi.out)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ x, family = poisson)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  4.80136    0.08490   56.55   &lt;2e-16 ***
## x           -0.92213    0.07704  -11.97   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 232.430  on 4  degrees of freedom
## Residual deviance:  12.437  on 3  degrees of freedom
## AIC: 38.911
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<blockquote>
<p>The summary output from the Poisson regression model is, in its structure,
identical to that of logistic regression. But there are some differences
in how values are defined. For instance, the deviance residual is
<span class="math display">\[
d_i = \mbox{sign}(Y_i - \hat{Y}_i) \sqrt{2[Y_i \log (Y_i/\hat{Y}_i) - (Y_i - \hat{Y}_i)]} \,,
\]</span>
where
<span class="math display">\[
\hat{Y}_i = \hat{\lambda}_i = \exp(\hat{\beta}_0+\hat{\beta}_1 x_i) \,.
\]</span>
(Note that when <span class="math inline">\(Y_i = 0\)</span>, <span class="math inline">\(Y_i \log (Y_i/\hat{Y}_i)\)</span> is assumed to
be zero.)
Because there are only five data points in the fit, all the deviance
residual values are displayed, rather than a five-number summary.
Also, the residual deviance (here, 12.437) is <em>not</em>
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>, as it was for logistic regression. There
are two ways to determine <span class="math inline">\(\mathcal{L}_{max}\)</span>; one is to take the AIC
value (38.911), subtract 2 times the number of model terms (2 here,
thus yielding 34.911), and then dividing by <span class="math inline">\(-2\)</span>. The more straightforward
way, however, is to utilize the <code>logLik()</code> function:</p>
</blockquote>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="the-poisson-and-related-distributions.html#cb288-1" tabindex="-1"></a><span class="fu">logLik</span>(poi.out)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -17.45551 (df=2)</code></pre>
<blockquote>
<p>As we did in the context of logistic regression, we can use the
the difference in the values of the null and residual deviances
(here, 232.430-12.437 = 219.993) to test the null hypothesis that
<span class="math inline">\(\beta_1 = 0\)</span>. The difference is assumed to be sampled from a chi-square
distribution for 4-3 = 1 degree of freedom. The <span class="math inline">\(p\)</span>-value is
<code>1 - pchisq(220.0,1)</code> or effectively zero: we emphatically reject the
null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
</blockquote>
<blockquote>
<p>As a final note, unlike the case of logistic regression where determining
the quality of fit of the learned model is not particularly straightforward,
for Poisson regression we can simply assume that the residual deviance
is chi-square-distributed for the given number of degrees of freedom.
Here, the <span class="math inline">\(p\)</span>-value is</p>
</blockquote>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="the-poisson-and-related-distributions.html#cb290-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="fl">12.437</span>,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.006026712</code></pre>
<blockquote>
<p>or 0.0060. Because this value is less than, e.g., <span class="math inline">\(\alpha = 0.05\)</span>, we
would (in this instance) reject the null hypothesis that the observed
data are truly Poisson distributed.</p>
</blockquote>
<hr />
</div>
<div id="negative-binomial-regression-example" class="section level3 hasAnchor" number="4.9.2">
<h3><span class="header-section-number">4.9.2</span> Negative Binomial Regression Example<a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the code chunk below, we simulate 100 data at each of four
different values of <span class="math inline">\(x\)</span>: 1, 2, 3, and 4. The data are simulated
with a Poisson overdispersion factor of 2.</p>
</blockquote>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="the-poisson-and-related-distributions.html#cb292-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">236</span>)</span>
<span id="cb292-2"><a href="the-poisson-and-related-distributions.html#cb292-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>               <span class="co"># 100 data per x value</span></span>
<span id="cb292-3"><a href="the-poisson-and-related-distributions.html#cb292-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>),n)</span>
<span id="cb292-4"><a href="the-poisson-and-related-distributions.html#cb292-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,<span class="fu">length</span>(x))</span>
<span id="cb292-5"><a href="the-poisson-and-related-distributions.html#cb292-5" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x) ) {</span>
<span id="cb292-6"><a href="the-poisson-and-related-distributions.html#cb292-6" tabindex="-1"></a>  Y[ii] <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">1</span>,<span class="fu">rgamma</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="at">scale=</span>x[ii]<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb292-7"><a href="the-poisson-and-related-distributions.html#cb292-7" tabindex="-1"></a>}</span></code></pre></div>
<blockquote>
<p>For these data, <span class="math inline">\(E[Y \vert x] = x\)</span> and
<span class="math inline">\(V[Y \vert x] = x + x^2/2\)</span>, meaning that the
overdispersion factor is, again, <span class="math inline">\(\theta = 2\)</span>. Lets see
how overdispersion affects the learning of a
Poisson regression model.</p>
</blockquote>
<blockquote>
<p>First, the Poisson regression model itself:</p>
</blockquote>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="the-poisson-and-related-distributions.html#cb293-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm</span>(Y<span class="sc">~</span>x,<span class="at">family=</span>poisson))</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ x, family = poisson)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.08336    0.09240  -0.902    0.367    
## x            0.38014    0.02944  12.913   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 1107  on 399  degrees of freedom
## Residual deviance:  930  on 398  degrees of freedom
## AIC: 1805
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<blockquote>
<p>and second, the negative binomial regression model,
as learned using the <code>glm.nb()</code> function of the <code>MASS</code>
package. (Note that <code>MASS</code> does not represent the state
of Massachusetts, but rather stands for Modern Applied
Statistics with Swith <code>S</code> being the precursor
software to <code>R</code>.)</p>
</blockquote>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="the-poisson-and-related-distributions.html#cb295-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb295-2"><a href="the-poisson-and-related-distributions.html#cb295-2" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm.nb</span>(Y<span class="sc">~</span>x))</span></code></pre></div>
<pre><code>## 
## Call:
## glm.nb(formula = Y ~ x, init.theta = 1.84882707, link = log)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.10769    0.13101  -0.822    0.411    
## x            0.38908    0.04483   8.679   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(1.8488) family taken to be 1)
## 
##     Null deviance: 530.52  on 399  degrees of freedom
## Residual deviance: 454.45  on 398  degrees of freedom
## AIC: 1633.9
## 
## Number of Fisher Scoring iterations: 1
## 
## 
##               Theta:  1.849 
##           Std. Err.:  0.258 
## 
##  2 x log-likelihood:  -1627.868</code></pre>
<blockquote>
<p>The negative binomial model is displayed in Figure <a href="the-poisson-and-related-distributions.html#fig:nbmodel">4.6</a>.
When we compare the output, we first look for the lines
beginning with <code>AIC</code>:</p>
</blockquote>
<pre><code>AIC: 1805    [Poisson regression]
AIC: 1633.9  [negative binomial regression]</code></pre>
<blockquote>
<p>The Akaike Information Criterion, or AIC, as we will recall, is
a quality-of-fit metric that penalizes model complexity.
If we learn a suite of models, we would generally adopt that associated
with the lowest
AIC value. Here, the negative binomial model has a much lower AIC value
than the Poisson model, so we would definitely adopt it!
But is the negative binomial model a good model in an absolute sense?
To determine that, we would compare the residual deviance value of
454.45 against a chi-square distribution with 398 degrees of freedom;
the result is</p>
</blockquote>
<pre><code>1 - pchisq(454.45,398) = 0.026</code></pre>
<blockquote>
<p>We would reject the null hypothesis that the negative binomial model
is the correct representation of the data-generating process, but we note
that the <span class="math inline">\(p\)</span>-value is only slightly smaller than 0.05: the model may
not be technically correct, but it appears to be at least approximately
correct!</p>
</blockquote>
<blockquote>
<p>The estimated value of the overdispersion parameter is
<span class="math inline">\(\hat{\theta} = 1.849\)</span>, with estimated standard error
<span class="math inline">\(0.258\)</span>. This is consistent with the true value
<span class="math inline">\(\theta = 2\)</span> and is definitely inconsistent with
<span class="math inline">\(\theta = \infty\)</span> (the value for truly Poisson-distributed data).
(Note that when the data are <em>not</em> overdispersed, <span class="math inline">\(\hat{\theta}\)</span>
will usually be a large number, but not actually infinity.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbmodel"></span>
<img src="_main_files/figure-html/nbmodel-1.png" alt="\label{fig:nbmodel}The negative binomial regression line superimposed on the simulated data. The `jitter()` function is applied to the $x$ values to allow us to more easily see the number of counts as a function of $x$ and $Y$." width="50%" />
<p class="caption">
Figure 4.6: The negative binomial regression line superimposed on the simulated data. The <code>jitter()</code> function is applied to the <span class="math inline">\(x\)</span> values to allow us to more easily see the number of counts as a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>.
</p>
</div>
</div>
</div>
<div id="chi-square-based-hypothesis-testing-1" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Chi-Square-Based Hypothesis Testing<a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter, we introduced the chi-square goodness-of-fit test
as a way of conducting hypothesis tests given multinomial data. Given <span class="math inline">\(k\)</span>
data recorded in <span class="math inline">\(m\)</span> separate bins, we can compute the test statistic
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} \,,
\]</span>
where <span class="math inline">\(X_i\)</span> is the number of data observed in bin <span class="math inline">\(i\)</span>, and where
<span class="math inline">\(p_i\)</span> is the probability of any one datum being observed in that bin
under the null.
If <span class="math inline">\(k\)</span> is sufficiently large, then <span class="math inline">\(W\)</span> converges in distribution to
a chi-square random variable for <span class="math inline">\(m-1\)</span> degrees of freedom. (Hence the
name of the test!)</p>
<p>But: what if the overall observed number of data <span class="math inline">\(X\)</span>
is a random variable instead of being a set constant <span class="math inline">\(X=k\)</span>?
Imagine a very simple digital camera that has four light collecting
bins, each of the same size, and we point it towards a light
source that emits an average of <span class="math inline">\(\lambda\)</span> photons in a particular
time period. If we open the shutter for that time period, what we
would observe is <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>) photons, with
the numbers in each bin being <span class="math inline">\(\{X_1,X_2,X_3,X_4\}\)</span>. Now lets
say we want to test the hypothesis that the probability of a photon
falling into each bin is the same, i.e., <span class="math inline">\(H_o: p_1 = p_2 = p_3 = p_4 = 1/4\)</span>.
If we were to use the chi-square GoF test directly, we would compute
<span class="math display">\[
w_{obs} = \sum_{i=1}^4 \frac{(X_i - \lambda p_i)^2}{\lambda p_i}
\]</span>
and then compute the <span class="math inline">\(p\)</span>-value <span class="math inline">\(1-F_W(w_{obs})\)</span> assume 3 degrees of freedom.
(In <code>R</code>, this would be computed via <code>1 - pchisq(w.obs,3)</code>.) We can do
this<span class="math inline">\(-\)</span>the computer will not stop us from doing so<span class="math inline">\(-\)</span>but is this valid?</p>
<p>The answer is that this <em>is</em> valid so long as each <span class="math inline">\(X_i\)</span> converges in
distribution to a normal random variable. And Poisson random variables
<em>do</em> converge in distribution to normal random variables. Thus, in short,
yes, use of the chi-square GoF test is valid if <span class="math inline">\(\lambda p_i\)</span> is large.
The question is, how large?</p>
<hr />
<p>The Poisson probability mass function is
<span class="math display">\[
p_X(x \vert \lambda) = \frac{\lambda^x}{x!} e^{-\lambda} \,.
\]</span>
We note that the normal probability density function does not have
a factorial in it, so we will start by using Stirlings approximation:
<span class="math display">\[
x! \approx \sqrt{2 \pi x} x^x e^{-x} \,.
\]</span>
This approximation has an error of 1.65% for <span class="math inline">\(x = 5\)</span> and 0.83% for <span class="math inline">\(x = 10\)</span>,
with the percentage error continuing to shrink as <span class="math inline">\(x \rightarrow \infty\)</span>.
With this approximation, we can write that
<span class="math display">\[
p_X(x \vert \lambda) \approx \frac{\lambda^x}{\sqrt{2 \pi x}} x^{-x} e^{x-\lambda} \,.
\]</span>
This still does not quite look like a normal pdf. So there is more work
to do. We compute the logarithm of this quantity:
<span class="math display">\[
\log p_X(x \vert \lambda) \approx x \log \lambda - \frac{1}{2} \log (2 \pi x) - x \log x + x - \lambda = -x ( \log x - \log \lambda ) - \frac{1}{2} \log (2 \pi x) + x - \lambda \,,
\]</span>
and then look at <span class="math inline">\(\log x - \log \lambda\)</span>:
<span class="math display">\[
\log x - \log \lambda = \log \frac{x}{\lambda} = \log \left( 1 - \frac{\lambda-x}{\lambda}\right) \approx -\frac{\delta}{\sqrt{\lambda}} - \frac{\delta^2}{2 \lambda} - \cdots \,.
\]</span>
Here, <span class="math inline">\(\delta = (\lambda - x)/\sqrt{\lambda}\)</span>. Plugging this result
into the expression for <span class="math inline">\(\log p_X(x \vert \lambda)\)</span>, we find that
<span class="math display">\[
\log p_X(x \vert \lambda) \approx -\frac{1}{2} \log (2 \pi x) + x \left( \frac{\delta}{\sqrt{\lambda}} + \frac{\delta^2}{2\lambda} \right) - \delta \sqrt{\lambda} \,.
\]</span>
The next step is plug in <span class="math inline">\(x = \lambda - \sqrt{\lambda}\delta\)</span>:
<span class="math display">\[\begin{align*}
\log p_X(x \vert \lambda) &amp;\approx -\frac{1}{2} \log (2 \pi x) + (\lambda - \sqrt{\lambda}\delta)\left( \frac{\delta}{\sqrt{\lambda}} + \frac{\delta^2}{2\lambda} \right) - \delta \sqrt{\lambda} \\
&amp;= -\frac{1}{2} \log (2 \pi x) + \sqrt{\lambda}\delta - \delta^2 + \frac{\delta^2}{2} - \frac{\delta^3}{2 \lambda^{3/2}} - \sqrt{\lambda}\delta \\
&amp;\approx -\frac{1}{2} \log (2 \pi x) - \frac{\delta^2}{2} \,,
\end{align*}\]</span>
where we drop the <span class="math inline">\(\mathcal{O}(\delta^3)\)</span> term.
When we exponentiate both sides, the final result is
<span class="math display">\[
p_X(x \vert \lambda) \approx \frac{1}{\sqrt{2 \pi x}} \exp\left( -\frac{\delta^2}{2} \right) = \frac{1}{\sqrt{2 \pi x}} \exp\left( -\frac{(x-\lambda)^2}{2\sqrt{\lambda}} \right) \,.
\]</span>
This has the (approximate) form of a normal pdf, at least for values
<span class="math inline">\(x \approx \lambda\)</span>. So, in the end, the Poisson probability mass function
<span class="math inline">\(p_X(x \vert \lambda)\)</span>
has approximately the same shape as the normal probability density function
<span class="math inline">\(f_X(x \vert \mu=\lambda,\sigma^2=\lambda)\)</span> for <span class="math inline">\(x \gg 1\)</span> and
<span class="math inline">\(x \approx \lambda\)</span>.</p>
<p>The conventional rule-of-thumb is that one can utilize the chi-square
GoF test with Poisson data so long as <span class="math inline">\(\lambda p_i \geq 5\)</span> counts
in each bin.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example-3" class="section level3 hasAnchor" number="4.10.1">
<h3><span class="header-section-number">4.10.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In previous examples, we have shown that over the course of 20 years,
in 10 separate Prussian army corps, soldiers died as a result of horse
kicks at a rate of <span class="math inline">\(\hat{\lambda} = \bar{X} = 0.61\)</span> deaths per corps
per year, and that a 95% confidence interval for <span class="math inline">\(\lambda\)</span> is [0.51,0.73].
However, we never examined what is perhaps the most important question of
all: is it plausible that the data are Poisson-distributed in the first place?</p>
</blockquote>
<blockquote>
<p>We last looked at the idea of performing hypothesis tests regarding
distributions in Chapter 2, where we introduce the Kolmogorov-Smirnov
test for use with arbitrary distributions and the Shapiro-Wilk test
to assess the plausibility that our data are normally distributed. So
it would seem that here we should work with the KS test, as the Poisson
distribution is not the normal distributionbut we can only use
the KS test in the context of <em>continuous</em> distributions. So we need
a new method!</p>
</blockquote>
<blockquote>
<p>We can utilize the chi-square goodness-of-fit test. Lets assume
<span class="math inline">\(\lambda = 0.61\)</span>. Then the probabilities <span class="math inline">\(P(X=x)\)</span> are as follows:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>P(X=x)</th>
<th><span class="math inline">\(E_x\)</span></th>
<th><span class="math inline">\(O_x\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.543</td>
<td>108.67</td>
<td>109</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.331</td>
<td>66.29</td>
<td>65</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.101</td>
<td>20.29</td>
<td>22</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.021</td>
<td>4.11</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.003</td>
<td>0.63</td>
<td>1</td>
</tr>
</tbody>
</table>
<blockquote>
<p>The conventional rule-of-thumb is that the expected number of counts
in each bin must be <span class="math inline">\(\geq 5\)</span>. Here, we will break that rule slightly
by combining bins 3 and 4 into one bin with expected counts 4.11 + 0.63
= 4.74. The chi-square GOF test statistic is thus
<span class="math display">\[
W = \frac{(109-108.67)^2}{108.67} + \frac{(65-66.29)^2}{66.29} + \frac{(22-20.29)^2}{20.29} + \frac{(4-4.74)^2}{4.74} = 0.298 \,.
\]</span>
This figure can be found using <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="the-poisson-and-related-distributions.html#cb299-1" tabindex="-1"></a>e    <span class="ot">&lt;-</span> <span class="dv">200</span><span class="sc">*</span><span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>,<span class="fl">0.61</span>)</span>
<span id="cb299-2"><a href="the-poisson-and-related-distributions.html#cb299-2" tabindex="-1"></a>e[<span class="dv">4</span>] <span class="ot">&lt;-</span> e[<span class="dv">4</span>]<span class="sc">+</span>e[<span class="dv">5</span>]</span>
<span id="cb299-3"><a href="the-poisson-and-related-distributions.html#cb299-3" tabindex="-1"></a>e    <span class="ot">&lt;-</span> e[<span class="sc">-</span><span class="dv">5</span>]</span>
<span id="cb299-4"><a href="the-poisson-and-related-distributions.html#cb299-4" tabindex="-1"></a>o    <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">109</span>,<span class="dv">65</span>,<span class="dv">22</span>,<span class="dv">4</span>)</span>
<span id="cb299-5"><a href="the-poisson-and-related-distributions.html#cb299-5" tabindex="-1"></a></span>
<span id="cb299-6"><a href="the-poisson-and-related-distributions.html#cb299-6" tabindex="-1"></a>(<span class="at">W =</span> <span class="fu">sum</span>((o<span class="sc">-</span>e)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>e))</span></code></pre></div>
<pre><code>## [1] 0.2980418</code></pre>
<blockquote>
<p>When using the chi-square GOF test to assess the viability of a distribution
whose parameters are estimated, we lose additional degrees of freedom,
so the number of degrees of freedom here is 4 - 2 = 2. The <span class="math inline">\(p\)</span>-value is</p>
</blockquote>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="the-poisson-and-related-distributions.html#cb301-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(W,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.8615511</code></pre>
<blockquote>
<p>We find that we fail to reject the null hypothesis and thus that it is
plausible that Bortkiewiczs horse-kick data are indeed Poisson-distributed.</p>
</blockquote>
</div>
</div>
<div id="exercises-3" class="section level2 hasAnchor" number="4.11">
<h2><span class="header-section-number">4.11</span> Exercises<a href="the-poisson-and-related-distributions.html#exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Suppose that you receive phone calls at a rate of 2 per hour. (a) Let <span class="math inline">\(X\)</span> be the number of phone calls received in a randomly selected 15-minute period. Compute <span class="math inline">\(P(\mu \leq X \leq \mu+2\sigma)\)</span>. (b) Let <span class="math inline">\(Y\)</span> be the length of time between when the phone first rings from one call and when the phone first rings for the next call. What is <span class="math inline">\(P(Y &gt; 1/2 \, \vert \, Y &gt; 1/4)\)</span>? (c) When the phone rings, there is probability 0.2 that the caller is your friend, who you always talk to for 10 minutes. (All other calls are one minute in length.) What is the expected overall time spent on the phone during any given set of 10 phone calls?</p></li>
<li><p>A particular basketball player shoots, <em>on average</em>, one time every two minutes, and exactly half of her shots are successful. In an upcoming game, she is to play for exactly eight minutes. (a) What is the probability that the player will have one successful shot, or less? (b) What is the probability that the number of successful shots will be within one standard deviation of the mean?</p></li>
<li><p>A particular event happens, on average, three times per year in California, four times per year in Nevada, and one time per year in Arizona. Instances of these events are independent of each other. What the mean and variance for the total number of events, summed over the three states, that would be
observed in a single two-year window of time?</p></li>
<li><p>We sample <span class="math inline">\(n\)</span> iid data from the following distribution:
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\sigma &gt; 0\)</span>. For this distribution, <span class="math inline">\(E[X] = \sigma\sqrt{\pi/2}\)</span> and <span class="math inline">\(V[X] = (4-\pi)\sigma^2/2\)</span>. (a) What is <span class="math inline">\(E[X^2]\)</span>? (b) What is the method of moments estimator for <span class="math inline">\(\sigma\)</span> that utilizes the first population and sample moments? (c) What is the bias of <span class="math inline">\(\hat{\sigma}_{MoM}\)</span>? (d) What is the variance of <span class="math inline">\(\hat{\sigma}_{MoM}\)</span>? (e) What is the method of moments estimator of <span class="math inline">\(\sigma^2\)</span> that utilizes the <em>second</em> population and sample moments?</p></li>
<li><p>We sample a datum <span class="math inline">\(X\)</span> from a geometric distribution. (a) Find a method-of-moments estimator of <span class="math inline">\(p\)</span>, based on the first population and sample moments. (b) Find a second method-of-moments estimator of <span class="math inline">\(p\)</span>, using the second moments.</p></li>
<li><p>We sample <span class="math inline">\(n\)</span> iid data from a beta distribution with <em>known</em> value <span class="math inline">\(\alpha\)</span>. (a) Write down the method-of-moments estimator for <span class="math inline">\(\beta\)</span> that is based on the first sample moment. (b) Can we use our answer to part (a) to write down the method-of-moments estimator for <span class="math inline">\(\sqrt{\beta}\)</span>? If yes, write down that estimator.</p></li>
<li><p>We have been given the code shown below for computing a 95% one-sided lower bound on <span class="math inline">\(\lambda\)</span>, given <span class="math inline">\(n\)</span> iid data sampled from a Poisson distribution. Apparently someone was in a hurry. (a) Is the argument list for <code>f</code> valid as is? If not, specify how it should be fixed. (b) Is the input to <code>ppois()</code> valid as is? If not, specify how it should be fixed. (c) The code writer forgot to FIX the input value of <code>q</code> in the <code>uniroot()</code> call. Specify the number that should go here. (d) Will the coverage of the confidence intervals found with a properly fixed code be exactly 95%, greater than or equal to 95%, or less than or equal to 95%?</p></li>
</ol>
<pre><code>X &lt;- ... # this is a vector of n iid Poisson r.v.&#39;s
n &lt;- length(X)
f &lt;- function(X,lambda,n,q)
{
  ppois(mean(X),lambda) - q
}
uniroot(f,interval=c(0.001,1000),X=X,n=n,q=FIX)$root</code></pre>
<ol start="8" style="list-style-type: decimal">
<li><p>The Bernoulli probability mass function is
<span class="math display">\[\begin{align*}
p_X(x) = p^x (1-p)^{1-x} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \in \{0,1\}\)</span> and <span class="math inline">\(p \in (0,1)\)</span>. Assume that we draw <span class="math inline">\(n\)</span> iid data from this distribution. (a) Write down the likelihood ratio test statistic <span class="math inline">\(\lambda_{LR}\)</span> that arises when testing the hypothesis <span class="math inline">\(H_o : p = p_o\)</span> versus the hypothesis <span class="math inline">\(H_a : p \neq p_o\)</span>. (b) In this situation, is the LRT the most powerful test?</p></li>
<li><p>In an experiment, you sample <em>one datum</em> from a Poisson distribution; the value of that datum is <span class="math inline">\(x_{\rm obs}\)</span>. You wish to test <span class="math inline">\(H_o : \lambda = \lambda_o\)</span> versus the alternative <span class="math inline">\(H_a : \lambda \neq \lambda_o\)</span>, and you will do this by utilizing the likelihood ratio test. However, instead of identifying a sufficient statistic and constructing an exact test, you decide to use Wilks theorem. (In real life: dont do thisconstruct an exact test.) (a) Write down <span class="math inline">\(\hat{\lambda}_{MLE}\)</span>. (b) Write down an expression for <span class="math inline">\(\lambda_{LR}\)</span>. (c) Use the answer for part (a) to write down an expression for the Wilks theorem test statistic. (d) Regarding the statistic written down for part (c): what is its sampling distribution? Give the name and the values of its parameters.</p></li>
<li><p>In an experiment, you sample <span class="math inline">\(n\)</span> iid data
from a Poisson distribution with unknown mean <span class="math inline">\(\lambda\)</span>. You wish
to conduct a likelihood-ratio test for which <span class="math inline">\(H_a : \lambda &lt; \lambda_o\)</span>. Assume
<span class="math inline">\(\alpha = 0.05\)</span>. (a) Write down the sufficient statistic for <span class="math inline">\(\lambda\)</span> that
appears directly in likelihood factorization. (b) Write down the sampling
distribution for the statistic you found in part (a), the
name and the value(s) of its parameters. (c) Write down the appropriate null
hypothesis, given the stated alternative (and given that you are performing an
LRT). (d) Write down the rejection region, using
<code>R</code> code (e.g., <code>qnorm(0.975,mean=mu.o,sd=sqrt(sigma2))</code>).
Assume the null value is to be written as <code>lambda.o</code>, and that
a number should be provided for <span class="math inline">\(q\)</span>.</p></li>
<li><p>You are given <span class="math inline">\(n\)</span> iid data sampled from an exponential distribution with pdf
<span class="math display">\[\begin{align*}
f_X(x \vert \beta) = \frac{1}{\beta}e^{-x/\beta} \,,
\end{align*}\]</span>
where <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>. (a) We wish to test <span class="math inline">\(H_o : \beta = 1\)</span> versus <span class="math inline">\(H_a : \beta \neq 1\)</span> at level <span class="math inline">\(\alpha = 0.05\)</span>. Write down an expression for the likelihood ratio test statistic <span class="math inline">\(\lambda\)</span>. Recall that the MLE for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\bar{X}\)</span>. (b) Assume that you do not know the sampling distribution for any sufficient statistic that you can define. Use Wilks theorem to write down a rejection region for this test.</p></li>
<li><p>Suppose you flipped a coin 1000 times and observe heads 550 times. You wish to conduct a likelihood-ratio test to check whether this coin is fair, at the level <span class="math inline">\(\alpha = 0.05\)</span>. (a) Specify the null and alternative hypotheses. (b) What are the corresponding sets <span class="math inline">\(\Theta_0\)</span> and <span class="math inline">\(\Theta_a\)</span>? (c) Determine the number of free parameters <span class="math inline">\(r_0\)</span> and <span class="math inline">\(r\)</span> in <span class="math inline">\(\Theta_0\)</span> and <span class="math inline">\(\Theta = \Theta_0\, \cup\, \Theta_a\)</span>, respectively. (d) Determine the likelihood ratio test statistic, and its value for the observed data. (e) Assume that you do not know the sampling distribution for any sufficient statistic that you can define, and use Wilks theorem to determine the corresponding <span class="math inline">\(p\)</span>-value. (f) Give your conclusion. Is the coin fair?</p></li>
<li><p>You are given the following probability density function:
<span class="math display">\[\begin{align*}
f_X(x) = \left\{ \begin{array}{cl} \frac{27}{16} x^2 e^{-\frac{3x}{2}} &amp; x &gt; 0 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{align*}\]</span>
What are <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(V[X]\)</span>?</p></li>
<li><p>Assume you are working with a gamma distribution for which <span class="math inline">\(E[X] = 5\)</span> and <span class="math inline">\(V[X] = 10\)</span>. (a) What are the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>? (b) What other name is given to this particular distribution, given the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> you derived in (a)?</p></li>
<li><p>Compute <span class="math inline">\(E[X^{-1}]\)</span> for a chi-square distribution, leaving your answer in terms of the number of degrees of freedom <span class="math inline">\(\nu\)</span>.</p></li>
<li><p>The inverse-gamma distribution has the probability density function
<span class="math display">\[\begin{align*}
f_X(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{1}{x^{\alpha+1}} e^{-\beta/x} \,,
\end{align*}\]</span>
for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(\alpha,\beta &gt; 0\)</span>. Derive <span class="math inline">\(E[X]\)</span> for this distribution. Your final answer should be in terms of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> only (with no gamma or beta functions appearing in the answer).</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be a random variable sampled from a Gamma(3/2,<span class="math inline">\(\beta\)</span>) distribution. (a) If <span class="math inline">\(\beta = 2\)</span>, what sub-family of gamma distributions is <span class="math inline">\(X\)</span> sampled from? (b) Now, assume <span class="math inline">\(\beta\)</span> is unknown, and that we sample <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>. What is a sufficient statistic for <span class="math inline">\(\beta\)</span>? (c) What is the MVUE for <span class="math inline">\(\beta\)</span>? (d) What is the method of moments estimator for <span class="math inline">\(\beta\)</span> that utilizes the first population and sample moments? (e) What is the bias of <span class="math inline">\(\hat{\beta}_{MoM}\)</span>?</p></li>
<li><p>What is <span class="math inline">\(E[X^{1/2}]\)</span> for the random variable <span class="math inline">\(X \sim\)</span> Gamma(<span class="math inline">\(\alpha,\beta\)</span>)?</p></li>
<li><p>Below is (edited) output from the application of Poisson and negative binomial regression to a dataset in <code>R</code>:
<img src="figures/PoiNB.png" width="35%" style="display: block; margin: auto;" />
(a) Identify the value for the overdispersion parameter. (b) Is the value of the overdispersion parameter such that we can conclude that the data are indeed overdispersed? (c) What procedure utilizes the two deviance values in each model to make inferences about one or more of the model coefficients? (d) What specific conclusion can we make given both the procedure in (c) and the deviance values that are observed? Recall what the null and alternative hypotheses are, and state a conclusion based on these and the observed deviance values.</p></li>
</ol>
<hr />
<ol start="20" style="list-style-type: decimal">
<li><p>We sample 100 iid data from a Gamma(1,1). (a) What is the sampling distribution for <span class="math inline">\(\bar{X}\)</span>? (Provide the name of the distribution and its parameter value(s).) (b) Using <code>R</code>, determine <span class="math inline">\(P(\bar{X} \geq E[X])\)</span>.</p></li>
<li><p>The entropy and the probability-generating function for a discrete distribution are given by <span class="math inline">\(E[-\log p_X(X)]\)</span> and <span class="math inline">\(E[z^X]\)</span> respectively. (a) Write down an evaluated expression for the entropy of a Poisson distribution. Do not evaluate <span class="math inline">\(E[X!]\)</span> and/or <span class="math inline">\(E[\log X!]\)</span> if they appear. (b) Write down an evaluated expression for the probability-generating function of a Poisson distribution.</p></li>
<li><p>We are given the following probability density function:
<span class="math display">\[\begin{align*}
f_X(x) = 4x^2e^{-2x} ~~~~~~ x \geq 0 \,.
\end{align*}\]</span>
(a) <span class="math inline">\(f_X(x)\)</span> belongs to what family of distributions that we have studied in class? Just provide the name. (b) What are the parameter value(s) for this distribution? (c) Determine <span class="math inline">\(E[X]\)</span>.</p></li>
<li><p>A half-normal distribution has expected value <span class="math inline">\(\sigma\sqrt{2/\pi}\)</span> and variance <span class="math inline">\(\sigma^2(1-2/\pi)\)</span>. (a) What is the method-of-moments estimator for <span class="math inline">\(\sigma\)</span> that is based on the first population and sample moments? (b) What is the method-of-moments estimator for <span class="math inline">\(\sigma^2\)</span> that is based on the second population and sample moments?</p></li>
<li><p>In an experiment, we sample <em>one datum</em> according to the following unnamed distribution:
<span class="math display">\[\begin{align*}
f_X(x) = \frac{\theta}{3^\theta} x^{\theta-1} ~~~~~~ x \in [0,3] ~~~~~~ \theta &gt; 0 \,.
\end{align*}\]</span>
We then test the hypothesis <span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta \neq \theta_o\)</span> at level <span class="math inline">\(\alpha\)</span>. (a) What is the maximum likelihood estimate for <span class="math inline">\(\theta\)</span>? (b) Write down the likelihood-ratio test statistic <span class="math inline">\(\lambda_{\rm LR}\)</span>. Leave the symbol <span class="math inline">\(\hat{\theta}_{MLE}\)</span> in the answer. (c) We decide that we will utilize Wilks theorem to compute the <span class="math inline">\(p\)</span>-value for the test. Write down an expression for the test statistic. Again, leave <span class="math inline">\(\hat{\theta}_{MLE}\)</span> in the answer. (d) Write down the <code>R</code> function call that would return the <span class="math inline">\(p\)</span>-value (in the context of using Wilks theorem). Assume the statistic is stored as the variable <code>stat</code>. If there are one or more additional arguments to the function, specify its/their numerical value(s).</p></li>
<li><p>Below we show the output from when we learn two separate models, given the same data. (a) Identify the specific type of generalized linear model whose output is shown to the right. (b) Which model, the one to the left or the one to the right, should we select as the better representation of the data-generating process, and why? As for the why, point out a specific metric that we would use to make our choice. (c) Is the model to the left a valid representation of the data-generating process? Write down an <code>R</code> function call that would return a metric that would allow us to decide yes or no.</p></li>
</ol>
<pre><code>            Estimate Std. Error z value Pr(&gt;|z|)                        Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  1.02442    0.15411   6.647 2.98e-11 ***        (Intercept)   1.0090     0.1733   5.823 5.79e-09 ***
x            0.46731    0.05674   8.236  &lt; 2e-16 ***        x             0.4747     0.0676   7.023 2.17e-12 ***

    Null deviance: 111.21  on 29  degrees of freedom            Null deviance: 82.671  on 29  degrees of freedom
Residual deviance:  41.94  on 28  degrees of freedom        Residual deviance: 31.527  on 28  degrees of freedom
AIC: 152.92                                                 AIC: 153.07

                                                                          Theta:  21.5
                                                                      Std. Err.:  20.9</code></pre>

<div style="page-break-after: always;"></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-binomial-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-uniform-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

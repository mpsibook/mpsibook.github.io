<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 The Poisson (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-binomial-and-related-distributions.html"/>
<link rel="next" href="the-uniform-distribution.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance*</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-sample-mean-and-standard-deviation-are-independent-random-variables"><i class="fa fa-check"></i><b>2.10.1</b> The Sample Mean and Standard Deviation Are Independent Random Variables</a></li>
<li class="chapter" data-level="2.10.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval for the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#example-of-the-ks-test-in-r"><i class="fa fa-check"></i><b>2.14.1</b> Example of the KS Test in R</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-for-normality-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> Testing for Normality: Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-test-about-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Test About the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Extension to Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#how-many-data-do-we-need-to-achieve-a-given-test-power"><i class="fa fa-check"></i><b>2.15.6</b> How Many Data Do We Need to Achieve a Given Test Power?</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Extension to Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.1</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.3</b> The Expected Value of a Negative Binomial Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-binomial-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Binomial Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-moment-generating-function-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The Moment-Generating Function for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-minimum-variance-unbiased-estimator-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.1</b> The Minimum Variance Unbiased Estimator for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-maximum-likelihood-estimator-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.6.3</b> The Maximum Likelihood Estimator for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-confidence-intervals-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.3</b> Large-Sample Confidence Intervals for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-beta1theta-distribution"><i class="fa fa-check"></i><b>3.8.1</b> Defining the Uniformly Most-Powerful Test for the Beta(1,<span class="math inline">\(\theta\)</span>) Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-of-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.8.2</b> Defining the Uniformly Most-Powerful Test of the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining the Uniformly Most-Powerful Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-proportion"><i class="fa fa-check"></i><b>3.8.4</b> Large-Sample Tests of the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R: Star-Quasar Classification</a></li>
<li class="chapter" data-level="3.9.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification-1"><i class="fa fa-check"></i><b>3.9.2</b> Logistic Regression in R: Star-Quasar Classification</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#testing-hypotheses-using-the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Testing Hypotheses Using the Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-poisson-random-variables"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Poisson Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
<li class="chapter" data-level="4.4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean-1"><i class="fa fa-check"></i><b>4.4.3</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-cramer-rao-lower-bound-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The Cramer-Rao Lower Bound on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-coviarance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Coviarance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.4</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-poisson-and-related-distributions" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> The Poisson (and Related) Distributions<a href="the-poisson-and-related-distributions.html#the-poisson-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation-2" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Motivation<a href="the-poisson-and-related-distributions.html#motivation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the challenges of a Prussian soldiers life in the
19th century was avoiding being kicked by horses. This was
no trivial matter: over one 20-year period, 122 soldiers
died from injuries sustained while being kicked by horses.
The statistician Ladislaus Bortkiewicz compiled the
following data, showing the number of soldiers killed by horse
kicks in any one Prussian army corps in any one year:</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(N(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>109</td>
</tr>
<tr class="even">
<td>1</td>
<td>65</td>
</tr>
<tr class="odd">
<td>2</td>
<td>22</td>
</tr>
<tr class="even">
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(x\)</span> is the number of deaths observed in any one Prussian
army corp in any one year, and <span class="math inline">\(N(x)\)</span> is the number of corps-years
in which <span class="math inline">\(x\)</span> deaths were observed. (<span class="math inline">\(N(x)\)</span>
sums to <span class="math inline">\(20 \cdot 10 = 200\)</span>, reflecting that the compiled data represent
10 army corps observed over a 20-year period.)</p>
<p>The data presented above are an example of a process, i.e., a sequence of
observations, but we can immediately see that unlike the case with
coin flips, this process is <em>not</em> a Bernoulli process.
Thats because the number of possible outcomes is greater than 2 (0 and 1);
in fact, the number of possible outcomes is countably
infinite, so this is not even a multinomial process.
Well, the reader might say, we <em>could</em> simply
discretize the data more finely, so that the number of
possible outcomes is at least finite (multinomial) or better yet
falls to two (Bernoulli). Lets get monthly data, or daily data, or hourly data.
However, there is no time period <span class="math inline">\(\Delta t\)</span> for which the number of possible
outcomes is limited to some maximum value: in theory, an infinite number of
soldiers could die in the same second, or even the same nanosecond, etc.</p>
<p>But lets keep playing with this idea of making the time periods smaller
and smaller. Let the number of time periods into which we divide our
observation interval, <span class="math inline">\(k\)</span>, go to infinity such that the probability of
observing a horse-kick death <span class="math inline">\(p \rightarrow 0\)</span> and such that
<span class="math inline">\(kp \rightarrow \lambda\)</span>, where <span class="math inline">\(\lambda\)</span> is a constant.
Under these conditions, as we will see in the next section,
the binomial distribution transforms
into the Poisson distribution, which Bortkiewicz dubbed
the <em>law of small numbers</em>. Before we go to the section, though, lets
define the Poisson distribution in words: it gives the probability
of observing a particular number of events (counts) in a fixed
interval of space and/or time, assuming there is a constant mean rate
of events and the occurrence of any one event is independent of the
occurrence of other events.</p>
<p>(For those who do not wish to wait until the section on point estimation
to know the final answer: <span class="math inline">\(\hat{\lambda}_{MLE} = 0.61\)</span>, i.e., if the
data are plausibly Poisson distributed [which is another question to ask
entirely!], the rate of death was 0.61 soldiers per corps per year.)</p>
</div>
<div id="probability-mass-function-1" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Probability Mass Function<a href="the-poisson-and-related-distributions.html#probability-mass-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a probability mass function is one way to represent a discrete probablity distribution, and it has the properties (a) <span class="math inline">\(0 \leq P(X=x) \leq 1\)</span> and (b) <span class="math inline">\(\sum_x P(X=x) = 1\)</span>, where the sum is over all possible values of <span class="math inline">\(x\)</span>.</em></p>
<p><span class="math display">\[\begin{align*}
P(X=x) &amp;= \binom{k}{x} p^x (1-p)^{k-x} \\
&amp;= \frac{k!}{x!(k-x)!} \left(\frac{\lambda}{k}\right)^x \left(1-\frac{\lambda}{k}\right)^{k-x} \\
&amp;= \frac{k!}{(k-x)! k^x} \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^{k-x} \\
&amp;= \left(\frac{k}{k}\right) \left(\frac{k-1}{k}\right) \cdots \left(\frac{k-x+1}{k}\right) \left(\frac{\lambda^x}{x!}\right) \left(1-\frac{\lambda}{k}\right)^{k-x} \rightarrow \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^{k-x} ~\mbox{as}~~ k \rightarrow \infty \\
&amp;= \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^k \left(1-\frac{\lambda}{k}\right)^{-x} \rightarrow \frac{\lambda^x}{x!} \left(1-\frac{\lambda}{k}\right)^k ~\mbox{as}~~ k \rightarrow \infty \,.
\end{align*}\]</span>
At this point, we concentrate on the parenthetical term above. Given that
<span class="math display">\[
\lim_{k \rightarrow \infty} \left(1 - \frac{1}{k}\right)^k = e^{-1} \,,
\]</span>
we can state that
<span class="math display">\[
\lim_{k \rightarrow \infty} \left(1 - \frac{1}{k/\lambda}\right)^{k/\lambda} = e^{-1} \implies \lim_{k \rightarrow \infty} \left(1 - \frac{1}{k/\lambda}\right)^k = e^{-k} \,.
\]</span>
We are now in a position to write down the probability mass function
for a Poisson random variable (see Figure <a href="the-poisson-and-related-distributions.html#fig:ppmf">4.1</a>):
<span class="math display">\[
P(X=x) = p(x) = \frac{\lambda^x}{x!} e^{-\lambda} ~\mbox{where}~ \lambda &gt; 0 ~\mbox{and}~ x \in [0,\infty) \,.
\]</span>
A Poisson random variable converges in distribution to a normal random variable
as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, a result which affects how the Poisson has historically
been implemented in hypothesis testing. (We will elaborate on this point when we
return to the chi-square goodness of fit test later in the chapter.)
To indicate that we have sampled a datum from a Poisson
distribution, we write <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>).
The expected value and variance of the Poisson distribution are
<span class="math inline">\(E[X] = \lambda\)</span> and <span class="math inline">\(V[X] = \lambda\)</span>, respectively; the former is
derived below in an example.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ppmf"></span>
<img src="_main_files/figure-html/ppmf-1.png" alt="\label{fig:ppmf}Poisson probability mass functions for $\lambda = 1$ (red), 5 (green), and 10 (blue)." width="50%" />
<p class="caption">
Figure 4.1: Poisson probability mass functions for <span class="math inline">\(\lambda = 1\)</span> (red), 5 (green), and 10 (blue).
</p>
</div>
<table>
<caption>Poisson Distribution - <code>R</code> Functions</caption>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PMF</td>
<td><code>dpois(x,lambda)</code></td>
</tr>
<tr class="even">
<td>CDF</td>
<td><code>ppois(x,lambda)</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td><code>qpois(q,lambda)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rpois(n,lambda)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="the-expected-value-of-a-poisson-random-variable" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> The Expected Value of a Poisson Random Variable<a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the expected value of a discretely distributed random variable is</em>
<span class="math display">\[
E[X] = \sum_x x P(X=x) = \sum_x x p_X(x) \,,
\]</span>
<em>where the sum is over all <span class="math inline">\(x\)</span> for which <span class="math inline">\(p_X(x) &gt; 0\)</span>.</em></p>
<blockquote>
<p>For a Poisson distribution, the expected value is
<span class="math display">\[
E[X] = \sum_{x=0}^\infty x \frac{\lambda^x}{x!} e^{-\lambda} = \sum_{x=1}^\infty x \frac{\lambda^x}{x!} e^{-\lambda} = \sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!} e^{-\lambda} \,.
\]</span>
The goal is to move constants into or out of
the summation so that the summation becomes one of a
probability mass function over the entire domain of a
distribution. Here, we move <span class="math inline">\(\lambda\)</span> out of the summation,
and make the substitution <span class="math inline">\(y = x-1\)</span>; the summand then takes
on the form of a Poisson pmf, summed over its entire domain,
so the summation evaluates to 1:
<span class="math display">\[
E[X] = \lambda \sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!} e^{-\lambda} = \lambda \sum_{y=0}^\infty \frac{\lambda^{y}}{y!} e^{-\lambda} = \lambda \,.
\]</span>
Note that a similar calculation that starts with the
derivation of <span class="math inline">\(E[X(X-1)]\)</span> yields the variance.</p>
</blockquote>
</div>
</div>
<div id="cumulative-distribution-function-2" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Cumulative Distribution Function<a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as <span class="math inline">\(F_X(x) = \sum_{y\leq x} p_Y(y)\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>For the Poisson distribution, the cdf is
<span class="math display">\[
F_X(x) = \sum_{y=0}^{\lfloor x \rfloor} p_Y(y) = \sum_{y=0}^{\lfloor x \rfloor} \frac{\lambda^y}{y!} \exp(-\lambda) = \frac{\Gamma(\lfloor x+1 \rfloor,\lambda)}{\lfloor x \rfloor !} \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the largest integer that is less than or
equal to <span class="math inline">\(x\)</span> (e.g., if <span class="math inline">\(x\)</span> = 8.33, <span class="math inline">\(\lfloor x \rfloor\)</span> = 8),
and <span class="math inline">\(\Gamma(\cdot,\cdot)\)</span> is the upper incomplete gamma function
<span class="math display">\[
\Gamma(\lfloor x+1 \rfloor,\lambda) = \int_{\lambda}^\infty u^{\lfloor x \rfloor} e^{-u} du \,.
\]</span>
(An example of an <code>R</code> function which computes the upper incomplete
gamma function is <code>incgam()</code> in the <code>pracma</code> package.)
As we are dealing with a probability mass function, the
cdf is a step function, as illustrated in the left panel of
Figure <a href="the-poisson-and-related-distributions.html#fig:poicdf">4.2</a>.
Recall that because of the step-function nature of the cdf,
the form of inequalities in a probabilistic statement matter: e.g.,
<span class="math inline">\(P(X &lt; x)\)</span> and <span class="math inline">\(P(X \leq x)\)</span> will not be the same
if <span class="math inline">\(x\)</span> is zero or a positive integer.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(F_X^{-1}(\cdot)\)</span>
takes as input the total probability
<span class="math inline">\(q \in [0,1]\)</span> in the range <span class="math inline">\((-\infty,x]\)</span> and returns the value of <span class="math inline">\(x\)</span>.
A discrete distribution has no unique inverse cdf; it is convention to
utilize the generalized inverse cdf,</em>
<span class="math inline">\(x = F_X^{-1}(q) = \mbox{inf}\{x : F_X(x) \geq q\}\)</span>,
<em>where inf indicates the return
the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q\)</span>.</em></p>
<p>In the right panel of Figure <a href="the-poisson-and-related-distributions.html#fig:poicdf">4.2</a>, we display the inverse cdf
for the same distribution used to generate the figure in the left panel
(<span class="math inline">\(\lambda = 2\)</span>). Like the cdf, the inverse cdf for a discrete distribution
is a step function.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poicdf"></span>
<img src="_main_files/figure-html/poicdf-1.png" alt="\label{fig:poicdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a Poisson distribution with $\lambda=2$. Note that because the domain of the Poisson distribution is countably infinite, we do not reach $F_X(x) = 1$ in this example." width="45%" /><img src="_main_files/figure-html/poicdf-2.png" alt="\label{fig:poicdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a Poisson distribution with $\lambda=2$. Note that because the domain of the Poisson distribution is countably infinite, we do not reach $F_X(x) = 1$ in this example." width="45%" />
<p class="caption">
Figure 4.2: Illustration of the cumulative distribution function <span class="math inline">\(F_X(x)\)</span> (left) and inverse cumulative distribution function <span class="math inline">\(F_X^{-1}(q)\)</span> (right) for a Poisson distribution with <span class="math inline">\(\lambda=2\)</span>. Note that because the domain of the Poisson distribution is countably infinite, we do not reach <span class="math inline">\(F_X(x) = 1\)</span> in this example.
</p>
</div>
<hr />
<div id="computing-probabilities-8" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Computing Probabilities<a href="the-poisson-and-related-distributions.html#computing-probabilities-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Poisson(5), which is <span class="math inline">\(P(4 \leq X &lt; 6)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We first note that due to the form of the inequality, we do <em>not</em> include <span class="math inline">\(X=6\)</span>
in the computation. Thus <span class="math inline">\(P(4 \leq X &lt; 6) = p_X(4) + p_X(5)\)</span>, which equals
<span class="math display">\[
\frac{5^4}{4!}e^{-5} + \frac{5^5}{5!}e^{-5} = \frac{5^4}{4!}e^{-5} \left( 1 + \frac{5}{5} \right) = 2\frac{5^4}{4!}e^{-5} = 0.351\,.
\]</span>
If we call on <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="the-poisson-and-related-distributions.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dpois</span>(<span class="dv">4</span>,<span class="at">lambda=</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">dpois</span>(<span class="dv">5</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.3509347</code></pre>
<blockquote>
<p>We can also utilize cdf functions here: <span class="math inline">\(P(4 \leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \leq 5) - P(X \leq 3) = F_X(5) - F_X(3)\)</span>, which in <code>R</code> is computed via</p>
</blockquote>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="the-poisson-and-related-distributions.html#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">5</span>,<span class="at">lambda=</span><span class="dv">5</span>) <span class="sc">-</span> <span class="fu">ppois</span>(<span class="dv">3</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.3509347</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(X \sim\)</span> Poisson(5), what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(X \leq a) = 0.9\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we set up the inverse cdf formula:
<span class="math display">\[
P(X \leq a) = F_X(a) = 0.9 ~~ \Rightarrow ~~ a = F_X^{-1}(0.9)
\]</span>
Note that we didnt do anything differently here than we would have done
in a continuous distribution settingand we can proceed directly to
<code>R</code> because it utilizes the generalized inverse cdf algorithm.</p>
</blockquote>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="the-poisson-and-related-distributions.html#cb223-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qpois</span>(<span class="fl">0.9</span>,<span class="at">lambda=</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
</div>
</div>
<div id="linear-functions-of-poisson-random-variables" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Linear Functions of Poisson Random Variables<a href="the-poisson-and-related-distributions.html#linear-functions-of-poisson-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid Poisson random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>).
What is the distribution of the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>?</p>
<div style="page-break-after: always;"></div>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(m_X(t) = E[e^{tX}]\)</span>. Also, if <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> as the mgf for a known family of distributions, then we can immediately identify the distribution of <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for a Poisson random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[
m_X(t) = \exp\left[\lambda\left(e^t-1\right)\right] \,.
\]</span>
(We derive this in an example below.)
Thus the mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is
<span class="math display">\[
m_Y(t) = \exp\left[\lambda\left(e^t-1\right)\right] \cdots \exp\left[\lambda\left(e^t-1\right)\right] = \exp\left[(\lambda+\cdots+\lambda)(e^t-1)\right] = \exp\left[n\lambda(e^t-1)\right] \,.
\]</span>
This mgf retains the form of a Poisson mgf. We thus
see that the sum of Poisson-distributed random variables is
itself Poisson distributed with parameter <span class="math inline">\(n\lambda\)</span>, i.e.,
<span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Poisson(<span class="math inline">\(n\lambda\)</span>).</p>
<p>While we can identify the distribution of the sum, we cannot
identify the distribution of the sample mean by name: if
<span class="math inline">\(\bar{X} = \left(\sum_{i=1}^n X_i\right)/n\)</span>, then
<span class="math display">\[
m_{\bar{X}}(t) = \exp\left[n\lambda(e^{t/n}-1)\right] \,.
\]</span>
We know of no family of distributions with this mgf. We are left in the
exact same situation that we faced in Chapter 3 when working with
the binomial distribution,
and as was the case in that chapter, we can pursue two alternatives
to learn more about the distribution for <span class="math inline">\(\bar{X}\)</span>:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(n \gtrsim 30\)</span>, we can utilize the Central Limit Theorem to state that
<span class="math display">\[
\bar{X} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(\lambda,\frac{\lambda}{n}\right) \,,
\]</span>
i.e., that the random variable <span class="math inline">\(\bar{X}\)</span> converges in distribution to
a normal random variable with mean <span class="math inline">\(E[\bar{X}] = \mu = \lambda\)</span> and
variance <span class="math inline">\(V[\bar{X}] = \sigma^2/n = \lambda/n\)</span>.</li>
<li>We can note that when we divide by <span class="math inline">\(n\)</span>, we are simply transforming the
domain of the pmf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> without changing the values of the
probability masses. Since we know <span class="math inline">\(Y \sim\)</span> Poisson(<span class="math inline">\(n\lambda\)</span>), we can write
<span class="math display">\[
p_{\bar{X}}(\bar{x}) = \frac{(n\lambda)^{n\bar{x}}}{(n\bar{x})!} e^{-n\lambda} ~~~~ \bar{x} \in [0,1/n,2/n,\ldots,\infty) \,.
\]</span>
This has the form of a Poisson pmf but not the domain, and thus the sampling
distribution is unnamed. However, as usual we can apply the general rule for
<span class="math inline">\(\bar{X}\)</span> and state immediately that <span class="math inline">\(E[\bar{X}] = \mu = \lambda\)</span> and
<span class="math inline">\(V[\bar{X}] = \sigma^2/n = \lambda/n\)</span>.</li>
</ol>
<hr />
<div id="the-moment-generating-function-of-a-poisson-random-variable" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> The Moment-Generating Function of a Poisson Random Variable<a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The moment-generating function for a random variable <span class="math inline">\(X\)</span> is found by
utilizing the Law of the Unconscious Statistician and computing
<span class="math inline">\(E[e^{tX}]\)</span>. If <span class="math inline">\(X\)</span> is a Poisson random variable, then
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^\infty e^{tx} p_X(x) \\
&amp;= \sum_{x=0}^\infty e^{tx} \frac{\lambda^x}{x!} e^{-\lambda} \\
&amp;= e^{-\lambda} \sum_{x=0}^\infty \frac{\lambda^x}{x!} e^{tx} \\
&amp;= e^{-\lambda} \left[ 1 + \lambda e^t + \frac{\lambda^2}{2!}e^{2t} + \ldots \right] \\
&amp;= e^{-\lambda} \left[ 1 + y + \frac{y^2}{2!} + \ldots \right] \\
&amp;= e^{-\lambda} e^y = \exp(-\lambda) \exp(\lambda e^t) = \exp[\lambda(e^t-1)] \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-distribution-of-the-difference-of-two-poisson-random-variables" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> The Distribution of the Difference of Two Poisson Random Variables<a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume we point a camera at an object, such as a star. A star gives off
photons at a particular rate <span class="math inline">\(\alpha_S\)</span> (with units, e.g., photons per second)
and thus if we open the shutter for a length of time <span class="math inline">\(t\)</span>, the number
of photons we observe from the star is a Poisson random variable <span class="math inline">\(S \sim\)</span>
Poisson(<span class="math inline">\(\alpha_St\)</span>). But the star is not the only object in the field
of view; there may be other objects in the background that give off photons
at a rate <span class="math inline">\(\alpha_B\)</span>, and the number of photons we observe from the
background will be <span class="math inline">\(B \sim\)</span> Poisson(<span class="math inline">\(\alpha_Bt\)</span>). Thus what we record
is not <span class="math inline">\(S\)</span>, but <span class="math inline">\(T = S+B\)</span>so how can we statistical inferences about
<span class="math inline">\(S\)</span> itself?</p>
</blockquote>
<blockquote>
<p>One possibility is to point the camera to an empty field near the star,
and record some number of photons <span class="math inline">\(B\)</span>. Then we can estimate <span class="math inline">\(S\)</span> using
<span class="math inline">\(S = T - B\)</span>. What is the distribution of <span class="math inline">\(S\)</span>?</p>
</blockquote>
<blockquote>
<p>We utilize the method of moment-generating functions and write
<span class="math display">\[\begin{align*}
m_S(t) = m_T(t) m_B(-t) &amp;= \exp[\lambda_T(e^t-1)] \exp[\lambda_B(e^{-t}-1)] \\
&amp;= \exp[\lambda_T(e^t-1) + \lambda_B(e^{-t}-1)] \\
&amp;= \exp[-(\lambda_T+\lambda_B) + \lambda_Te^t + \lambda_Be^{-t}] \,,
\end{align*}\]</span>
where <span class="math inline">\(\lambda_T = \lambda_S+\lambda_B = (\alpha_S+\alpha_B)t\)</span> and
<span class="math inline">\(\lambda_B = \alpha_Bt\)</span>. At first, utilizing the method of mgfs appears
to be a fools errand: this is not an mgf we know. But it turns out that
the family of distributions associated with this mgf <em>does</em> have a name:
<span class="math inline">\(S = T-B\)</span> is a Skellam-distributed random variable, with mean
<span class="math inline">\(\lambda_T-\lambda_B\)</span> and variance <span class="math inline">\(\lambda_T+\lambda_B\)</span>. We can work
with this distribution to, e.g., construct confidence intervals for
<span class="math inline">\(\mu_S\)</span>, etc.</p>
</blockquote>
<hr />
</div>
<div id="the-probability-mass-function-for-the-sample-mean-1" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> The Probability Mass Function for the Sample Mean<a href="the-poisson-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The derivation of the pmf for the sample mean proceeds entirely
analogously with the derivation given in the previous chapter for
the sample mean of <span class="math inline">\(n\)</span> iid binomial random variables: we can use the
method of moment generating functions to find that the sum of
<span class="math inline">\(n\)</span> iid Poisson random variables is itself a Poisson random variable
with mean and variance <span class="math inline">\(n\lambda\)</span>, and then use the general transformation
framework to find the pmf for <span class="math inline">\(\bar{X} = (1/n)\sum_{i=1}^n X_i\)</span>.</p>
</blockquote>
<blockquote>
<p>However, as we did in the last chapter, we can view the problem as
derive the pmf for the sum and then tranform the domain. By doing this,
we find that we can write that
<span class="math display">\[
p_{\bar{X}}(\bar{x}) = \frac{(n\lambda)^{n\bar{x}}}{(n\bar{x})!} e^{-n\lambda} ~~ \bar{x} \in [0,1/n,2/n,\ldots,\infty) \,.
\]</span>
This pmf has the functional form of a Poisson pmf but not the domain of
one, thus it has no name and no tabulated properties. See the
example in Figure <a href="the-poisson-and-related-distributions.html#fig:xbarpois">4.3</a>.</p>
</blockquote>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="the-poisson-and-related-distributions.html#cb225-1" aria-hidden="true" tabindex="-1"></a>lambda  <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb225-2"><a href="the-poisson-and-related-distributions.html#cb225-2" aria-hidden="true" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">10</span>  </span>
<span id="cb225-3"><a href="the-poisson-and-related-distributions.html#cb225-3" aria-hidden="true" tabindex="-1"></a>x.bar   <span class="ot">&lt;-</span> <span class="fu">seq</span>(lambda<span class="dv">-5</span><span class="sc">*</span><span class="fu">sqrt</span>(lambda<span class="sc">/</span>n),lambda<span class="sc">+</span><span class="dv">5</span><span class="sc">*</span><span class="fu">sqrt</span>(lambda<span class="sc">/</span>n),<span class="at">by=</span><span class="dv">1</span><span class="sc">/</span>n)</span>
<span id="cb225-4"><a href="the-poisson-and-related-distributions.html#cb225-4" aria-hidden="true" tabindex="-1"></a>p.x.bar <span class="ot">&lt;-</span> <span class="fu">dpois</span>(n<span class="sc">*</span>x.bar,n<span class="sc">*</span>lambda)</span>
<span id="cb225-5"><a href="the-poisson-and-related-distributions.html#cb225-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x.bar,p.x.bar,<span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;x.bar&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;p(x.bar)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xbarpois"></span>
<img src="_main_files/figure-html/xbarpois-1.png" alt="\label{fig:xbarpois}Probability mass function for the sample mean of $n = 10$ iid Poisson random variables, for $\lambda = 10$." width="50%" />
<p class="caption">
Figure 4.3: Probability mass function for the sample mean of <span class="math inline">\(n = 10\)</span> iid Poisson random variables, for <span class="math inline">\(\lambda = 10\)</span>.
</p>
</div>
</div>
</div>
<div id="point-estimation-3" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Point Estimation<a href="the-poisson-and-related-distributions.html#point-estimation-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous chapters, we describe two commonly used approaches for
defining good estimators (as opposed to simply defining one ourselves
and hoping for the best!): maximum likelihood estimation
and finding the minimum variance unbiased estimator. We review both
below, in the context of estimating the Poisson <span class="math inline">\(\lambda\)</span> parameter, and
then for completeness introduce one last, less-commonly used approach,
the so-called <em>method of moments</em>.</p>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.</em></p>
<p>First, lets take the logarithm of the likelihood function written out above:
<span class="math display">\[
\ell(\lambda \vert \mathbf{x}) = \left(\sum_{i=1}^n x_i\right) \log \lambda - n \lambda - \log\left(\prod_{i=1}^n x_i!\right) \,.
\]</span>
The derivative of <span class="math inline">\(\ell(\lambda \vert \mathbf{x})\)</span> with
respect to <span class="math inline">\(\lambda\)</span> is
<span class="math display">\[
\frac{d\ell}{d\lambda} = \left(\frac{1}{\lambda}\sum_{i=1}^n x_i \right) - n \,.
\]</span>
Setting the derivative to zero and rearranging terms, we find that
<span class="math display">\[
\hat{\lambda}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
\]</span>
is the MLE for <span class="math inline">\(\lambda\)</span>. By the general rule introduced in Chapter 1,
<span class="math inline">\(E[\hat{\lambda}_{MLE}] = E[\bar{X}] = \lambda\)</span> (so <span class="math inline">\(\hat{\lambda}_{MLE}\)</span>
is an unbiased estimator), and <span class="math inline">\(V[\hat{\lambda}_{MLE}] = \lambda/n\)</span> (so
<span class="math inline">\(\hat{\lambda}_{MLE}\)</span> is a consistent estimator, since
<span class="math inline">\(\hat{\lambda}_{MLE} \rightarrow \lambda\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.
(There is no guarantee that the MLE will produce an unbiased estimator; it
just happens to do so here. It will produce at least an asymptotically
unbiased estimator, and it will always produce a consistent estimator.)</p>
<p>Recall that if we wish to find the MLE for a function of the parameter,
e.g., <span class="math inline">\(\lambda^2\)</span>, we simply apply that function to <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.
Hence <span class="math inline">\(\hat{\lambda^2}_{MLE}\)</span> is <span class="math inline">\(\bar{X}^2\)</span>. This is the invariance
property of the MLE.</p>
<p>Also, recall that the MLE converges in distribution to a normal random
variable with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(1/I_n(\theta)\)</span>, where
<span class="math inline">\(I_n(\theta)\)</span> is the Fisher information content of the data sample.</p>
<p><strong>Recall</strong>: <em>deriving the minimum variance unbiased estimator involves two steps:</em></p>
<ol style="list-style-type: decimal">
<li><em>factorizing the likelihood function to uncover a sufficient statistic <span class="math inline">\(U\)</span> (that we assume is both minimal and complete); and</em></li>
<li><em>finding a function <span class="math inline">\(h(U)\)</span> such that <span class="math inline">\(E[h(U)] = \lambda\)</span>.</em></li>
</ol>
<p>The likelihood function is
<span class="math display">\[
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda} = \left(\prod_{i=1}^n \frac{1}{x_i!}\right) \left(\prod_{i=1}^n \lambda^{x_i}e^{-\lambda}\right) = \underbrace{\left(\prod_{i=1}^n \frac{1}{x_i!}\right)}_{h(\mathbf{x})} \underbrace{\lambda^{\sum_{i=1}^n x_i}e^{-n\lambda}}_{g\left(\sum_{i=1}^n x_i,\lambda\right)} \,.
\]</span>
We see that the sufficient statistic is <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>. Lets determine the expected value for <span class="math inline">\(U\)</span>:
<span class="math display">\[
E[U] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \lambda = n\lambda \,.
\]</span>
Thus <span class="math inline">\(h(U) = U/n = \bar{X}\)</span> is the MVUE for <span class="math inline">\(\lambda\)</span>. As this matches the
MLE, we know already that the MVUE is an unbiased (by definition here!)
and consistent estimator. The next question is whether the variance
of the MVUE achieves the CRLB. We show that it does in an example below.</p>
<p>Note: the MVUE does not possess the invariance property, and it may be
the case that it does not achieve the Cramer-Rao Lower Bound! Its primary
advantage over the MLE is that the MVUE is the best estimator of those that
are always unbiased, for all sample sizes.</p>
<hr />
<p>The <em>method of moments</em> is a classic (read: old) means by which to define
estimators that can be useful when, for instance, working with the likelihood
function itself is difficult. (As such, it is an alternative to working
with likelihood functions numerically.)</p>
<p>Recall that by definition, <span class="math inline">\(\mu_k&#39; = E[X^k]\)</span> is the <span class="math inline">\(k^{\rm th}\)</span> moment
of the distribution of the random variable <span class="math inline">\(X\)</span>. For instance,
<span class="math display">\[
\mu_1&#39; = E[X] ~~ \mbox{and} ~~ \mu_2&#39; = E[X^2] = V[X] + (E[X])^2 \,.
\]</span>
If we shift from the population to the data sample, we can define
analagous <em>sample moments</em>, e.g.,
<span class="math display">\[
m_1&#39; = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} ~~ \mbox{and} ~~ m_2&#39; = \frac{1}{n} \sum_{i=1}^n X_i^2 = \overline{X^2} \,.
\]</span>
Lets suppose that we have <span class="math inline">\(p\)</span> parameters that we are trying to estimate.
In method of moments estimation, we generally set the first <span class="math inline">\(p\)</span> population
moments equal to the first <span class="math inline">\(p\)</span> sample moments and solve the system of equations
to determine parameter estimates. These estimates are generally consistent,
but also may be biased. (Situations may exist where higher-order moments
may be preferable to use, such as when the one parameter of a distribution
is <span class="math inline">\(\sigma^2\)</span> and thus we might derive a better estimator using the second
moments, but typically we will use the first <span class="math inline">\(p\)</span> moments.)</p>
<p>For the Poisson distribution, there is one parameter to estimate and
thus we set <span class="math inline">\(\mu_1&#39; = E[X] = \lambda = m_1&#39; = \bar{X}\)</span>. We thus find that
<span class="math inline">\(\hat{\lambda}_{MoM} = \bar{X}\)</span>. For a more relevant example of method
of moments usage, see below.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We begin this chapter by displaying the number of deaths per
Prussian army corps per year resulting from horse kicks. Leaving
aside the question of whether the data are truly Poisson distributed
(a question we will try to answer later in this chapter), what is
the estimated rate of death per corps per year?</p>
</blockquote>
<blockquote>
<p>The total number of events observed are
<span class="math display">\[
0 \times 109 + 1 \times 65 + 2 \times 22 + 3 \times 3 + 4 \times 1 = 65 + 44 + 9 + 4 = 122 \,,
\]</span>
and the total sample size is <span class="math inline">\(n = 200\)</span>, so
<span class="math display">\[
\hat{\lambda} = \bar{X} = \frac{1}{n} \sum_{i=1}^n X_i = \frac{122}{200} = 0.61
\,.
\]</span>
This is the MLE, the MVUE, and the MoM estimate for <span class="math inline">\(\lambda\)</span>.
In the next section, we will use these data to estimate a 95%
confidence interval for <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-cramer-rao-lower-bound-on-the-variance-of-lambda-estimators" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> The Cramer-Rao Lower Bound on the Variance of <span class="math inline">\(\lambda\)</span> Estimators<a href="the-poisson-and-related-distributions.html#the-cramer-rao-lower-bound-on-the-variance-of-lambda-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall</strong>: <em>the Cramer-Rao Lower Bound (or CRLB) is the lower bound on the variance of any unbiased estimator. If an unbiased estimator achieves the CRLB, it is the MVUEbut it can be the case that the MVUE does not achieve the CRLB. For a discrete distribution, the CRLB is given by</em>
<span class="math display">\[
V[\hat{\theta}] \geq -\frac{1}{nE\left[\frac{d^2}{d\theta^2} \log p_X(X \vert p) \right]} = \frac{1}{nI(\theta)}
\]</span>
<em>where <span class="math inline">\(I(\theta)\)</span> is the Fisher information.</em></p>
<blockquote>
<p>For the Poisson distribution,
<span class="math display">\[\begin{align*}
p(x \vert \lambda) &amp;= \frac{\lambda^x}{x!}e^{-\lambda} \\
\log p(x \vert \lambda) &amp;= x \log \lambda - \lambda - \log x! \\
\frac{d}{d\lambda} \log p(x \vert \lambda) &amp;= \frac{x}{\lambda} - 1 \\
\frac{d^2}{d\lambda^2} \log p(x \vert \lambda) &amp;= -\frac{x}{\lambda^2} \\
E \left[ \frac{d^2}{d\lambda^2} \log p(X \vert \lambda) \right] &amp;= -\frac{1}{\lambda^2} E[X] \\
&amp;= -\frac{1}{\lambda^2} \lambda = -\frac{1}{\lambda}
\end{align*}\]</span>
and
<span class="math display">\[
V[\hat{\lambda}] \geq -\frac{1}{-n/\lambda} = \frac{\lambda}{n} \,.
\]</span>
Thus <span class="math inline">\(\hat{\lambda}_{MLE}\)</span>, <span class="math inline">\(\hat{\lambda}_{MVUE}\)</span>, <em>and</em>
<span class="math inline">\(\hat{\lambda}_{MoM}\)</span> all achieve the CRLB.</p>
</blockquote>
<hr />
</div>
<div id="minimum-variance-unbiased-estimation-and-the-invariance-property" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Minimum Variance Unbiased Estimation and the Invariance Property<a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As stated above, the MVUE does not possess the property of invariance.
(This severely limits the general applicability of the algorithm!)
To demonstrate the lack of invariance, we will define the MVUE for
<span class="math inline">\(\lambda^2\)</span>.</p>
</blockquote>
<blockquote>
<p>The first thing to notice is that we cannot fall back on factorization
to determine an appropriate sufficient statistic, since <span class="math inline">\(\lambda^2\)</span>
does not appear directly in the likelihood function. So we iterate:
we make an initial guess and see where that guess takes us, and we
guess again if our initial guess is wrong, etc.</p>
</blockquote>
<blockquote>
<p>An appropriate guess for <span class="math inline">\(\lambda^2\)</span> is <span class="math inline">\(\bar{X}^2\)</span>:
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{\lambda}{n} + \lambda^2
\]</span>
We do get the term <span class="math inline">\(\lambda^2\)</span> herebut we also get <span class="math inline">\(\lambda/n\)</span>.
Hmmso lets try <span class="math inline">\(\bar{X}^2 - \bar{X}/n\)</span> instead:
<span class="math display">\[
E\left[\bar{X}^2 - \frac{\bar{X}}{n}\right] = E[\bar{X}^2] - \frac{1}{n}E[\bar{X}] = \frac{\lambda}{n} + \lambda^2 - \frac{\lambda}{n} = \lambda^2 \,.
\]</span>
Done! The MVUE for <span class="math inline">\(\lambda^2\)</span> is thus <span class="math inline">\(\hat{\lambda^2}_{MVUE} = \bar{X}^2-\bar{X}/n\)</span>,
which is <em>not</em> equal to <span class="math inline">\(\hat{\lambda^2}_{MLE} = \bar{X}^2\)</span> (except
asymptotically, in the limit <span class="math inline">\(n \rightarrow \infty\)</span>).</p>
</blockquote>
<hr />
</div>
<div id="method-of-moments-estimation-for-the-gamma-distribution" class="section level3 hasAnchor" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> Method of Moments Estimation for the Gamma Distribution<a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We will not officially introduce the gamma distribution until later in
this chapter, but it is a good one to use when exploring method of
moments estimation. The probability density function for a gamma
random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}}{\beta^{\alpha}} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\alpha,\beta &gt; 0\)</span>.
The expected value is <span class="math inline">\(E[X] = \alpha \beta\)</span> while the variance
is <span class="math inline">\(V[X] = \alpha \beta^2\)</span> (and thus <span class="math inline">\(E[X^2] = \alpha \beta^2 + \alpha^2 \beta^2\)</span>). To compute a maximum likelihood
estimate here, we would need to be able to differentiate the gamma
function <span class="math inline">\(\Gamma(\alpha)\)</span>and to find the MVUE, we would need to
determine how to utilize <em>joint</em> sufficient statistics in the MVUE algorithm,
which we do not know how to do. Thus we fall back on the method of moments.</p>
</blockquote>
<blockquote>
<p>(Well, in real life, we would probably actually fall back on numerical
optimization of the likelihood function, but if we seek equations to
write down)</p>
</blockquote>
<blockquote>
<p>Lets assume we have <span class="math inline">\(n\)</span> iid gamma-distributed random variables.
Because there are two parameters, we match the first two moments:
<span class="math display">\[\begin{align*}
\mu_1&#39; = E[X] = \alpha \beta &amp;= m_1&#39; = \bar{X} \\
\mu_2&#39; = E[X^2] = \alpha \beta^2 + \alpha^2 \beta^2 &amp;= m_2&#39; = \frac{1}{n}\sum_{i=1}^n X_i^2 = \overline{X^2} \,.
\end{align*}\]</span>
Let <span class="math inline">\(\beta = \bar{X}/\alpha\)</span>. Then
<span class="math display">\[\begin{align*}
\alpha \left( \frac{\bar{X}}{\alpha} \right)^2 + \alpha^2 \left( \frac{\bar{X}}{\alpha} \right)^2 &amp;= \overline{X^2} \\
\frac{(\bar{X})^2}{\alpha} &amp;= \overline{X^2} - (\bar{X})^2 \\
\Rightarrow ~~ \hat{\alpha}_{MoM} &amp;= \frac{(\bar{X})^2}{\overline{X^2} - (\bar{X})^2} \,,
\end{align*}\]</span>
and thus
<span class="math display">\[
\hat{\beta}_{MoM} = \frac{\bar{X}}{\hat{\alpha}_{MoM}} = \frac{\overline{X^2} - (\bar{X})^2}{\bar{X}} \,.
\]</span></p>
</blockquote>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="confidence-intervals-3" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Confidence Intervals<a href="the-poisson-and-related-distributions.html#confidence-intervals-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_H\)</span> by, e.g., solving for the root
<span class="math inline">\(\theta_q\)</span> in each of the following equations:</em>
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta_{\alpha/2}) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{\rm obs} \vert \theta_{1-\alpha/2}) - \left(1-\frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
<em>The construction of confidence intervals thus relies on knowing the
sampling distribution of the adopted statistic <span class="math inline">\(Y\)</span>. One maps
<span class="math inline">\(\theta_{\alpha/2}\)</span> and <span class="math inline">\(\theta_{1-\alpha/2}\)</span> to
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_H\)</span> by taking into account how
the expected value <span class="math inline">\(E[Y]\)</span> varies with the parameter <span class="math inline">\(\theta\)</span>. (See the
table in section 14 of Chapter 1.)</em></p>
<p>As far as the construction of confidence intervals given a discrete
sampling distribution goes, nothing changes algorithmically from
Chapter 3. Below, in an example,
we review how to construct such an interval for the
Poisson parameter <span class="math inline">\(\lambda\)</span>.</p>
<hr />
<p>What we will do here is answer the question, what do we do if we
neither know nor are willing to assume the distribution from which our
data are sampled? After all, our root-finding algorithm relies upon knowing
the sampling distribution of an observed statistic, and that in turn relies on
knowing the distribution from which we draw each of our <span class="math inline">\(n\)</span> iid data.</p>
<p>What we can do, in some situations, is fall back upon <em>bootstrapping</em>.</p>
<p>The bootstrap, invented by Bradley Efron
in 1979, uses the observed data themselves to build up empirical
sampling distributions for statistics.
Lets suppose we are handed the following data:
<span class="math display">\[
\mathbf{X} = \{X_1,X_2,\ldots,X_n\} \overset{iid}{\sim} P \,,
\]</span>
where the distribution <span class="math inline">\(P\)</span> is unknown. Now, lets suppose further that
from these data we compute a statistic: a single number.
How can we build up an empirical sampling distribution from a single
number? The answer is to repeatedly <em>resample</em> the data we observe,
<em>with replacement</em>. For instance, if we have as data the numbers <span class="math inline">\(\{1,2,3\}\)</span>,
a bootstrap sample might be <span class="math inline">\(\{1,1,3\}\)</span> or <span class="math inline">\(\{2,3,3\}\)</span>, etc. Every time
we resample the data, we compute the statistic we are interested in and
record its value. Voila: we have an empirical sampling distribution.
And if we can link the elements of that sampling distribution to a population
parameter, we can immediately write down a confidence interval. For instance,
if we have the <span class="math inline">\(n_{\rm boot}\)</span> statistics <span class="math inline">\(\{\bar{X}_1,\ldots,\bar{X}_k\}\)</span>, we
can put bounds on the population mean <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\begin{align*}
\hat{\mu}_L &amp;= \bar{X}_{\alpha/2} \\
\hat{\mu}_H &amp;= \bar{X}_{1-\alpha/2} \,,
\end{align*}\]</span>
where <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> represent sample percentiles, e.g.,
the 2.5<span class="math inline">\(^{\rm th}\)</span> and 97.5<span class="math inline">\(^{\rm th}\)</span> percentiles.</p>
<hr />
<div id="confidence-interval-for-the-poisson-parameter-lambda" class="section level3 hasAnchor" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As we did in Chapters 2 and 3, below we will adapt the general-purpose
<code>R</code> code for constructing confidence intervals that we provide in
Appendix B to a specific problem: here, putting a confidence interval
on the Poisson parameter <span class="math inline">\(\lambda\)</span>. Assume that we sample <span class="math inline">\(n\)</span> iid data.
Then, as shown above, <span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Poisson(<span class="math inline">\(n\lambda\)</span>);
our observed test statistic is <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i\)</span>.
For this statistic, <span class="math inline">\(E[Y] = n\lambda\)</span> increases with <span class="math inline">\(\lambda\)</span>, so
<span class="math inline">\(\lambda_{1-\alpha/2}\)</span> maps to the lower bound, while
<span class="math inline">\(\lambda_{\alpha/2}\)</span> maps to the upper bound.</p>
</blockquote>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="the-poisson-and-related-distributions.html#cb226-1" aria-hidden="true" tabindex="-1"></a>confint <span class="ot">&lt;-</span> <span class="cf">function</span>(y.obs,n,<span class="at">alpha=</span><span class="fl">0.05</span>)</span>
<span id="cb226-2"><a href="the-poisson-and-related-distributions.html#cb226-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb226-3"><a href="the-poisson-and-related-distributions.html#cb226-3" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">&lt;-</span> <span class="cf">function</span>(lambda,y.obs,n,q)</span>
<span id="cb226-4"><a href="the-poisson-and-related-distributions.html#cb226-4" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb226-5"><a href="the-poisson-and-related-distributions.html#cb226-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ppois</span>(y.obs,<span class="at">lambda=</span>n<span class="sc">*</span>lambda)<span class="sc">-</span>q</span>
<span id="cb226-6"><a href="the-poisson-and-related-distributions.html#cb226-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb226-7"><a href="the-poisson-and-related-distributions.html#cb226-7" aria-hidden="true" tabindex="-1"></a>  lo <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10000</span>),y.obs,n,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb226-8"><a href="the-poisson-and-related-distributions.html#cb226-8" aria-hidden="true" tabindex="-1"></a>  hi <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10000</span>),y.obs,n,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb226-9"><a href="the-poisson-and-related-distributions.html#cb226-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(lo,hi))</span>
<span id="cb226-10"><a href="the-poisson-and-related-distributions.html#cb226-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb226-11"><a href="the-poisson-and-related-distributions.html#cb226-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb226-12"><a href="the-poisson-and-related-distributions.html#cb226-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Let&#39;s assume we observe ten years of data in a Poisson process</span></span>
<span id="cb226-13"><a href="the-poisson-and-related-distributions.html#cb226-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb226-14"><a href="the-poisson-and-related-distributions.html#cb226-14" aria-hidden="true" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb226-15"><a href="the-poisson-and-related-distributions.html#cb226-15" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb226-16"><a href="the-poisson-and-related-distributions.html#cb226-16" aria-hidden="true" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n,<span class="at">lambda=</span>lambda)</span>
<span id="cb226-17"><a href="the-poisson-and-related-distributions.html#cb226-17" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">sum</span>(X),n)</span></code></pre></div>
<pre><code>## [1] 5.810583 9.178655</code></pre>
<blockquote>
<p>We find that the interval is <span class="math inline">\([\hat{\lambda}_L,\hat{\lambda}_H] = [5.81,9.18]\)</span>, which overlaps the true value of 8. (See
Figure <a href="the-poisson-and-related-distributions.html#fig:poici">4.4</a>.)
Note that the interval over which we
search for the root is [0,10000], which is
(effectively) the range of possible values for <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poici"></span>
<img src="_main_files/figure-html/poici-1.png" alt="\label{fig:poici}Probability mass functions for Poisson distributions with $n=10$ (left) $\lambda=5.81$ and (right) $\lambda=9.18$. We assume that we observe $y_{\rm obs} = \sum_{i=1}^n x_i = 73$ events in total and that we want to construct a 95\% confidence interval. $\lambda=5.81$ is the smallest value of $\lambda$ such that $F_Y^{-1}(0.975) = 73$, while $\lambda=9.18$ is the largest value of $\lambda$ such that $F_Y^{-1}(0.025) = 73$." width="45%" /><img src="_main_files/figure-html/poici-2.png" alt="\label{fig:poici}Probability mass functions for Poisson distributions with $n=10$ (left) $\lambda=5.81$ and (right) $\lambda=9.18$. We assume that we observe $y_{\rm obs} = \sum_{i=1}^n x_i = 73$ events in total and that we want to construct a 95\% confidence interval. $\lambda=5.81$ is the smallest value of $\lambda$ such that $F_Y^{-1}(0.975) = 73$, while $\lambda=9.18$ is the largest value of $\lambda$ such that $F_Y^{-1}(0.025) = 73$." width="45%" />
<p class="caption">
Figure 4.4: Probability mass functions for Poisson distributions with <span class="math inline">\(n=10\)</span> (left) <span class="math inline">\(\lambda=5.81\)</span> and (right) <span class="math inline">\(\lambda=9.18\)</span>. We assume that we observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 73\)</span> events in total and that we want to construct a 95% confidence interval. <span class="math inline">\(\lambda=5.81\)</span> is the smallest value of <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 73\)</span>, while <span class="math inline">\(\lambda=9.18\)</span> is the largest value of <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 73\)</span>.
</p>
</div>
<blockquote>
<p>In Chapter 3, we find that when we are constructing exact intervals
with discrete sampling distributions (exact meaning that the sampling
distribution is the correct one, not an approximation), the actual
coverage can differ, either positively or negatively, from what we
expect. This is a discreteness effect that goes away as the
sample size <span class="math inline">\(n\)</span> increases (i.e., as the discrete sampling distribution
tends more and more to having a continuous appearance).
Thus when we deal with a sufficiently small data sample, it
is good practice to run a simulation to try to estimate the
actual coverage, as we do below.</p>
</blockquote>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="the-poisson-and-related-distributions.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb228-2"><a href="the-poisson-and-related-distributions.html#cb228-2" aria-hidden="true" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb228-3"><a href="the-poisson-and-related-distributions.html#cb228-3" aria-hidden="true" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb228-4"><a href="the-poisson-and-related-distributions.html#cb228-4" aria-hidden="true" tabindex="-1"></a>lambda  <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb228-5"><a href="the-poisson-and-related-distributions.html#cb228-5" aria-hidden="true" tabindex="-1"></a>lower   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb228-6"><a href="the-poisson-and-related-distributions.html#cb228-6" aria-hidden="true" tabindex="-1"></a>upper   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb228-7"><a href="the-poisson-and-related-distributions.html#cb228-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb228-8"><a href="the-poisson-and-related-distributions.html#cb228-8" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n,<span class="at">lambda=</span>lambda)</span>
<span id="cb228-9"><a href="the-poisson-and-related-distributions.html#cb228-9" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">confint</span>(<span class="fu">sum</span>(X),n)</span>
<span id="cb228-10"><a href="the-poisson-and-related-distributions.html#cb228-10" aria-hidden="true" tabindex="-1"></a>  lower[ii] <span class="ot">&lt;-</span> b[<span class="dv">1</span>]</span>
<span id="cb228-11"><a href="the-poisson-and-related-distributions.html#cb228-11" aria-hidden="true" tabindex="-1"></a>  upper[ii] <span class="ot">&lt;-</span> b[<span class="dv">2</span>]</span>
<span id="cb228-12"><a href="the-poisson-and-related-distributions.html#cb228-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb228-13"><a href="the-poisson-and-related-distributions.html#cb228-13" aria-hidden="true" tabindex="-1"></a>truth    <span class="ot">&lt;-</span> lambda</span>
<span id="cb228-14"><a href="the-poisson-and-related-distributions.html#cb228-14" aria-hidden="true" tabindex="-1"></a>in.bound <span class="ot">&lt;-</span> (lower <span class="sc">&lt;=</span> truth) <span class="sc">&amp;</span> (upper <span class="sc">&gt;=</span> truth)</span>
<span id="cb228-15"><a href="the-poisson-and-related-distributions.html#cb228-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The estimated coverage is &quot;</span>,<span class="fu">sum</span>(in.bound)<span class="sc">/</span>num.sim,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The estimated coverage is  0.9547</code></pre>
<blockquote>
<p>Our estimated coverage is 0.9547: we observe 47 more simulated confidence
intervals that overlap the true value than we expect. Is this consistent
with expectation? We have <span class="math inline">\(k = 10000\)</span> trials, where the expected
success proportion is <span class="math inline">\(p = 0.95\)</span>. Let <span class="math inline">\(X\)</span> be the number of simulations
in which the confidence intervals overlap the true parameter value. Then
<span class="math inline">\(E[X] = kp\)</span> (here, 9500), <span class="math inline">\(V[X] = kp(1-p)\)</span> (here, 475), and
<span class="math inline">\(\sigma_X = \sqrt{V[X]}\)</span> (here, 21.79). Our observed value is <span class="math inline">\(X = 9547\)</span>,
which is <span class="math inline">\(47/21.79 = 2.16\)</span> standard deviations away from what we expect.
This is implausible; the probability of sampling a value that deviates
from the expectation by 47 (or more), conditional on our expectation being
correct, is <span class="math inline">\(\approx\)</span> 0.03 (as the reader can confirm using appropriate calls
to <code>pbinom()</code>. We can conclude that in this particular case, the actual
coverage differs from what we expect.</p>
</blockquote>
<blockquote>
<p>We note that we can reduce the level of uncertainty (here, 21.79) by
running more simulations. For instance, if we run one million simulations
insted of 10,000, the
level of uncertainty will be reduced by a factor of 10, to 2.179.
As a general rule, we should always try to run as many simulations as
time will allow!</p>
</blockquote>
<hr />
</div>
<div id="revisiting-the-death-by-horse-kick-example-1" class="section level3 hasAnchor" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the last section above, we determined that the rate of death from
horse kicks per Prussian army corps per year was <span class="math inline">\(\hat{\lambda} = 0.61\)</span>.
Here, we determine a 95% interval estimate for <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="the-poisson-and-related-distributions.html#cb230-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">109</span>),<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">65</span>),<span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">22</span>),<span class="fu">rep</span>(<span class="dv">3</span>,<span class="dv">3</span>),<span class="fu">rep</span>(<span class="dv">4</span>,<span class="dv">1</span>))</span>
<span id="cb230-2"><a href="the-poisson-and-related-distributions.html#cb230-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(X)</span>
<span id="cb230-3"><a href="the-poisson-and-related-distributions.html#cb230-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">sum</span>(X),n)</span></code></pre></div>
<pre><code>## [1] 0.5111476 0.7283388</code></pre>
<blockquote>
<p>The 95% confidence interval is <span class="math inline">\([0.511,0.728]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="determining-a-confidence-interval-using-the-bootstrap" class="section level3 hasAnchor" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> Determining a Confidence Interval Using the Bootstrap<a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume we have the same data as in the first example above.</p>
</blockquote>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="the-poisson-and-related-distributions.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb232-2"><a href="the-poisson-and-related-distributions.html#cb232-2" aria-hidden="true" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb232-3"><a href="the-poisson-and-related-distributions.html#cb232-3" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb232-4"><a href="the-poisson-and-related-distributions.html#cb232-4" aria-hidden="true" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">rpois</span>(n,<span class="at">lambda=</span>lambda)</span>
<span id="cb232-5"><a href="the-poisson-and-related-distributions.html#cb232-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(X)</span></code></pre></div>
<pre><code>##  [1] 7 4 9 9 6 6 8 7 9 8</code></pre>
<blockquote>
<p>The confidence interval that we construct for <span class="math inline">\(\lambda\)</span>, which is
the mean of the distribution, is <span class="math inline">\([5.81,9.18]\)</span>. How does the bootstrap
estimate of the mean compare?</p>
</blockquote>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="the-poisson-and-related-distributions.html#cb234-1" aria-hidden="true" tabindex="-1"></a>n.boot <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb234-2"><a href="the-poisson-and-related-distributions.html#cb234-2" aria-hidden="true" tabindex="-1"></a>x.bar  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,n.boot)</span>
<span id="cb234-3"><a href="the-poisson-and-related-distributions.html#cb234-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.boot ) {</span>
<span id="cb234-4"><a href="the-poisson-and-related-distributions.html#cb234-4" aria-hidden="true" tabindex="-1"></a>  s         <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">length</span>(X),<span class="fu">length</span>(X),<span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb234-5"><a href="the-poisson-and-related-distributions.html#cb234-5" aria-hidden="true" tabindex="-1"></a>  x.bar[ii] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X[s])</span>
<span id="cb234-6"><a href="the-poisson-and-related-distributions.html#cb234-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb234-7"><a href="the-poisson-and-related-distributions.html#cb234-7" aria-hidden="true" tabindex="-1"></a>q <span class="ot">&lt;-</span> <span class="fu">quantile</span>(x.bar,<span class="at">probs=</span><span class="fu">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))</span>
<span id="cb234-8"><a href="the-poisson-and-related-distributions.html#cb234-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The estimated interval is [&quot;</span>,<span class="fu">round</span>(q[<span class="dv">1</span>],<span class="dv">2</span>),<span class="st">&quot;,&quot;</span>,<span class="fu">round</span>(q[<span class="dv">2</span>],<span class="dv">2</span>),<span class="st">&quot;].</span><span class="sc">\n</span><span class="st">&quot;</span>,<span class="at">sep=</span><span class="st">&quot;&quot;</span>)</span></code></pre></div>
<pre><code>## The estimated interval is [6.3,8.2].</code></pre>
<blockquote>
<p>The estimated interval is <span class="math inline">\([6.3,8.2]\)</span>. This is substantially smaller than
what we found above, and makes sense: for instance, the largest observed
datum is 9, so the largest possible value of the bootstrap sample mean
is 9which is smaller than the upper bound of 9.18. What we are seeing
is the effect of a small sample size: in the limit of small <span class="math inline">\(n\)</span>,
the length of bootstrap confidence intervals is on average smaller than
that of exact ones, with greater variability in lengths. As <span class="math inline">\(n\)</span>
increases, the mean lengths converge, but the variability in lengths remains
larger for bootstrap intervals than for exact ones.</p>
</blockquote>
<hr />
</div>
<div id="the-proportion-of-observed-data-in-a-bootstrap-sample" class="section level3 hasAnchor" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> The Proportion of Observed Data in a Bootstrap Sample<a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we sample <span class="math inline">\(n\)</span> iid data from some distribution <span class="math inline">\(P\)</span>.
When we create a bootstrap sample of these data, some of the observed data
appear multiple times, while other data do not appear at all. What is
the average proportion of observed data in any given bootstrap sample?</p>
</blockquote>
<blockquote>
<p>Let <span class="math inline">\(i\)</span> be the index of an arbitrary datum, where the indices are
<span class="math inline">\(\{1,2,\ldots,n-1,n\}\)</span>. Let <span class="math inline">\(X\)</span> be the number of times <span class="math inline">\(i\)</span> is chosen
when we construct a bootstrap sample of size <span class="math inline">\(n\)</span>:
<span class="math inline">\(X \sim\)</span> Binom(<span class="math inline">\(n,1/n\)</span>). <span class="math inline">\(P(X \geq 1)\)</span> then represents
the average proportion of observed data in a bootstrap sample:
<span class="math display">\[
P(X \geq 1) = 1 - P(X = 0) = 1 - (1-1/n)^n \,,
\]</span>
which, as <span class="math inline">\(n \rightarrow \infty\)</span>, approaches <span class="math inline">\(1-1/e = 0.632\)</span>.
Thus, for a sufficiently large sample, 63.2% of the observed data
will appear at least once in a bootstrapped dataset.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-2" class="section level2 hasAnchor" number="4.7">
<h2><span class="header-section-number">4.7</span> Hypothesis Testing<a href="the-poisson-and-related-distributions.html#hypothesis-testing-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-sided test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bounds given by <span class="math inline">\(y_{\alpha/2}\)</span> and <span class="math inline">\(y_{1-\alpha/2}\)</span>, which are solutions to the equations</em>
<span class="math display">\[\begin{align*}
F_Y(y_{\alpha/2} \vert \theta_o) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{1-\alpha/2} \vert \theta_o) - \left(1 - \frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
<em>The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic <span class="math inline">\(Y\)</span>. One maps, e.g., <span class="math inline">\(y_{\alpha/2}\)</span> to either the lower or upper rejection region boundary by taking into account how the expected value <span class="math inline">\(E[Y]\)</span> varies with the parameter <span class="math inline">\(\theta\)</span>. (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven.</em></p>
<p>In Chapter 3, we build upon the framework outlined above by introducing the
Neyman-Pearson lemma. This result allows us to bypass the guesswork that goes
into defining a hypothesis test statistic, by defining for us the most powerful
test of a simple null hypothesis versus a simple specified alternative.</p>
<p><strong>Recall:</strong> <em>when we test the simple hypotheses <span class="math inline">\(H_o: \theta = \theta_o\)</span> versus <span class="math inline">\(H_a: \theta = \theta_a\)</span>, the Neyman-Pearson lemma allows us to state that the hypothesis test with maximum power has a rejection region of the form</em>
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} \leq c(\alpha) \,,
\]</span>
<em>where <span class="math inline">\(c(\alpha)\)</span> is a constant whose value depends on the specified Type I error <span class="math inline">\(\alpha\)</span>. In practice, we determine the sufficient statistic <span class="math inline">\(U\)</span>, examine the form of the likelihood ratio to determine the form of the rejection region (<span class="math inline">\(U \leq u_\alpha\)</span> versus <span class="math inline">\(U \geq u_{1-\alpha}\)</span>), and use the sampling distribution of <span class="math inline">\(U\)</span> to derive the rejection region boundary (as <span class="math inline">\(F_U^{-1}(\alpha)\)</span> or <span class="math inline">\(F_U^{-1}(1-\alpha)\)</span>). If the rejection region does not depend on <span class="math inline">\(\theta_a\)</span>, then the test is said to be a uniformly most powerful (UMP) test.</em></p>
<p>In an example, we demonstrate how to apply the NP lemma to construct a
hypothesis test for the Poisson parameter <span class="math inline">\(\lambda\)</span>, given a sample of
<span class="math inline">\(n\)</span> iid data. However, here we describe a more general hypothesis
test framework, dubbed the <em>likelihood ratio test</em> (or <em>LRT</em>).</p>
<p>Waitthe NP lemma had a likelihood ratio. How is the LRT different?
That is a good question.
It differs in how the null and alternative hypotheses are specified:
<span class="math display">\[
H_o: \theta \in \Theta_o ~~\mbox{vs.}~~ H_a: \theta \in \Theta_o^c \,,
\]</span>
where <span class="math inline">\(\Theta_o\)</span> (capital theta naught) represents a set of possible null
values for <span class="math inline">\(\theta\)</span>, while <span class="math inline">\(\Theta_o^c\)</span> is the complement of that set. For
instance, for tests involving the Poisson parameter <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\Theta_o\)</span> could
be <span class="math inline">\(\lambda \in [5,10]\)</span>, so that <span class="math inline">\(\Theta_o^c\)</span> is <span class="math inline">\(\lambda &lt; 5\)</span> or
<span class="math inline">\(\lambda &gt; 10\)</span>. (The null hypothesis in this example is a composite hypothesis,
although it can be specified as a simple one, and usually is.)
Let <span class="math inline">\(\Theta = \Theta_o \cup \Theta_o^c\)</span>, i.e., the union of
the null and alternative sets. The rejection region for the LRT is
<span class="math display">\[
\lambda_{LR} = \frac{\mbox{sup}_{\theta \in \Theta_o} \mathcal{L}(\theta \vert \mathbf{x})}{\mbox{sup}_{\theta \in \Theta} \mathcal{L}(\theta \vert \mathbf{x})} \leq c(\alpha) \,,
\]</span>
where, like it is in the context of the NP lemma,
<span class="math inline">\(c(\alpha)\)</span> is a constant that depends on the specified Type I error <span class="math inline">\(\alpha\)</span>.</p>
<p>Since the LRT is more general, why did we ever utilize the NP lemma?
That is another good question. There are three primary points to make about why
we would use the NP lemma:</p>
<ol style="list-style-type: decimal">
<li>The NP lemma tests two simple hypotheses. For the LRT, the hypotheses
can be composite hypotheses.</li>
<li>The NP lemma allows us to define the <em>most powerful test</em> for
disambiguating two simple hypotheses. The LRT is generally a powerful test,
but given the composite nature of one (or both) of the hypotheses,
it comes with no guarantee of being the most powerful test.
(For instance, perhaps the score test or the Wald test, which can define
different statistics from the LRT, yields the most powerful test.)</li>
<li>The NP lemma framework ultimately allows us to define exact rejection
regions, assuming we know the functional form of the sampling distribution
of the sufficient statistic <span class="math inline">\(U\)</span>. Sometimes we know the sampling distribution
in the context of the LRT, but often we do not, which potentially leads
us to fall back on Wilks theorem (which we discuss below).</li>
</ol>
<p>When we <em>can</em> specify the sampling distribution for <span class="math inline">\(U\)</span>, LRT problems
proceed like NP lemma problems: we use the ratios to determine the form
of the rejection regions and then find the boundaries of those regions.
See the second example below.
But how would we proceed if we do <em>not</em> know the sampling distribution?
We might perform simulations, but as stated above
we might also fall back on Wilks theorem.
Let <span class="math inline">\(r_o\)</span> denote the number of free parameters in
<span class="math inline">\(H_o: \theta \in \Theta_o\)</span> and let <span class="math inline">\(r\)</span>
denote the number of free parameters in
<span class="math inline">\(\theta \in \Theta = \Theta_o \cup \Theta_a\)</span>.
(Note that <span class="math inline">\(\Theta\)</span> must include <em>all possible values of the parameters</em>.
For instance, if <span class="math inline">\(H_o\)</span> is <span class="math inline">\(\theta = \theta_o\)</span>, then <span class="math inline">\(H_a\)</span> must be <span class="math inline">\(\theta \neq \theta_o\)</span> and not <span class="math inline">\(\theta &lt; \theta_o\)</span> or <span class="math inline">\(\theta &gt; \theta_o\)</span>.)
Then, for large <span class="math inline">\(n\)</span>,
<span class="math display">\[
-2\log \lambda_{LR} \stackrel{d}{\rightarrow} W \sim \chi_{r-r_o}^2 \,.
\]</span>
Since this result is related to the central limit theorem, large <span class="math inline">\(n\)</span>
would be, by rule-of-thumb, 30 or more. In the language of <code>R</code>, if
<code>w.obs = -2*log(lambda.LR)</code>, then we would reject the null if
<code>1-pchisq(w.obs,r-r.o)</code> is less than <span class="math inline">\(\alpha\)</span>. Note that when we apply
Wilks theorem, the results of all tests are
contingent upon whether or not <span class="math inline">\(W \geq w_{1-\alpha}\)</span>.</p>
<p>Note that we say we mightfall back on Wilks theorem. Thats
because it has limited use in the context of problems that we are
dealing with in this book. First of all, as noted above, <span class="math inline">\(\Theta\)</span> has
to encompass the full parameter space, thus Wilks theorem cannot be
used to carry out upper- or lower-tail tests when the null is
<span class="math inline">\(\theta = \theta_o\)</span>. Also, it is most useful when
data are sampled from distributions with two or more freely varying parameters
(e.g., the normal distribution, with <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> both unknown),
and where the null hypothesis fixes more parameter values than the
alternative hypothesis (so that <span class="math inline">\(r-r_o &gt; 0\)</span>). When there are <span class="math inline">\(p\)</span> freely varying
parameters, then there are <span class="math inline">\(p\)</span> joint sufficient statistics, and thus
we would have a <span class="math inline">\(p\)</span>-dimensional sampling distribution that would be
difficult to work with. We <em>can</em> use Wilks theorem when there is only
one parameter, but again, we only consider doing so
in cases where the sampling distribution for the one sufficient
statistic <span class="math inline">\(U\)</span> is unknown.</p>
<hr />
<div id="the-uniformly-most-powerful-test-of-poisson-lambda" class="section level3 hasAnchor" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets say that we are counting the number of students that enter
a classroom each minute. We assume that the entry of students is
a homogeneous Poisson process (i.e., <span class="math inline">\(\lambda\)</span>, the expected number
of students, does not change from minute to minute). We think that
five students, on average, will pass through the door each minute, while
someone else thinks the number will be three. We collect data during
five independent one-minute intervals: 4, 4, 3, 2, 3. Can we
reject our null hypothesis at the level <span class="math inline">\(\alpha = 0.05\)</span>?
What is the <span class="math inline">\(p\)</span>-value?
And what is the power of the test for <span class="math inline">\(\lambda_a = 3\)</span>?</p>
</blockquote>
<blockquote>
<p>We test the simple hypotheses <span class="math inline">\(H_o: \lambda_o = 5\)</span> and <span class="math inline">\(H_a: \lambda_a = 3\)</span>.
The factorized likelihood for our data sample is
<span class="math display">\[
\mathcal{L}(\lambda \vert \mathbf{x}) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} e^{-\lambda} = \underbrace{\lambda^{\sum_{i=1}^n x_i} e^{-n\lambda}}_{g(\sum x_i,\lambda}) \cdot \underbrace{\frac{1}{\prod_{i=1}^n x_i!}}_{h(\mathbf{x})} \,.
\]</span>
Our sufficient statistic is thus <span class="math inline">\(U = \sum_{i=1}^n X_i = n\bar{X}\)</span>,
and the ratio of likelihoods is
<span class="math display">\[
\frac{\mathcal{L}(\lambda_o \vert \mathbf{x})}{\mathcal{L}(\lambda_a \vert \mathbf{x})} = \frac{(1/\prod_{i=1}^n x_i!) \lambda_o^{u_{\rm obs}} e^{-\lambda_o}}{(1/\prod_{i=1}^n x_i!) \lambda_a^{u_{\rm obs}} e^{-\lambda_a}} \propto \left(\frac{\lambda_o}{\lambda_a}\right)^{u_{\rm obs}} \,.
\]</span>
If <span class="math inline">\(\lambda_a &lt; \lambda_o\)</span>, the ratio goes towards zero as
<span class="math inline">\(u_{\rm obs} \rightarrow 0\)</span>. Thus the rejection region is
<span class="math inline">\(U \leq u_\alpha = F_U^{-1}(\alpha)\)</span>.</p>
</blockquote>
<blockquote>
<p>Using the method of moment-generating functions, we find that
<span class="math inline">\(U = \sum_{i=1}^n X_i\)</span> is Poisson-distributed with parameter <span class="math inline">\(n\lambda\)</span>.
Hence we can adapt a result from Chapter 3 and determine <span class="math inline">\(u_\alpha\)</span> in <code>R</code>
via the function call <code>qpois(alpha,lambda=n*lambda.o)-1</code>.
Since <span class="math inline">\(u_{\rm obs} = 16\)</span>
and <span class="math inline">\(u_\alpha = 16\)</span>, so we reject the null hypothesis that <span class="math inline">\(\lambda = 5\)</span>.
We further adapt results from Chapter 3 to write down that the
<span class="math inline">\(p\)</span>-value is <code>ppois(u.obs,lambda=n*lambda.o)</code>, which is 0.038.</p>
</blockquote>
<blockquote>
<p>Thus far, the only way that weve utilized the alternative hypothesis
<span class="math inline">\(H_a: \lambda_a = 3\)</span> is when determining the orientation of the rejection
region. Now we will use this value to determine the power of the test.
The power is the probability of rejecting the null hypothesis given
a specific value of <span class="math inline">\(\lambda\)</span>, i.e., <span class="math inline">\(P(U \leq u_\alpha \vert \lambda)\)</span>.
Here, that value is given by <code>ppois(u.lo,n*lambda)</code>, which for
<span class="math inline">\(\lambda = \lambda_a = 3\)</span> is 0.664: if <span class="math inline">\(\lambda\)</span> is truly equal to 3,
we would reject the null hypothesis that <span class="math inline">\(\lambda_o = 5\)</span> after collecting
five data about two-thirds of the time.</p>
</blockquote>
<blockquote>
<p>For completeness, we write down results for Poisson distributed data that
are analogous to what we write down
in Chapter 3 for binomially distributed data.</p>
</blockquote>
<table>
<colgroup>
<col width="14%" />
<col width="28%" />
<col width="57%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Rejection Region(s)</th>
<th><code>R</code> Code for Poisson Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(u_{\rm obs} \leq u_{\alpha}\)</span></td>
<td><code>u.lo &lt;- qpois(alpha,lambda=n*lambda.o) - 1</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(u_{\rm obs} \geq u_{1-\alpha}\)</span></td>
<td><code>u.hi &lt;- qpois(1-alpha,lambda=n*lambda.o) + 1</code></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Formula</th>
<th><code>R</code> Code for Poisson Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(F_U(u_{\rm obs} \vert p_o)\)</span></td>
<td><code>p &lt;- ppois(u.obs,lambda=n*lambda.o)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(1-F_U(u_{\rm obs}-1 \vert p_o)\)</span></td>
<td><code>p &lt;- 1-ppois(u.obs-1,lambda=n*lambda.o)</code></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Formula</th>
<th><code>R</code> Code for Poisson Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(F_U(u_\alpha \vert p_a)\)</span></td>
<td><code>power &lt;- ppois(u.lo,lambda=n*lambda.a)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(1-F_U(u_{1-\alpha}-1 \vert p_a)\)</span></td>
<td><code>power &lt;- 1-ppois(u.hi-1,lambda=n*lambda.a)</code></td>
</tr>
</tbody>
</table>
<blockquote>
<p>Note that since the rejection region does not depend on the value
<span class="math inline">\(\lambda_a\)</span>, we have defined the uniformly most powerful test of
<span class="math inline">\(\lambda_o\)</span> versus <span class="math inline">\(\lambda_a\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="likelihood-ratio-test-of-the-poisson-parameter-lambda" class="section level3 hasAnchor" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume the same setting as for the last example, but here,
lets say that we will test <span class="math inline">\(H_o: \lambda = \lambda_o = 5\)</span> versus
<span class="math inline">\(H_a: \lambda \neq \lambda_o\)</span>. The alternative
hypothesis is a composite hypothesis, because it does not uniquely
specify the shape of the probability mass function. As we will see,
because we know the sampling distribution for the sufficient
statistic <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>, we can adopt the NP lemma algorithm
within the LRT to derive the two rejection regionsbut because the
alternative hypothesis is composite, there is no longer a guarantee
that the test that we define<span class="math inline">\(-\)</span> <span class="math inline">\(H_o: \lambda = \lambda_o = 5\)</span> versus <span class="math inline">\(H_a: \lambda \neq \lambda_o\)</span> <span class="math inline">\(-\)</span>is
the most powerful test of these hypotheses. It may be, it may not be.</p>
</blockquote>
<blockquote>
<p>Let the set of possible values of <span class="math inline">\(\lambda\)</span> be denoted <span class="math inline">\(\Lambda\)</span> (capital
lambda). <span class="math inline">\(\Lambda = \Lambda_o \cup \Lambda_a \in (0,\infty)\)</span>,
so <span class="math inline">\(\mbox{sup}_{\lambda \in \Lambda} \mathcal{L}(\lambda \vert \mathbf{x})\)</span>
is the value of the likelihood for <span class="math inline">\(\hat{\lambda}_{MLE} = \bar{X}\)</span>.
Thus the denominator of the likelihood ratio is
<span class="math display">\[
\prod_{i=1}^n \frac{\hat{\lambda}^{x_i}}{x_i!}e^{-\hat{\lambda}} = \frac{1}{\prod_{i=1}^n x_i!} \hat{\lambda}^{\sum_{i=1}^n x_i} e^{-n\hat{\lambda}} \,.
\]</span>
The form of the numerator is the same, with <span class="math inline">\(\lambda_o\)</span> replacing
<span class="math inline">\(\hat{\lambda}_{MLE}\)</span>. Thus
<span class="math display">\[
\lambda_{LR} = \left(\frac{\lambda_o}{\hat{\lambda}}\right)^{\sum_{i=1}^n x_i} e^{-n(\lambda_o - \hat{\lambda})} = \left(\frac{\lambda_o}{\hat{\lambda}}\right)^{u_{\rm obs}} e^{-n(\lambda_o - \hat{\lambda})}\,,
\]</span>
where <span class="math inline">\({u_{\rm obs}}\)</span> is the sufficient statistic. We note that if
<span class="math inline">\(\hat{\lambda} &lt; \lambda_o\)</span>, then the ratio goes towards zero as
<span class="math inline">\(u_{\rm obs} \rightarrow 0\)</span>, while if <span class="math inline">\(\hat{\lambda} &gt; \lambda_o\)</span>,
the ratio goes towards zero as <span class="math inline">\(u_{\rm obs} \rightarrow \infty\)</span>.
Thus there are two
rejection regions: <span class="math inline">\(U \leq u_{\alpha/2} = F_U^{-1}(\alpha/2)\)</span>,
and <span class="math inline">\(U \geq u_{1-\alpha/2} = F_U^{-1}(\alpha/2)\)</span>.</p>
</blockquote>
<blockquote>
<p>The sampling distribution for <span class="math inline">\(U\)</span> is Poisson(<span class="math inline">\(n\lambda\)</span>), and so we
know how to determine the boundaries: <span class="math inline">\(u_{\alpha/2}\)</span> is given by
<code>qpois(alpha/2,n*lambda.o)-1</code>, or 15, while <span class="math inline">\(u_{1-\alpha/2}\)</span> is given
by <code>qpois(1-alpha/2,n*lambda.o)+1</code>, or 36. Our observed statistic
is <span class="math inline">\(U = 16\)</span>, thus we fail to reject the null hypothesis.</p>
</blockquote>
<blockquote>
<p>As a reminder, because the alternative hypothesis is a composite
hypothesis, the NP lemma does not apply here, and thus <em>we cannot
guarantee that the likelihood ratio test we have just constructed is
the most powerful of all possible tests</em> of <span class="math inline">\(H_o: \lambda = \lambda_o\)</span>
versus <span class="math inline">\(H_a: \lambda \neq \lambda_o\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="using-wilks-theorem-to-test-hypotheses-about-the-normal-mean" class="section level3 hasAnchor" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Using Wilks Theorem to Test Hypotheses About the Normal Mean<a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We have collected <span class="math inline">\(n\)</span> iid data from a normal distribution and
we wish to test the hypothesis <span class="math inline">\(H_o: \mu = \mu_o\)</span> versus
the hypothesis <span class="math inline">\(H_a: \mu \neq \mu_o\)</span> using the likelihood ratio test.
(We assume the variance is unknown.)</p>
</blockquote>
<blockquote>
<p>For this problem,
<span class="math display">\[\begin{align*}
\Theta_o &amp;= \{ \mu,\sigma^2 : \mu = \mu_o, \sigma^2 &gt; 0 \} \\
\Theta_a &amp;= \{ \mu,\sigma^2 : \mu \neq \mu_o, \sigma^2 &gt; 0 \} \,,
\end{align*}\]</span>
and so <span class="math inline">\(\Theta = \Theta_o \cup \Theta_a = \{ \mu,\sigma^2 : \mu \in (-\infty,\infty), \sigma^2 &gt; 0 \}\)</span>, with
<span class="math inline">\(r_o = 1\)</span> (<span class="math inline">\(\sigma^2\)</span>) and <span class="math inline">\(r = 2\)</span> (<span class="math inline">\(\mu,\sigma^2\)</span>).</p>
</blockquote>
<blockquote>
<p>The likelihood for the normal pdf is
<span class="math display">\[
\mathcal{L}(\mu,\sigma \vert \mathbf{x}) = \prod_{i=1}^n \frac{1}{2 \pi \sigma^2} \exp\left( -\frac{(x_i-\mu)^2}{2\sigma^2} \right)
\]</span>
and the log-likelihood is
<span class="math display">\[
\ell(\mu,\sigma \vert \mathbf{x}) = -\frac{n}{2} \log(2 \pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2 \,.
\]</span>
The test statistic for Wilks theorem is
<span class="math display">\[
W = -2 \left[ \ell(\mu_o,\widehat{\sigma^2}_{MLE} \vert \mathbf{x}) - \ell(\hat{\mu}_{MLE},\widehat{\sigma^2}_{MLE} \vert \mathbf{x}) \right] \,,
\]</span>
where <span class="math inline">\(\hat{\mu}_{MLE}\)</span> and <span class="math inline">\(\widehat{\sigma^2}_{MLE}\)</span> are the MLEs for <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\sigma\)</span>. We know these results from previous derivations:
<span class="math display">\[\begin{align*}
\hat{\mu}_{MLE} &amp;= \bar{X} \\
\widehat{\sigma^2}_{MLE} &amp;= \frac{n-1}{n}S^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \,.
\end{align*}\]</span>
(Wait a second, the reader says. Shouldnt we use <span class="math inline">\(\mu_o\)</span> instead of
<span class="math inline">\(\bar{X}\)</span> for <span class="math inline">\(\widehat{\sigma^2}_{MLE}\)</span> when
optimizing the likelihood in the numerator above? In other words,
shouldnt we use
<span class="math display">\[
\widehat{\sigma^2}_{MLE,o} = \frac{1}{n} \sum_{i=1}^n (X_i - \mu_o)^2
\]</span>
instead of <span class="math inline">\(\widehat{\sigma^2}_{MLE}\)</span>? No, for the simple reason that
<span class="math inline">\(\widehat{\sigma^2}_{MLE} \geq \widehat{\sigma^2}_{MLE,o}\)</span>: we achieve
a more likely value of the distribution width when we center on where
the data actually are, rather than where we hypothesize they are.)
Ultimately, we compare the value of <span class="math inline">\(W\)</span> against the chi-square
distribution for <span class="math inline">\(r-r_o = 1\)</span> degree of freedom, and thus we reject the
null (at level <span class="math inline">\(\alpha = 0.05\)</span>) if <span class="math inline">\(W &gt; 3.841\)</span> (= <code>qchisq(0.95,1)</code>).</p>
</blockquote>
<hr />
</div>
<div id="simulating-the-likelihood-ratio-test" class="section level3 hasAnchor" number="4.7.4">
<h3><span class="header-section-number">4.7.4</span> Simulating the Likelihood Ratio Test<a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Wilks theorem generates an approximate result<span class="math inline">\(-\)</span>it assumes that the
test statistic is chi-square-distributed<span class="math inline">\(-\)</span>and a major issue is that we
do not know how good the approximation is. For instance, lets say
<span class="math inline">\(n = 20\)</span>this might be an insufficient sample size
for the Wilks theorem machinery to yield an accurate and precise result
(at least in terms of a rejection-region boundary).</p>
</blockquote>
<blockquote>
<p>To get a sense as to how well Wilks theorem works for us, we can
run simulations. We simulate sets of data under the null (<span class="math inline">\(\mu = 5\)</span>),
and for each, we compute <span class="math inline">\(W\)</span>, and we determine the
<span class="math inline">\(100(1-\alpha)^{\rm th}\)</span> percentile value. This is our estimate
of the rejection-region boundary.</p>
</blockquote>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="the-poisson-and-related-distributions.html#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb236-2"><a href="the-poisson-and-related-distributions.html#cb236-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-3"><a href="the-poisson-and-related-distributions.html#cb236-3" aria-hidden="true" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb236-4"><a href="the-poisson-and-related-distributions.html#cb236-4" aria-hidden="true" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb236-5"><a href="the-poisson-and-related-distributions.html#cb236-5" aria-hidden="true" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb236-6"><a href="the-poisson-and-related-distributions.html#cb236-6" aria-hidden="true" tabindex="-1"></a>mu.o    <span class="ot">&lt;-</span> <span class="dv">5</span>      </span>
<span id="cb236-7"><a href="the-poisson-and-related-distributions.html#cb236-7" aria-hidden="true" tabindex="-1"></a>sigma2  <span class="ot">&lt;-</span> <span class="dv">9</span>      <span class="co"># an arbitrary value</span></span>
<span id="cb236-8"><a href="the-poisson-and-related-distributions.html#cb236-8" aria-hidden="true" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb236-9"><a href="the-poisson-and-related-distributions.html#cb236-9" aria-hidden="true" tabindex="-1"></a>W       <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,k)</span>
<span id="cb236-10"><a href="the-poisson-and-related-distributions.html#cb236-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-11"><a href="the-poisson-and-related-distributions.html#cb236-11" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(X,n,mu.o)</span>
<span id="cb236-12"><a href="the-poisson-and-related-distributions.html#cb236-12" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb236-13"><a href="the-poisson-and-related-distributions.html#cb236-13" aria-hidden="true" tabindex="-1"></a>  hat.mu.mle     <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb236-14"><a href="the-poisson-and-related-distributions.html#cb236-14" aria-hidden="true" tabindex="-1"></a>  hat.sigma2.mle <span class="ot">&lt;-</span> (n<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">var</span>(X)<span class="sc">/</span>n</span>
<span id="cb236-15"><a href="the-poisson-and-related-distributions.html#cb236-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-16"><a href="the-poisson-and-related-distributions.html#cb236-16" aria-hidden="true" tabindex="-1"></a>  logL.o <span class="ot">&lt;-</span> <span class="sc">-</span>(n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>hat.sigma2.mle)<span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>hat.sigma2.mle)<span class="sc">*</span><span class="fu">sum</span>((X<span class="sc">-</span>mu.o)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb236-17"><a href="the-poisson-and-related-distributions.html#cb236-17" aria-hidden="true" tabindex="-1"></a>  logL   <span class="ot">&lt;-</span> <span class="sc">-</span>(n<span class="sc">/</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>hat.sigma2.mle)<span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>hat.sigma2.mle)<span class="sc">*</span><span class="fu">sum</span>((X<span class="sc">-</span>hat.mu.mle)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb236-18"><a href="the-poisson-and-related-distributions.html#cb236-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>(logL.o <span class="sc">-</span> logL))</span>
<span id="cb236-19"><a href="the-poisson-and-related-distributions.html#cb236-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb236-20"><a href="the-poisson-and-related-distributions.html#cb236-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-21"><a href="the-poisson-and-related-distributions.html#cb236-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb236-22"><a href="the-poisson-and-related-distributions.html#cb236-22" aria-hidden="true" tabindex="-1"></a>  X     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb236-23"><a href="the-poisson-and-related-distributions.html#cb236-23" aria-hidden="true" tabindex="-1"></a>  W[ii] <span class="ot">&lt;-</span> <span class="fu">f</span>(X,n,mu.o)</span>
<span id="cb236-24"><a href="the-poisson-and-related-distributions.html#cb236-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb236-25"><a href="the-poisson-and-related-distributions.html#cb236-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb236-26"><a href="the-poisson-and-related-distributions.html#cb236-26" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(W[W<span class="sc">&lt;</span><span class="dv">8</span>],<span class="at">probability=</span><span class="cn">TRUE</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;w&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.5</span>),<span class="at">main=</span><span class="cn">NULL</span>)</span>
<span id="cb236-27"><a href="the-poisson-and-related-distributions.html#cb236-27" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">quantile</span>(W,<span class="at">probs=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>alpha)),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;green&quot;</span>)</span>
<span id="cb236-28"><a href="the-poisson-and-related-distributions.html#cb236-28" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">qchisq</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="dv">1</span>),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;green&quot;</span>)</span>
<span id="cb236-29"><a href="the-poisson-and-related-distributions.html#cb236-29" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>,<span class="dv">8</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb236-30"><a href="the-poisson-and-related-distributions.html#cb236-30" aria-hidden="true" tabindex="-1"></a>f.x <span class="ot">&lt;-</span> <span class="fu">dchisq</span>(x,<span class="dv">1</span>)</span>
<span id="cb236-31"><a href="the-poisson-and-related-distributions.html#cb236-31" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x,f.x,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lrtsim"></span>
<img src="_main_files/figure-html/lrtsim-1.png" alt="\label{fig:lrtsim}The empirical distribution of the statistic $-2(\log\mathcal{L}_o - \log\mathcal{L}_a)$, with the chi-square distribution for $\Delta r = 1$ degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks' theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks' theorem at best provides approximate results and that simulations can provide more accurate and precise results." width="50%" />
<p class="caption">
Figure 4.5: The empirical distribution of the statistic <span class="math inline">\(-2(\log\mathcal{L}_o - \log\mathcal{L}_a)\)</span>, with the chi-square distribution for <span class="math inline">\(\Delta r = 1\)</span> degree of freedom overlaid (red curve). The dashed vertical green line represents the rejection region boundary according to Wilks theorem, and the solid vertical green line represents the 95th percentile of simulated statistic values. The divergence of the two green lines indicates that Wilks theorem at best provides approximate results and that simulations can provide more accurate and precise results.
</p>
</div>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="the-poisson-and-related-distributions.html#cb237-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical rejection region boundary is&quot;</span>,<span class="fu">round</span>(<span class="fu">quantile</span>(W,<span class="at">probs=</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">-</span>alpha)),<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical rejection region boundary is 4.584</code></pre>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="the-poisson-and-related-distributions.html#cb239-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The proportion of simulated statistic values in the Wilks&#39; theorem rejection region&quot;</span>,<span class="fu">round</span>(<span class="fu">sum</span>(W<span class="sc">&gt;=</span><span class="fu">qchisq</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="dv">1</span>))<span class="sc">/</span>num.sim,<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The proportion of simulated statistic values in the Wilks&#39; theorem rejection region 0.071</code></pre>
<blockquote>
<p>The boundary value via simulation is 4.584, which is sufficiently
far from the Wilks theorem value of 3.841 to be concerning. The
upshot: if <span class="math inline">\(n\)</span> is small, it is best <em>not</em> to assume that <span class="math inline">\(W\)</span> is
chi-square-distributed; run simulations to determine rejection region
boundaries and <span class="math inline">\(p\)</span>-values instead.</p>
</blockquote>
</div>
</div>
<div id="the-gamma-distribution" class="section level2 hasAnchor" number="4.8">
<h2><span class="header-section-number">4.8</span> The Gamma Distribution<a href="the-poisson-and-related-distributions.html#the-gamma-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>gamma distribution</em> is a continuous distribution that is commonly
used to, e.g., model the waiting times between discrete events. Its
probability density function is given by
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}}{\beta^\alpha} \frac{\exp\left(-x/\beta\right)}{\Gamma(\alpha)} \,,
\]</span>
where <span class="math inline">\(x \in [0,\infty)\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are both <span class="math inline">\(&gt;\)</span> 0,
and <span class="math inline">\(\Gamma(\alpha)\)</span> is the gamma function:
<span class="math display">\[
\Gamma(\alpha) = \int_0^\infty u^{\alpha-1} e^{-u} du \,.
\]</span>
(See Figure <a href="the-poisson-and-related-distributions.html#fig:gammapdf">4.6</a>.)
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are referred to as shape and scale parameters,
respectively. The gamma family of distributions
exhibits a wide variety of functional shapes
and it is the parent family to a number of other
distributions, some of which we have met before.
One in particular is the <em>exponential distribution</em>,
<span class="math display">\[
f_X(x) = \frac{1}{\beta} \exp\left(-\frac{x}{\beta}\right) \,,
\]</span>
which is a gamma distribution with <span class="math inline">\(\alpha = 1\)</span>. Note how we lead off
above by saying that the gamma distribution is commonly used to
model the waiting times between discrete events. The
exponential distribution specifically models the waiting time between
one event and the next in a Poisson process. The number of strong
earthquakes that occur in California in one year? That can be modeled as
a Poisson random variable. The time that elapses between two consecutive
strong earthquakes in California? That can be modeled using the
exponential distribution. (For completeness: the Erlang distribution
is a generalization of the exponential distribution, in the sense
that we can use it to model the waiting time between the <span class="math inline">\(i^{\rm th}\)</span>
and <span class="math inline">\((i+\alpha)^{\rm th}\)</span> events, where <span class="math inline">\(\alpha\)</span> is a positive integer,
in a Poisson process.)</p>
<table>
<caption>Distributions Within the Gamma Family of Distributions</caption>
<thead>
<tr class="header">
<th>distribution</th>
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(\beta\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>exponential</td>
<td>1</td>
<td><span class="math inline">\((0,\infty)\)</span></td>
</tr>
<tr class="even">
<td>Erlang</td>
<td><span class="math inline">\(\{1,2,3,\ldots\}\)</span></td>
<td><span class="math inline">\((0,\infty)\)</span></td>
</tr>
<tr class="odd">
<td>chi-square</td>
<td><span class="math inline">\(\{1/2,1,3/2,\ldots\}\)</span></td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Lets conclude this section by repeating the exercise we did in the
last chapter while discussing the beta distribution, the one in which
we examined the functional form of the likelihood function
<span class="math inline">\(\mathcal{L}(p \vert k,x)\)</span>. Here, we write down the Poisson likelihood
function
<span class="math display">\[
\mathcal{L}(\lambda \vert x) = \frac{\lambda^x}{x!} e^{-\lambda}
\]</span>
and compare it with the gamma pdf. We can match the gamma pdf if
we map the Poisson <span class="math inline">\(\lambda\)</span> to the gamma <span class="math inline">\(x\)</span>, and
the Poisson <span class="math inline">\(x\)</span> to the gamma <span class="math inline">\(\alpha-1\)</span> (and if we set <span class="math inline">\(\beta\)</span> to 1).
But because the Poisson <span class="math inline">\(x\)</span> is an integer with values <span class="math inline">\(\{0,1,2,\ldots\}\)</span>,
we find that the integrand specifically
matches the Erlang pdf, for which <span class="math inline">\(\alpha = \{1,2,3,\ldots\}\)</span>.
So, if we observe a random variable <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>),
then the likelihood function <span class="math inline">\(\mathcal{L}(\lambda \vert x)\)</span> has
the shape (and normalization!) of a Gamma(<span class="math inline">\(x+1,1\)</span>) (or Erlang(<span class="math inline">\(x+1\)</span>))
distribution.</p>
<p>(About the normalization: if we integrate the likelihood function over
its domain, we find that
<span class="math display">\[
\frac{1}{x!} \int_0^\infty \lambda^x e^{-\lambda} d\lambda = \frac{1}{x!} \Gamma(x+1) = \frac{x!}{x!} = 1 \,.
\]</span>
The mathematics works out because <span class="math inline">\(x\)</span> is integer valued and thus
<span class="math inline">\(\Gamma(x+1) = x!\)</span>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gammapdf"></span>
<img src="_main_files/figure-html/gammapdf-1.png" alt="\label{fig:gammapdf}Three examples of gamma probability density functions: Gamma(2,2) (red), Gamma(4,2) (blue), and Gamma(2,3) (green)." width="50%" />
<p class="caption">
Figure 4.6: Three examples of gamma probability density functions: Gamma(2,2) (red), Gamma(4,2) (blue), and Gamma(2,3) (green).
</p>
</div>
<table>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PDF</td>
<td><code>dgamma(x,shape,scale)</code></td>
</tr>
<tr class="even">
<td>CDF</td>
<td><code>pgamma(x,shape,scale)</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td><code>qgamma(p,shape,scale)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rgamma(n,shape,scale)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="the-expected-value-of-a-gamma-random-variable" class="section level3 hasAnchor" number="4.8.1">
<h3><span class="header-section-number">4.8.1</span> The Expected Value of a Gamma Random Variable<a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a gamma random variable is found by introducing
constants into the expected value integral so that a gamma pdf integrand
is formed. Specifically
<span class="math display">\[\begin{align*}
E[X] = \int_0^\infty x f_X(x) dx &amp;= \int_0^\infty x \frac{x^{\alpha-1}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^\alpha} \frac{\exp(-x/\beta)}{\Gamma(\alpha)} \frac{\Gamma(\alpha+1)}{\Gamma(\alpha+1)} \frac{\beta^{\alpha+1}}{\beta^{\alpha+1}} dx \\
&amp;= \int_0^\infty \frac{x^{\alpha}}{\beta^{\alpha+1}} \frac{\exp(-x/\beta)}{\Gamma(\alpha+1)} \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \frac{\beta^{\alpha+1}}{\beta^\alpha} dx \\
&amp;= \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)} \frac{\beta^{\alpha+1}}{\beta^\alpha} \int_0^\infty \frac{x^{\alpha}}{\beta^{\alpha+1}} \frac{\exp(-x/\beta)}{\Gamma(\alpha+1)} dx \\
&amp;= \frac{\alpha \Gamma(\alpha)}{\Gamma(\alpha)} \beta \times 1 \\
&amp;= \alpha \beta \,.
\end{align*}\]</span>
By introducing the constants, we are able to transform the integrand
to that of a Gamma(<span class="math inline">\(\alpha+1,\beta\)</span>) distribution, and because the
integral is over the entire domain of a gamma distribution, the
integral evaluates to 1.</p>
</blockquote>
<blockquote>
<p>A similar calculation involving the derivation of <span class="math inline">\(E[X^2]\)</span> allows us
to determine that the variance of a gamma random variable is
<span class="math inline">\(V[X] = \alpha \beta^2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-distribution-of-the-sum-of-exponential-random-variables" class="section level3 hasAnchor" number="4.8.2">
<h3><span class="header-section-number">4.8.2</span> The Distribution of the Sum of Exponential Random Variables<a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As stated above, the exponential distribution, i.e., the gamma
distribution with <span class="math inline">\(\alpha = 1\)</span>, is used to model the waiting time
between two successive events in a Poisson process. Lets assume
that we have recorded <span class="math inline">\(n\)</span> separate times between <span class="math inline">\(n\)</span> separate pairs
of events. What is the distribution of <span class="math inline">\(T = T_1 + \cdots + T_n\)</span>?</p>
</blockquote>
<blockquote>
<p>As we typically do when faced with a linear function of <span class="math inline">\(n\)</span> iid
random variables, we utilize the method of moment-generating functions:
<span class="math display">\[
m_T(t) = \prod_{i=1}^n m_{T_i}(t) \,,
\]</span>
where <span class="math inline">\(m_{T_i}(t) = (1-\beta t)^{-1}\)</span> is the mgf for the exponential
distribution. Thus
<span class="math display">\[
m_T(t) = \prod_{i=1}^n (1-\beta t)^{-1} = (1-\beta t)^{-n} \,.
\]</span>
This has the form of the mgf for a Gamma(<span class="math inline">\(n,\beta\)</span>) distribution,
or, equivalently, an Erlang(<span class="math inline">\(n,\beta\)</span>) distribution. In other words
the sum of <span class="math inline">\(n\)</span> iid waiting times has the same distribution as
the waiting time between the <span class="math inline">\(i^{\rm th}\)</span> and <span class="math inline">\((i+n)^{\rm th}\)</span>
events of a Poisson process.</p>
</blockquote>
<hr />
</div>
<div id="memorylessness-and-the-exponential-distribution" class="section level3 hasAnchor" number="4.8.3">
<h3><span class="header-section-number">4.8.3</span> Memorylessness and the Exponential Distribution<a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>An important feature of the exponential distribution is that when we
use it to model, e.g., the lifetimes of components in a system, it
exhibits <em>memorylessness</em>. In other words, if <span class="math inline">\(T\)</span> is the random variable
representing a components lifetime, where <span class="math inline">\(T \sim\)</span> Exponential(<span class="math inline">\(\beta\)</span>)
<span class="math inline">\(E[T] = \beta\)</span>, it doesnt matter how old the component is when
you first examine it: <em>from that point onward</em>, the average lifetime
will be <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets demonstrate how this works via a word problem. We examine a component
born at time <span class="math inline">\(t_0=0\)</span> at a later time <span class="math inline">\(t_1\)</span>, and we wish to determine
the probability that it will live beyond an even later time <span class="math inline">\(t_2\)</span>. In other
words, we wish to compute
<span class="math display">\[
P(T \geq t_2-t_0 \vert T \geq t_1-t_0) \,.
\]</span>
(We know the component lived to time <span class="math inline">\(t_1\)</span>, hence the added condition.)
Let <span class="math inline">\(T \sim\)</span> Exponential(<span class="math inline">\(\beta\)</span>). Then
<span class="math display">\[\begin{align*}
P(T \geq t_2-t_0 \vert T \geq t_1-t_0) &amp;= \frac{P(T \geq t_2-t_0 \cap T \geq t_1-t_0)}{P(T \geq t_1-t_0)} \\
&amp;= \frac{P(T \geq t_2-t_0)}{P(T \geq t_1-t_0)} \\
&amp;= \frac{\int_{t_2-t_0}^\infty (1/\beta) \exp(-t/\beta) dt}{\int_{t_1-t_0}^\infty (1/\beta) \exp(-t/\beta) dt} \\
&amp;= \frac{\left. -\exp(-t/\beta) \right|_{t_2-t_0}^\infty}{\left. -\exp(-t/\beta) \right|_{t_1-t_0}^\infty} \\
&amp;= \frac{0 + \exp(-(t_2-t_0)/\beta)}{0 + \exp(-(t_1-t_0)/\beta)} \\
&amp;= \exp[-(t_2-t_1)/\beta] = P(T \geq t_2-t_1) \,.
\end{align*}\]</span>
Note that <span class="math inline">\(t_0\)</span> drops out of the final result: no matter how
long ago <span class="math inline">\(t_0\)</span> might have been, the probability that the component will
live <span class="math inline">\(t_2-t_1\)</span> units longer is the same, and the average additional lifetime
is still <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
</div>
</div>
<div id="poisson-regression" class="section level2 hasAnchor" number="4.9">
<h2><span class="header-section-number">4.9</span> Poisson Regression<a href="the-poisson-and-related-distributions.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that for a given measurement <span class="math inline">\(x\)</span>, we record a random
variable <span class="math inline">\(Y\)</span> that is a number of counts that we observe. For instance,
<span class="math inline">\(x\)</span> might be the time of day, and <span class="math inline">\(Y\)</span> might be the observed number
of cars parked in a lot at that time. Because <span class="math inline">\(Y\)</span> is (a) integer valued,
and (b) non-negative, an appropriate distribution for the random variable
<span class="math inline">\(Y \vert x\)</span> might be the Poisson distribution, and thus to model these
data, we may want to pursue Poisson regression.</p>
<div style="page-break-after: always;"></div>
<p><strong>Recall:</strong> <em>To implement a generalized linear model (or GLM),
we need to do two things:</em></p>
<ol style="list-style-type: decimal">
<li><em>examine the response values and select an appropriate distribution for them
(are they discretely or continuously valued?
what is the functional domain?); and</em></li>
<li><em>define a link function <span class="math inline">\(g(\theta \vert x)\)</span> that maps the line
<span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, which has infinite range, into a more limited
range (e.g., <span class="math inline">\([0,\infty)\)</span>).</em></li>
</ol>
<p>Because <span class="math inline">\(\lambda &gt; 0\)</span>, in Poisson regression we adopt a link function
that maps <span class="math inline">\(\beta_0 + \beta_1 x\)</span> from the range <span class="math inline">\((-\infty,\infty)\)</span>
to <span class="math inline">\((0,\infty)\)</span>. There is no unique choice of link function, but the
conventionally applied one is the logarithm:
<span class="math display">\[
g(\lambda \vert x) = \log (\lambda \vert x) = \beta_0 + \beta_1 x ~~\implies~~ \lambda \vert x = e^{\beta_0 + \beta_1 x} \,.
\]</span>
Similar to logistic regression, our goal is to estimate
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which is done via numerical optimization of the
likelihood function
<span class="math display">\[
\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y}) = \prod_{i=1}^n p_{Y \vert \beta_0,\beta_1}(y_i \vert \beta_0,\beta_1) \,.
\]</span></p>
<hr />
<p>For the Poisson distribution, <span class="math inline">\(E[X] = V[X] = \lambda\)</span>, so the expectation
is that for any given value of <span class="math inline">\(x\)</span>, <span class="math inline">\(E[Y \vert x] = V[Y \vert x]\)</span>. However,
it is commonly seen in real-life data that the sample variance of
<span class="math inline">\(Y \vert x\)</span> exceeds the sample mean. This is dubbed <em>overdispersion</em>, and
it can arise when, e.g., the observed Poisson process is inhomogeneousor
differently stated, when <span class="math inline">\(\lambda\)</span> varies as a function of space and/or time.
A standard way to deal with overdispersion is to move from Poisson regression
to <em>negative binomial regression</em>.</p>
<p>Before we say more, however, we note that while the name is technically
correct (in the sense that the model assumes that the response data are
negative binomially distributed), it can be very confusing for those new
to regression, who might view the negative binomial as a distribution that is
only useful when, e.g., modeling failures in Bernoulli trials.
How could that distribution possibly apply here? The answer is that the
negative binomial probability mass function is just a function (and as
such, it is allowed to have more general use than just modeling failures),
but more to the point, it is a function that arises naturally when we
apply the Law of Total Probability to the Poisson pmf.</p>
<p>Lets suppose that <span class="math inline">\(Y \vert x \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>), but that <span class="math inline">\(\lambda\)</span>
itself is a random variable. There is no unique way to model the
distribution of <span class="math inline">\(\lambda\)</span>, but the gamma distribution provides a flexible
means by which to do so (since the family encompasses a wide variety of
shapes, unlike, say, the normal distribution, which can never be skew).
Lets assume that <span class="math inline">\(\lambda \sim\)</span> Gamma(<span class="math inline">\(\theta,p/\theta\)</span>). The distribution of <span class="math inline">\(Y\)</span>
is found with the LoTP:
<span class="math display">\[\begin{align*}
p_{Y \vert \theta,p}(y \vert \theta,p) &amp;= \int_0^\infty p_{Y \vert \lambda}(y \vert \lambda) f_{\lambda}(\lambda \vert \theta,p) d\lambda \\
&amp;= \int_0^\infty \frac{\lambda^y}{y!} e^{-\lambda} \frac{\lambda^{\theta-1}}{(p/\theta)^\theta} \frac{e^{-\lambda/(p/\theta)}}{\Gamma(\theta)} d\lambda \\
&amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \int_0^\infty \lambda^{y+\theta+1} e^{-\lambda(1+\theta/p)} d\lambda \,.
\end{align*}\]</span>
The integrand looks suspiciously like the integrand of a gamma function
integral, but we have to change <span class="math inline">\(\lambda(1+\theta/p)\)</span> in the exponential to
just <span class="math inline">\(\lambda&#39;\)</span>:
<span class="math display">\[
\lambda&#39; = \lambda(1+\theta/p) ~~ \mbox{and} ~~ d\lambda&#39; = d\lambda (1+\theta/p) \,.
\]</span>
The bounds of the integral do not change. The integral now becomes
<span class="math display">\[\begin{align*}
p_{Y \vert \theta,p}(y \vert \theta,p) &amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \frac{1}{(1+\theta/p)^{y+\theta}} \int_0^\infty (\lambda&#39;)^{y+\theta-1} e^{-\lambda&#39;} d\lambda&#39; \\
&amp;= \frac{1}{y!}\left(\frac{\theta}{p}\right)^\theta \frac{1}{\Gamma(\theta)} \frac{1}{(1+\theta/p)^{y+\theta}} \Gamma(y+\theta) \\
&amp;= \frac{(y+\theta-1)!}{y! (\theta-1)!} \left(\frac{\theta}{p}\right)^\theta \left(\frac{p}{p+\theta}\right)^{y+\theta} \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{p}{p+\theta}\right)^y \left(\frac{\theta}{p}\right)^\theta \left(\frac{p}{p+\theta}\right)^\theta \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{p}{p+\theta}\right)^y \left(\frac{\theta}{p+\theta}\right)^\theta \\
&amp;= \binom{y+\theta-1}{y} \left(\frac{\theta}{p+\theta}\right)^\theta \left(1 - \frac{\theta}{p+\theta}\right)^y \,.
\end{align*}\]</span>
This has the functional form of a negative binomial pmf in which
<span class="math inline">\(y\)</span> represents the number of failures
<span class="math inline">\(\theta\)</span> is the number of successes, and <span class="math inline">\(\theta/(p+\theta)\)</span> is the
probability of success. Again, to reiterate: <span class="math inline">\(Y\)</span> might be distributed
as a negative
binomial random variable, but what we are really modeling is the
<span class="math inline">\(\lambda\)</span> parameter of the Poisson distribution.</p>
<p>Now, why do we choose this form of the negative binomial distribution?
We do so because it so happens that
<span class="math display">\[\begin{align*}
E[Y] &amp;= p \\
V[Y] &amp;= p + \frac{p^2}{\theta} \,.
\end{align*}\]</span>
(One can derive these quantities starting with, e.g.,
<span class="math inline">\(E[Y] = \theta(1-p&#39;)/p&#39;\)</span> and <span class="math inline">\(V[Y] = \theta(1-p&#39;)/(p&#39;)^2\)</span> and
plugging in <span class="math inline">\(p&#39; = \theta/(p+\theta)\)</span>.)
Varying <span class="math inline">\(\theta\)</span> allows us to model the overdispersion with a single
variable. (We assume that <span class="math inline">\(p\)</span> takes the place of <span class="math inline">\(\lambda\)</span>, in the
sense that now <span class="math inline">\(p \vert x = \exp(\beta_0 + \beta_1x\)</span>).)
In the limit that <span class="math inline">\(\theta \rightarrow \infty\)</span>, negative binomial regression
becomes Poisson regression. Because the overdispersion is represented
in a single variable, we can examine the results of learning both Poisson and
negative binomial regression models to determine whether or not
the quality of fit improves enough to justify the extra model complexity.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example-2" class="section level3 hasAnchor" number="4.9.1">
<h3><span class="header-section-number">4.9.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Modeling Bortkiewiczs horse-kick dataset provides a simple example
of the use of Poisson regression.</p>
</blockquote>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="the-poisson-and-related-distributions.html#cb241-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb241-2"><a href="the-poisson-and-related-distributions.html#cb241-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">109</span>,<span class="dv">65</span>,<span class="dv">22</span>,<span class="dv">3</span>,<span class="dv">1</span>)</span>
<span id="cb241-3"><a href="the-poisson-and-related-distributions.html#cb241-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb241-4"><a href="the-poisson-and-related-distributions.html#cb241-4" aria-hidden="true" tabindex="-1"></a>poi.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y<span class="sc">~</span>x,<span class="at">family=</span>poisson)</span>
<span id="cb241-5"><a href="the-poisson-and-related-distributions.html#cb241-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(poi.out)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ x, family = poisson)
## 
## Deviance Residuals: 
##       1        2        3        4        5  
## -1.1700   2.2681   0.6145  -1.9199  -1.3639  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  4.80136    0.08490   56.55   &lt;2e-16 ***
## x           -0.92213    0.07704  -11.97   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 232.430  on 4  degrees of freedom
## Residual deviance:  12.437  on 3  degrees of freedom
## AIC: 38.911
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<blockquote>
<p>The summary output from the Poisson regression model is, in its structure,
identical to that of logistic regression. But there are some differences
in how values are defined. For instance, the deviance residual is
<span class="math display">\[
d_i = \mbox{sign}(Y_i - \hat{Y}_i) \sqrt{2[Y_i \log (Y_i/\hat{Y}_i) - (Y_i - \hat{Y}_i)]} \,,
\]</span>
where
<span class="math display">\[
\hat{Y}_i = \hat{\lambda}_i = \exp(\hat{\beta}_0+\hat{\beta}_1 x_i) \,.
\]</span>
(Note that when <span class="math inline">\(Y_i = 0\)</span>, <span class="math inline">\(Y_i \log (Y_i/\hat{Y}_i)\)</span> is assumed to
be zero.)
Because there are only five data points in the fit, all the deviance
residual values are displayed, rather than a five-number summary.
Also, the residual deviance (here, 12.437) is <em>not</em>
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>, as it was for logistic regression. There
are two ways to determine <span class="math inline">\(\mathcal{L}_{max}\)</span>; one is to take the AIC
value (38.911), subtract 2 times the number of model terms (2 here,
thus yielding 34.911), and then dividing by <span class="math inline">\(-2\)</span>. The more straightforward
way, however, is to utilize the <code>logLik()</code> function:</p>
</blockquote>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="the-poisson-and-related-distributions.html#cb243-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(poi.out)</span></code></pre></div>
<pre><code>## &#39;log Lik.&#39; -17.45551 (df=2)</code></pre>
<blockquote>
<p>As a final note, unlike the case of logistic regression where determining
the quality of fit of the learned model is not particularly straightforward,
for Poisson regression we can simply assume that the residual deviance
is chi-square-distributed for the given number of degrees of freedom under
the null. Here, the <span class="math inline">\(p\)</span>-value is</p>
</blockquote>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="the-poisson-and-related-distributions.html#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="fl">12.437</span>,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.006026712</code></pre>
<blockquote>
<p>or 0.0060. Because this value is less than, e.g., <span class="math inline">\(\alpha = 0.05\)</span>, we
would (in this instance) reject the null hypothesis that the observed
data are truly Poisson distributed.</p>
</blockquote>
<hr />
</div>
<div id="negative-binomial-regression-example" class="section level3 hasAnchor" number="4.9.2">
<h3><span class="header-section-number">4.9.2</span> Negative Binomial Regression Example<a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the code chunk below, we simulate 100 data at each of four
different values of <span class="math inline">\(x\)</span>: 1, 2, 3, and 4. The data are simulated
with a Poisson overdispersion factor of 2.</p>
</blockquote>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="the-poisson-and-related-distributions.html#cb247-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">236</span>)</span>
<span id="cb247-2"><a href="the-poisson-and-related-distributions.html#cb247-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>               <span class="co"># 100 data per x value</span></span>
<span id="cb247-3"><a href="the-poisson-and-related-distributions.html#cb247-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>),n)</span>
<span id="cb247-4"><a href="the-poisson-and-related-distributions.html#cb247-4" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,<span class="fu">length</span>(x))</span>
<span id="cb247-5"><a href="the-poisson-and-related-distributions.html#cb247-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x) ) {</span>
<span id="cb247-6"><a href="the-poisson-and-related-distributions.html#cb247-6" aria-hidden="true" tabindex="-1"></a>  Y[ii] <span class="ot">&lt;-</span> <span class="fu">rpois</span>(<span class="dv">1</span>,<span class="fu">rgamma</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="at">scale=</span>x[ii]<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb247-7"><a href="the-poisson-and-related-distributions.html#cb247-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<blockquote>
<p>For these data, <span class="math inline">\(E[Y \vert x] = x\)</span> and
<span class="math inline">\(V[Y \vert x] = x + x^2/2\)</span>, meaning that the
overdispersion factor is, again, <span class="math inline">\(\theta = 2\)</span>. Lets see
how overdispersion affects the learning of a
Poisson regression model.</p>
</blockquote>
<blockquote>
<p>First, the Poisson regression model itself:</p>
</blockquote>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="the-poisson-and-related-distributions.html#cb248-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm</span>(Y<span class="sc">~</span>x,<span class="at">family=</span>poisson))</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Y ~ x, family = poisson)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9013  -1.6404  -0.3122   0.6825   5.6697  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.08336    0.09240  -0.902    0.367    
## x            0.38014    0.02944  12.913   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 1107  on 399  degrees of freedom
## Residual deviance:  930  on 398  degrees of freedom
## AIC: 1805
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<blockquote>
<p>and second, the negative binomial regression model,
as learned using the <code>glm.nb()</code> function of the <code>MASS</code>
package. (Note that <code>MASS</code> does not represent the state
of Massachusetts, but rather stands for Modern Applied
Statistics with Swith <code>S</code> being the precursor
software to <code>R</code>.)</p>
</blockquote>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="the-poisson-and-related-distributions.html#cb250-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb250-2"><a href="the-poisson-and-related-distributions.html#cb250-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">glm.nb</span>(Y<span class="sc">~</span>x))</span></code></pre></div>
<pre><code>## 
## Call:
## glm.nb(formula = Y ~ x, init.theta = 1.84882707, link = log)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1019  -1.2027  -0.2296   0.4198   2.8666  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.10769    0.13101  -0.822    0.411    
## x            0.38908    0.04483   8.679   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(1.8488) family taken to be 1)
## 
##     Null deviance: 530.52  on 399  degrees of freedom
## Residual deviance: 454.45  on 398  degrees of freedom
## AIC: 1633.9
## 
## Number of Fisher Scoring iterations: 1
## 
## 
##               Theta:  1.849 
##           Std. Err.:  0.258 
## 
##  2 x log-likelihood:  -1627.868</code></pre>
<blockquote>
<p>The negative binomial model is displayed in Figure <a href="the-poisson-and-related-distributions.html#fig:nbmodel">4.7</a>.
When we compare the output, we first look for the lines
beginning with <code>(Dispersion parameter...)</code>:</p>
</blockquote>
<pre><code>(Dispersion parameter for poisson family taken to be 1)
(Dispersion parameter for Negative Binomial(1.8488) family taken to be 1)</code></pre>
<blockquote>
<p>The second line is perhaps sub-optimally worded, as the estimate of
the dispersion parameter <span class="math inline">\(\theta\)</span> is not 1, but the value in
parentheses: <span class="math inline">\(\hat{\theta} = 1.8488\)</span>. This is close to the true value
<span class="math inline">\(\theta = 2\)</span> and is definitely different from
<span class="math inline">\(\theta = \infty\)</span> (the value for truly Poisson-distributed data), but
is it statistically significantly different, such that we know we
should adopt the negative binomial model? To answer that question,
we need to utilize the standard error for <span class="math inline">\(\hat{\theta}\)</span>; here, thet
is 0.258. By the empirical rule, we would expect the true value of
<span class="math inline">\(\theta\)</span> to lay within three standard errors of <span class="math inline">\(\hat{\theta}\)</span>and
<span class="math inline">\(1.849 + 3 \times 0.258 = 2.623\)</span>, so <span class="math inline">\(\theta\)</span> is definitely smaller
than infinity. So it is clear that we would adopt
the negative binomial model here. And theres
another bit of evidence that supports our adoption of that model:</p>
</blockquote>
<pre><code>AIC: 1805   # Poisson
AIC: 1633.9 # NB</code></pre>
<blockquote>
<p>The Akaike Information Criterion, or AIC, as you will recall, is
a quality-of-fit metric that penalizes model complexity.
If we learn a suite of models, we would generally adopt that associated
with the lowest
AIC value. Here, the negative binomial model has a much lower AIC value
than the Poisson model, so we would definitely adopt it!</p>
</blockquote>
<blockquote>
<p>(Note that when the data are <em>not</em> overdispersed, <span class="math inline">\(\hat{\theta}\)</span>
will usually be a large number, but not actually infinity. If in doubt
about whether large is large enough, we can always fall back on
the outputted AIC values so as to make a decision.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbmodel"></span>
<img src="_main_files/figure-html/nbmodel-1.png" alt="\label{fig:nbmodel}The negative binomial regression line superimposed on the simulated data. The `jitter()` function is applied to the $x$ values to allow us to more easily see the number of counts as a function of $x$ and $Y$." width="50%" />
<p class="caption">
Figure 4.7: The negative binomial regression line superimposed on the simulated data. The <code>jitter()</code> function is applied to the <span class="math inline">\(x\)</span> values to allow us to more easily see the number of counts as a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>.
</p>
</div>
</div>
</div>
<div id="chi-square-based-hypothesis-testing-1" class="section level2 hasAnchor" number="4.10">
<h2><span class="header-section-number">4.10</span> Chi-Square-Based Hypothesis Testing<a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter, we introduced the chi-square goodness-of-fit test
as a way of conducting hypothesis tests given multinomial data. Given <span class="math inline">\(k\)</span>
data recorded in <span class="math inline">\(m\)</span> separate bins, we can compute the test statistic
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} \,,
\]</span>
where <span class="math inline">\(X_i\)</span> is the number of data observed in bin <span class="math inline">\(i\)</span>, and where
<span class="math inline">\(p_i\)</span> is the probability of any one datum being observed in that bin
under the null.
If <span class="math inline">\(k\)</span> is sufficiently large, then <span class="math inline">\(W\)</span> converges in distribution to
a chi-square random variable for <span class="math inline">\(m-1\)</span> degrees of freedom. (Hence the
name of the test!)</p>
<p>But: what if the overall observed number of data <span class="math inline">\(X\)</span>
is a random variable instead of being a set constant <span class="math inline">\(X=k\)</span>?
Imagine a very simple digital camera that has four light collecting
bins, each of the same size, and we point it towards a light
source that emits an average of <span class="math inline">\(\lambda\)</span> photons in a particular
time period. If we open the shutter for that time period, what we
would observe is <span class="math inline">\(X \sim\)</span> Poisson(<span class="math inline">\(\lambda\)</span>) photons, with
the numbers in each bin being <span class="math inline">\(\{X_1,X_2,X_3,X_4\}\)</span>. Now lets
say we want to test the hypothesis that the probability of a photon
falling into each bin is the same, i.e., <span class="math inline">\(H_o: p_1 = p_2 = p_3 = p_4 = 1/4\)</span>.
If we were to use the chi-square GoF test directly, we would compute
<span class="math display">\[
w_{obs} = \sum_{i=1}^4 \frac{(X_i - \lambda p_i)^2}{\lambda p_i}
\]</span>
and then compute the <span class="math inline">\(p\)</span>-value <span class="math inline">\(1-F_W(w_{obs})\)</span> assume 3 degrees of freedom.
(In <code>R</code>, this would be computed via <code>1 - pchisq(w.obs,3)</code>.) We can do
this<span class="math inline">\(-\)</span>the computer will not stop us from doing so<span class="math inline">\(-\)</span>but is this valid?</p>
<p>The answer is that this <em>is</em> valid so long as each <span class="math inline">\(X_i\)</span> converges in
distribution to a normal random variable. And Poisson random variables
<em>do</em> converge in distribution to normal random variables. Thus, in short,
yes, use of the chi-square GoF test is valid if <span class="math inline">\(\lambda p_i\)</span> is large.
The question is, how large?</p>
<hr />
<p>The Poisson probability mass function is
<span class="math display">\[
p_X(x \vert \lambda) = \frac{\lambda^x}{x!} e^{-\lambda} \,.
\]</span>
We note that the normal probability density function does not have
a factorial in it, so we will start by using Stirlings approximation:
<span class="math display">\[
x! \approx \sqrt{2 \pi x} x^x e^{-x} \,.
\]</span>
This approximation has an error of 1.65% for <span class="math inline">\(x = 5\)</span> and 0.83% for <span class="math inline">\(x = 10\)</span>,
with the percentage error continuing to shrink as <span class="math inline">\(x \rightarrow \infty\)</span>.
With this approximation, we can write that
<span class="math display">\[
p_X(x \vert \lambda) \approx \frac{\lambda^x}{\sqrt{2 \pi x}} x^{-x} e^{x-\lambda} \,.
\]</span>
This still does not quite look like a normal pdf. So there is more work
to do. We compute the logarithm of this quantity:
<span class="math display">\[
\log p_X(x \vert \lambda) \approx x \log \lambda - \frac{1}{2} \log (2 \pi x) - x \log x + x - \lambda = -x ( \log x - \log \lambda ) - \frac{1}{2} \log (2 \pi x) + x - \lambda \,,
\]</span>
and then look at <span class="math inline">\(\log x - \log \lambda\)</span>:
<span class="math display">\[
\log x - \log \lambda = \log \frac{x}{\lambda} = \log \left( 1 - \frac{\lambda-x}{\lambda}\right) \approx -\frac{\delta}{\sqrt{\lambda}} - \frac{\delta^2}{2 \lambda} - \cdots \,.
\]</span>
Here, <span class="math inline">\(\delta = (\lambda - x)/\sqrt{\lambda}\)</span>. Plugging this result
into the expression for <span class="math inline">\(\log p_X(x \vert \lambda)\)</span>, we find that
<span class="math display">\[
\log p_X(x \vert \lambda) \approx -\frac{1}{2} \log (2 \pi x) + x \left( \frac{\delta}{\sqrt{\lambda}} + \frac{\delta^2}{2\lambda} \right) - \delta \sqrt{\lambda} \,.
\]</span>
The next step is plug in <span class="math inline">\(x = \lambda - \sqrt{\lambda}\delta\)</span>:
<span class="math display">\[\begin{align*}
\log p_X(x \vert \lambda) &amp;\approx -\frac{1}{2} \log (2 \pi x) + (\lambda - \sqrt{\lambda}\delta)\left( \frac{\delta}{\sqrt{\lambda}} + \frac{\delta^2}{2\lambda} \right) - \delta \sqrt{\lambda} \\
&amp;= -\frac{1}{2} \log (2 \pi x) + \sqrt{\lambda}\delta - \delta^2 + \frac{\delta^2}{2} - \frac{\delta^3}{2 \lambda^{3/2}} - \sqrt{\lambda}\delta \\
&amp;\approx -\frac{1}{2} \log (2 \pi x) - \frac{\delta^2}{2} \,,
\end{align*}\]</span>
where we drop the <span class="math inline">\(\mathcal{O}(\delta^3)\)</span> term.
When we exponentiate both sides, the final result is
<span class="math display">\[
p_X(x \vert \lambda) \approx \frac{1}{\sqrt{2 \pi x}} \exp\left( -\frac{\delta^2}{2} \right) = \frac{1}{\sqrt{2 \pi x}} \exp\left( -\frac{(x-\lambda)^2}{2\sqrt{\lambda}} \right) \,.
\]</span>
This has the (approximate) form of a normal pdf, at least for values
<span class="math inline">\(x \approx \lambda\)</span>. So, in the end, the Poisson probability mass function
<span class="math inline">\(p_X(x \vert \lambda)\)</span>
has approximately the same shape as the normal probability density function
<span class="math inline">\(f_X(x \vert \mu=\lambda,\sigma^2=\lambda)\)</span> if <span class="math inline">\(x \gg 1\)</span> and
<span class="math inline">\(x \approx \lambda\)</span>.</p>
<p>The conventional rule-of-thumb is that one can utilize the chi-square
GoF test with Poisson data so long as <span class="math inline">\(\lambda p_i \geq 5\)</span> counts
in each bin.</p>
<hr />
<div id="revisiting-the-death-by-horse-kick-example-3" class="section level3 hasAnchor" number="4.10.1">
<h3><span class="header-section-number">4.10.1</span> Revisiting the Death-by-Horse-Kick Example<a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In previous examples, we have shown that over the course of 20 years,
in 10 separate Prussian army corps, soldiers died as a result of horse
kicks at a rate of <span class="math inline">\(\hat{\lambda} = \bar{X} = 0.61\)</span> deaths per corps
per year, and that a 95% confidence interval for <span class="math inline">\(\lambda\)</span> is [0.51,0.73].
However, we never examined what is perhaps the most important question of
all: is it plausible that the data are Poisson-distributed in the first place?</p>
</blockquote>
<blockquote>
<p>We last looked at the idea of performing hypothesis tests regarding
distributions in Chapter 2, where we introduce the Kolmogorov-Smirnov
test for use with arbitrary distributions and the Shapiro-Wilk test
to assess the plausibility that our data are normally distributed. So
it would seem that here we should work with the KS test, as the Poisson
distribution is not the normal distributionbut we can only use
the KS test in the context of <em>continuous</em> distributions. So we need
a new method!</p>
</blockquote>
<blockquote>
<p>We can utilize the chi-square goodness-of-fit test. Lets assume
<span class="math inline">\(\lambda = 0.61\)</span>. Then the probabilities <span class="math inline">\(P(X=x)\)</span> are as follows:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th>x</th>
<th>P(X=x)</th>
<th><span class="math inline">\(E_x\)</span></th>
<th><span class="math inline">\(O_x\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.543</td>
<td>108.67</td>
<td>109</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.331</td>
<td>66.29</td>
<td>65</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.101</td>
<td>20.29</td>
<td>22</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.021</td>
<td>4.11</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.003</td>
<td>0.63</td>
<td>1</td>
</tr>
</tbody>
</table>
<blockquote>
<p>The conventional rule-of-thumb is that the expected number of counts
in each bin must be <span class="math inline">\(\geq 5\)</span>. Here, we will break that rule slightly
by combining bins 3 and 4 into one bin with expected counts 4.11 + 0.63
= 4.74. The chi-square GOF test statistic is thus
<span class="math display">\[
W = \frac{(109-108.67)^2}{108.67} + \frac{(65-66.29)^2}{66.29} + \frac{(22-20.29)^2}{20.29} + \frac{(4-4.74)^2}{4.74} = 0.298 \,.
\]</span>
This figure can be found using <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="the-poisson-and-related-distributions.html#cb254-1" aria-hidden="true" tabindex="-1"></a>e    <span class="ot">&lt;-</span> <span class="dv">200</span><span class="sc">*</span><span class="fu">dpois</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">4</span>,<span class="fl">0.61</span>)</span>
<span id="cb254-2"><a href="the-poisson-and-related-distributions.html#cb254-2" aria-hidden="true" tabindex="-1"></a>e[<span class="dv">4</span>] <span class="ot">&lt;-</span> e[<span class="dv">4</span>]<span class="sc">+</span>e[<span class="dv">5</span>]</span>
<span id="cb254-3"><a href="the-poisson-and-related-distributions.html#cb254-3" aria-hidden="true" tabindex="-1"></a>e    <span class="ot">&lt;-</span> e[<span class="sc">-</span><span class="dv">5</span>]</span>
<span id="cb254-4"><a href="the-poisson-and-related-distributions.html#cb254-4" aria-hidden="true" tabindex="-1"></a>o    <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">109</span>,<span class="dv">65</span>,<span class="dv">22</span>,<span class="dv">4</span>)</span>
<span id="cb254-5"><a href="the-poisson-and-related-distributions.html#cb254-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb254-6"><a href="the-poisson-and-related-distributions.html#cb254-6" aria-hidden="true" tabindex="-1"></a>(<span class="at">W =</span> <span class="fu">sum</span>((o<span class="sc">-</span>e)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>e))</span></code></pre></div>
<pre><code>## [1] 0.2980418</code></pre>
<blockquote>
<p>When using the chi-square GOF test to assess the viability of a distribution
whose parameters are estimated, we lose additional degrees of freedom,
so the number of degrees of freedom here is 4 - 2 = 2. The <span class="math inline">\(p\)</span>-value is</p>
</blockquote>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="the-poisson-and-related-distributions.html#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(W,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.8615511</code></pre>
<blockquote>
<p>We find that we fail to reject the null hypothesis and thus that it is
plausible that Bortkiewiczs horse-kick data are indeed Poisson-distributed.</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-binomial-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-uniform-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

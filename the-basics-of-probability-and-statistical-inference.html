<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 The Basics of Probability and Statistical Inference | Modern Probability and Statistical Inference</title>
  <meta name="description" content="1 The Basics of Probability and Statistical Inference | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="1 The Basics of Probability and Statistical Inference | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 The Basics of Probability and Statistical Inference | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>
<link rel="next" href="the-normal-and-related-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.3</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.6</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-basics-of-probability-and-statistical-inference" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> The Basics of Probability and Statistical Inference<a href="the-basics-of-probability-and-statistical-inference.html#the-basics-of-probability-and-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="data-and-statistical-populations" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Data and Statistical Populations<a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Data surround us, in the form of numbers, texts, images, and more that
are collected and analyzed across disciplines. Tweets contain data about
user sentiments. Receipts contain data about peoples buying habits.
Pictures help us differentiate between, e.g., goldfish and dogs. These
data<span class="math inline">\(-\)</span>tweets, receipts, pictures<span class="math inline">\(-\)</span>are <em>unstructured</em> data, so-called
because we generally cannot visualize or analyze them directly. So what
can we do? We can provide structure: to determine if a tweet
indicates that a film was liked or disliked, we can extract counts of
words indicating sentiments (e.g., good and bad). To determine the
whether an image is that of a goldfish or a dog, it can be passed through
appropriate filters that break down the images to a series of analyzable
numbers. Etc.</p>
<p>The result of all this pre-processing is generation of <em>structured data</em>,
data in the form of a table in which the columns represent particular
measurements (e.g., the number of instances of the word good) and the
rows representing the objects of study (e.g., individual films). Lets
focus on a single table column. Perhaps its data look like this:</p>
<pre><code>34.1 28.6 37.7 52.1 26.6 28.9 ...</code></pre>
<p>Such data are dubbed <em>quantitative</em> data. Quantitative data are numbers that
might be discretely valued (e.g., 1, 2, and 3) or continuously valued and
measured to arbitrary precision (e.g., 15.4959735).
Data may also look like this:</p>
<pre><code>Heads Heads Tails Heads ...</code></pre>
<p>These data are <em>categorical</em> data; each outcome is one element from a
set of categories. Here that set is {Heads,Tails}.</p>
<p>An <em>experiment</em> is the act of measuring and recording a datum (like when
after each flip of a coin we record <span class="math inline">\(H\)</span> for heads and <span class="math inline">\(T\)</span> for tails). The data
we generate from experiments are drawn from <em>populations</em>, the sets
of all possible experimental outcomes. A population can be an existing group
of objects (e.g., 52 cards in a deck, eight socks of different colors in a
drawer),
but it can also be hypothetical (e.g., a mathematical function, like a bell
curve, which indicates the relative rates at which we would draw samples with
particular values). To boil down the discipline of statistics to its essence,
our goal is to use the data we have drawn from a population to say something
(i.e., to <em>infer</em> something) about the population itself. If we record the
heights of 100 people, we would like to say something about the average height
of humans. If we record the ice-cream flavor preferences of 500 people, we
would like to infer the proportion of humans that prefer chocolate to vanilla.
Etc. Data surround us and the possibilities for inference are plentiful.</p>
<p>We pictorially summarize what we write above in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:cycle1">1.1</a>.
One might immediately
notice the word statistic, which we have yet to define. As we see later in
this chapter, a statistic is simply a function of data (such as their average
value) that helps reduce data volume while (hopefully!) retaining sufficient
information to allow useful inferences to happen.
Defining and understanding useful statistics
is a <em>major</em> part of this course!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cycle1"></span>
<img src="figures/inference_cycle.png" alt="\label{fig:cycle1}The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, and use statistics, or functions of the data, to infer population properties." width="60%" />
<p class="caption">
Figure 1.1: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, and use statistics, or functions of the data, to infer population properties.
</p>
</div>
<p>But, the reader says: the course is called
<em>Modern Probability and Statistical Inference</em>.
Where is probability in all of this? Probability is the so-called language of
statistics, and it provides the mathematical framework upon which we can
build statistical inference.
Remember how above we say that a population might be a mathematical function
indicating the relative rates of observing experimental outcomes? Those
relative rates <em>are</em> probabilities (or at least probability densities).
Thus the structure of this chapter (and mathemtical statistics courses
as a whole): we discuss
probability first, and then use our newfound knowledge to show how the
enterprise of statistical inference works, both algorithmically and
mathematically.</p>
</div>
<div id="sample-spaces-and-the-axioms-of-probability" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Sample Spaces and the Axioms of Probability<a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probability is the long-term frequency of the occurrence of an event. For instance, what is the probability of flipping a coin
and observing heads? (Intuitively, this probability is 1/2, if the coin is fair.) Or: what is the probability that a student finishes a particular
test in between 30 and 40 minutes? Etc.</p>
<p>To build up an understanding of probability, it is conventional to start with the concept of a <em>sample space</em>. A sample space is the set of all
possible outcomes of an experiment (or trial), which is simply some process that can, in theory, be repeated an infinite number of times.
(For instance, the flipping of a coin.) For instance, if our experiment is to flip a single coin twice, the sample space would be
<span class="math display">\[
\Omega = \{HH,HT,TH,TT\} \,,
\]</span>
where <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span> represent observing heads and tails, respectively.
(The Greek letter <span class="math inline">\(\Omega\)</span> is a capital omega, or oh-MAY-gah.)
See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:ss">1.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ss"></span>
<img src="figures/samplespace.png" alt="\label{fig:ss}This is an example of a sample space $\Omega$, representing the experimental outcomes of flipping a single coin twice and recording the observed side of the coin. For purposes of intuition, it is common to associate the area shown for each outcome with that outcome's probability of occurrence, so here we may view the coin as an unfair one that favors tails." width="40%" />
<p class="caption">
Figure 1.2: This is an example of a sample space <span class="math inline">\(\Omega\)</span>, representing the experimental outcomes of flipping a single coin twice and recording the observed side of the coin. For purposes of intuition, it is common to associate the area shown for each outcome with that outcomes probability of occurrence, so here we may view the coin as an unfair one that favors tails.
</p>
</div>
<p>The members of the set <span class="math inline">\(\Omega\)</span> are dubbed <em>events</em> and they come
in two varieties:</p>
<ol style="list-style-type: decimal">
<li><em>simple events</em>: specific experimental outcomes (e.g., <span class="math inline">\(HH\)</span>); any two
simple events in <span class="math inline">\(\Omega\)</span> are <em>mutually exclusive</em>, or <em>disjoint</em>, as they
cannot be observed simultaneously in a single experiment.</li>
<li><em>compound events</em>: sets of two or more simple events (e.g., <span class="math inline">\(\{HH,HT,TH\}\)</span>,
which represents the set of outcomes where heads was observed at least once).</li>
</ol>
<p>As stated above, a sample space is a set of possible experimental
outcomes; thus we can apply set notation to, e.g., define specific events
as functions of others:</p>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="center">notation</th>
<th align="center">intuitive terminology</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">superset</td>
<td align="center"><span class="math inline">\(A \supset B\)</span></td>
<td align="center">encompasses</td>
</tr>
<tr class="even">
<td align="left">subset</td>
<td align="center"><span class="math inline">\(A \subset B\)</span></td>
<td align="center">within</td>
</tr>
<tr class="odd">
<td align="left">union</td>
<td align="center"><span class="math inline">\(A \cup B\)</span></td>
<td align="center">or</td>
</tr>
<tr class="even">
<td align="left">intersection</td>
<td align="center"><span class="math inline">\(A \cap B\)</span></td>
<td align="center">and</td>
</tr>
<tr class="odd">
<td align="left">complement</td>
<td align="center"><span class="math inline">\(\bar{A}\)</span></td>
<td align="center">not</td>
</tr>
</tbody>
</table>
<p>We show examples of how we use set notation in the context of samples spaces
below.</p>
<p>Here are a few more things to keep in mind regarding sample spaces:</p>
<ul>
<li>The number of simple events in <span class="math inline">\(\Omega\)</span> (i.e., the sets <em>cardinality</em>)
may be finite (as in the example above) or either countably or uncountably
infinite (e.g., the set of all non-negative integers versus the set of
real numbers). (For instance, the simple events in the experiment of repeating
a task until one fails are <span class="math inline">\(\{F,SF,SSF,SSSF,\ldots\}\)</span>, where <span class="math inline">\(S\)</span> denotes
success and <span class="math inline">\(F\)</span> denotes failure.)</li>
<li>The definition of a sample space can depend upon whether the order of
outcomes matters. For instance, if the order of outcomes does not matter,
we could rewrite our two-coin-flip sample space as
<span class="math inline">\(\Omega = \{HH,HT,TT\}\)</span> (or <span class="math inline">\(\{HH,TH,TT\}\)</span>).</li>
<li>At no point thus far have we indicated the probability of observing
any simple event. It is not the case in general that each experimental
outcome is equally likely!</li>
</ul>
<p>Regarding the last point above: while we may not know the probability of
observing any simple event, there are some things we can say about
its long-term relative frequency of occurrence:</p>
<ol style="list-style-type: decimal">
<li>it must be <span class="math inline">\(&gt; 0\)</span> and <span class="math inline">\(\leq 1\)</span>;</li>
<li>the relative frequencies of all simple events in <span class="math inline">\(\Omega\)</span> must sum to 1; and</li>
<li>the relative frequency of a compound event must equal the sum of the
relative frequencies of its component simple events.</li>
</ol>
<p>These statements appear to be self-evident, and as thus may be dubbed
<em>axiomatic</em>. (A mathematical axiom is a statement accepted without proof.)
In probability theory, these statements were recast as the so-called
Kolmogorov axioms, introduced by Andrey Kolmogorov in 1933. Let <span class="math inline">\(A\)</span> denote an
event within <span class="math inline">\(\Omega\)</span> (i.e., <span class="math inline">\(A \subset S\)</span>), either simple or compound.
A <em>probability measure</em> on <span class="math inline">\(\Omega\)</span> is a function <span class="math inline">\(P\)</span> from subsets of
<span class="math inline">\(\Omega\)</span> to <span class="math inline">\(\mathbb{R}^n\)</span> that satisfies the following:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(A) \in [0,1]\)</span>;</li>
<li><span class="math inline">\(P(\Omega) = 1\)</span>; and</li>
<li>if <span class="math inline">\(\{B_1,\ldots,B_k\}\)</span> is a set of mutually exclusive simple or
compound events, then <span class="math inline">\(P(\bigcup_{i=1}^k B_i) = \sum_{i=1}^k P(B_i)\)</span>,
where the symbol <span class="math inline">\(\bigcup\)</span> refers to the <em>union</em> of the set of events,
i.e., the combination of all the events in the set into a single compound event.</li>
</ol>
<hr />
<div id="utilizing-set-notation" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Utilizing Set Notation<a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we have in our hand one six-sided die, with
faces numbered 1 through 6. We roll it once, and observe the value
on the uppermost face. The sample space is
<span class="math display">\[
\Omega = \{1,2,3,4,5,6\} \,.
\]</span>
Let the event <span class="math inline">\(A\)</span> be all odd-numbered outcomes, and let the event
<span class="math inline">\(B\)</span> be all outcomes less than 4. Thus <span class="math inline">\(A = \{1,3,5\}\)</span> and <span class="math inline">\(B = \{1,2,3\}\)</span>.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is <span class="math inline">\(\bar{A}\)</span>, the complement of <span class="math inline">\(A\)</span>? It is the set of all outcomes not
in <span class="math inline">\(A\)</span>, i.e., the set of all even-numbered faces: <span class="math inline">\(\bar{A} = \{2,4,6\}\)</span>.</li>
<li>What is <span class="math inline">\(A \cup B\)</span>, the union of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>? It is the set of all
outcomes observed in either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, without double counting:
<span class="math inline">\(A \cup B = \{1,2,3,5\}\)</span> (and not <span class="math inline">\(A \cup B = \{1,1,2,3,3,5\}\)</span>it is
meaningless to write out the same experimental outcome twice).</li>
<li>What is <span class="math inline">\(A \cap B\)</span>, the intersection of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>? It is the set of all
outcomes observed in both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, again without double counting:
<span class="math inline">\(A \cap B = \{1,3\}\)</span>.</li>
</ol>
</blockquote>
<blockquote>
<p>We note that we can combine unions, intersections, and complements in, e.g.,
the <em>distributive law</em>,
<span class="math display">\[
A \cap (B \cup C) = (A \cap B) \cup (A \cap C) \,,
\]</span>
the <em>associative law</em>,
<span class="math display">\[
A \cup (B \cap C) = (A \cup B) \cap (A \cup C) \,,
\]</span>
and <em>De Morgans laws</em>,
<span class="math display">\[
\overline{A \cup B} = \bar{A} \cap \bar{B} ~~\mbox{and}~~ \overline{A \cap B} = \bar{A} \cup \bar{B} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="working-with-contingency-tables" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Working With Contingency Tables<a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we are given the following information about two events
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="center"><span class="math inline">\(A\)</span></th>
<th align="center"><span class="math inline">\(\bar{A}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(B\)</span></td>
<td align="center">0.45</td>
<td align="center">0.12</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\bar{B}\)</span></td>
<td align="center">0.21</td>
<td align="center">0.22</td>
</tr>
</tbody>
</table>
<blockquote>
<p>This is dubbed a <em>contingency table</em> (or, more specifically here,
a two-by-two contigency table). The numbers in each cell represent
probabilities; for instance, <span class="math inline">\(P(A \cap B) = 0.45\)</span>. A contingency table
is appropriate to work with if the probabilities associated with each
event do not change from experiment to experiment.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(A)\)</span>? We can determine this by summing down the <span class="math inline">\(A\)</span> column:
<span class="math inline">\(P(A) = P[A \cap \Omega] = P[A \cap (B \cup \bar{B})] =
P[(A \cap B) \cup (A \cap \bar{B})]\)</span>; since <span class="math inline">\(A \cap B\)</span> and <span class="math inline">\(A \cap \bar{B}\)</span>
are disjoint, we can view the <span class="math inline">\(\cup\)</span> as addition, and so
<span class="math inline">\(P(A) = P(A \cap B) + P(A \cap \bar{B}) = 0.45 + 0.21 = 0.66\)</span>.</li>
<li>What is <span class="math inline">\(P(A \cup B)\)</span>? Utilizing De Morgans laws, this would be
<span class="math inline">\(1 - P(\overline{A \cup B}) = 1 - P(\bar{A} \cap \bar{B}) = 1 - 0.22 = 0.78\)</span>.</li>
<li>What is the probability of observing <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, but not both? This
would be <span class="math inline">\(P(A \cup B) - P(A \cap B)\)</span>, which is <span class="math inline">\(0.78 - 0.45 = 0.33\)</span>.</li>
</ol>
</blockquote>
<blockquote>
<p>It is well worth taking the time to see how one could derive each of these
answers through visual inspection of the table. For instance, <span class="math inline">\(P(A \cup B)\)</span>
is 1 minus the value in the cell at lower right, which does not lie in
the row for <span class="math inline">\(B\)</span> or the column for <span class="math inline">\(A\)</span>.</p>
</blockquote>
</div>
</div>
<div id="conditional-probability-and-the-independence-of-events" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Conditional Probability and the Independence of Events<a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Intuitively, we can picture a sample space <span class="math inline">\(\Omega\)</span> and two of its constituent events as looking something like what we show in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:nondj">1.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nondj"></span>
<img src="figures/conditional_event.png" alt="\label{fig:nondj}A sample space with non-disjoint events $A$ and $B$." width="27.5%" />
<p class="caption">
Figure 1.3: A sample space with non-disjoint events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.
</p>
</div>
<p>We can imagine that the geometric areas of each region
represent probability, with <span class="math inline">\(P(\Omega) = 1\)</span>
(given the second Kolmogorov axiom) and <span class="math inline">\(P(A \cap B) &gt; 0\)</span> being the probability
that both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur during an experiment.
(Perhaps <span class="math inline">\(A\)</span> is the event of speaking French and <span class="math inline">\(B\)</span> is the event of living
is Brussels. The symbol <span class="math inline">\(\cap\)</span> denotes the intersection or overlap between two sets, whereas the
analogous symbol <span class="math inline">\(\cup\)</span> represents the union of two sets.)</p>
<p>We can use this intuitive picture to illustrate the concept of
<em>conditional probability</em>. A stated event probability, such
as <span class="math inline">\(P(A)\)</span>, is an <em>unconditional probability</em>: its occurrence does not
depend on whether or not other events occur. To denote
a conditional probability, we add a vertical bar and place the
conditions to the right of it. For instance, <span class="math inline">\(P(A \vert B)\)</span>
denotes the probability that the event <span class="math inline">\(A\)</span> is observed,
<em>given</em> that the event <span class="math inline">\(B\)</span> is observed. (Note that there is no
implied causality: it is not necessarily the case that <span class="math inline">\(B\)</span>
occurring is causing changes to the probability that <span class="math inline">\(A\)</span> will
occur.) To illustrate why <span class="math inline">\(P(A)\)</span> may not equal <span class="math inline">\(P(A \vert B)\)</span>,
we first point out that <span class="math inline">\(P(A) = P(A \vert \Omega)\)</span>, which
we may think of as the probability of observing the event <span class="math inline">\(A\)</span>
if we observe the event <span class="math inline">\(\Omega\)</span>, which is the ratio of
geometric areas of <span class="math inline">\(A \cap \Omega\)</span> and <span class="math inline">\(\Omega\)</span>:
<span class="math display">\[
P(A) = P(A \vert \Omega) = \frac{P(A \cap \Omega)}{P(\Omega)} \,.
\]</span>
When we condition on the event <span class="math inline">\(B\)</span>, we are reducing the set of
possible outcomes from the full sample space <span class="math inline">\(\Omega\)</span> to
<span class="math inline">\(B\)</span>, i.e., we are replacing <span class="math inline">\(\Omega\)</span> in the expression above with <span class="math inline">\(B\)</span>:
<span class="math display">\[
P(A \vert B) = \frac{P(A \cap B)}{P(B)} \,,
\]</span></p>
<p>In the context of our intuitive picture, we are changing the one
shown above to the one we show in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sscond">1.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sscond"></span>
<img src="figures/conditional_event_ii.png" alt="\label{fig:sscond}The new sample space that arises when we condition on the event $B$." width="15%" />
<p class="caption">
Figure 1.4: The new sample space that arises when we condition on the event <span class="math inline">\(B\)</span>.
</p>
</div>
<p><span class="math inline">\(B\)</span> thus defines a new sample space.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if the probability of observing one does not depend on the probability of observing the other. The intuitive picture
many have of independence is the one shown in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:dj">1.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dj"></span>
<img src="figures/disjoint_event.png" alt="\label{fig:dj}To many, $A$ and $B$ appear to be independent events...but they are simply disjoint." width="27.5%" />
<p class="caption">
Figure 1.5: To many, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> appear to be independent eventsbut they are simply disjoint.
</p>
</div>
<p>The events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> do not overlaphence they are independent events, right? No: they are simply disjoint events. Also, with reflection, we realize that if, e.g.,
the event <span class="math inline">\(A\)</span> is observed in a given experiment,
then we know that <span class="math inline">\(B\)</span> <em>cannot be observed</em>.
So these events are very much dependent!
Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:ind">1.6</a> shows how we can actually represent <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as independent events.
In this figure, the ratio of the geometric area associated with the event <span class="math inline">\(A\)</span> to the geometric area of <span class="math inline">\(\Omega\)</span> is equal to the ratio of the areas of <span class="math inline">\(A \cap B\)</span> and <span class="math inline">\(B\)</span>. Thus we can write that
<span class="math display">\[
P(A) = P(A \vert \Omega) = \frac{P(A \cap \Omega)}{P(\Omega)} = \frac{P(A \cap B)}{P(B)} = P(A \vert B) \,.
\]</span>
The probability of observing the event <span class="math inline">\(A\)</span> is unchanged if the event <span class="math inline">\(B\)</span> occurs: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent events.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ind"></span>
<img src="figures/independent_event.png" alt="\label{fig:ind}$A$ and $B$ are independent events." width="27.5%" />
<p class="caption">
Figure 1.6: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent events.
</p>
</div>
<hr />
<div id="visualizing-conditional-probabilities-contingency-tables" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Visualizing Conditional Probabilities: Contingency Tables<a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets recall the two-by-two contingency table we defined in the previous
section, but with some additional information added:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="center"><span class="math inline">\(A\)</span></th>
<th align="center"><span class="math inline">\(\bar{A}\)</span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(B\)</span></td>
<td align="center">0.45</td>
<td align="center">0.12</td>
<td align="center">0.57</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\bar{B}\)</span></td>
<td align="center">0.21</td>
<td align="center">0.22</td>
<td align="center">0.43</td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="center">0.67</td>
<td align="center">0.33</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<blockquote>
<p>The numbers in the so-called margins are the row and column sums, so,
for instance, <span class="math inline">\(P(A) = 0.67\)</span>. This information is useful to have when
computing conditional probabilities. In analogy with what was stated above
about imposing conditions and what that does to the sample space, here
we can say that imposing a condition will restrict us to a given row or
a given column. For instance, what is <span class="math inline">\(P(\bar{A} \vert B)\)</span>? The condition
restricts us to the top row, and within that row, the probability of
observing the event <span class="math inline">\(\bar{A}\)</span> is 0.12/0.57 = 0.21. So
<span class="math inline">\(P(\bar{A} \vert B) = P(\bar{A} \cap B) / P(B) = 0.21\)</span>.</p>
</blockquote>
<blockquote>
<p>Now, are <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> independent events? The easy way to visually infer
this given a two-by-two table is to see if the rows (or columns)
are multiples of each othermeaning, here, is there a number <span class="math inline">\(a\)</span> such
that <span class="math inline">\(0.45 = 0.21 a\)</span> <em>and</em> <span class="math inline">\(0.12 = 0.22 a\)</span>? The answer here is noso
the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are dependent events. (The conventional, yet
longer way to determine independence is to see if, e.g.,
<span class="math inline">\(P(A \vert B) = P(A)\)</span>; if so, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent.)</p>
</blockquote>
<hr />
</div>
<div id="conditional-independence" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Conditional Independence<a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent events, is it automatically the case
that the events <span class="math inline">\(A \vert C\)</span> and <span class="math inline">\(B \vert C\)</span> are also independent events?</p>
</blockquote>
<blockquote>
<p>Recall the figure above that shows how independent events appear in
a Venn diagram. Recall also that if we impose a condition <span class="math inline">\(C\)</span>, we
effectively change the sample space from <span class="math inline">\(\Omega\)</span> to <span class="math inline">\(C\)</span>. Imagine <span class="math inline">\(C\)</span>
as an arbitrarily shaped region superimposed on the last figure, so
that now we have a situation like the one in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:condind">1.7</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:condind"></span>
<img src="figures/cond_ind_event.png" alt="\label{fig:condind}$A$ and $B$ are not necessarily independent events, given $C$." width="27.5%" />
<p class="caption">
Figure 1.7: <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not necessarily independent events, given <span class="math inline">\(C\)</span>.
</p>
</div>
<blockquote>
<p>The events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are conditionally independent given <span class="math inline">\(C\)</span> if
<span class="math inline">\(P(C) &gt; 0\)</span> and <span class="math inline">\(P(A \cap B \vert C) = P(A \vert C) P(B \vert C)\)</span>. As
we can see in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:condind">1.7</a>, <span class="math inline">\(C\)</span> can be made to overlap <span class="math inline">\(B\)</span>, <span class="math inline">\(A\)</span>, and
<span class="math inline">\(A \cap B\)</span> in any number of ways such that
<span class="math inline">\(P(A \cap B \vert C) \neq P(A \vert C) P(B \vert C)\)</span>so it is <em>not</em>
the case that if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, <span class="math inline">\(A \vert C\)</span> and <span class="math inline">\(B \vert C\)</span>
are always independent. (If <span class="math inline">\(C\)</span> had a rectangular shape with a horizontal
base and top and vertical sides,
conditional independence <em>would</em> hold. Think through
why this would be true)</p>
</blockquote>
</div>
</div>
<div id="further-laws-of-probability" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Further Laws of Probability<a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that we have learned about the concepts of conditional probabilities and independence, we can write down some useful laws that one can use to solve an array of probability-based problems.</p>
<ul>
<li><em>Multiplicative Law</em>. This follows simply from rearranging the definition of conditional probability:
<span class="math display">\[
P(A \cap B) = P(A) P(B \vert A) = P(B) P(A \vert B)
\]</span>
We can generalize this law given an arbitrary number of events <span class="math inline">\(k\)</span>:
<span class="math display">\[\begin{align*}
P(A_1 \cap A_2 \cap \cdots \cap A_k) &amp;= P(A_1 \vert A_2 \cap \cdots \cap A_k) P(A_2 \cap \cdots \cap A_k) = \cdots \\
&amp;= P(A_k) \prod_{i=1}^{k-1} P(A_i \vert A_{i+1} \cap \cdots \cap A_k) \,,
\end{align*}\]</span>
where <span class="math inline">\(\prod\)</span> is the product symbol, the multiplicative analogue to the summation symbol <span class="math inline">\(\sum\)</span>.</li>
<li><em>Additive Law</em>. The probability of the union of two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is
<span class="math display">\[
P(A \cup B) = P(A) + P(B) - P(A \cap B) \,.
\]</span>
If the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not disjoint, then if we add their probabilities, we count the probability of <span class="math inline">\(A \cap B\)</span> twicehence the subtracted term.</li>
<li><em>Law of Total Probability (LoTP)</em>. Assume that we partition the sample space <span class="math inline">\(\Omega\)</span> into <span class="math inline">\(k\)</span> disjoint (simple or compound) events <span class="math inline">\(\{B_1,\ldots,B_k\}\)</span>,
all of which have non-zero probability of occurring. Then, given any event <span class="math inline">\(A\)</span>, we can write
<span class="math display">\[
P(A) = \sum_{i=1}^k P(A \vert B_i) P(B_i) \,.
\]</span></li>
<li><em>Bayes Rule</em>. Continue to assume that the sample space is partitioned into the events <span class="math inline">\(\{B_1,\ldots,B_k\}\)</span>. The conditional probability of each
of these events, given that <span class="math inline">\(A\)</span> occurs, is
<span class="math display">\[
P(B_i \vert A) = \frac{P(A \vert B_i) P(B_i)}{\sum_{j=1}^k P(A \vert B_j)P(B_j)} = \frac{P(A \vert B_i)P(B_i)}{P(A)} \,.
\]</span></li>
</ul>
<hr />
<div id="the-additive-law-for-independent-events" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> The Additive Law for Independent Events<a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that for a given experiment, we can define the independent
events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, with <span class="math inline">\(P(A) = 0.6\)</span> and <span class="math inline">\(P(B) = 0.4\)</span>. What is
<span class="math inline">\(P(A \cup B)\)</span>?</p>
</blockquote>
<blockquote>
<p>In general, when solving probability problems, we look at all the rules
and relationships at our disposal and see which one (or more!) contains
the probabilities we know and the one we dont know, and we use that rule
or relationship to derive the solution. Here, there is nothing that
directly relates <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> to <span class="math inline">\(A \cup B\)</span>the events may overlap when
represented on a Venn diagram, and
we dont know by how much. Exceptwe are given the word independent.
That allows us to say that <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>and now we know that
the additive law is in play:
<span class="math display">\[\begin{align*}
P(A \cup B) &amp;= P(A) + P(B) - P(A \cap B) \\
&amp;= P(A) + P(B) - P(A \vert B)P(B) \\
&amp;= P(A) + P(B) - P(A)P(B) = 0.6 + 0.4 - 0.6 \cdot 0.4 = 0.76 \,.
\end{align*}\]</span>
Is this the only way to solve the problem? Nowe know from
De Morgans laws that <span class="math inline">\(\overline{A \cup B} = \bar{A} \cap \bar{B}\)</span>, and
thus
<span class="math display">\[\begin{align*}
P(A \cup B) &amp;= 1 - P(\overline{A \cup B}) = 1 - P(\bar{A} \cap \bar{B}) = 1 - P(\bar{A} \vert \bar{B})P(\bar{B}) \\
&amp;= 1 - P(\bar{A})P(\bar{B}) = 1 - (1-0.6)(1-0.4) = 0.76 \,.
\end{align*}\]</span>
There is no right way to solve a probability problemjust correct ones.</p>
</blockquote>
<hr />
</div>
<div id="the-monty-hall-problem" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> The Monty Hall Problem<a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p><em>Lets Make a Deal</em> is a game show that has appeared on television at
various times since 1963. During one part of the show, contestants are
brought on stage and presented with three closed doors; behind one is an
expensize prize (say, a car or an around-the-world cruise), and behind
the other two are inexpensive prizes (like a years supply of
Turtle Wax). The contestant is asked to pick a door (say, Door
#1), at which point the shows host will open another door (say, Door #3)
and show the inexpensive prize behind that door (thereby
taking that door out of play). The contestant is then asked if they
want to stick with the door theyve chosen (here, Door #1), or switch their
choice to the other unopened door (here, Door #2). What should we advise
the constestant to do?</p>
</blockquote>
<blockquote>
<p>The original, and most famous, host of <em>Lets Make a Deal</em> was Monty
Hall. Hence: the Monty Hall Problem. (Note that the problem is often
stated such that there is a car being behind one door and goats behind the
other two. The author is old enough to have seen the show in its heyday
and he recalls
seeing no goats. Or maybe they made no impression at the time)</p>
</blockquote>
<blockquote>
<p>Assume, without loss of generality, that Door #1 is chosen. Then, let</p>
</blockquote>
<blockquote>
<ul>
<li><span class="math inline">\(O_i\)</span> = Monty Hall opens Door #<span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(C_i\)</span> = The car is behind Door #<span class="math inline">\(i\)</span></li>
</ul>
</blockquote>
<blockquote>
<p>and assume that <span class="math inline">\(P(C_i) = 1/3\)</span> for all <span class="math inline">\(i\)</span>. (The car could have been placed
behind any door before the show was filmed.) The sample space
of experimental outcomes is
<span class="math display">\[
\Omega = \{ O_2 \cap C_1 , O_2 \cap C_3 , O_3 \cap C_1 , O_3 \cap C_2\} \,.
\]</span>
Why not <span class="math inline">\(O_2 \cap C_2\)</span> and <span class="math inline">\(O_3 \cap C_3\)</span>? Monty is not stupid: he wont
open the door the car is behind. (He knows where it is!)</p>
</blockquote>
<blockquote>
<p>Lets assume, again without any loss of generality, that Monty opens
Door #3. The probability we want to compute is <span class="math inline">\(P(C_2 \vert O_3)\)</span>: what
is the probability that the car is actually behind Door #2? (Note that
this is <span class="math inline">\(1 - P(C_1 \vert O_3)\)</span>again, <span class="math inline">\(P(C_3 \vert O_3) = 0\)</span>, as Monty
is not stupid.) Is this probability 1/2?
We utilize Bayes rule and the LoTP to write
<span class="math display">\[
P(C_2 \vert O_3) = \frac{P(O_3 \vert C_2) P(C_2)}{P(O_3)} =  \frac{P(O_3 \vert C_2) P(C_2)}{P(O_3 \vert C_2) P(C_2) + P(O_3 \vert C_1) P(C_1)} = \frac{P(O_3 \vert C_2)}{P(O_3 \vert C_2) + P(O_3 \vert C_1)}\,.
\]</span>
What do we know?</p>
</blockquote>
<blockquote>
<ul>
<li><span class="math inline">\(P(O_3 \vert C_2) = 1\)</span>: if the car is behind Door #2, Monty has to open Door #3</li>
<li><span class="math inline">\(P(O_3 \vert C_1) = 1/2\)</span>: Monty can open either Door #2 or #3 if the car is behind Door #1</li>
</ul>
</blockquote>
<blockquote>
<p>Hence
<span class="math display">\[
P(C_2 \vert O_3) = \frac{1}{1 + 1/2} = \frac{2}{3} \,.
\]</span>
<em>We should advise the contestant to open Door #2!</em></p>
</blockquote>
<blockquote>
<p>Confused? Think about the solution this way: the contestant has a one-third
chance of correctly picking the door the car is behind, and a two-thirds
chance of being wrong. Opening one of the other doors (while knowing there
is no car behind it) doesnt change these conditions at all: the
contestant still has a one-third chance of having initially picked
the correct door.
Thus the contestant should change their pick to the other unopened door.</p>
</blockquote>
<hr />
</div>
<div id="visualizing-conditional-probabilities-tree-diagrams" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Visualizing Conditional Probabilities: Tree Diagrams<a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the previous sections, we show how one can use contingency tables
to aid the visualization of probabilities (and to solve for probabilities
of simple and/or compound events). Here we show another, somewhat more
general probability visualizer: the <em>tree diagram</em>. Why somewhat more
general? First, a tree in a tree diagram can have arbitrary depth: if we have
events <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>, the table would be three-dimensional, with
the axes representing the experimental outcome in terms of <span class="math inline">\(A\)</span> and <span class="math inline">\(\bar{A}\)</span>,
<span class="math inline">\(B\)</span> and <span class="math inline">\(\bar{B}\)</span>, and <span class="math inline">\(C\)</span> and <span class="math inline">\(\bar{C}\)</span>. A table is not an optimal means
to represent probabilities. And second, a tree is arguably a more natural
means to represent probabilities when an experiment represents sequential
outcomes, particularly when we sample <em>without</em> replacement.</p>
</blockquote>
<blockquote>
<p>Lets elaborate on that second point. Lets say we have a drawer with
five socks, three of which are red and two of which are blue. We plan to draw
three socks in succession from the drawer without placing the socks back
into the drawer, but we will stop early if we draw two socks of the
same color on the first two draws.
What is the probability that our final sample contains two blue socks?</p>
</blockquote>
<blockquote>
<p>We can write out the following: if <span class="math inline">\(B_i\)</span> and <span class="math inline">\(R_i\)</span> are the probabilities
of drawing a blue and red sock from the drawer when taking out the
<span class="math inline">\(i^{\rm th}\)</span> sock, then <span class="math inline">\(P(B_1) = 2/5\)</span> and <span class="math inline">\(P(R_1) = 3/5\)</span>and
<span class="math inline">\(P(B_2 \vert B_1) = 1/4\)</span> because there is one less blue sock in the drawer,
and Actually, this gets tiring quickly. Lets use a tree diagram
instead.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tree"></span>
<img src="figures/decision_tree.png" alt="\label{fig:tree}An example of visualizing probabilities using a decision tree." width="65%" />
<p class="caption">
Figure 1.8: An example of visualizing probabilities using a decision tree.
</p>
</div>
<blockquote>
<p>In Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:tree">1.8</a>, we show the tree diagram for this problem. We
note some aspects of this diagram:</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>the tree can be truncated along some branches (here, thats because
we stop removing socks from the drawer if we remove two of the same color
in the first two draws);</li>
<li>at any branching point, the (conditional) probabilities of going
down each branch sum to one; and</li>
<li>the probability of ending up at a particular leaf (where the leaves
collectively represent the simple events of the experiment) is the
product of all the branch probabilities leading to that leaf.</li>
</ol>
</blockquote>
<blockquote>
<p>So, now, what is the probability of drawing two blue socks in this
experiment? To find that, we determine which leaves are associated
with drawing two blue socks; from the top, that would be leaves 1, 2,
and 4, with probabilities 1/10, 1/10, and 1/10. Because simple events
are disjoint by definition, the probability of the compound event is
simply the sum of the probabilities of the simple events, which here is 3/10.
In any given replication of this experiment, we have a 30% chance of
ending up with two blue socks.</p>
</blockquote>
</div>
</div>
<div id="random-variables" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Random Variables<a href="the-basics-of-probability-and-statistical-inference.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets say that we perform an experiment in which we flip a fair coin
three times. Let <span class="math inline">\(H\)</span> denote observing heads, and <span class="math inline">\(T\)</span> tails.
The sample space of outcomes <span class="math inline">\(\Omega\)</span> is
<span class="math display">\[
\{ HHH,HHT,HTH,THH,HTT,THT,TTH,TTT\}
\]</span>
and each outcome is observed with probability 1/8.
What is the probability of observing exactly one tail? We can determine this
by laboriously generating a table of probabilities, like so:
<span class="math display">\[\begin{align*}
P(\mbox{``no tails&#39;&#39;}) &amp;= P(HHH) = 1/8 \\
P(\mbox{``one tail&#39;&#39;}) &amp;= P(HHT \cup HTH \cup THH) = 3/8 \\
P(\mbox{``two tails&#39;&#39;}) &amp;= P(HTT \cup THT \cup TTH) = 3/8 \\
P(\mbox{``three tails&#39;&#39;}) &amp;= P(TTT) = 1/8 \,.
\end{align*}\]</span>
One can easily imagine how, if we were to flip a coin 50 times, or 500 times,
the generation of tables would be onerous. A better
way to portray the information in a sample space is to use a <em>random variable</em>.
In probability theory, a random variable <span class="math inline">\(X\)</span> is a
measurable function mapping from a set of outcomes (here, <span class="math inline">\(\Omega\)</span>) to a
measurable space (here, <span class="math inline">\(\mathbb{R}^n\)</span>, where
<span class="math inline">\(\mathbb{R}^1 = \mathbb{R}\)</span> is the real-number line).
(See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:rv">1.9</a>.)
While <span class="math inline">\(X\)</span> is a function, it is natural in an undergraduate context
to think of it as a variable whose
value is an experimental outcome. For instance, if we define <span class="math inline">\(X\)</span> as being
the number of tails observed in three flips of a
fair coin, then <span class="math inline">\(P(X=1) = 3/8\)</span>. (Below, we will complete our transition
away from laboriously built probability tables by
introducing mathematical functions<span class="math inline">\(-\)</span><em>probability mass functions</em> or
<em>probability density functions</em> associated with
<em>distributions</em><span class="math inline">\(-\)</span>that allow us to compute probabilities more generally,
as a function of an arbitrary observed value
<span class="math inline">\(X=x\)</span> or a range of observed values <span class="math inline">\(X \in [a,b]\)</span>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rv"></span>
<img src="figures/measurablespace.png" alt="\label{fig:rv}A random variable is a function that maps events in $\Omega$ to the real-number line $\mathbb{R}$." width="60%" />
<p class="caption">
Figure 1.9: A random variable is a function that maps events in <span class="math inline">\(\Omega\)</span> to the real-number line <span class="math inline">\(\mathbb{R}\)</span>.
</p>
</div>
<p>There are a few initial things to note about random variables.
First, they are conventionally denoted with capital Latin letters
(e.g., <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, <span class="math inline">\(Z\)</span>).
Second, note the words if we define above. There is no unique random
variable associated with a sample space. In addition to <span class="math inline">\(X\)</span>, we
could just as easily have defined <span class="math inline">\(Y\)</span> as the number of heads observed,
or <span class="math inline">\(Z\)</span> as having value 0 if at least one head and at
least one tail are observed, and 1 otherwise, etc. Third, and most
important, is that random variables come in two types,
<em>discrete</em> and <em>continuous</em>:</p>
<ul>
<li>A discrete random variable <span class="math inline">\(X\)</span> maps the sample space <span class="math inline">\(\Omega\)</span> to countably
finite (e.g., <span class="math inline">\(\{0,1\}\)</span>) or infinite (e.g., <span class="math inline">\(\{0,1,\ldots,\}\)</span>) outcomes.</li>
<li>A continuous random variable <span class="math inline">\(X\)</span> maps the sample space <span class="math inline">\(\Omega\)</span> to an
outcome that is uncountably infinite (e.g., <span class="math inline">\([0,1]\)</span> or <span class="math inline">\([0,\infty)\)</span>).</li>
</ul>
</div>
<div id="probability-distributions" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Probability Distributions<a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <em>probability distribution</em> is a mapping
<span class="math inline">\(P: \Omega \rightarrow \mathbb{R}^n\)</span> that describes how probabilities are
distributed across the values of a random variable.
(A random variable simply maps events in a sample space
to a measurable space like the real-number line,
without regard to the probability of the event. A distribution adds
this additional layer of information.) There are different ways to
mathematically define a distribution; here, we concentrate upon</p>
<ul>
<li>the <em>probability mass function</em> (or <em>pmf</em>): if <span class="math inline">\(X\)</span> is a discrete random
variable, this represents the probability that <span class="math inline">\(X\)</span> takes on a particular
value <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(p_X(x) = P(X = x)\)</span>; or</li>
<li>the <em>probability density function</em> (or <em>pdf</em>): if <span class="math inline">\(X\)</span> is a continuous
random variable, this represents the probability density (think of this
as the probability per unit interval) at the value <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(f_X(x)\)</span>.</li>
</ul>
<p>To be clear, we can represent a given distribution with a pmf or a pdf, but
not both simultaneously; the choice is dictated by whether <span class="math inline">\(X\)</span> is discretely
or continuously valued. (It is possible to mix probability masses and densities
into a single distribution, however. See the example below.)
Later, we introduce two alternatives to pmfs/pdfs:
the cumulative distribution function (cdf),
and the moment-generating function (mgf).</p>
<p>Probability mass and density functions have two fundamental constraints:
(a) they are non-negative; and (b) they sum or integrate to 1:</p>
<table>
<thead>
<tr class="header">
<th align="center">pmf</th>
<th align="center">pdf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_X(x) \in [0,1]\)</span></td>
<td align="center"><span class="math inline">\(f_X(x) \in [0,\infty)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\sum_x p_X(x) = 1\)</span></td>
<td align="center"><span class="math inline">\(\int_x f_X(x) dx = 1\)</span></td>
</tr>
</tbody>
</table>
<p>Before continuing on to discussing properties of distributions,
we reiterate the point that one cannot interpret
a pdf <span class="math inline">\(f_X(x)\)</span> as the probability of sampling the value <span class="math inline">\(x\)</span>! It is,
again, a probability <em>density</em> function
and not a probability itself; to determine a probability, we
utilize integration:
<span class="math display">\[
P(a \leq X \leq b) = \int_a^b f_X(x) dx \,.
\]</span>
To drive home the point that a pdf does not itself represent probability,
we note that for any value <span class="math inline">\(a\)</span>,
<span class="math display">\[
P(X = a) = \int_a^a f_X(x) dx = 0 \,.
\]</span></p>
<hr />
<div id="a-simple-probability-density-function" class="section level3 hasAnchor" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> A Simple Probability Density Function<a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have defined the following pdf:
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{cl} 2x &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
We visualize this pdf in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:twox">1.10</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twox"></span>
<img src="_main_files/figure-html/twox-1.png" alt="\label{fig:twox}The probability density function $f_X(x) = 2x$, for $0 \leq x \leq 1$." width="50%" />
<p class="caption">
Figure 1.10: The probability density function <span class="math inline">\(f_X(x) = 2x\)</span>, for <span class="math inline">\(0 \leq x \leq 1\)</span>.
</p>
</div>
<blockquote>
<p>This pdf helps to illustrate many of the points made above. Note that it is
(a) non-negative and although its maximum value is <span class="math inline">\(&gt; 1\)</span>,
(b) it integrates &gt; to 1.
(We need not actually integrate here, as geometry is sufficient: the
area under the curve is 1/2 <span class="math inline">\(\times\)</span> 1 <span class="math inline">\(\times\)</span> 2 = 1.)</p>
</blockquote>
<blockquote>
<p>How would one interpret this pdf? Where its value is larger, we
are more likely
to sample data. Full stop. What is the probability of sampling a datum between
0 and 1/2? Again, we can use geometry and see that the area under the curve
is 1/2 <span class="math inline">\(\times\)</span> 1/2 <span class="math inline">\(\times\)</span> 1 = 1/4. (Which means the probability of sampling
a datum between 1/2 and 1 must be <span class="math inline">\(1 - 1/4 = 3/4\)</span>.)</p>
</blockquote>
<blockquote>
<p>Lets extend this example a bit by adding a condition. For instance, what
is the probability of sampling a datum between 1/4 and 1/2, <em>given</em> that
we sample a datum between 0 and 3/4? In analogy with how we worked with
conditional probabilities above, we can write that
<span class="math display">\[
P(1/4 \leq X \leq 1/2 \, \vert \, 0 \leq X \leq 3/4) = \frac{P(1/4 \leq X \leq 1/2 \cap 0 \leq X \leq 3/4)}{P(0 \leq X \leq 3/4)} =  \frac{P(1/4 \leq X \leq 1/2)}{P(0 \leq X \leq 3/4)} \,.
\]</span>
(How does this differ from computing the unconditional probability
<span class="math inline">\(P(1/4 \leq X \leq 1/2)\)</span>? Technically, it does notwe could write out
a similar expression to the one above. But we note that the denominator
would be <span class="math inline">\(P(0 \leq X \leq 1) = 1\)</span> and thus it would go away.)
Using geometrical arguments, we should be able to convince ourselves that
the answer we seek is 1/3.</p>
</blockquote>
<blockquote>
<p>One last point we will make here is that for a continuous distribution,
it is meaningless to compute <span class="math inline">\(P(X = a)\)</span>. For instance:
<span class="math display">\[
P\left(X = \frac{1}{2}\right) = \int_{1/2}^{1/2} 2 x dx = \left. x^2 \right|_{1/2}^{1/2} = \frac{1}{4} - \frac{1}{4} = 0 \,.
\]</span>
What are we to make of this? Recall that a pdf is a probability <em>density</em>
function, and that one can think of it as having units of
probability per unit intervalso one has to integrate the pdf over an
interval of length greater than zero to derive a non-zero probability value.</p>
</blockquote>
<hr />
</div>
<div id="shape-parameters-and-families-of-distributions" class="section level3 hasAnchor" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Shape Parameters and Families of Distributions<a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the previous example,
the stated pdf was the stated pdf: there was no means by which to change
its shape. We can generalize it by utilizing a <em>shape parameter</em>:
<span class="math display">\[
f_X(x \vert \theta) = \left\{ \begin{array}{cl} \theta x^{\theta-1} &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
where in the previous example, <span class="math inline">\(\theta = 2\)</span>.
It is conventional to denote a population parameter or a
set of such parameters with the Greek letter <span class="math inline">\(\theta\)</span>
(theta, pronounced thay-tah).
Here, <span class="math inline">\(\theta\)</span> represents a single, constant parameter
whose value is <span class="math inline">\(&gt; 0\)</span>. (If <span class="math inline">\(\theta\)</span> were
negative, for instance, <span class="math inline">\(f_X(x \vert \theta)\)</span> would be <span class="math inline">\(&lt; 0\)</span>,
which is not allowed!) <span class="math inline">\(f_X(x \vert \theta)\)</span>, with
<span class="math inline">\(\theta \in \Theta = (0,\infty)\)</span>, is perhaps confusingly dubbed a <em>family of
distributions</em>. (One might think that a family would refer to a set of different
mathematical forms for pdfs, like <span class="math inline">\(\theta x^{\theta-1}\)</span> and
<span class="math inline">\(e^{-x/\theta}/\theta\)</span>,
etc., but it actually refers to the fact that <span class="math inline">\(\theta\)</span> can take on more than
one value, yielding a family of shapes as illustrated in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:family">1.11</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:family"></span>
<img src="_main_files/figure-html/family-1.png" alt="\label{fig:family}Examples of the family of pdfs $f_X(x \vert \theta) = \theta x^{\theta-1}$ for $0 \leq x \leq 1$, with parameters $\theta =$ 1/2 (solid red line), 1 (dashed green line), and 2 (dotted blue line)." width="50%" />
<p class="caption">
Figure 1.11: Examples of the family of pdfs <span class="math inline">\(f_X(x \vert \theta) = \theta x^{\theta-1}\)</span> for <span class="math inline">\(0 \leq x \leq 1\)</span>, with parameters <span class="math inline">\(\theta =\)</span> 1/2 (solid red line), 1 (dashed green line), and 2 (dotted blue line).
</p>
</div>
<hr />
</div>
<div id="a-simple-probability-mass-function" class="section level3 hasAnchor" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> A Simple Probability Mass Function<a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets play a game: we throw a dart at a board that has ten numbers on it,
1 through 10. Assume that we are guaranteed to hit the board, and that
the regions associated with each number have the exact same size. If we hit
an even number, we get 0 points, while if we hit an odd number, we get
2 points. Furthermore, if we hit a prime number, we get a bonus of 1 point.
What is the probability mass function for the number of points we will
score given a single throw of the dart?</p>
</blockquote>
<blockquote>
<p>If we hit the 4, 6, 8, or 10, we get 0 points. (2 is prime, so wed get
a bonus of 1 point by hitting that.) If we hit the 9, we get 2 points, and
if we hit the 1, 3, 5, or 7, we get 3 points. Hence the probability mass
function is</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(p_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">4/10</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">1/10</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">1/10</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">4/10</td>
</tr>
</tbody>
</table>
<blockquote>
<p>We see that this pmf is (a) non-negative and (b) has values <span class="math inline">\(p_X(x)\)</span> that
lie between 0 and 1. Because we are dealing with masses and not densities,
probability calculations involve summations (while taking care to note
whether one or both limits of summation lie at a mass, and if so, whether
or not the inequality is, e.g., <span class="math inline">\(&gt;\)</span> or <span class="math inline">\(\geq\)</span>). For instance, what is
the probability of achieving a score greater than 1 point? <span class="math inline">\(P(X &gt; 1) =
p_X(2) + p_X(3) = 1/2\)</span>. What about a score of 3 points, given a score greater
than 0 points?
<span class="math display">\[
P(X = 3 \vert X &gt; 0) = \frac{P(X = 3 \cap X &gt; 0)}{P(X &gt; 0)} = \frac{P(X = 3)}{P(X &gt; 0)} = \frac{p_X(3)}{p_X(1)+p_X(2)+p_X(3)} = \frac{4}{1+1+4} = \frac{2}{3} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="a-more-complex-example-involving-both-masses-and-densities" class="section level3 hasAnchor" number="1.6.4">
<h3><span class="header-section-number">1.6.4</span> A More Complex Example Involving Both Masses and Densities<a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>There is no reason why masses and densities cannot be combined into a
single probability distribution. For instance, perhaps we have the following:
<span class="math display">\[
h_X(x) = \left\{ \begin{array}{cc} 1/2 &amp; x \in [0,1] \\ 1/2 &amp; x = 2 \end{array} \right. \,.
\]</span>
There is nothing special about this function; the mathematics of probability
calculations is just a tad more complicated than before.
For instance, what is the probability of sampling a value greater than 3/4?
<span class="math display">\[
P(X &gt; 3/4) = \int_{3/4}^1 h_X(x) dx + h_X(2) = \frac{1}{2} \left. x \right|_{3/4}^1 + \frac{1}{2} = \frac{1}{8} + \frac{1}{2} = \frac{5}{8} \,.
\]</span>
Integrate over the domain(s) where densities are defined and sum over the
domain(s) where masses are defined. Done!</p>
</blockquote>
</div>
</div>
<div id="characterizing-probability-distributions" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Characterizing Probability Distributions<a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A probability distribution represents the rates of occurrence of different experimental outcomes. Can we determine an average outcome? In other words, can we determine what value
to expect when we next run the experiment? The answer is yes: this is the <em>expected value</em> of a random variable (or <em>expectation</em>) and it is the
weighted average of all possible experimental outcomes:
<span class="math display">\[\begin{align*}
E[X] &amp;= \frac{\sum_x x p_X(x)}{\sum_x p_X(x)} = \sum_x x p_X(x) ~~ \mbox{(discrete r.v.)} \\
     &amp;= \frac{\int_x x f_X(x) dx}{\int_x f_X(x) dx} = \int_x x f_X(x) dx ~~ \mbox{(continuous r.v.)} \,.
\end{align*}\]</span>
In each case, the denominator disappears because it equals 1, by definition. Note that Greek letter <span class="math inline">\(\mu\)</span> (mu, pronounced myoo), which conventionally denotes the mean value of a pdf or pmf, is also sometimes used interchangeably with <span class="math inline">\(E[X]\)</span>.
See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:exvx">1.12</a>.</p>
<p>It is important here to note the following:</p>
<ol style="list-style-type: decimal">
<li>The input to the expected value operator is (usually!) a random variable,
so that input is capitalized. In other words, we always write <span class="math inline">\(E[X]\)</span> and
not <span class="math inline">\(E[x]\)</span>. (<span class="math inline">\(x\)</span> is just a coordinate on the real-number lineits expected
value is simply <span class="math inline">\(x\)</span> itself. See Expected Value Tricks in the examples below.)</li>
<li><em>The expected value is a constant; it is not random!</em> For any given pmf or
pdf, the average value of a sampled datum does not change from experiment
to experimentthere is no randomness.</li>
</ol>
<p>Now, because the expected value is simply a weighted average,
we can write down a more general expression for it:
<span class="math display">\[\begin{align*}
E[g(X)] &amp;= \frac{\sum_x g(x)p_X(x)}{\sum_x p_X(x)} = \sum_x g(x) p_X(x) ~~ \mbox{(discrete r.v.)} \\
        &amp;= \frac{\int_x g(x)f_X(x) dx}{\int_x f_X(x) dx} = \int_x g(x) f_X(x) dx ~~ \mbox{(continuous r.v.)} \,.
\end{align*}\]</span>
This has been dubbed the Law of the Unconscious Statistician
(e.g., Ross 1988, as noted by Casella &amp; Berger 2002)
due to the fact that we all think of it a definitionand
not the result of a theorem.</p>
<p>A probability distribution may have an extended domain (e.g., <span class="math inline">\([0,\infty)\)</span>)
but often the probability mass or density is concentrated in a relatively
small interval. A metric that represents the square of the width of
that interval is the <em>variance</em>, which is defined as
<span class="math display">\[
V[X] = \sigma^2 = E[(X-\mu)^2] = E[X^2] - (E[X])^2 \,.
\]</span>
The width itself<span class="math inline">\(-\)</span>the square root of the variance<span class="math inline">\(-\)</span>is dubbed the
<em>standard deviation</em> and is denoted with the Greek letter <span class="math inline">\(\sigma\)</span> (sigma,
pronounced SIG-muh).
Note that because the variance is the expected value of a squared quantity,
it is always non-negative. (And like the expected value, it is a <em>constant</em>.)
See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:exvx">1.12</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:exvx"></span>
<img src="_main_files/figure-html/exvx-1.png" alt="\label{fig:exvx}Examples of a probability mass function (left) and a probability density function (right), with the expected values $E[X]$ indicated by the vertical lines and the distribution &quot;widths&quot;  (here, $E[X]-\sqrt{V[X]}$ to $E[X]+\sqrt{V[X]}$) indicated by the horizontal lines." width="45%" /><img src="_main_files/figure-html/exvx-2.png" alt="\label{fig:exvx}Examples of a probability mass function (left) and a probability density function (right), with the expected values $E[X]$ indicated by the vertical lines and the distribution &quot;widths&quot;  (here, $E[X]-\sqrt{V[X]}$ to $E[X]+\sqrt{V[X]}$) indicated by the horizontal lines." width="45%" />
<p class="caption">
Figure 1.12: Examples of a probability mass function (left) and a probability density function (right), with the expected values <span class="math inline">\(E[X]\)</span> indicated by the vertical lines and the distribution widths (here, <span class="math inline">\(E[X]-\sqrt{V[X]}\)</span> to <span class="math inline">\(E[X]+\sqrt{V[X]}\)</span>) indicated by the horizontal lines.
</p>
</div>
<p>Both the expected value and variance are examples of <em>moments</em> of
probability distributions. Moments represent elements of a distributions
location and shape. In the end, moments are just expected values
computed via the Law of the Unconscious Statistician, ones
that are defined around the coordinate origin (<span class="math inline">\(E[X^k]\)</span>),
and ones that are defined around the distributions
mean, <span class="math inline">\(\mu\)</span> (<span class="math inline">\(E[(X-\mu)^k]\)</span>).
Other metrics used to describe a probability distribution, such as its
<em>skewness</em>, are also related to moments. (One definition of skewness
is Fishers moment coefficient: <span class="math inline">\(E[(X-\mu)^3]/\sigma^3\)</span>.)</p>
<hr />
<div id="expected-value-tricks" class="section level3 hasAnchor" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> Expected Value Tricks<a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value operator <span class="math inline">\(E[X]\)</span> has the following properties.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If we multiply <span class="math inline">\(X\)</span> by a constant <span class="math inline">\(a\)</span>, that constant can be moved out
of the operator, i.e.,
<span class="math display">\[
E[aX] = aE[X] \,.
\]</span></li>
<li>The expected value of a constant is simply that constant, i.e.,
<span class="math display">\[
E[b] = b \,.
\]</span></li>
<li>The expected value operator is a linear operator, which means that we
can split it at <span class="math inline">\(+\)</span>s and <span class="math inline">\(-\)</span>s, with the sign being preserved:
<span class="math display">\[
E[aX - b] = E[aX] - E[b] = aE[X] - b \,.
\]</span></li>
</ol>
</blockquote>
<blockquote>
<p>If, for example, we define a random variable <span class="math inline">\(Y = 10X - 5\)</span> and we
know that <span class="math inline">\(E[X] = 4\)</span>, then we can write that <span class="math inline">\(E[Y] = 10E[X] - 5 = 35\)</span>.</p>
</blockquote>
<blockquote>
<p>Note that we have said nothing about <span class="math inline">\(E[XY]\)</span> here. In general, we cannot
simplify this expression at all, <em>unless</em> <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent
random variables (a concept we havent discussed yet), in which case
<span class="math inline">\(E[XY] = E[X]E[Y]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="variance-tricks" class="section level3 hasAnchor" number="1.7.2">
<h3><span class="header-section-number">1.7.2</span> Variance Tricks<a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The variance operator <span class="math inline">\(V[X]\)</span> has the following properties.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If we multiply <span class="math inline">\(X\)</span> by a constant <span class="math inline">\(a\)</span>, that constant can be moved out
of the operator, but it is then squared, i.e.,
<span class="math display">\[
V[aX] = a^2V[X] \,.
\]</span></li>
<li>The variance of a constant is zero:
<span class="math display">\[
V[b] = 0 \,.
\]</span></li>
<li>The variance operator is a linear operator, which means that we can
split it at <span class="math inline">\(+\)</span>s and <span class="math inline">\(-\)</span>s, with all signs becoming <em>positive</em>:
<span class="math display">\[
V[aX - b] = V[aX] + V[b] = a^2V[X] + 0 = a^2V[X] \,.
\]</span></li>
</ol>
</blockquote>
<p>If, again, <span class="math inline">\(Y = 10X - 5\)</span>, and if <span class="math inline">\(V[X] = 2\)</span>, then <span class="math inline">\(V[Y] = 100V[X] = 200\)</span>.</p>
<hr />
</div>
<div id="the-shortcut-formula-for-variance" class="section level3 hasAnchor" number="1.7.3">
<h3><span class="header-section-number">1.7.3</span> The Shortcut Formula for Variance<a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Above, we indicate that
<span class="math display">\[
V[X] = E[(X-\mu)^2] = E[X^2] - (E[X])^2 \,.
\]</span>
This is the so-called <em>shortcut formula</em> for determining the variance
of a distribution. We can derive it as follows, making use of the tricks
we show above:
<span class="math display">\[\begin{align*}
V[X] = E[(X-\mu)^2] &amp;= E[X^2 - 2X\mu + \mu^2] ~~\mbox{(expand)} \\
&amp;= E[X^2] - E[2X\mu] + E[\mu^2] ~~\mbox{(split on + and -)} \\
&amp;= E[X^2] - 2\mu E[X] + \mu^2 ~~\mbox{(slide constants out)}\\
&amp;= E[X^2] - 2(E[X])^2 + (E[X])^2 = E[X^2] - (E[X])^2 \,,
\end{align*}\]</span>
where in the last line we make use of the fact that <span class="math inline">\(E[X] = \mu\)</span>.
Note what this shortcut formula means: it means that to compute
a variance, it is sufficient to compute both <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span> and
combine the results. It also means that if we are given any
two of the quantities <span class="math inline">\(E[X]\)</span>, <span class="math inline">\(E[X^2]\)</span>, and <span class="math inline">\(V[X]\)</span>, we can immediately
derive the third one.</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-and-variance-of-a-probability-density-function" class="section level3 hasAnchor" number="1.7.4">
<h3><span class="header-section-number">1.7.4</span> The Expected Value and Variance of a Probability Density Function<a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the last section, we define the pdf
<span class="math display">\[
f_X(x \vert \theta) = \left\{ \begin{array}{cl} \theta x^{\theta-1} &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
For this pdf, the expected value is
<span class="math display">\[
E[X] = \int_0^1 x f_X(x) dx = \int_0^1 \theta x^\theta dx = \frac{\theta}{\theta+1} \left. x^{\theta+1} \right|_0^1 = \frac{\theta}{\theta+1} \,.
\]</span>
As for the variance, we utilize the shortcut formula, which means that we
compute <span class="math inline">\(E[X^2]\)</span> first:
<span class="math display">\[
E[X^2] = \int_0^1 x^2 f_X(x) dx = \int_0^1 \theta x^{\theta+1} dx = \frac{\theta}{\theta+2} \left. x^{\theta+2} \right|_0^1 = \frac{\theta}{\theta+2} \,.
\]</span>
Hence the variance is
<span class="math display">\[
V[X] = E[X^2] - (E[X])^2 = \frac{\theta}{\theta+2} - \frac{\theta^2}{(\theta+1)^2} = \frac{\theta}{(\theta+2)(\theta+1)^2} \,.
\]</span></p>
</blockquote>
<blockquote>
<p>We see that the value for our new pdf is similar: 0.643.</p>
</blockquote>
</div>
</div>
<div id="working-with-r-probability-distributions" class="section level2 hasAnchor" number="1.8">
<h2><span class="header-section-number">1.8</span> Working With R: Probability Distributions<a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we introduce <code>R</code> as a tool with which to, e.g.,
visualize and numerically manipulate probability distributions.</p>
<p>We start with the concept of the <em>vector</em>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb4-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Hello, world!&quot;</span>)</span></code></pre></div>
<p>(The reader should feel free to open <code>R</code> and type in these lines
at the Console prompt.)
In this example, we define a vector of character strings which we
name <code>x</code>; here, <code>x</code> has length 1:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb5-1" tabindex="-1"></a><span class="fu">length</span>(x)</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p><code>c()</code> is an <code>R</code> function whose arguments (e.g., <code>"Hello, world!"</code>) are
what are to be the constituents of the vector.
The arrow is an assignment operator; <code>=</code> is equally valid. We can create
a numeric vector as follows:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb7-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>)</span>
<span id="cb7-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb7-2" tabindex="-1"></a><span class="fu">print</span>(x)</span></code></pre></div>
<pre><code>## [1] 1 2 4 8</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb9-1" tabindex="-1"></a><span class="fu">length</span>(x)</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<p>but when the numbers follow a (long) sequence, it can be easier to utilize
<code>seq()</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb11-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,pi,<span class="at">by=</span><span class="fl">0.01</span>)  <span class="co"># 0, 0.01, 0.02, ..., 3.14 (but not 3.15)</span></span>
<span id="cb11-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb11-2" tabindex="-1"></a>                        <span class="co"># pi and Inf are built-in constants</span></span>
<span id="cb11-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb11-3" tabindex="-1"></a><span class="fu">length</span>(x)</span></code></pre></div>
<pre><code>## [1] 315</code></pre>
<p>When it comes to probability distributions, what might we want to do first?</p>
<p>Lets suppose that our data are sampled from this pdf:
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{cl} c x \sin x &amp; 0 \leq x \leq \pi \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
<span class="math inline">\(c\)</span> is a <em>normalization constant</em>, meaning it has some value (to be
determined) such that the integral of <span class="math inline">\(f_X(x)\)</span> from 0 to <span class="math inline">\(\pi\)</span> is 1.
Below, we will show how we can determine the value of <span class="math inline">\(c\)</span> using <code>R</code> code.
But first, we will assume <span class="math inline">\(c = 1\)</span> and
determine if <span class="math inline">\(f_X(x)\)</span> is non-negative (as it should be!):</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb13-1" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,pi,<span class="at">by=</span>pi<span class="sc">/</span><span class="dv">100</span>)</span>
<span id="cb13-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb13-2" tabindex="-1"></a>f.x <span class="ot">&lt;-</span> x<span class="sc">*</span><span class="fu">sin</span>(x)            </span>
<span id="cb13-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb13-3" tabindex="-1"></a><span class="fu">min</span>(f.x)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>Note how we do not have to use a <code>for</code>-loop here, as one of the hallmarks of
<code>R</code> is <em>vectorization</em>: if <code>R</code> sees that <code>x</code> is a vector, it will work with
the vector directly and thus <code>f.x</code> will itself be a vector with the same
length as <code>x</code> (and with the first element of <code>x</code> corresponding to the first
element of <code>f.x</code>, etc.). We see that the minimum value is 0.</p>
<p>If we want to go further, we can make a simple plot
(see Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:xsinx">1.13</a>):</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-1" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,pi,<span class="at">by=</span>pi<span class="sc">/</span><span class="dv">100</span>)</span>
<span id="cb15-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-2" tabindex="-1"></a>f.x <span class="ot">&lt;-</span> x<span class="sc">*</span><span class="fu">sin</span>(x)</span>
<span id="cb15-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-3" tabindex="-1"></a>df  <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb15-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x)) <span class="sc">+</span></span>
<span id="cb15-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-5" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb15-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-6" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb15-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(f[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb15-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb15-8" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xsinx"></span>
<img src="_main_files/figure-html/xsinx-1.png" alt="\label{fig:xsinx}The function $x \sin x$." width="50%" />
<p class="caption">
Figure 1.13: The function <span class="math inline">\(x \sin x\)</span>.
</p>
</div>
<p>The <code>ggplot()</code> function puts <code>x</code> on the <span class="math inline">\(x\)</span>-axis and <code>f.x</code> on the <span class="math inline">\(y\)</span>-axis.
We then connect each point with a line (<code>geom_line()</code>),
make the line blue (<code>col="blue"</code>),
overlay a horizontal red line at <span class="math inline">\(y = 0\)</span> (<code>geom_hline()</code>,
with <code>yintercept=0</code>),
and change the default <span class="math inline">\(y\)</span>-axis label to one that includes the subscript X
(<code>labs()</code>).</p>
<p>The next step is to determine the normalization constant. Lets suppose
that we have forgotten integration by parts and thus we are not sure
how to integrate <span class="math inline">\(f_X(x)\)</span>. We can code numerical integration in <code>R</code> using a
combination of a function that evaluates <span class="math inline">\(f_X(x)\)</span> and a call to the
built-in function <code>integrate()</code>, which performs numerical integration:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb16-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb16-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb16-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">*</span><span class="fu">sin</span>(x))</span>
<span id="cb16-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb16-3" tabindex="-1"></a>}</span>
<span id="cb16-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb16-4" tabindex="-1"></a><span class="fu">integrate</span>(f,<span class="dv">0</span>,pi) <span class="co"># integrate the function f between 0 and pi</span></span></code></pre></div>
<pre><code>## 3.141593 with absolute error &lt; 3.5e-14</code></pre>
<blockquote>
<p>We see that the integral is <span class="math inline">\(\pi\)</span>, so to make
the pdf valid, we have to set <span class="math inline">\(c\)</span> to <span class="math inline">\(1/\pi\)</span>:
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{cl} \frac{1}{\pi} x \sin x &amp; 0 \leq x \leq \pi \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span></p>
</blockquote>
<p>Lets suppose we sample data from this distribution.
What is the probability that the next observed datum will
have a value between 1 and 2? We can use <code>integrate</code> to figure that
out:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb18-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb18-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb18-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)     <span class="co"># we now include the normalization constant</span></span>
<span id="cb18-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb18-3" tabindex="-1"></a>}</span>
<span id="cb18-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb18-4" tabindex="-1"></a><span class="fu">integrate</span>(f,<span class="dv">1</span>,<span class="dv">2</span>) <span class="co"># integrate the function f between 1 and 2</span></span></code></pre></div>
<pre><code>## 0.4585007 with absolute error &lt; 5.1e-15</code></pre>
<p>The answer is 0.4585there is a 45.85% chance that the next datum will have
a value between 1 and 2.</p>
<p>What is the expected value, <span class="math inline">\(E[X]\)</span>, of <span class="math inline">\(f_X(x)\)</span>?</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb20-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb20-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb20-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)     <span class="co"># add an additional x</span></span>
<span id="cb20-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb20-3" tabindex="-1"></a>}</span>
<span id="cb20-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb20-4" tabindex="-1"></a><span class="fu">integrate</span>(f,<span class="dv">0</span>,pi)</span></code></pre></div>
<pre><code>## 1.868353 with absolute error &lt; 2.1e-14</code></pre>
<p>The expected value is 1.868. Given the appearance of the pdf,
this number makes sense.</p>
<hr />
<div id="numerical-integration-and-conditional-probability" class="section level3 hasAnchor" number="1.8.1">
<h3><span class="header-section-number">1.8.1</span> Numerical Integration and Conditional Probability<a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we would like to numerically evaluate
<span class="math display">\[
P(1 \leq X \leq 2 \vert X &gt; 0.5) = \frac{P(1 \leq X \leq 2 \cap X &gt; 0.5)}{P(X &gt; 0.5} = \frac{P(1 \leq X \leq 2)}{P(X &gt; 0.5} \,.
\]</span> As we have already defined
At first, it would appear that
all we have to do is to call <code>integrate()</code> twice</p>
</blockquote>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb22-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb22-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb22-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)</span>
<span id="cb22-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb22-3" tabindex="-1"></a>}</span>
<span id="cb22-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb22-4" tabindex="-1"></a><span class="fu">integrate</span>(f,<span class="dv">1</span>,<span class="dv">2</span>) <span class="sc">/</span> <span class="fu">integrate</span>(f,<span class="fl">0.5</span>,pi)</span></code></pre></div>
<blockquote>
<p>However, this will not work, since <code>integrate()</code> returns a <em>list</em>, not
a single numerical value. So we have
to figure out where the value of the integral value is stored:</p>
</blockquote>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb23-1" tabindex="-1"></a><span class="fu">names</span>(<span class="fu">integrate</span>(f,<span class="dv">1</span>,<span class="dv">2</span>))  <span class="co"># return the names of each list element</span></span></code></pre></div>
<pre><code>## [1] &quot;value&quot;        &quot;abs.error&quot;    &quot;subdivisions&quot; &quot;message&quot;      &quot;call&quot;</code></pre>
<blockquote>
<p>What we want is <code>value</code>. To reference the value directly, we use a dollar
sign, as shown here:</p>
</blockquote>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb25-1" tabindex="-1"></a><span class="fu">integrate</span>(f,<span class="dv">1</span>,<span class="dv">2</span>)<span class="sc">$</span>value <span class="sc">/</span> <span class="fu">integrate</span>(f,<span class="fl">0.5</span>,pi)<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.3836833</code></pre>
<blockquote>
<p>Done. Our conditional probability is 0.4645.</p>
</blockquote>
<hr />
</div>
<div id="numerical-integration-and-variance" class="section level3 hasAnchor" number="1.8.2">
<h3><span class="header-section-number">1.8.2</span> Numerical Integration and Variance<a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Above, we compute the expected value of <span class="math inline">\(f_X(x)\)</span>.
For the variance, we adapt the same code to compute <span class="math inline">\(E[X^2]\)</span>,
then utilize the shortcut formula:</p>
</blockquote>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb27-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)     <span class="co"># same code as above</span></span>
<span id="cb27-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-3" tabindex="-1"></a>}</span>
<span id="cb27-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-4" tabindex="-1"></a>E.X <span class="ot">&lt;-</span> <span class="fu">integrate</span>(f,<span class="dv">0</span>,pi)<span class="sc">$</span>value</span>
<span id="cb27-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-5" tabindex="-1"></a></span>
<span id="cb27-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-6" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb27-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-7" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">^</span><span class="dv">3</span><span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)     <span class="co"># add one more power of x</span></span>
<span id="cb27-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-8" tabindex="-1"></a>}</span>
<span id="cb27-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-9" tabindex="-1"></a>V.X <span class="ot">&lt;-</span> <span class="fu">integrate</span>(f,<span class="dv">0</span>,pi)<span class="sc">$</span>value <span class="sc">-</span> E.X<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb27-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb27-10" tabindex="-1"></a>V.X </span></code></pre></div>
<pre><code>## [1] 0.3788611</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb29-1" tabindex="-1"></a><span class="fu">sqrt</span>(V.X)</span></code></pre></div>
<pre><code>## [1] 0.6155169</code></pre>
<blockquote>
<p>The variance is 0.379 and the standard deviation is 0.616. We interpret
these numbers as saying that the majority of the observed data will lie
between <span class="math inline">\(1.868 - 0.616 = 1.252\)</span> and <span class="math inline">\(1.868 + 0.616 = 2.484\)</span>. If we recall
introductory statistics, the proportion
of values within one standard deviation of the mean for a normal distribution
(i.e., a bell curve) is 0.683but that value changes from distribution
to distribution. What is the value here?</p>
</blockquote>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb31-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb31-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb31-2" tabindex="-1"></a>  <span class="fu">return</span>(x<span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi)       <span class="co"># back to the original pdf</span></span>
<span id="cb31-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb31-3" tabindex="-1"></a>}</span>
<span id="cb31-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb31-4" tabindex="-1"></a><span class="fu">integrate</span>(f,E.X<span class="sc">-</span><span class="fu">sqrt</span>(V.X),E.X<span class="sc">+</span><span class="fu">sqrt</span>(V.X))<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.642609</code></pre>
<blockquote>
<p>We see that the value for our new pdf is similar: 0.643.</p>
</blockquote>
</div>
</div>
<div id="cumulative-distribution-functions" class="section level2 hasAnchor" number="1.9">
<h2><span class="header-section-number">1.9</span> Cumulative Distribution Functions<a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A cumulative distribution function (a cdf) is another means by which to
mathematically
express a probability distribution, which is to say, if we have a cdf, we
can derive the associated pmf/pdf and vice-versa.
A cdf is, in the discrete case, a sum of probability masses that lie to
the left of a chosen coordinate <span class="math inline">\(x\)</span> on the real-number line<span class="math inline">\(-\)</span>
<span class="math display">\[
F_X(x) = \sum_{y \leq x} p_Y(y)
\]</span>
<span class="math inline">\(-\)</span>while in the continuous case it is an integral of the probability density
that lies to the left of <span class="math inline">\(x\)</span><span class="math inline">\(-\)</span>
<span class="math display">\[
F_X(x) = \int_{y \leq x} f_Y(y) dy \,.
\]</span>
In both cases, we utilize a dummy variable for the pmf/pdf itself
because <span class="math inline">\(x\)</span> is the upper limit of summation/integration. See Figure
<a href="the-basics-of-probability-and-statistical-inference.html#fig:pdfcdf">1.14</a>,
which illustrates how a cdf collects all the probability masses or density
to the left of a given value of <span class="math inline">\(x\)</span>. Given this figure,
it should be clear that
<span class="math inline">\(F_X(-\infty) = 0\)</span> (there is nothing to collect to the left of <span class="math inline">\(-\infty\)</span>)
and <span class="math inline">\(F_X(\infty) = 1\)</span> (since, by the time we reach
<span class="math inline">\(x = \infty\)</span>, <em>all</em> masses or density have been collected).
Another thing to keep in mind is that even if a random variable is discrete, its
associated cdf <span class="math inline">\(F_X(x)\)</span> is continuously valued, because it is defined at all
values of <span class="math inline">\(x\)</span> (although it is technically not mathematically continuous
due to the steps that <span class="math inline">\(F_X(x)\)</span> takes at each value of <span class="math inline">\(x\)</span> where
there is a probability mass).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdfcdf"></span>
<img src="_main_files/figure-html/pdfcdf-1.png" alt="\label{fig:pdfcdf}Illustration of the relationship between a probability mass function (left) and a probability density function (right) and its associated cdf (evaluated here at $x = 2.5$). For the pmf, the cdf is the sum of the probability masses to the left of $x = 2.5$ (the masses marked in green), while for the pdf, the cdf is the integral over $x \in [0,2.5]$ (the area under curve shown in green)." width="45%" /><img src="_main_files/figure-html/pdfcdf-2.png" alt="\label{fig:pdfcdf}Illustration of the relationship between a probability mass function (left) and a probability density function (right) and its associated cdf (evaluated here at $x = 2.5$). For the pmf, the cdf is the sum of the probability masses to the left of $x = 2.5$ (the masses marked in green), while for the pdf, the cdf is the integral over $x \in [0,2.5]$ (the area under curve shown in green)." width="45%" />
<p class="caption">
Figure 1.14: Illustration of the relationship between a probability mass function (left) and a probability density function (right) and its associated cdf (evaluated here at <span class="math inline">\(x = 2.5\)</span>). For the pmf, the cdf is the sum of the probability masses to the left of <span class="math inline">\(x = 2.5\)</span> (the masses marked in green), while for the pdf, the cdf is the integral over <span class="math inline">\(x \in [0,2.5]\)</span> (the area under curve shown in green).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cdfs"></span>
<img src="_main_files/figure-html/cdfs-1.png" alt="\label{fig:cdfs}Examples of the cdfs $F_X(x)$ for the probability mass function (left) and the probability density function (right) shown in Figure \@ref(fig:pdfcdf)." width="45%" /><img src="_main_files/figure-html/cdfs-2.png" alt="\label{fig:cdfs}Examples of the cdfs $F_X(x)$ for the probability mass function (left) and the probability density function (right) shown in Figure \@ref(fig:pdfcdf)." width="45%" />
<p class="caption">
Figure 1.15: Examples of the cdfs <span class="math inline">\(F_X(x)\)</span> for the probability mass function (left) and the probability density function (right) shown in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:pdfcdf">1.14</a>.
</p>
</div>
<p>A cdf is useful to have when our goal is to compute the probability of
that the value of a sampled random variable lies between <span class="math inline">\(x = a\)</span> and <span class="math inline">\(x = b\)</span>.
For the case of a continuous random variable,
<span class="math display">\[
P(a &lt; X &lt; b) = F_X(b) - F_X(a) \,.
\]</span>
As we can see, if we have the cdf, we do not need to perform integration
to compute the probabilitywe just plug in coordinate values.
(Note that the form of the inequality, i.e., whether we have <span class="math inline">\(&lt;\)</span> or <span class="math inline">\(\leq\)</span>,
does not matter.)
However, when we are dealing with a discrete random variable, we need
to tread far more carefully, because
the form of the inequality can matter.
Lets suppose we have a pmf with masses given at
<span class="math inline">\(x = \{0,1\}\)</span>. Then, e.g.,
<span class="math display">\[\begin{align*}
P(0 \leq X \leq 1) &amp;= \sum_{x \in [0,1]} p_X(x) = p_X(0) + p_X(1) = F_X(1) \\
P(0 &lt; X \leq 1) &amp;= \sum_{x \in (0,1]} p_X(x) = p_X(1) = F_X(1) - F_X(0) \\
P(0 &lt; X &lt; 1) &amp;= \sum_{x \in (0,1)} p_X(x) = 0 \,.
\end{align*}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pmfcdf"></span>
<img src="_main_files/figure-html/pmfcdf-1.png" alt="\label{fig:pmfcdf}An illustration of the relationship between a cdf and probability. The probability $P(1 &lt; X &lt; 3)$ is given by the distance between the two red lines (i.e., $F_X(3)-F_X(1)$)." width="50%" />
<p class="caption">
Figure 1.16: An illustration of the relationship between a cdf and probability. The probability <span class="math inline">\(P(1 &lt; X &lt; 3)\)</span> is given by the distance between the two red lines (i.e., <span class="math inline">\(F_X(3)-F_X(1)\)</span>).
</p>
</div>
<p>We will make two final points here about cdfs.</p>
<p>First, as indicated above, given a cdf, we can find the associated pmf/pdf.
If a pmf has non-zero masses at values <span class="math inline">\(x - \Delta x\)</span> and <span class="math inline">\(x\)</span>, and
none in between, then
<span class="math display">\[
p_X(x) = F_X(x) - F_X(x-\Delta x) \,,
\]</span>
while in the continuous case,
<span class="math display">\[
f_X(x) = \frac{d}{dx}F_X(x) \,,
\]</span>
assuming <span class="math inline">\(F_X(x)\)</span> is differentiable at <span class="math inline">\(x\)</span>.</p>
<p>Second, we can define an inverse cumulative distribution function, or
inverse cdf. The inverse cdf takes as input the total probability collected
to the left of <span class="math inline">\(x\)</span> (e.g., the green region shown in the right panel of
Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:pdfcdf">1.14</a>) and
returns the associated value of <span class="math inline">\(x\)</span>. In other words,
if <span class="math inline">\(q = F_X(x)\)</span>, then <span class="math inline">\(x = F_X^{-1}(q)\)</span>.</p>
<p>One issue that arises with the inverse cdf is that if <span class="math inline">\(F_X(x)\)</span> is not
strictly monotonically increasing
(i.e., if for some range of values, <span class="math inline">\(\frac{d}{dx}F_X(x) = 0\)</span>) then
there is no unique inverse. For instance, see the left panel
of Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:cdfs">1.15</a>: if we input <span class="math inline">\(F_X(x) = 0.35\)</span>, then <span class="math inline">\(x \in [2,3)\)</span>. We
can circumvent this issue by utilizing the <em>generalized inverse cdf</em>
instead, for which
<span class="math display">\[
x = F_X^{-1}(q) = \mbox{inf}\{ x : F_X(x) \geq q \} \,.
\]</span>
The symbol inf indicates that we are finding the infimum, or smallest value,
of the indicated set of values.
Here, the output <span class="math inline">\(x\)</span> is the smallest value for which <span class="math inline">\(F_X(x) \geq q\)</span> holds.
For our given example, <span class="math inline">\(x = 2\)</span>. On the other hand, if we pick a value of
<span class="math inline">\(F_X(x)\)</span> that lies <em>between</em> the steps, we would choose the smallest <span class="math inline">\(x\)</span> value
associated with the next higher step. For instance, if for our example we
want the inverse cdf for <span class="math inline">\(F_X(x) = 0.5\)</span>, which lies between the steps at
0.35 and 0.6, we would take the smallest value of <span class="math inline">\(x\)</span> associated with
<span class="math inline">\(F_X(x) = 0.6\)</span>, which is <span class="math inline">\(x = 3\)</span>. (Note that <code>R</code> utilizes the generalized
form of the inverse cdf.)</p>
<hr />
<div id="the-cumulative-distribution-function-for-a-probability-density-function" class="section level3 hasAnchor" number="1.9.1">
<h3><span class="header-section-number">1.9.1</span> The Cumulative Distribution Function for a Probability Density Function<a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We work again with our simple parameterized pdf:
<span class="math display">\[
f_X(x \vert \theta) = \left\{ \begin{array}{cl} \theta x^{\theta-1} &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
The cdf for this function is simply the integral of the pdf to the left
of the coordinate <span class="math inline">\(x\)</span>:
<span class="math display">\[
F_X(x \vert \theta) = \int_0^x f_Y(y \vert \theta) dy \,.
\]</span>
Because the upper bound of the integral is <span class="math inline">\(x\)</span>, we replace <span class="math inline">\(x\)</span> in the
integrand with a dummy variable. (Here, <span class="math inline">\(y\)</span> was chosen arbitrarily.)
Thus
<span class="math display">\[
F_X(x \vert \theta) = \int_0^x \theta y^{\theta-1} dy = \left. y^\theta \right|_0^x = x^\theta \,.
\]</span>
We can answer a variety of questions given this cdf. For example</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is the median of this distribution?</li>
</ol>
<p>The median <span class="math inline">\(\tilde{x}\)</span> is the point on the real-number line where
<span class="math display">\[
P(X \leq \tilde{x}) = \frac{1}{2} \,.
\]</span>
For our distribution,
<span class="math display">\[
\tilde{x}^\theta = \frac{1}{2} ~\Rightarrow~ \tilde{x} = \left( \frac{1}{2} \right)^{1/\theta} \,.
\]</span></p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Now let <span class="math inline">\(\theta = 3\)</span>. What is the probability of sampling a datum between
<span class="math inline">\(x = 1/4\)</span> and <span class="math inline">\(x = 3/4\)</span>?
<span class="math display">\[
P\left(\frac{1}{4} \leq X \leq \frac{3}{4}\right) = F_X\left(\frac{3}{4} \vert \theta=3\right) - F_X\left(\frac{1}{4} \vert \theta=3\right) = \left(\frac{3}{4}\right)^3 - \left(\frac{1}{4}\right)^3 = \frac{26}{64} = \frac{13}{32} \,.
\]</span></li>
</ol>
</blockquote>
<hr />
</div>
<div id="visualizing-the-cumulative-distribution-function-in-r" class="section level3 hasAnchor" number="1.9.2">
<h3><span class="header-section-number">1.9.2</span> Visualizing the Cumulative Distribution Function in R<a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We continue with the pdf we use above, with <span class="math inline">\(\theta = 3\)</span>. To show the
region being integrated over to compute a cdf value, for say <span class="math inline">\(x = 0.6\)</span>,
we utilize <code>R</code>s <code>polygon()</code> function. (See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:cdfpoly">1.17</a>.)</p>
</blockquote>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-1" tabindex="-1"></a>x        <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb33-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-2" tabindex="-1"></a>f.x      <span class="ot">&lt;-</span> <span class="dv">3</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb33-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-3" tabindex="-1"></a>x.o      <span class="ot">&lt;-</span> <span class="fl">0.6</span></span>
<span id="cb33-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-4" tabindex="-1"></a></span>
<span id="cb33-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-5" tabindex="-1"></a>df       <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb33-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-6" tabindex="-1"></a>df.shade <span class="ot">&lt;-</span> <span class="fu">subset</span>(df,x<span class="sc">&lt;=</span>x.o)</span>
<span id="cb33-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-7" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x)) <span class="sc">+</span></span>
<span id="cb33-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-8" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb33-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-9" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data =</span> df.shade,<span class="fu">aes</span>(x,<span class="at">y=</span>f.x),<span class="at">fill=</span><span class="st">&quot;green&quot;</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">outline.type=</span><span class="st">&quot;full&quot;</span>) <span class="sc">+</span></span>
<span id="cb33-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-10" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>x.o,<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb33-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(f[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb33-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb33-12" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cdfpoly"></span>
<img src="_main_files/figure-html/cdfpoly-1.png" alt="\label{fig:cdfpoly}The cdf for $f_X(x) = 3x^2$ at $x = 0.6$ is the area represented in green." width="50%" />
<p class="caption">
Figure 1.17: The cdf for <span class="math inline">\(f_X(x) = 3x^2\)</span> at <span class="math inline">\(x = 0.6\)</span> is the area represented in green.
</p>
</div>
<blockquote>
<p>What is happening in this code chunk? We first define a sequence of
values for <code>x</code> (via <code>seq()</code>), then compute the pdf for each <code>x</code> value
(<code>f.x</code>). We then define a data frame with <code>x</code> and <code>f.x</code> as columns,
and determine which rows correspond
to values of <code>x</code> that are less than or equal to 0.6 (via <code>subset()</code>).
To create the polygon, we pass the subset data frame <code>df.shade</code> to
the function <code>geom_area()</code>.</p>
</blockquote>
<blockquote>
<p>If we wish to visualize the full cdf, we can do the following.
(See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:cdfcurve">1.18</a>.)</p>
</blockquote>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-1" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb34-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-2" tabindex="-1"></a>F.x <span class="ot">&lt;-</span> x<span class="sc">^</span><span class="dv">3</span></span>
<span id="cb34-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-3" tabindex="-1"></a>df  <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">F.x=</span>F.x)</span>
<span id="cb34-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-4" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>F.x)) <span class="sc">+</span></span>
<span id="cb34-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-5" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb34-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-6" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb34-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb34-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-8" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="sc">-</span><span class="dv">1</span>,<span class="at">xend=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">yend=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb34-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-9" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="dv">1</span>,<span class="at">xend=</span><span class="dv">2</span>,<span class="at">y=</span><span class="dv">1</span>,<span class="at">yend=</span><span class="dv">1</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb34-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(F[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb34-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb34-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cdfcurve"></span>
<img src="_main_files/figure-html/cdfcurve-1.png" alt="\label{fig:cdfcurve}The cdf for $f_X(x) = 3x^2$." width="50%" />
<p class="caption">
Figure 1.18: The cdf for <span class="math inline">\(f_X(x) = 3x^2\)</span>.
</p>
</div>
<hr />
</div>
<div id="the-cdf-for-a-mathematically-discontinuous-distribution" class="section level3 hasAnchor" number="1.9.3">
<h3><span class="header-section-number">1.9.3</span> The CDF for a Mathematically Discontinuous Distribution<a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume that we are handed the following pdf:
<span class="math display">\[
f_X(x \vert \theta) = \left\{ \begin{array}{cl} 1/2 &amp; 0 \leq x \leq 1 \\ 2-x &amp; 1 \leq x \leq 2 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
which we display in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:discon">1.19</a>.</p>
</blockquote>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-1" tabindex="-1"></a>df  <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.25</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="fl">2.25</span>),<span class="at">f.x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb35-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x)) <span class="sc">+</span></span>
<span id="cb35-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-3" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb35-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-4" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb35-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(f[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb35-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb35-6" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:discon"></span>
<img src="_main_files/figure-html/discon-1.png" alt="\label{fig:discon}A continuous probability density function that is mathematically discontinuous at $x=1$." width="50%" />
<p class="caption">
Figure 1.19: A continuous probability density function that is mathematically discontinuous at <span class="math inline">\(x=1\)</span>.
</p>
</div>
<blockquote>
<p>This is a completely valid, continuous pdf that has a mathematical
discontinuity at <span class="math inline">\(x = 1\)</span>. What is the cdf for this function?</p>
</blockquote>
<blockquote>
<p>The key insight is that we should not try to evaluate the integral of
<span class="math inline">\(f_X(x)\)</span> from 0 to <span class="math inline">\(x\)</span> when <span class="math inline">\(x &gt; 1\)</span> with a single integralthis will not
work! We simply have to break the problem up so as to define the cdf
over the domain [0,1), and then over the domain [1,2].
<span class="math display">\[\begin{align*}
F_X(x \vert x &lt; 1) &amp;= \int_0^x f_Y(y) dy = \frac{1}{2} \int_0^y dy = \frac{x}{2} \\
F_X(x \vert x \geq 1) &amp;= \int_0^1 f_Y(y) dy + \int_1^x f_Y(y) dy = \left. \frac{y}{2} \right|_0^1 + \int_1^x (2-y) dy = \frac{1}{2} - \left. \frac{(2-y)^2}{2} \right|_1^x \\
&amp;= \frac{1}{2} - \left( \frac{(2-x)^2}{2} - \frac{1}{2} \right) = 1 - \frac{(2-x)^2}{2} \,.
\end{align*}\]</span>
(Not sure if this is right? We can at the very least do sanity checking, as
we know <span class="math inline">\(F_X(1) = 1/2\)</span> and <span class="math inline">\(F_X(2) = 1\)</span>and our formula produces these
results! Alternatively, we can take the derivative of <span class="math inline">\(F_X(x)\)</span> and see if
it matches <span class="math inline">\(f_X(x)\)</span>.) We display the cdf in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:disconcdf">1.20</a>.</p>
</blockquote>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-1" tabindex="-1"></a>x.seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">1.01</span>,<span class="dv">2</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb36-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-2" tabindex="-1"></a>x     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,x.seq)</span>
<span id="cb36-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-3" tabindex="-1"></a>F.x   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span><span class="sc">-</span>(<span class="dv">2</span><span class="sc">-</span>x.seq)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb36-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-4" tabindex="-1"></a>df    <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">F.x=</span>F.x)</span>
<span id="cb36-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>F.x)) <span class="sc">+</span></span>
<span id="cb36-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-6" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb36-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb36-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-8" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb36-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-9" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="sc">-</span><span class="dv">1</span>,<span class="at">xend=</span><span class="dv">0</span>,<span class="at">y=</span><span class="dv">0</span>,<span class="at">yend=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb36-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-10" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="dv">2</span>,<span class="at">xend=</span><span class="dv">3</span>,<span class="at">y=</span><span class="dv">1</span>,<span class="at">yend=</span><span class="dv">1</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb36-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(F[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb36-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb36-12" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:disconcdf"></span>
<img src="_main_files/figure-html/disconcdf-1.png" alt="\label{fig:disconcdf}The cdf for our mathematically discontinuous pdf." width="50%" />
<p class="caption">
Figure 1.20: The cdf for our mathematically discontinuous pdf.
</p>
</div>
</div>
</div>
<div id="the-law-of-total-probability" class="section level2 hasAnchor" number="1.10">
<h2><span class="header-section-number">1.10</span> The Law of Total Probability<a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the laws of probability that we introduce earlier in this chapter
is the Law of Total Probability, or LoTP: if we partition a sample space
<span class="math inline">\(\Omega\)</span> into <span class="math inline">\(k\)</span> disjoint events <span class="math inline">\(\{B_1,\ldots,B_k\}\)</span>, then for any event
<span class="math inline">\(A\)</span> we can write
<span class="math display">\[
P(A) = \sum_{i=1}^k P(A \vert B_i) P(B_i) \,.
\]</span>
We are in a position now, having introduced random variables and
probability distributions, to update how we think of this law: it can
express the probability of a random variable <span class="math inline">\(X\)</span> when it is sampled from
a discrete distribution with parameter <span class="math inline">\(\theta\)</span>and when <span class="math inline">\(\theta\)</span> itself
is not a fixed constant (as it has been up until now),
but <em>is itself a discrete random variable</em>. To
see this, lets rewrite the LoTP given this description:
<span class="math display">\[
p_X(x) = \sum_\theta p_{X \vert \theta}(x \vert \theta) p_{\Theta}(\theta) \,.
\]</span>
This equation is saying that the probability mass associated with the
coordinate <span class="math inline">\(x\)</span> is the value of the mass for <span class="math inline">\(x\)</span>, given the value <span class="math inline">\(\theta\)</span>,
weighted by the probability that we would even observe the value
<span class="math inline">\(\theta\)</span> in the first place. Or, that <span class="math inline">\(p_X(x)\)</span> is a weighted average of
the values of
the conditional distribution <span class="math inline">\(p_{X \vert \theta}(x \vert \theta)\)</span>,
where the weights are given by <span class="math inline">\(p_{\Theta}(\theta)\)</span>.</p>
<p>What if <span class="math inline">\(\theta\)</span> is actually a continuous random variable? We can extend the
LoTP to handle that possibility by replacing the summation over a discrete
random variable with an integral over a continuous one:
<span class="math display">\[
p_X(x) = \int_\theta p_{X \vert \theta}(x \vert \theta) f_{\Theta}(\theta) d\theta \,.
\]</span>
And what if the distribution of <span class="math inline">\(X \vert \theta\)</span> is continuous? We would
just replace the <span class="math inline">\(p_X\)</span> and the <span class="math inline">\(p_{X \vert \theta}\)</span> in the equations above
with <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_{X \vert \theta}\)</span>, i.e., we would use the LoTP to define
a probability density instead of a probability mass.</p>
<hr />
<div id="the-lotp-with-two-simple-discrete-distributions" class="section level3 hasAnchor" number="1.10.1">
<h3><span class="header-section-number">1.10.1</span> The LoTP With Two Simple Discrete Distributions<a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where the probability
mass function for <span class="math inline">\(Y\)</span> is</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(y\)</span></th>
<th align="center"><span class="math inline">\(p_Y(y)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">2/3</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">1/3</td>
</tr>
</tbody>
</table>
<blockquote>
<p>and where, if <span class="math inline">\(Y = 0\)</span>, the pmf for <span class="math inline">\(X\)</span> is</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x \vert y=0\)</span></th>
<th align="center"><span class="math inline">\(p_{X \vert Y}(x \vert y=0)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">4/5</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">1/5</td>
</tr>
</tbody>
</table>
<blockquote>
<p>and if <span class="math inline">\(Y = 1\)</span> the pmf for <span class="math inline">\(X\)</span> is</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x \vert y=1\)</span></th>
<th align="center"><span class="math inline">\(p_{X \vert Y}(x \vert y=1)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">3/5</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">2/5</td>
</tr>
</tbody>
</table>
<blockquote>
<p>What is the pmf <span class="math inline">\(p_X(x)\)</span>?</p>
</blockquote>
<blockquote>
<p>The Law of Total Probability tells us that
<span class="math display">\[
p_X(x) = \sum_y p_{X \vert Y}(x \vert y) p_{Y}(y) \,,
\]</span>
so
<span class="math display">\[\begin{align*}
p_X(0) &amp;= p_{X \vert Y}(0 \vert 0) p_{Y}(0) + p_{X \vert Y}(0 \vert 1) p_{Y}(1) = \frac{4}{5} \cdot \frac{2}{3} + \frac{3}{5} \cdot \frac{1}{3} = \frac{11}{15} \\
p_X(1) &amp;= p_{X \vert Y}(1 \vert 0) p_{Y}(0) + p_{X \vert Y}(1 \vert 1) p_{Y}(1) = \frac{1}{5} \cdot \frac{2}{3} + \frac{2}{5} \cdot \frac{1}{3} = \frac{4}{15} \,.
\end{align*}\]</span>
The pmf is thus</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(p_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">11/15</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">4/15</td>
</tr>
</tbody>
</table>
<blockquote>
<p>The masses sum to 1, so indeed this is a proper pmf.</p>
</blockquote>
<hr />
</div>
<div id="the-law-of-total-expectation" class="section level3 hasAnchor" number="1.10.2">
<h3><span class="header-section-number">1.10.2</span> The Law of Total Expectation<a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>If we inspect the tables above, we see that, e.g.,
<span class="math display">\[\begin{align*}
E[X \vert Y=0] &amp;= 0 \cdot \frac{4}{5} + 1 \cdot \frac{1}{5} = \frac{1}{5} \,.
\end{align*}\]</span>
A similar calculation yields <span class="math inline">\(E[X \vert Y=1] = 2/5\)</span>. What then is the
expected value of <span class="math inline">\(X\)</span> itself?</p>
</blockquote>
<blockquote>
<p>A result related to the Law of Total Probability is the Law of Total
Expectation (LoTE), which states that when <span class="math inline">\(Y\)</span> is finite and countable,
<span class="math display">\[
E[X] = E[E[X \vert Y]] = \sum_y E[X \vert Y=y] ~ p_Y(y) \,,
\]</span>
i.e., the overall expected value is a weighted average of the
individual values <span class="math inline">\(E[X \vert Y=y]\)</span>. Here, the LoTE yields
<span class="math display">\[
E[X] = \frac{1}{5} \cdot \frac{2}{3} + \frac{2}{5} \cdot \frac{1}{3} = \frac{4}{15} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-lotp-with-two-continuous-distributions" class="section level3 hasAnchor" number="1.10.3">
<h3><span class="header-section-number">1.10.3</span> The LoTP With Two Continuous Distributions<a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we have two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(\theta\)</span>, such
that
<span class="math display">\[\begin{align*}
f_{X \vert \Theta}(x \vert \theta) &amp;= \theta \exp(-\theta x) \\
f_{\Theta}(\theta) &amp;= \exp(-\theta) \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,\infty)\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. What is <span class="math inline">\(f_X(x)\)</span>?</p>
</blockquote>
<blockquote>
<p>As mentioned above, the primary change to the LoTP would be that we use
integrate over all possible values of <span class="math inline">\(\theta\)</span>, rather than sum, so the
LoTP looks like this:
<span class="math display">\[
f_X(x) = \int_0^\infty f_{X \vert \Theta}(x \vert \theta) f_{\Theta}(\theta) d\theta \,.
\]</span>
Now that weve established this equation, the rest is mathexcept
as well see, we need to use integration by parts.
<span class="math display">\[\begin{align*}
f_X(x) &amp;= \int_0^\infty \theta \exp(-\theta x) \exp(-\theta) d\theta \\
&amp;= \int_0^\infty \theta \exp(-\theta (x+1)) d\theta \,.
\end{align*}\]</span>
We set up the integration as follows:
<span class="math display">\[\begin{align*}
u = \theta ~~~ &amp; ~~~ dv = \exp(-\theta (x+1)) d\theta \\
du = d\theta ~~~ &amp; ~~~ v = -\frac{1}{x+1}\exp(-\theta (x+1)) \,.
\end{align*}\]</span>
Then
<span class="math display">\[\begin{align*}
f_X(x) &amp;= \left.(u v)\right|_0^\infty - \int_0^\infty v du \\
&amp;= -\left.\frac{\theta}{x+1}\exp(-\theta (x+1))\right|_0^\infty + \int_0^\infty \frac{1}{x+1}\exp(-\theta (x+1)) d\theta \\
&amp;= 0 + \int_0^\infty \frac{1}{x+1}\exp(-\theta (x+1)) d\theta \,.
\end{align*}\]</span>
(We will stop here momentarily to remind the reader that when we evaluate
an expression of the form <span class="math inline">\(x e^{-x}\)</span>, the result as <span class="math inline">\(x \rightarrow \infty\)</span>
is zero
because <span class="math inline">\(e^{-x} \rightarrow 0\)</span> faster than <span class="math inline">\(x \rightarrow \infty\)</span>. We
now carry on)
<span class="math display">\[\begin{align*}
f_X(x) &amp;= \int_0^\infty \frac{1}{x+1}\exp(-\theta (x+1)) d\theta \\
&amp;= \left. -\frac{1}{(x+1)^2} \exp(-\theta (x+1)) \right|_0^\infty \\
&amp;= \frac{1}{(x+1)^2} \,,
\end{align*}\]</span>
for <span class="math inline">\(x \in [0,\infty)\)</span>. Done. We will leave it as an exercise to the reader
to confirm that <span class="math inline">\(f_X(x)\)</span> is a valid pdf that integrates to one.</p>
</blockquote>
<blockquote>
<p>Above, we say that we need to use integration by parts. This is not
quite true. A handy result that we will utilize as the book goes on
is that
<span class="math display">\[
\Gamma(t) = \int_0^\infty u^{t-1} \exp(-u) du \,.
\]</span>
This is the gamma function. (The symbol <span class="math inline">\(\Gamma\)</span> represents a capital gamma.)
One of the properties that makes
this function useful is that when <span class="math inline">\(x\)</span> is a non-negative integer,
the gamma function is related to the factorial function:
<span class="math inline">\(\Gamma(x) = (x-1)! = (x-1) (x-2) \cdots 1\)</span>. But the reason why the
gamma function is useful here is that we can use it to avoid integration
by parts.</p>
</blockquote>
<blockquote>
<p>Our integral is
<span class="math display">\[
f_X(x) = \int_0^\infty \frac{1}{x+1}\exp(-\theta (x+1)) d\theta \,.
\]</span>
To solve this, we implement <em>variable substitution</em>. The three steps
of variable substitution are</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>to write down a viable substitution <span class="math inline">\(u = g(\theta)\)</span>;</li>
<li>to then derive <span class="math inline">\(du = h(u,\theta) d\theta\)</span>; and finally</li>
<li>to use <span class="math inline">\(u = g(\theta)\)</span> to transform the bounds of the integral.</li>
</ol>
<blockquote>
<p>For our integral
<span class="math display">\[
(1) ~~ u = (x+1)\theta ~~ \implies ~~ (2) ~~ du = (x+1)d\theta
\]</span>
and
<span class="math display">\[
(3) ~~ \theta = 0 ~\implies~ u = 0 ~~~ \mbox{and} ~~~ \theta = \infty ~\implies~ u = \infty \,,
\]</span>
We see from point (3) that
making the variable substitution will not affect the bounds of the integral.
Thus we have that
<span class="math display">\[\begin{align*}
f_X(x) &amp;= \int_0^\infty \frac{1}{x+1}\exp(-u) \frac{du}{x+1} \\
f_X(x) &amp;= \frac{1}{(x+1)^2} \int_0^\infty u^0 \exp(-u) du \\
f_X(x) &amp;= \frac{1}{(x+1)^2} \Gamma(1) = \frac{1}{(x+1)^2} 0! = \frac{1}{(x+1)^2} \,.
\end{align*}\]</span>
(Here, we utilize the fact that zero factorial is one.)</p>
</blockquote>
</div>
</div>
<div id="working-with-r-data-sampling" class="section level2 hasAnchor" number="1.11">
<h2><span class="header-section-number">1.11</span> Working With R: Data Sampling<a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the primary uses of <code>R</code> is to perform <em>simulations</em> in which
we repeatedly create mock datasets and analyze them. But: how do we
create such datasets?</p>
<p>Below, we will describe two methods for randomly sampling data given
a probability distribution. The first, <em>rejection sampling</em>, is appropriate
to use when we cannot work with the cumulative distribution function of the
assumed distribution analytically (i.e., with pencil and paper).
As we will see, rejection sampling
is (relatively) computationally inefficient, but it does have the benefit
that we can apply it in just about any sampling situation. The second
method, <em>inverse transform sampling</em>, is efficient and should always be
our first choice when the cdf is tractable. To head off a question the
reader may have: no, we do not always have to hand-code samplers when working
in <code>R</code>for commonly used distributions, <code>R</code> supplies wrapper functions
that effectively abstract away the details of inverse transform sampling.
However, knowing how to code a sampler is a good skill to have!</p>
<hr />
<p>Lets suppose we are working with one of the pdfs that we define above:
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{cl} \frac{1}{\pi} x \sin x &amp; 0 \leq x \leq \pi \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
The cdf for this distribution is
<span class="math display">\[
F_X(x) = \frac{1}{\pi}\left( \sin x - x \cos x \right) \,,
\]</span>
which is not easily inverted.
Thus to sample data from this distribution,
we utilize the following algorithmic steps.</p>
<ol style="list-style-type: decimal">
<li>Determine the range of values over which we will sample
data values: <span class="math inline">\([x_{lo},x_{hi}]\)</span>. Nominally this will be the domain of
the distribution, but sometimes thats not viable, such as when the
domain is semi- or fully infinite. (Here, the range is easily specified:
<span class="math inline">\([0,\pi]\)</span>.)</li>
<li>Within <span class="math inline">\([x_{lo},x_{hi}]\)</span>, determine the maximum value of <span class="math inline">\(f_X(x)\)</span>.
(For our assumed distributione, this is not necessarily a simple
calculation, as the derivative of <span class="math inline">\(f_X(x)\)</span> is <span class="math inline">\((\sin x + x \cos x)/\pi\)</span>.
We can solve for the root using, e.g., <code>R</code>s
<code>uniroot()</code> function:</li>
</ol>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb37-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x)</span>
<span id="cb37-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb37-2" tabindex="-1"></a>{</span>
<span id="cb37-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb37-3" tabindex="-1"></a>  (<span class="fu">sin</span>(x) <span class="sc">+</span> x<span class="sc">*</span><span class="fu">cos</span>(x))<span class="sc">/</span>pi</span>
<span id="cb37-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb37-4" tabindex="-1"></a>}</span>
<span id="cb37-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb37-5" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.01</span>,pi))<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 2.028758</code></pre>
<p>This looks for the root of the given function within the stated interval;
since there is a root at 0, corresponding to a functional <em>minimum</em>, we
exclude that point by setting the interval lower bound to 0.01. <code>uniroot()</code>
is an extremely useful function and we <em>will</em> see it again throughout the
rest of this book. The root is <span class="math inline">\(x_{max} = 2.0288\)</span> and
<span class="math inline">\(f_X(x_{max}) = 0.5792\)</span>.)
3. We repeat the following steps until we reach our target
sample size <span class="math inline">\(n\)</span>: (a) sample
a random number <span class="math inline">\(u\)</span> assuming uniform weighting between <span class="math inline">\(x_{lo}\)</span> and <span class="math inline">\(x_{hi}\)</span>;
(b) sample another random number <span class="math inline">\(v\)</span> assuming uniform weighting
between 0 and <span class="math inline">\(f_X(x_{max})\)</span>;
and (c) keep <span class="math inline">\(u\)</span> as part of our sample if <span class="math inline">\(v \leq f_X(u)\)</span>.
(a) and (b) are summed up by the statement draw a rectangle whose vertices
are <span class="math inline">\((x_{lo},0)\)</span>, <span class="math inline">\((x_{hi},0)\)</span>, <span class="math inline">\(x_{hi},f_X(x_{max}))\)</span>, and
<span class="math inline">\(x_{lo},f_X(x_{max}))\)</span> and pick a random point inside the rectangle,
while (c) is summed up by saying keep the random point if it lies <em>below</em>
<span class="math inline">\(f_X(x)\)</span>.
Note that we will
assume that at the very least, we can use an <code>R</code> wrapper function to sample
a numbers with uniform weighting; without this assumption, we would have
to wade into the quagmire that is random number generation, which is
well beyond the scope of this book!</p>
<p>In a code chunk and in
Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:rejsamp">1.21</a> we show how we sample
<span class="math inline">\(n = 1000\)</span> data sampled from our distribution, and the final result.
(We dub the observed distribution the <em>empirical distribution</em> of the
data, where empirical simply means what we actually observe.)
Rejection sampling seems quick
and easyshould we always use it when we are not already provided a
sampling function for our pmf or pdf? No, not necessarily, because as noted
above it is computationally inefficient: we might have to sample <span class="math inline">\(m \gg n\)</span>
points in order to populate a sample of size <span class="math inline">\(n\)</span>.</p>
<p>(We will also note here that this is the first time that we are running
across the <code>R</code> function <code>set.seed()</code>. This initializes the underlying random
number generator such that we generate the same numerical results every time
we run the subsequent codewhich is useful when doing analyses that
we want to be <em>reproducible</em>.
If we leave out <code>set.seed()</code>, then every time we run the subsequent code, we
get a different data sample. The number that we pass to <code>set.seed()</code>
can be anythingwe adopt 101 here, but it can any real number.)</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb39-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-2" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb39-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-3" tabindex="-1"></a>x.lo     <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb39-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-4" tabindex="-1"></a>x.hi     <span class="ot">&lt;-</span> pi</span>
<span id="cb39-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-5" tabindex="-1"></a>f.x.hi   <span class="ot">&lt;-</span> <span class="fl">0.58</span> <span class="co"># rounding up is OK, it just decreases algorithm efficiency</span></span>
<span id="cb39-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-6" tabindex="-1"></a></span>
<span id="cb39-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-7" tabindex="-1"></a>X.sample <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,n)                <span class="co"># rejection sampling</span></span>
<span id="cb39-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-8" tabindex="-1"></a>ii       <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb39-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-9" tabindex="-1"></a><span class="cf">while</span> ( ii <span class="sc">&lt;</span> n ) {</span>
<span id="cb39-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-10" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>,<span class="at">min=</span>x.lo,<span class="at">max=</span>x.hi)</span>
<span id="cb39-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-11" tabindex="-1"></a>  v <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span>f.x.hi)</span>
<span id="cb39-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-12" tabindex="-1"></a>  <span class="cf">if</span> ( v <span class="sc">&lt;</span> u<span class="sc">*</span><span class="fu">sin</span>(u)<span class="sc">/</span>pi ) {</span>
<span id="cb39-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-13" tabindex="-1"></a>    ii           <span class="ot">&lt;-</span> ii<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb39-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-14" tabindex="-1"></a>    X.sample[ii] <span class="ot">&lt;-</span> u</span>
<span id="cb39-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-15" tabindex="-1"></a>  }</span>
<span id="cb39-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-16" tabindex="-1"></a>}</span>
<span id="cb39-17"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-17" tabindex="-1"></a></span>
<span id="cb39-18"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-18" tabindex="-1"></a>empirical.dist <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X.sample=</span>X.sample)</span>
<span id="cb39-19"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-19" tabindex="-1"></a>x              <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,pi,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb39-20"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-20" tabindex="-1"></a>f.x            <span class="ot">&lt;-</span> x<span class="sc">*</span><span class="fu">sin</span>(x)<span class="sc">/</span>pi</span>
<span id="cb39-21"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-21" tabindex="-1"></a>true.dist      <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb39-22"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-22" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>empirical.dist,<span class="fu">aes</span>(<span class="at">x=</span>X.sample)) <span class="sc">+</span></span>
<span id="cb39-23"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-23" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb39-24"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-24" tabindex="-1"></a>                 <span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="fl">3.2</span>,<span class="at">by=</span><span class="fl">0.2</span>)) <span class="sc">+</span></span>
<span id="cb39-25"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-25" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>true.dist,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb39-26"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-26" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;x&quot;</span>) <span class="sc">+</span></span>
<span id="cb39-27"><a href="the-basics-of-probability-and-statistical-inference.html#cb39-27" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rejsamp"></span>
<img src="_main_files/figure-html/rejsamp-1.png" alt="\label{fig:rejsamp}$n = 1000$ data sampled from the distribution $f_X(x) = (x \sin x)/\pi$ via the rejection sampling algorithm. We observe that our empirical distribution follows the true distribution well." width="50%" />
<p class="caption">
Figure 1.21: <span class="math inline">\(n = 1000\)</span> data sampled from the distribution <span class="math inline">\(f_X(x) = (x \sin x)/\pi\)</span> via the rejection sampling algorithm. We observe that our empirical distribution follows the true distribution well.
</p>
</div>
<hr />
<p>A primary alternative to rejection sampling is <em>inverse transform sampling</em>,
in which we utilize the inverse cdf function to generate appropriately
distributed data. Inverse transform sampling is efficient in that every
proposal point is kept.</p>
<p>Lets suppose we are working with the pdf:
<span class="math display">\[
f_X(x) = \theta x^{\theta-1} \,,
\]</span>
where <span class="math inline">\(\theta &gt; 0\)</span> and <span class="math inline">\(x \in [0,1]\)</span>. The inverse cdf, as derived
in an example above, is <span class="math inline">\(F_X^{-1}(q) = q^{1/\theta}\)</span>.
Inverse transform sampling utilizes the following algorithmic steps.</p>
<ol style="list-style-type: decimal">
<li>Pick the target sample size <span class="math inline">\(n\)</span>.</li>
<li>Sample <span class="math inline">\(n\)</span> data with uniform weighting between 0 and 1.
These are the cdf bounds. Call these data <span class="math inline">\(q\)</span>.</li>
<li>Transform the data <span class="math inline">\(q\)</span> to be <span class="math inline">\(x = F_X^{-1}(q)\)</span>.</li>
</ol>
<p>In a code chunk and in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:invsamp">1.22</a> we display our
inverse-transform sampling code as well as the empirical distribution of
<span class="math inline">\(n = 1000\)</span> data sampled from our distribution (assuming <span class="math inline">\(\theta = 3\)</span>). We
note that the code to generate our sample is <em>much</em> simpler than the
code needed to perform rejection sampling!</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb40-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-2" tabindex="-1"></a>theta    <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb40-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-3" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb40-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-4" tabindex="-1"></a>q        <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>)</span>
<span id="cb40-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-5" tabindex="-1"></a>X.sample <span class="ot">&lt;-</span> q<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>theta)</span>
<span id="cb40-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-6" tabindex="-1"></a></span>
<span id="cb40-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-7" tabindex="-1"></a>empirical.dist <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X.sample=</span>X.sample)</span>
<span id="cb40-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-8" tabindex="-1"></a>x              <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb40-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-9" tabindex="-1"></a>f.x            <span class="ot">&lt;-</span> theta<span class="sc">*</span>x<span class="sc">^</span>(theta<span class="dv">-1</span>)</span>
<span id="cb40-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-10" tabindex="-1"></a>true.dist      <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb40-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-11" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>empirical.dist,<span class="fu">aes</span>(<span class="at">x=</span>X.sample)) <span class="sc">+</span></span>
<span id="cb40-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-12" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb40-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-13" tabindex="-1"></a>                 <span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.1</span>)) <span class="sc">+</span></span>
<span id="cb40-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-14" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>true.dist,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb40-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-15" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;x&quot;</span>) <span class="sc">+</span></span>
<span id="cb40-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb40-16" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:invsamp"></span>
<img src="_main_files/figure-html/invsamp-1.png" alt="\label{fig:invsamp}$n = 1000$ data sampled from the distribution $f_X(x) = 3x^2$ via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well." width="50%" />
<p class="caption">
Figure 1.22: <span class="math inline">\(n = 1000\)</span> data sampled from the distribution <span class="math inline">\(f_X(x) = 3x^2\)</span> via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well.
</p>
</div>
<hr />
<div id="more-inverse-transform-sampling" class="section level3 hasAnchor" number="1.11.1">
<h3><span class="header-section-number">1.11.1</span> More Inverse-Transform Sampling<a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we are to sample <span class="math inline">\(n\)</span> data from the following distribution:
<span class="math display">\[
f_X(x) = 2(1-x) ~~~ x \in [0,1] \,.
\]</span>
Can we do this via inverse-transform sampling? The answer is yes, if (a)
we can derive the cdf <span class="math inline">\(F_X(x)\)</span>, and (b) we can invert it. Here,
<span class="math display">\[
F_X(x) = \int_0^x f_V(v) dv = \int_0^x 2(1-v) dv = \left. -(1-v)^2 \right|_0^x = -(1-x)^2 - (-1) = 1 - (1-x)^2 \,.
\]</span>
To invert the cdf, we set it equal to <span class="math inline">\(q\)</span> and solve for <span class="math inline">\(x\)</span>:
<span class="math display">\[\begin{align*}
q &amp;= 1 - (1-x)^2 \\
\Rightarrow ~~~ 1 - q &amp;= (1-x)^2 \\
\Rightarrow ~~~ \sqrt{1 - q} &amp;= 1-x \\
\Rightarrow ~~~ x &amp;= 1 - \sqrt{1 - q} \.,
\end{align*}\]</span>
To check for the correctness of our inversion, we utilize a code like
the one in the main body of the section above and compare our sampled
data against the pdf. See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:invsamp2">1.23</a>.</p>
</blockquote>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb41-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-2" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb41-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-3" tabindex="-1"></a>q        <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>)</span>
<span id="cb41-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-4" tabindex="-1"></a>X.sample <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">-</span>q)</span>
<span id="cb41-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-5" tabindex="-1"></a></span>
<span id="cb41-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-6" tabindex="-1"></a>empirical.dist <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X.sample=</span>X.sample)</span>
<span id="cb41-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-7" tabindex="-1"></a>x              <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb41-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-8" tabindex="-1"></a>f.x            <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>x)</span>
<span id="cb41-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-9" tabindex="-1"></a>true.dist      <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb41-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-10" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>empirical.dist,<span class="fu">aes</span>(<span class="at">x=</span>X.sample)) <span class="sc">+</span></span>
<span id="cb41-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-11" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb41-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-12" tabindex="-1"></a>                 <span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.1</span>)) <span class="sc">+</span></span>
<span id="cb41-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-13" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>true.dist,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb41-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-14" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;x&quot;</span>) <span class="sc">+</span></span>
<span id="cb41-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb41-15" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:invsamp2"></span>
<img src="_main_files/figure-html/invsamp2-1.png" alt="\label{fig:invsamp2}$n = 1000$ data sampled from the distribution $f_X(x) = 2(1-x)$ via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well." width="50%" />
<p class="caption">
Figure 1.23: <span class="math inline">\(n = 1000\)</span> data sampled from the distribution <span class="math inline">\(f_X(x) = 2(1-x)\)</span> via the inverse transform sampling algorithm. We observe that our empirical distribution follows the true distribution well.
</p>
</div>
</div>
</div>
<div id="statistics-and-sampling-distributions" class="section level2 hasAnchor" number="1.12">
<h2><span class="header-section-number">1.12</span> Statistics and Sampling Distributions<a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets say that we run an experiment in which we randomly sample many data from
some distribution <span class="math inline">\(P\)</span>:
<span class="math display">\[
\mathbf{X} = \{X_1,X_2,\ldots,X_n\} \overset{iid}{\sim} P \,.
\]</span>
The expected value for this distribution is <span class="math inline">\(E[X] = \mu\)</span>, while the
variance is <span class="math inline">\(V[X] = \sigma^2\)</span> (assumed to be finite).</p>
<p>So we have datanow what?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cycle2"></span>
<img src="figures/inference_cycle.png" alt="\label{fig:cycle2}The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, assume that the population can be represented by some family of distributions parameterized by $\theta$, and compute and use statistics to infer the value(s) of $\theta$." width="60%" />
<p class="caption">
Figure 1.24: The canonical experiment-and-infer cycle. We gather data sampled from an unknown population, assume that the population can be represented by some family of distributions parameterized by <span class="math inline">\(\theta\)</span>, and compute and use statistics to infer the value(s) of <span class="math inline">\(\theta\)</span>.
</p>
</div>
<p>The answer, typically, is that we would use these
data to <em>infer</em> the (unknown) properties of the population from which
they are drawn. A simple picture of the experiment-and-infer cycle is
given in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:cycle2">1.24</a>.
Notice how in this figure we use the term statistical inference.
(Fitting, as it is part of the name of this course!)
This is an appropriate term to use because we utilize statistics
when trying to infer the properties of the unknown underlying population,
like its true mean <span class="math inline">\(\mu\)</span>. But this motivates a next question</p>
<p><em>What is a statistic?</em></p>
<p>A statistic is simply a function of the data we observe. It can be any
function of the data<span class="math inline">\(-\)</span><span class="math inline">\(X_1\)</span>, sin<span class="math inline">\((X_1) + \pi X_2\)</span>, etc.<span class="math inline">\(-\)</span>and
it provides a useful means by which to summarize data (i.e., reduce <span class="math inline">\(n\)</span>
numbers to a single number). But it should
be intuitively obvious (we would hope!) that some statistics are going to be
more informative than others: for instance, if we are trying to infer what
<span class="math inline">\(\mu\)</span> might be, <span class="math inline">\(X_1\)</span> is probably going to be more useful to us than
sin<span class="math inline">\((X_1) + \pi X_2\)</span>. But it may not (nay, will not) be the most useful
quantity when the sample size <span class="math inline">\(n &gt; 1\)</span>. We could say that much of what we do
as statisticians is to pick appropriate (and optimal!) statistics to
perform inference.</p>
<p>What are some common statistics?</p>
<ol style="list-style-type: decimal">
<li>The <em>sample mean</em>:
<span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i \,.
\]</span>
This is always useful for inferring a population mean. Why it is better
than, e.g., <span class="math inline">\(X_1\)</span> when <span class="math inline">\(n &gt; 1\)</span>
will become more clear below. (Foreshadowing:
there are metrics we can compute that provide numerical assessments of the
usefulness of a statistic for performing inference.
We introduce some of these metrics in the next
section.)</li>
<li>The <em>sample variance</em>:
<span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_1 - \bar{X})^2 \,.
\]</span>
As we might guess, this one helps us infer population variances, and thus
helps us make sense of the width of the pmf or pdf from which the data
are sampled. The square root of <span class="math inline">\(S^2\)</span> is the <em>sample standard deviation</em>.
(One might ask: why <span class="math inline">\(n-1\)</span>? Metrics, againwe will return to this point
below when we discuss point estimation.)</li>
<li>The <em>sample range</em>:
<span class="math display">\[
R = X_{(n)} - X_{(1)} \,.
\]</span>
Here, we introduce a notational wrinkle: <span class="math inline">\(X_{(i)}\)</span> represents the <span class="math inline">\(i^{\rm th}\)</span>
<em>smallest</em> datum. So <span class="math inline">\(X_{(1)}\)</span> is the observed datum with the smallest value
(but not necessarily the first one to be recorded in our experiment),
and <span class="math inline">\(X_{(n)}\)</span> is the one with the largest value.
<span class="math inline">\(X_{(\cdot)}\)</span> is dubbed an <em>order statistic</em> and we will illustrate
its use as we go on, beginning in Chapter 3.</li>
<li>There are a myriad of others: the interquartile range, the median, etc.</li>
</ol>
<p>We have stated what a statistic is: it is a function of the observed data.
But what does this imply? <strong>It implies that statistics, which are functions
of random variables, are themselves random variables, and thus are sampled
from a pmf or pdf.</strong> It is convention to call the pmf or pdf associated
with a given statistic the <em>sampling distribution</em>, but we are not necessarily
fans of the term: it makes it sound like something new and different, when
in reality a sampling distribution is just another pmf or pdf, with properties
equivalent to those discussed earlier in the chapter (e.g., a sampling
distribution has an expected value, a variance, etc.).</p>
<p>As we will see in the first
example below, if the statistic is a linear function of random variables
(like the sample mean), we can derive its expected value, variance, and
<em>standard error</em> now given the tools we already have at our disposal.
The term standard error simply refers to the standard
deviation of a sampling distribution, i.e.,
<span class="math display">\[
se[Y] = \sqrt{V[Y]} \,,
\]</span>
where <span class="math inline">\(Y\)</span> is our statistic. Now, can we go beyond this and derive
the mathematical form of a statistics pmf or pdf now?
The short answer is nowe have not yet introduced methods for deriving
the functional forms of sampling distributions.</p>
<ul>
<li>In Chapter 2, we will introduce moment-generating
functions, which can help us derive sampling distributions for linear
functions of random variables (an example of which is the sample mean).</li>
<li>In Chapter 3, we will show how one can write down the pmf or pdf for order
statistics (like the sample median); once the sampling distribution is known,
then we can derive its expected value and variance.</li>
</ul>
<p>(As an aside, we should mention here the <em>empirical rule</em>. While nominally
about the normal distribution, we will think of it as stating that nearly
all statistics should be observed as laying within three standard errors
of their population means. For instance, as we show below in an example,
<span class="math inline">\(E[\bar{X}] = \mu\)</span>, so virtually all values of <span class="math inline">\(\bar{X}\)</span>, as observed over
repetitions of an experiment, should lie in the range
<span class="math inline">\([\mu - 3 \cdot se(\bar{X}),\mu + 3 \cdot se(\bar{X})]\)</span>, a range that
gets smaller and smaller as the sample size <span class="math inline">\(n\)</span> goes to infinity.)</p>
<p>There are, however, two paths that one could follow that allow us
to build up an empirical sampling distribution for a statistic.
The first assumes that we know (or are willing to assume) the pmf or pdf for
the individual data: we would repeatedly simulate data from the distribution
and record the values for the statistic. This builds off of the material
in the last section above. (See the example below, as well as the last
section of this chapter.)
The other is useful for situations where we do not know nor are willing
to assume the form of the pmf or pdf for the individual datathis is
the <em>bootstrap</em>. We discuss the bootstrap technique in Chapter 4.</p>
<hr />
<p><strong>Many important results in statistical inference assume that we have collected
a sample of <span class="math inline">\(n\)</span> iid random variables, so over the
course of the rest of this chapter and for the next several, we will assume
that when we have sampled two or more random variables, they will be iid random
variables.</strong> (We will discuss concepts related to simultaneously
sampling values for two or more <em>dependent</em> random variables in Chapter 6.)</p>
<hr />
<div id="expected-value-and-variance-of-the-sample-mean-for-all-distributions" class="section level3 hasAnchor" number="1.12.1">
<h3><span class="header-section-number">1.12.1</span> Expected Value and Variance of the Sample Mean (For All Distributions)<a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Given <span class="math inline">\(n\)</span> iid data from
some distribution, we can use the results from earlier in this chapter
to immediately show that
<span class="math display">\[
E[\bar{X}] = E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}E\left[\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n E[X_i] = \frac{1}{n}\sum_{i=1}^n \mu = \frac{1}{n} n \mu = \mu \,,
\]</span>
and
<span class="math display">\[
V[\bar{X}] = V\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n^2} V\left[\sum_{i=1}^n X_i\right] = \frac{1}{n^2} \sum_{i=1}^n V[X_i] = \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n} \,.
\]</span>
The standard error for the sample mean is thus
<span class="math inline">\(\sqrt{\bar{X}} = \sigma/\sqrt{n}\)</span>.
There are two important conclusions to take away from this simple example.
First, <em>we never state the distribution from which we sample the initial data, so this is a general result that holds for all distributions</em>.
Second, we see that the width of the
sampling distribution for the sample mean decreases as we collect
more and more
data, as <span class="math inline">\(1/\sqrt{n}\)</span>, meaning that any inferences that we make about the
population mean will become more and more accurate as the sample size
increases.</p>
</blockquote>
<hr />
</div>
<div id="visualizing-the-distribution-of-the-sample-mean" class="section level3 hasAnchor" number="1.12.2">
<h3><span class="header-section-number">1.12.2</span> Visualizing the Distribution of the Sample Mean<a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In an example in the last section above, we use inverse-transform
sampling to sample data from the pdf <span class="math inline">\(f_X(x) = 2(1-x)\)</span> for <span class="math inline">\(x \in [0,1]\)</span>.
Here, we extend our <code>R</code> code so as to visualize the distribution of
the sample mean of <span class="math inline">\(n = 10\)</span> data drawn from this distribution. We note
that some of the material below foreshadows that which we cover in the
last section of this chapter, when we discuss numerical simulation.</p>
</blockquote>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb42-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-2" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb42-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-3" tabindex="-1"></a></span>
<span id="cb42-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-4" tabindex="-1"></a>num.sim  <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb42-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-5" tabindex="-1"></a>X.bar    <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim) <span class="co"># set aside storage for X.bar</span></span>
<span id="cb42-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-6" tabindex="-1"></a>                            <span class="co"># NA == Not Available - this is overwritten</span></span>
<span id="cb42-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-7" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb42-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-8" tabindex="-1"></a>  q         <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>)</span>
<span id="cb42-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-9" tabindex="-1"></a>  X.sample  <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">-</span>q)</span>
<span id="cb42-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-10" tabindex="-1"></a>  X.bar[ii] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X.sample)</span>
<span id="cb42-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-11" tabindex="-1"></a>}</span>
<span id="cb42-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-12" tabindex="-1"></a></span>
<span id="cb42-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-13" tabindex="-1"></a>empirical.mean <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X.bar=</span>X.bar)</span>
<span id="cb42-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-14" tabindex="-1"></a>x              <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb42-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-15" tabindex="-1"></a>f.x            <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>x)</span>
<span id="cb42-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-16" tabindex="-1"></a>pdf            <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x)</span>
<span id="cb42-17"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-17" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>empirical.mean,<span class="fu">aes</span>(<span class="at">x=</span>X.bar)) <span class="sc">+</span></span>
<span id="cb42-18"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-18" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)),<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,</span>
<span id="cb42-19"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-19" tabindex="-1"></a>                 <span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.05</span>)) <span class="sc">+</span></span>
<span id="cb42-20"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-20" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>pdf,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb42-21"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-21" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="at">col=</span><span class="st">&quot;green&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb42-22"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-22" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span><span class="fl">-0.0745</span>,<span class="at">xend=</span><span class="dv">1</span><span class="sc">/</span><span class="dv">3</span><span class="fl">+0.0745</span>,<span class="at">y=</span><span class="fl">2.5</span>,<span class="at">yend=</span><span class="fl">2.5</span>,<span class="at">col=</span><span class="st">&quot;green&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb42-23"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-23" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;x&quot;</span>) <span class="sc">+</span></span>
<span id="cb42-24"><a href="the-basics-of-probability-and-statistical-inference.html#cb42-24" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampmean"></span>
<img src="_main_files/figure-html/sampmean-1.png" alt="\label{fig:sampmean} The empirical distribution of the sample mean of $n = 10$ data sampled from the distribution $f_X(x) = 2(1-x)$ for $x \in [0,1]$. The red line indicates $f_X(x)$, while the vertical and horizontal green lines indicate $E[\bar{X}] = \mu = 1/3$ and the range $[\mu-se(\bar{X}),\mu+se(\bar{X})] = [0.2588,0.4078]$. The shape of the empirical distribution is approaching that of a normal distribution, a result that we will discuss in Chapter 2 when introducing the Central Limit Theorem." width="50%" />
<p class="caption">
Figure 1.25:  The empirical distribution of the sample mean of <span class="math inline">\(n = 10\)</span> data sampled from the distribution <span class="math inline">\(f_X(x) = 2(1-x)\)</span> for <span class="math inline">\(x \in [0,1]\)</span>. The red line indicates <span class="math inline">\(f_X(x)\)</span>, while the vertical and horizontal green lines indicate <span class="math inline">\(E[\bar{X}] = \mu = 1/3\)</span> and the range <span class="math inline">\([\mu-se(\bar{X}),\mu+se(\bar{X})] = [0.2588,0.4078]\)</span>. The shape of the empirical distribution is approaching that of a normal distribution, a result that we will discuss in Chapter 2 when introducing the Central Limit Theorem.
</p>
</div>
<blockquote>
<p>See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sampmean">1.25</a>. The mean of our pdf is
<span class="math display">\[
E[X] = \int_0^1 2x(1-x) dx = \int_0^1 2xdx - \int_0^1 2x^2dx = \left. x^2 \right|_0^1 - \left. \frac{2}{3}x^3 \right|_0^1 = 1 - \frac{2}{3} = \frac{1}{3} \,.
\]</span>
This is indicated via the green vertical line in the figure.
The variance is
<span class="math display">\[\begin{align*}
V[X] &amp;= E[X^2] - (E[X])^2 \\
&amp;= \left[ \int_0^1 2x^2dx - \int_0^1 2x^3dx \right] - \left(\frac{1}{3}\right)^2 \\
&amp;= \left(\frac{2}{3} - \frac{1}{2}\right) - \frac{1}{9} \\
&amp;= \frac{1}{6} - \frac{1}{9} = \frac{1}{18}\\
\end{align*}\]</span>
The standard error for <span class="math inline">\(\bar{X}\)</span> is thus
<span class="math display">\[
se(\bar{X}) = \sqrt{\frac{V[X]}{n}} = \sqrt{\frac{1}{180}} = 0.0745 \,.
\]</span>
The range from the mean minus one standard error to the mean plus one
standard error is indicated via the green horizontal line segment in
the figure.</p>
</blockquote>
<blockquote>
<p>We observe that the distribution of the sample mean values matches the
expected mean and standard error well, and is <em>definitely different from</em>
the distribution of the individual data values (which is shown as the
red line in the figure). The sample mean distribution almost looks
like a normal distribution, but it isnt one exactlyand for now
we will have to content ourselves with only knowing the mean and
the standard error of the distribution, and not its mathematical details.
However, as well
see in Chapter 2, the sample mean distribution will look more and more
like a normal distribution as the sample size <span class="math inline">\(n\)</span> goes to infinity,
in a result dubbed the Central Limit Theorem.</p>
</blockquote>
</div>
</div>
<div id="the-likelihood-function" class="section level2 hasAnchor" number="1.13">
<h2><span class="header-section-number">1.13</span> The Likelihood Function<a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Assume we are given iid data <span class="math inline">\(\mathbf{X} = \{X_1,\ldots,X_n\}\)</span>,
with each datum sampled from a continuous distribution <span class="math inline">\(f_X(x \vert \theta)\)</span>.
(Recall that <span class="math inline">\(\theta\)</span> is the conventionally
used symbol for a population parameter or set of parameters. Here, without
loss of generality, we will assume that <span class="math inline">\(\theta\)</span> represents one parameter.)
The likelihood function for the entire sample is defined as
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = f_X(\mathbf{x} \vert \theta) = \prod_{i=1}^n f_X(x_i \vert \theta) \,.
\]</span>
The second equality holds because the data are assumed to be iid. Note
that the same definition holds for discrete distributions, with the
notational change <span class="math inline">\(f \rightarrow p\)</span>.
Additionally, recall that <span class="math inline">\(\prod\)</span> is the product symbol,
the multiplicative analogue
of the summation symbol <span class="math inline">\(\sum\)</span>: <span class="math inline">\(\prod_{i=1}^n f_X(x_i \vert \theta) =
f_X(x_1 \vert \theta) \cdot f_X(x_2 \vert \theta) \cdot \cdots \cdot f_X(x_n \vert \theta)\)</span>. As a last comment, we will find that we often work not
with the likelihood function itself, but with the <em>log-likelihood</em>
function <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span>; given iid data, the log-likelihood is
<span class="math display">\[
\ell(\theta \vert \mathbf{x}) = \log \left[ \prod_{i=1}^n f_X(x_i \vert \theta) \right] = \sum_{i=1}^n \log f_X(x_i \vert \theta) \,.
\]</span></p>
<p>Lets step back for an instant here, and assume our sample size is <span class="math inline">\(n = 1\)</span>.
At first blush, it would appear that a likelihood is the same as a probability
density function, because, after all, <span class="math inline">\(\mathcal{L}(\theta \vert x) =
f_X(x \vert \theta)\)</span>. But note what is
being conditioned upon in both functions: for the likelihood, we
consider that the datum is fixed to its
observed value (i.e., <span class="math inline">\(X = x\)</span>) and that we are free to vary the value of
<span class="math inline">\(\theta\)</span>.</p>
<p>To show the difference between a probability distribution and the likelihood
function, lets take a look at a simple example where we flip a potentially
unfair coin twice, with <span class="math inline">\(p\)</span> being the probability of observing heads in any
single flip.
The probability mass function for <span class="math inline">\(X\)</span>, the random variable denoting the number
of heads observed, is
<span class="math display">\[
p_X(x \vert p) = \left\{ \begin{array}{cl} 2 &amp; p^2 \\ 1 &amp; 2p(1-p) \\ 0 &amp; (1-p)^2 \end{array} \right. \,.
\]</span>
The probability mass functions that arise when
we set <span class="math inline">\(p\)</span> to, e.g., 0.3, 0.5, and 0.8 are shown in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:pmfs">1.26</a>, while
the likelihood functions for each observable value of <span class="math inline">\(x\)</span> are shown
in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:likes">1.27</a>. None of the plots in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:likes">1.27</a>
shows the pmf <span class="math inline">\(p_X(x \vert p)\)</span>; rather, they
each show the relative plausibility
of a particular probability <span class="math inline">\(p\)</span> given the number of observed heads. If we
observe zero (or two) heads, then a probability of <span class="math inline">\(p=0\)</span> (or <span class="math inline">\(p=1\)</span>) is the most
plausible valuebut
only <span class="math inline">\(p=1\)</span> (or <span class="math inline">\(p=0\)</span>) is impossible.
On the other hand, when we observe one head and one tail,
the most plausible value for <span class="math inline">\(p\)</span> is 0.5, although all other values (save
<span class="math inline">\(p=0\)</span> and <span class="math inline">\(p=1\)</span>) are possible as well.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pmfs"></span>
<img src="_main_files/figure-html/pmfs-1.png" alt="\label{fig:pmfs}From left to right, the probability mass functions $p_X(x \vert p)$ for probabilities $p =$ 0.3, 0.5, and 0.8." width="30%" /><img src="_main_files/figure-html/pmfs-2.png" alt="\label{fig:pmfs}From left to right, the probability mass functions $p_X(x \vert p)$ for probabilities $p =$ 0.3, 0.5, and 0.8." width="30%" /><img src="_main_files/figure-html/pmfs-3.png" alt="\label{fig:pmfs}From left to right, the probability mass functions $p_X(x \vert p)$ for probabilities $p =$ 0.3, 0.5, and 0.8." width="30%" />
<p class="caption">
Figure 1.26: From left to right, the probability mass functions <span class="math inline">\(p_X(x \vert p)\)</span> for probabilities <span class="math inline">\(p =\)</span> 0.3, 0.5, and 0.8.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likes"></span>
<img src="_main_files/figure-html/likes-1.png" alt="\label{fig:likes}From left to right, the likelihood function $\mathcal{L}(p \vert x)$ for the probability parameter $p$ given that we observe $x=0$ heads, $x=1$ head, and $x=2$ heads, respectively." width="30%" /><img src="_main_files/figure-html/likes-2.png" alt="\label{fig:likes}From left to right, the likelihood function $\mathcal{L}(p \vert x)$ for the probability parameter $p$ given that we observe $x=0$ heads, $x=1$ head, and $x=2$ heads, respectively." width="30%" /><img src="_main_files/figure-html/likes-3.png" alt="\label{fig:likes}From left to right, the likelihood function $\mathcal{L}(p \vert x)$ for the probability parameter $p$ given that we observe $x=0$ heads, $x=1$ head, and $x=2$ heads, respectively." width="30%" />
<p class="caption">
Figure 1.27: From left to right, the likelihood function <span class="math inline">\(\mathcal{L}(p \vert x)\)</span> for the probability parameter <span class="math inline">\(p\)</span> given that we observe <span class="math inline">\(x=0\)</span> heads, <span class="math inline">\(x=1\)</span> head, and <span class="math inline">\(x=2\)</span> heads, respectively.
</p>
</div>
<p>One might question at this point why the likelihood function is important, as
it is indeed not a pmf or pdf. We will see below that it is often used when
trying to uncover the truth about a population: e.g., given a set of data,
randomly sampled from some family of distributions parameterized by <span class="math inline">\(\theta\)</span>,
we can utilize
the likelihood to infer, or estimate, <span class="math inline">\(\theta\)</span>. Before we talk about
estimation, however,
we need to discuss how one might summarize a set of data so
as to make it mathematically more easy to work within other words, we need to
talk about statistics.</p>
<hr />
<div id="examples-of-likelihood-functions-for-iid-data" class="section level3 hasAnchor" number="1.13.1">
<h3><span class="header-section-number">1.13.1</span> Examples of Likelihood Functions for IID Data<a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>(For more information on the product-symbol manipulations utilized below,
see the material on useful product symbol tricks in Chapter 8.)</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>We are given <span class="math inline">\(n\)</span> iid samples from
<span class="math display">\[
f_X(x \vert \theta) = \left\{ \begin{array}{cl} \theta x^{\theta-1} &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
The likelihood function is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \theta x_i^{\theta-1} = \theta^n \left(\prod_{i=1}^n x_i\right)^{\theta-1} \,,
\]</span>
while the log-likelihood function is
<span class="math display">\[
\ell(\theta \vert \mathbf{x}) = \log \mathcal{L}(\theta \vert \mathbf{x}) = n \log \theta + (\theta-1) \log \prod_{i=1}^n x_i = n \log \theta + (\theta-1) \sum_{i=1}^n \log x_i \,.
\]</span></li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>We are given <span class="math inline">\(n\)</span> iid samples from
<span class="math display">\[
p_X(x \vert p) = \left\{ \begin{array}{cl} p &amp; x=1 \\ 1-p &amp; x=0 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
We observe <span class="math inline">\(k\)</span> values of 1, and <span class="math inline">\(n-k\)</span> values of 0. Thus the likelihood
function is
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n p_X(x_i \vert p) = \prod_{i=1}^k p \times \prod_{i=1}^{n-k} (1-p) = p^k(1-p)^{n-k} \,,
\]</span>
and the log-likelihood function is
<span class="math display">\[
\ell(p \vert \mathbf{x}) = \log \mathcal{L}(p \vert \mathbf{x}) = k \log p + (n-k) \log (1-p) \,.
\]</span></li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>We are given <span class="math inline">\(n\)</span> iid samples from the mixed distribution
<span class="math display">\[
h_X(x \vert \theta) = \left\{ \begin{array}{cl} \frac{x}{\theta} &amp; x \in [0,1] \\ 1-\frac{1}{2\theta} &amp; x=2 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\]</span>
We observe <span class="math inline">\(l\)</span> values between 0 and 1, along with <span class="math inline">\(n-l\)</span> values of 2.
Let <span class="math inline">\(j\)</span> denote the indices of those data with values between 0 and 1, and
<span class="math inline">\(k\)</span> denote the indices of those data with value 2. The
likelihood function is then
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n h_X(x_i \vert p) = \prod_{j=1}^l \frac{x_j}{\theta} \times \prod_{k=1}^{n-l} \left(1-\frac{1}{2\theta}\right) = \left( \frac{1}{\theta^l} \prod_{j=1}^l x_j \right) \left(1-\frac{1}{2\theta}\right)^{n-l}
\]</span>
<!--
This expression needs a little further explanation. $\mathbb{I}$ is
*indicator function*, which takes on the value 1 if a condition is met
and 0 otherwise. Here,
$$
\mathbb{I}_{x_i \in [0,1]} = \left\{ \begin{array}{cl} 1 & x_i \in [0,1] \\ 0 & x_i=2 \end{array} \right. \,.
$$
Use of the indicator function helps us keep track of what specific values
of $x_i$ to include in the first product of the likelihood function...in
other words, it helps us not mess up the indexing and to not include values
of $x = 2$ in the product.
We do not need an indicator function for the second product, as $x_i$ does
not explicitly appear there.
--></li>
</ol>
</blockquote>
<hr />
</div>
<div id="coding-the-likelihood-function-in-r" class="section level3 hasAnchor" number="1.13.2">
<h3><span class="header-section-number">1.13.2</span> Coding the Likelihood Function in R<a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets code and display the likelihood function for the mixed distribution
we introduce immediately above. The first thing we will do, however, is
generate data from this distribution using inverse transform sampling.</p>
</blockquote>
<blockquote>
<p>The cdf for this distribution is
<span class="math display">\[
H_X(x \vert \theta) = \left\{ \begin{array}{cl} 0 &amp; x &lt; 0 \\ x^2/2\theta &amp; 0 \leq x \leq 1 \\ 1/2\theta &amp; 1 &lt; x &lt; 2 \\ 1 &amp; x \geq 2 \end{array} \right. \,.
\]</span>
For cdf values <span class="math inline">\(q &lt; 1/(2\theta)\)</span>, the inverse function is
<span class="math inline">\(x = \sqrt{2 \theta q}\)</span>.
We thus code the inverse transform sampler as follows</p>
</blockquote>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb43-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-2" tabindex="-1"></a>n           <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb43-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-3" tabindex="-1"></a>theta       <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb43-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-4" tabindex="-1"></a>q           <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>)</span>
<span id="cb43-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-5" tabindex="-1"></a>w           <span class="ot">&lt;-</span> <span class="fu">which</span>(q <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>theta))</span>
<span id="cb43-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-6" tabindex="-1"></a>X.sample    <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">2</span>,n)</span>
<span id="cb43-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb43-7" tabindex="-1"></a>X.sample[w] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>theta<span class="sc">*</span>q[w])</span></code></pre></div>
<blockquote>
<p>Given our dataset, we can compute and visualize the likelihood
function <span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x})\)</span>butthis will
be problematic. If we examine the likelihood function, we see that
its values will be (a) tiny, and (b) spread over a large dynamic range.
(In fact, for a sufficiently large dataset, the likelihood function will
have a value too small to be recordable as a floating-point number on
a computer!)
Thus in practice it is often best to visualize the log-likelihood
function <span class="math inline">\(\ell(\theta \vert \mathbf{x}) =
\log \mathcal{L}(\theta \vert \mathbf{x})\)</span> as a function of <span class="math inline">\(\theta\)</span>.
We do this below in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:samplike">1.28</a>. (We will leave the
derivation of the log-likelihood as an exercise to the reader.)</p>
</blockquote>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-1" tabindex="-1"></a>loglike <span class="ot">&lt;-</span> <span class="cf">function</span>(theta,x)</span>
<span id="cb44-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-2" tabindex="-1"></a>{</span>
<span id="cb44-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-3" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">which</span>(x <span class="sc">&lt;</span> <span class="dv">1</span>)</span>
<span id="cb44-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-4" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb44-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-5" tabindex="-1"></a>  l <span class="ot">&lt;-</span> <span class="fu">length</span>(w)</span>
<span id="cb44-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-6" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">log</span>(<span class="fu">prod</span>(x[w])) <span class="sc">-</span> l<span class="sc">*</span><span class="fu">log</span>(theta) <span class="sc">+</span> (n<span class="sc">-</span>l)<span class="sc">*</span><span class="fu">log</span>(<span class="dv">1-1</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>theta))</span>
<span id="cb44-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-7" tabindex="-1"></a>}</span>
<span id="cb44-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-8" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.51</span>,<span class="fl">10.0</span>,<span class="at">by=</span><span class="fl">0.01</span>)</span>
<span id="cb44-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-9" tabindex="-1"></a>llike <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,<span class="fu">length</span>(theta))</span>
<span id="cb44-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-10" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(theta) ) {</span>
<span id="cb44-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-11" tabindex="-1"></a>  llike[ii] <span class="ot">&lt;-</span> <span class="fu">loglike</span>(theta[ii],X.sample)</span>
<span id="cb44-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-12" tabindex="-1"></a>}</span>
<span id="cb44-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-13" tabindex="-1"></a>w     <span class="ot">&lt;-</span> <span class="fu">which.max</span>(llike)</span>
<span id="cb44-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-14" tabindex="-1"></a>df    <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">theta=</span>theta,<span class="at">llike=</span>llike)</span>
<span id="cb44-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-15" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>theta,<span class="at">y=</span>llike)) <span class="sc">+</span></span>
<span id="cb44-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-16" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb44-17"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-17" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>theta[w],<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb44-18"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(theta),<span class="at">y=</span><span class="st">&quot;Log-Likelihood&quot;</span>) <span class="sc">+</span></span>
<span id="cb44-19"><a href="the-basics-of-probability-and-statistical-inference.html#cb44-19" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:samplike"></span>
<img src="_main_files/figure-html/samplike-1.png" alt="\label{fig:samplike}The log-likelihood function $\ell(\theta \vert \mathbf{x})$ for the pdf $h_X(x \vert \theta)$ defined in the text. The red line indicates the value of $\theta$ (1.56) for which $\ell(\theta \vert \mathbf{x})$ is maximized." width="50%" />
<p class="caption">
Figure 1.28: The log-likelihood function <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> for the pdf <span class="math inline">\(h_X(x \vert \theta)\)</span> defined in the text. The red line indicates the value of <span class="math inline">\(\theta\)</span> (1.56) for which <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> is maximized.
</p>
</div>
</div>
</div>
<div id="point-estimation" class="section level2 hasAnchor" number="1.14">
<h2><span class="header-section-number">1.14</span> Point Estimation<a href="the-basics-of-probability-and-statistical-inference.html#point-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets suppose we are given a sample of iid data <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>, sampled
from some distribution with mean <span class="math inline">\(E[X]\)</span> and finite variance <span class="math inline">\(V[X]\)</span>.
As we started
to discuss above, we can use functions of these data, or statistics, to
make inferences about population properties: what are plausible values
for the population mean
<span class="math inline">\(\mu\)</span>? or the population variance <span class="math inline">\(\sigma^2\)</span>? or, more generally,
any population parameter <span class="math inline">\(\theta\)</span>?</p>
<p>In <em>point estimation</em>, the statistic that we choose is an <em>estimator</em> of
<span class="math inline">\(\theta\)</span>. We dub the estimate <span class="math inline">\(\hat{\theta}\)</span>: this is a number that
is our best guess for what the true value of <span class="math inline">\(\theta\)</span> is, given the data
we have observed thus far.</p>
<p>Nowhow do we define an estimator? Well, to start, we can guess what might
be good estimators, and compare their properties. For instance, here, lets
propose two estimators for <span class="math inline">\(\mu\)</span>: <span class="math inline">\(\hat{\mu} = X_1\)</span> and <span class="math inline">\(\hat{\mu} = \bar{X}\)</span>.
Which is better? It may seem intuitively obvious that <span class="math inline">\(\bar{X}\)</span> is better,
because it incorporates more databut how do we quantify better?</p>
<p>In the last section, we indicated that we can assess the utility of a statistic
used as
an estimator by computing metrics, quantities that allow us to directly compare
estimators. Here, we will highlight two of them, the <em>bias</em> and the
<em>variance</em>; later, we will highlight others. (Recall that an estimator
is a statistic, and thus it is a random variable that is drawn
from a sampling distribution with some mean and some variance.)</p>
<ol style="list-style-type: decimal">
<li>Bias: does the
estimator yield the true value, <em>on average</em>? In other words, is
<span class="math inline">\(B[\hat{\theta}] = E[\hat{\theta}-\theta] = E[\hat{\theta}] - \theta =
0\)</span>? If so, we say that our estimator is <em>unbiased</em>. Here,
both estimators are unbiased: <span class="math inline">\(E[X_1-\mu] = E[X_1]-\mu = \mu-\mu = 0\)</span>,
and <span class="math inline">\(E[\bar{X}-\mu] = E[\bar{X}] - \mu = \mu-\mu = 0\)</span>.</li>
<li>Variance: is the spread of values that the estimator generates from
experiment to experiment relatively small, or relatively large? (Recall
that it cannot be zero, due to the randomness inherent in the data-generating
process!) Here, the first estimator has variance <span class="math inline">\(V[X_1] = \sigma^2\)</span>, while
the second has variance <span class="math inline">\(V[\bar{X}] = \sigma^2/n\)</span>. (Recall that we derive
the latter result in the previous section!)
If <span class="math inline">\(n &gt; 1\)</span>, the second
estimator, with its smaller variance, is the better one to use.
(If <span class="math inline">\(n = 1\)</span>, then <span class="math inline">\(\bar{X} = X_1\)</span>, so the two estimators are identical anyway.)</li>
</ol>
<p>See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:biasvar">1.29</a> for a graphical representation of bias and
variance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:biasvar"></span>
<img src="figures/bias_variance.png" alt="\label{fig:biasvar}A graphical representation of the concepts of bias (how far on average an estimate is from the truth...represented here as an offset from the bullseye) and variance (the spread of estimate values...represented here as the spatial spread of the plotted points)." width="50%" />
<p class="caption">
Figure 1.29: A graphical representation of the concepts of bias (how far on average an estimate is from the truthrepresented here as an offset from the bullseye) and variance (the spread of estimate valuesrepresented here as the spatial spread of the plotted points).
</p>
</div>
<p>Given a choice, we generally prefer unbiased estimators with smaller variances;
however, it is theoretically possible that we might be better off with an
estimator with a small bias if it has an even lower variance than the best
unbiased one. We can start making sense of this statement now by stating
that we can combine the information about bias and variance together into
a single metric, the <em>mean-squared error</em> (or <em>MSE</em>). The MSE is defined as
<span class="math display">\[
MSE[\hat{\theta}] = B[\hat{\theta}]^2 + V[\hat{\theta}] \,,
\]</span>
and smaller values are better. For <span class="math inline">\(\hat{\theta} = X_1\)</span>, the
MSE is <span class="math inline">\(0^2 + \sigma^2 = \sigma^2\)</span>, while for <span class="math inline">\(\hat{\theta} = \bar{X}\)</span>,
the MSE is <span class="math inline">\(0^2 + \sigma^2/n = \sigma^2/n\)</span>; <span class="math inline">\(\bar{X}\)</span> is still the
better estimator.</p>
<hr />
<p>At this point, one might be thinking that
guessing estimators would be a sub-optimal approach.
And one would be correct.
Over the remainder of the book, we introduce different algorithmic
approaches for
defining estimators with (presumably) good properties. Here we examine a first
one: <em>maximum likelihood estimation</em> (or <em>MLE</em>).</p>
<p>Above, in the section introducing the likelihood, we discussed how the
likelihood function <span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x})\)</span> encapsulates
the relative plausibilities of different values of <span class="math inline">\(\theta\)</span> given the
data that are observed. Maximum likelihood estimation takes
this idea to its natural conclusion: the most plausible value of <span class="math inline">\(\theta\)</span>,
the one that maximizes the likelihood function, <em>is</em> indeed a good way to
estimate the true value
of <span class="math inline">\(\theta\)</span>. Assuming that the likelihood function achieves a maximum away
from <span class="math inline">\(\theta_{\rm lo}\)</span> and <span class="math inline">\(\theta_{\rm hi}\)</span>, the parameter bounds, then
the steps to find <span class="math inline">\(\hat{\theta}_{MLE}\)</span> involve straightforward calculus,
albeit with a simplifying twist:</p>
<ol style="list-style-type: decimal">
<li>Write down the likelihood function: <span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta)\)</span>.</li>
<li>Take the natural logarithm of <span class="math inline">\(\mathcal{L}\)</span>: <span class="math inline">\(\ell(\theta \vert \mathbf{x}) = \log \mathcal{L}(\theta \vert \mathbf{x}) = \sum_{i=1}^n \log f_X(x_i \vert \theta)\)</span>.</li>
<li>Compute the first derivative of <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> with respect to <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(\theta\)</span> represents more than one parameter, take the first <em>partial</em> derivative with respect to the parameter of interest.</li>
<li>Set <span class="math inline">\(\ell&#39;(\theta \vert \mathbf{x}) = 0\)</span>.</li>
<li>Solve for <span class="math inline">\(\theta\)</span>. The solution is <span class="math inline">\(\hat{\theta}_{MLE}\)</span>, assuming that
the second derivative of <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> is negative (i.e.,
concave down); otherwise, we have actually found a local <em>minimum</em> of the
likelihood function.</li>
</ol>
<p>Note that the simplifying twist is transforming the likelihood
<span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x})\)</span>
to the log-likelihood <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span>; differentiating the
latter is often, if not always,
easier than differentiating the former. But even if we skip step 2 and
compute the first derivative of <span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x})\)</span> directly,
we will eventually get the same expression for <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.</p>
<hr />
<div id="comparing-two-estimators" class="section level3 hasAnchor" number="1.14.1">
<h3><span class="header-section-number">1.14.1</span> Comparing Two Estimators<a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that
we are given <span class="math inline">\(n\)</span> iid data sampled from the following pdf:
<span class="math display">\[
f_X(x) = \frac{1}{\theta}
\]</span>
for <span class="math inline">\(0 \leq x \leq \theta\)</span>, with <span class="math inline">\(\theta\)</span> unknown. The expected value and
variance of this distribution are <span class="math inline">\(\mu = E[X] = \theta/2\)</span> and <span class="math inline">\(\sigma^2 = V[X] =
\theta^2/12\)</span>, respectively. We propose two estimators
for <span class="math inline">\(\theta\)</span>: <span class="math inline">\(2\bar{X}\)</span>, and <span class="math inline">\(X_1+X_2\)</span>. Which is
better? Intuitively, we know the answer, but can we quantify it, i.e.,
can we determine which of the two
estimators has a smaller mean-squared error?</p>
</blockquote>
<blockquote>
<p>For <span class="math inline">\(\hat{\theta} = 2\bar{X}\)</span>, the expected value is
<span class="math display">\[
E[2\bar{X}] = 2E[\bar{X}] = 2\mu = \theta \,,
\]</span>
and thus we can see that <span class="math inline">\(2\bar{X}\)</span> is unbiased.
(Here, we utilize the general result that <span class="math inline">\(E[\bar{X}] = \mu\)</span>.)
Thus the MSE will simply be the variance of this estimator:
<span class="math display">\[
MSE[\hat{\theta}] = V[2\bar{X}] = 4V[\bar{X}] = 4\frac{\sigma^2}{n} = \frac{\theta^2}{3n} \,.
\]</span>
(Here, we utilize the general result that <span class="math inline">\(V[\bar{X}] = \frac{\sigma^2}{n}\)</span>.)</p>
</blockquote>
<blockquote>
<p>For <span class="math inline">\(\hat{\theta} = X_1 + X_2\)</span>, the expected value is
<span class="math display">\[
E[X_1+X_2] = E[X_1] + E[X_2] = \frac{\theta}{2} + \frac{\theta}{2} = \theta \,.
\]</span>
The estimator is unbiased. The MSE is thus
<span class="math display">\[
MSE[\hat{\theta}] = V[X_1+X_2] = V[X_1] + V[X_2] = \frac{\theta^2}{12} + \frac{\theta^2}{12} = \frac{\theta^2}{6} \,.
\]</span></p>
</blockquote>
<blockquote>
<p>Compare the two MSE expressions, keeping in mind that the second one
is meaningless if <span class="math inline">\(n=1\)</span>. For <span class="math inline">\(n=2\)</span> the MSEs are equivalent,
which makes sense since the estimators themselves are equivalent.
If <span class="math inline">\(n &gt; 2\)</span>, then the MSE for <span class="math inline">\(\hat{\theta} = 2\bar{X}\)</span>,
is smaller, and it will continue getting smaller as <span class="math inline">\(n\)</span> increases, unlike
the MSE for <span class="math inline">\(\hat{\theta} = X_1 + X_2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="maximum-likelihood-estimate-of-population-parameter" class="section level3 hasAnchor" number="1.14.2">
<h3><span class="header-section-number">1.14.2</span> Maximum Likelihood Estimate of Population Parameter<a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that
we are given <span class="math inline">\(n\)</span> iid data sampled from the following pdf:
<span class="math display">\[
f_X(x) = \frac{1}{\theta}\exp({-x/\theta}) \,,
\]</span>
with <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>.
(To be clear: in real-life situations, we do not know the form of the
pmf or pdf from which the data are sampled!
We <em>assume</em> a family of distributions, then estimate
the value of the population parameter of interest.)
For this distribution, <span class="math inline">\(E[X] = \theta\)</span> and <span class="math inline">\(V[X] = \theta^2\)</span>.
The likelihood function is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n \frac{1}{\theta}\exp\left(-\frac{x_i}{\theta}\right) = \frac{1}{\theta^n} \prod_{i=1}^n \exp\left(-\frac{x_i}{\theta}\right) = \frac{1}{\theta^n} \exp\left(-\frac{1}{\theta}\sum_{i=1}^n x_i\right) \,,
\]</span>
and the log-likelihood is
<span class="math display">\[
\ell(\theta \vert \mathbf{x}) = \log \left[ \frac{1}{\theta^n} \exp\left(-\frac{1}{\theta}\sum_{i=1}^n x_i\right) \right] = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^n x_i \,.
\]</span>
The next step in determining <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is to take the first
derivative with respect to <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
\ell&#39;(\theta \vert \mathbf{x}) = \frac{d}{d\theta} \ell(\theta \vert \mathbf{x}) = -\frac{n}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^n x_i \,,
\]</span>
and set the result equal to zero:
<span class="math display">\[
-\frac{n}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^n x_i = 0 = -n + \frac{1}{\theta} \sum_{i=1}^n x_i \,.
\]</span>
Solving for <span class="math inline">\(\theta\)</span>, we get
<span class="math display">\[
\hat{\theta}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X} \,.
\]</span>
When we switched from solving for the generic quantity <span class="math inline">\(\theta\)</span> to solving
for an estimator, which is a function of random variables, we switched from
using the lower-case generic variable <span class="math inline">\(x\)</span> to using an upper-case-denoted
random variable <span class="math inline">\(X\)</span>.</p>
</blockquote>
<blockquote>
<p>(Note that we should check to see whether the second derivative
is negative at the extremum,
indicating that <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is located at a local
maximum of the likelihood function rather than a minimum. So:
<span class="math display">\[
\ell&#39;&#39;(\theta \vert \mathbf{x}) = \frac{d}{d\theta} \ell&#39;(\theta \vert \mathbf{x}) = \frac{n}{\theta^2} - \frac{2}{\theta^3} \sum_{i=1}^n x_i = \frac{n}{\theta^2} \left( 1 - \frac{2n}{\theta}\bar{x} \right)\,.
\]</span>
Lets plug in <span class="math inline">\(\theta = \hat{\theta}_{MLE} = \bar{x}\)</span>:
<span class="math display">\[
\ell&#39;&#39;(\hat{\theta}_{MLE} \vert \mathbf{x}) = \frac{n}{\bar{x}^2} \left( 1 - 2n \right) \,.
\]</span>
We know that <span class="math inline">\(n\)</span> is positive and <span class="math inline">\(\geq 1\)</span> and that <span class="math inline">\(\bar{x} &gt; 0\)</span>,
so indeed <span class="math inline">\(\ell&#39;&#39;(\hat{\theta}_{MLE} \vert \mathbf{x}) &lt; 0\)</span> and
thus we have detected a maximum of the likelihood function.)</p>
</blockquote>
<blockquote>
<p>Now, is this estimate biased? We know from results shown above that
<span class="math inline">\(E[\bar{X}] = \theta\)</span>, thus <span class="math inline">\(B[\hat{\theta}_{MLE}] = E[\hat{\theta}_{MLE}] - \theta
= \theta - \theta = 0\)</span> and thus the estimator is unbiased. We also
know that <span class="math inline">\(V[\bar{X}] = \sigma^2/n = \theta^2/n\)</span>. The question, to be answered
in a future chapter, is whether we can possibly find an estimator with a
lower variance via some other estimation approachor if this indeed the
best that we can do.</p>
</blockquote>
<blockquote>
<p>Now, when we solve for, e.g., <span class="math inline">\(\hat{\theta}_{MLE}\)</span>, we can solve for
other quantities as wellit is just algebra. Meaning, for instance, that
if we want to estimate <span class="math inline">\(\hat{\theta}_{MLE}^2\)</span> for whatever reason, we
can just square both sides in the solution above:
<span class="math display">\[
(\hat{\theta}_{MLE})^2 = (\bar{X})^2 ~~\Rightarrow~~ \hat{\theta}_{MLE}^2 = \bar{X}^2 \,.
\]</span>
(This is a manifestation of the so-called <em>invariance property</em> of the MLE.)
Now, is <em>this</em> a biased estimator for <span class="math inline">\(\theta^2\)</span>? We can utilize the
shortcut formula for computing variance to find that
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{\sigma^2}{n} + \mu^2 = \frac{\theta^2}{n} + \theta^2 = \theta^2 \left( 1 + \frac{1}{n} \right) \neq \theta^2 \,.
\]</span>
This estimator <em>is</em> biasedbut the bias goes away as the sample size
<span class="math inline">\(n\)</span> increases. That means that we would call this estimator <em>asymptotically
unbiased</em>, i.e., it is unbiased when the sample size is infinite.
We note here that maximum likelihood estimates are always either unbiased
or asymptotically unbiased.</p>
</blockquote>
</div>
</div>
<div id="statistical-inference-with-sampling-distributions" class="section level2 hasAnchor" number="1.15">
<h2><span class="header-section-number">1.15</span> Statistical Inference with Sampling Distributions<a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A fundamental issue with point estimation is that, e.g., it does not
provide a notion of how uncertain an estimate is. By this, we do not
mean the standard error of the sampling distribution for the statistic
we use when making the estimate (which is a quantity that we can derive),
but rather how large (or small) is the range of plausible values of <span class="math inline">\(\theta\)</span>
given the observed value of the statistic?
Point estimation also does not allow
us to answer the question of is a particular hypothesized
value of <span class="math inline">\(\theta\)</span>
plausible given what we observe? We can resolve the first issue by
computing a <em>confidence interval</em> for <span class="math inline">\(\theta\)</span>, and the second via
<em>hypothesis testing</em>. We discuss each of these concepts in turn
in the next two sections.
Before doing so, however, we will show how the construction of confidence
intervals and the performance of hypothesis tests both boil down to
performing a root-finding exercise in which we work directly with
the sampling distribution.</p>
<p>Let <span class="math inline">\(Y = g(X_1,\ldots,X_n)\)</span> be a statistic, with the <span class="math inline">\(X_i\)</span>s being
independent and identically distributed (iid) random variables. (To be
clear, <span class="math inline">\(Y\)</span> does not have to be a function of <em>all</em> the observed data, but
rather at least some of them. Think of the sample median in contrast to
the sample mean.) As we describe above, <span class="math inline">\(Y\)</span> has a sampling distribution,
whose probability density function (pdf) is <span class="math inline">\(f_Y(y \vert \theta)\)</span> and
whose cumulative distribution function (cdf) is <span class="math inline">\(F_Y(y \vert \theta)\)</span>.
(For simplicity, and without loss
of generality, we assume <span class="math inline">\(Y\)</span> is a continuous random variable.)
We can alter the shape and/or location of the sampling distribution
by change the value(s) of <span class="math inline">\(\theta\)</span>. To illustrate this, lets make
up a sampling distribution pdf:
<span class="math display">\[
f_Y(y \vert \theta) = \frac{1}{2} ~~~ y \in [\theta-1,\theta+1]
\]</span>
In Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:samppdf">1.30</a>, we show how the pdf moves with <span class="math inline">\(\theta\)</span>;
as <span class="math inline">\(\theta\)</span> changes from 2.8 to 5.2, the pdf shifts (smoothly) from the
location indicated by the red lines to that indicated by the blue lines.
Now, lets assume that the green line in the figure, at value
<span class="math inline">\(y_{\rm obs} = 3.4\)</span>,
is the statistic value that we actually observe.
What can we conclude right away,
on the basis of this figure? We can conclude that this observed value
is <em>plausible</em> if <span class="math inline">\(\theta = 2.8\)</span>, but <em>implausible</em> if <span class="math inline">\(\theta = 5.2\)</span>.
In other words, 2.8 is an acceptable value for <span class="math inline">\(\theta\)</span>, but 5.2 is not.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:samppdf"></span>
<img src="_main_files/figure-html/samppdf-1.png" alt="\label{fig:samppdf}An illustration of how a sampling distribution pdf can move as the distribution parameter is changed. Here, $f_Y(y) = 1/2$ for $y \in [\theta-1,\theta+1]$; the red lines represent the pdf for $\theta = 2.8$ and the blue lines represents the pdf for $\theta=5.2$. The green vertical line represents the observed value of the statistic $Y$: $y_{\rm obs} = 3.4$." width="50%" />
<p class="caption">
Figure 1.30: An illustration of how a sampling distribution pdf can move as the distribution parameter is changed. Here, <span class="math inline">\(f_Y(y) = 1/2\)</span> for <span class="math inline">\(y \in [\theta-1,\theta+1]\)</span>; the red lines represent the pdf for <span class="math inline">\(\theta = 2.8\)</span> and the blue lines represents the pdf for <span class="math inline">\(\theta=5.2\)</span>. The green vertical line represents the observed value of the statistic <span class="math inline">\(Y\)</span>: <span class="math inline">\(y_{\rm obs} = 3.4\)</span>.
</p>
</div>
<p>To tie this illustration back to confidence intervals and hypothesis testing,
a confidence interval for <span class="math inline">\(\theta\)</span> is a range of acceptable values for
<span class="math inline">\(\theta\)</span> given what we observe (which initially we can think of here as
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U] = [2.4,4.4]\)</span>, since these values generate
pdfs that overlap <span class="math inline">\(y_{\rm obs}\)</span>), and a hypothesis test is the
determination of whether a stated value of <span class="math inline">\(\theta\)</span> is acceptable given
what we observe (here, a hypothesis that <span class="math inline">\(\theta = 3.1\)</span> would be acceptable,
while a hypothesis that <span class="math inline">\(\theta = 10\)</span> definitely would not be).</p>
<p>Lets generalize this a bit: we cannot, for instance, construct a
confidence interval simply by asking if the
sampling distribution pdf for our statistic
overlaps the observed value, because given its domain,
it may <em>always</em> overlap the observed value! (Albeit perhaps with
<span class="math inline">\(f_Y(y)\)</span> having a very small value.) So, by convention, we adopt a value
<span class="math inline">\(\alpha\)</span>
and ask the following questions for a two-sided interval:</p>
<ul>
<li>for what value of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\int_{-\infty}^{y_{\rm obs}} f_Y(y \vert \theta) = \alpha/2\)</span>
(in other words, for what value of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(F_Y(y_{\rm obs} \vert \theta) =
\alpha/2\)</span>); and</li>
<li>for what value of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\int_{y_{\rm obs}}^\infty f_Y(y \vert \theta) = \alpha/2\)</span>
(or is <span class="math inline">\(F_Y(y_{\rm obs} \vert \theta) = 1 - \alpha/2\)</span>)?</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampci"></span>
<img src="_main_files/figure-html/sampci-1.png" alt="\label{fig:sampci}Given the setting defined in Figure \@ref(fig:samppdf), we can ask (a) for what value of $\theta$ is the area under $f_Y(y)$ to the left of $y_{\rm obs}$ equal to $\alpha/2$, and (b) for what other value of $\theta$ is the area under $f_Y(y)$ to the right of $y_{\rm obs}$ equal to $\alpha/2$? These two values comprise a two-sided confidence interval. Here, $\alpha = 0.1$. For $y_{\rm obs} = 3.4$, the two values of $\theta$ are 2.5 and 4.3." width="30%" /><img src="_main_files/figure-html/sampci-2.png" alt="\label{fig:sampci}Given the setting defined in Figure \@ref(fig:samppdf), we can ask (a) for what value of $\theta$ is the area under $f_Y(y)$ to the left of $y_{\rm obs}$ equal to $\alpha/2$, and (b) for what other value of $\theta$ is the area under $f_Y(y)$ to the right of $y_{\rm obs}$ equal to $\alpha/2$? These two values comprise a two-sided confidence interval. Here, $\alpha = 0.1$. For $y_{\rm obs} = 3.4$, the two values of $\theta$ are 2.5 and 4.3." width="30%" />
<p class="caption">
Figure 1.31: Given the setting defined in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:samppdf">1.30</a>, we can ask (a) for what value of <span class="math inline">\(\theta\)</span> is the area under <span class="math inline">\(f_Y(y)\)</span> to the left of <span class="math inline">\(y_{\rm obs}\)</span> equal to <span class="math inline">\(\alpha/2\)</span>, and (b) for what other value of <span class="math inline">\(\theta\)</span> is the area under <span class="math inline">\(f_Y(y)\)</span> to the right of <span class="math inline">\(y_{\rm obs}\)</span> equal to <span class="math inline">\(\alpha/2\)</span>? These two values comprise a two-sided confidence interval. Here, <span class="math inline">\(\alpha = 0.1\)</span>. For <span class="math inline">\(y_{\rm obs} = 3.4\)</span>, the two values of <span class="math inline">\(\theta\)</span> are 2.5 and 4.3.
</p>
</div>
<p>(See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sampci">1.31</a>.)
And here we come back to the notion that this is a root-finding exercise.
To answer the first bullet-pointed question above, we would find the
root for the equation
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - \frac{\alpha}{2} = 0 \,.
\]</span>
Lets denote the root <span class="math inline">\(\hat{\theta}\)</span>.
As we can see in the left panel of the figure, <span class="math inline">\(\hat{\theta}\)</span>
represents the <em>upper bound</em> on <span class="math inline">\(\theta\)</span>, but in general that is not always
going to be true; in the next section well dig more into the details of
how we determine whether or not a given <span class="math inline">\(\hat{\theta}\)</span> is actually
a lower bound or an upper bound on <span class="math inline">\(\theta\)</span>.
(See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sampci2">1.32</a> for an alternative illustration showing what
is happening when we are finding roots.)
To answer the second question, we would find the
root for the equation
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - \left(1-\frac{\alpha}{2}\right) = 0 \,.
\]</span>
Referring back to the right panel of the figure, we see that here,
the root <span class="math inline">\(\hat{\theta}\)</span> represents the lower bound on <span class="math inline">\(\theta\)</span>.
We will note here that
in relatively rare circumstances, we can solve for the roots by hand, but
as we will see in examples sprinkled throughout the rest of the book,
we can always solve for the roots numerically using, e.g., <code>R</code>s <code>uniroot()</code>
function.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampci2"></span>
<img src="_main_files/figure-html/sampci2-1.png" alt="\label{fig:sampci2}In this alternative illustration to Figure \@ref(fig:sampci), we show how the interval bounds are found by changing the value of $\theta$ until the cumulative distribution function $F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.05$ (on the left, for $\theta = 4.3$, given that $y_{\rm obs} = 3.4$) and until $F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.95$ (on the right, for $\theta = 2.5$)." width="30%" /><img src="_main_files/figure-html/sampci2-2.png" alt="\label{fig:sampci2}In this alternative illustration to Figure \@ref(fig:sampci), we show how the interval bounds are found by changing the value of $\theta$ until the cumulative distribution function $F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.05$ (on the left, for $\theta = 4.3$, given that $y_{\rm obs} = 3.4$) and until $F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.95$ (on the right, for $\theta = 2.5$)." width="30%" />
<p class="caption">
Figure 1.32: In this alternative illustration to Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sampci">1.31</a>, we show how the interval bounds are found by changing the value of <span class="math inline">\(\theta\)</span> until the cumulative distribution function <span class="math inline">\(F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.05\)</span> (on the left, for <span class="math inline">\(\theta = 4.3\)</span>, given that <span class="math inline">\(y_{\rm obs} = 3.4\)</span>) and until <span class="math inline">\(F_Y(y_{\rm obs} \vert \theta) = \alpha/2 = 0.95\)</span> (on the right, for <span class="math inline">\(\theta = 2.5\)</span>).
</p>
</div>
<p>Now, what about hypothesis testing? As we will see below,
in a hypothesis test, we adopt a
<em>null hypothesis</em> (<span class="math inline">\(\theta = \theta_o\)</span>) and an
<em>alternative hypothesis</em> (e.g., <span class="math inline">\(\theta \neq \theta_o\)</span>), and we combine
this information with <span class="math inline">\(y_{\rm obs}\)</span> to determine whether the null hypothesis
is viable (in which case we fail to reject the null) or not
(in which case we reject the null). For a two-tail test, we would
determine the boundaries of the so-called <em>rejection region</em> by finding the
roots <span class="math inline">\(y_{\rm RR,1}\)</span> and <span class="math inline">\(y_{\rm RR,2}\)</span> for each of the following
equations
<span class="math display">\[\begin{align*}
F_Y(y_{\rm RR,1} \vert \theta_o) - \frac{\alpha}{2} &amp;= 0 ~~\Rightarrow~~ y_{\rm RR,1} = F_Y^{-1}(\alpha/2 \vert \theta_o) \\
F_Y(y_{\rm RR,2} \vert \theta_o) - \left(1-\frac{\alpha}{2}\right) &amp;= 0 ~~\Rightarrow~~ y_{\rm RR,2} = F_Y^{-1}(1-\alpha/2 \vert \theta_o)\,,
\end{align*}\]</span>
where <span class="math inline">\(F_Y^{-1}(\cdot)\)</span> is the inverse cdf function associated with the
sampling distribution.
Here, if <span class="math inline">\(y_{\rm obs}\)</span> is smaller than the lesser of <span class="math inline">\(y_{\rm RR,1}\)</span> and
<span class="math inline">\(y_{\rm RR,2}\)</span>, or larger than the greater of the two,
we would reject the null, i.e., we would deem <span class="math inline">\(\theta_o\)</span> implausible.
Note how the equations above are the same ones
we use when constructing confidence
intervals; here we fix <span class="math inline">\(\theta = \theta_o\)</span>
(as opposed to solving for <span class="math inline">\(\theta\)</span>)
and we solve for <span class="math inline">\(y\)</span> (as opposed to fixing <span class="math inline">\(y = y_{\rm obs}\)</span>).
As is the case for confidence intervals, sometimes we can find the
roots by hand, but if not, we can often find them by using off-the-shelf
inverse cdf codes like <code>R</code>s <code>qnorm()</code> (if the sampling distribution is
a normal distribution), etc.</p>
<hr />
<p>An important message to take away from this section is that
confidences interval estimation and hypothesis testing both involve
the same fundamental root-finding exercise: they only differ in
terms of what values are fixed and what values we are solving for!</p>
</div>
<div id="confidence-intervals" class="section level2 hasAnchor" number="1.16">
<h2><span class="header-section-number">1.16</span> Confidence Intervals<a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Interval estimation</em> is a mechanism to determine a range of possible
values for a distribution parameter <span class="math inline">\(\theta\)</span> that
overlaps the true but unknown value with some stated probability.
A two-sided interval estimate, or <em>confidence interval</em>, has the form
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
where <span class="math inline">\(1 - \alpha\)</span> is the user-set <em>confidence coefficient</em>, which
typically has the value 0.95 (or <span class="math inline">\(\alpha = 0.05\)</span>).
One can also define one-sided intervals:
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \right) = 1 - \alpha ~~\mbox{and}~~ P\left( \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,.
\]</span></p>
<p>It is important for us to interpret an interval estimate correctly:</p>
<ol style="list-style-type: decimal">
<li>it is a <em>random interval</em>, meaning that if we re-do the data-generating
experiment, the interval will change; and</li>
<li>we state that <em>the probability that, e.g.,
the two-sided interval estimate
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> will overlap <span class="math inline">\(\theta\)</span> is <span class="math inline">\(1 - \alpha\)</span></em>.</li>
</ol>
<p>In particular, one does not say the probability that <span class="math inline">\(\theta\)</span> lies in the
stated interval is <span class="math inline">\(1-\alpha\)</span>. <span class="math inline">\(\theta\)</span> is a population
quantity, and thus we cannot make probabilistic statements about it. It has the (unknown) value it has. Stated another way, in reference to point 2: the
probability of
<span class="math inline">\(1 - \alpha\)</span> refers to the reliability of the estimation procedure and not
to any one specific interval. For instance, if we compute an interval of
[1,2] and then re-do the experiment and compute an interval of [2.5,3.5],
and if we say there is a 95% chance that <span class="math inline">\(\theta\)</span> is in [1,2] and a
95% chance it is in [2.5,3.5], then there must be a 190% chance it is in either.
A computed interval either overlaps the true value, or it does not; it is no
longer a matter of probability. See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:overlap">1.33</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overlap"></span>
<img src="_main_files/figure-html/overlap-1.png" alt="\label{fig:overlap}Schematic illustration of ten confidence intervals constructed given ten separate independent datasets sampled from the same underlying population. We indicate the true parameter value $\theta$ with the vertical dashed line. Counting from the bottom, we observe that the fourth and seventh intervals do not overlap the true value; if we were to claim that the probability that $\theta$ lies within each interval is, e.g., 0.95, then the probability that $\theta$ lies in either the fourth or seventh interval is 1.9...which is clearly wrong. Thus the proper interpretation would be that 95 percent of evaluated intervals overlap the true value." width="50%" />
<p class="caption">
Figure 1.33: Schematic illustration of ten confidence intervals constructed given ten separate independent datasets sampled from the same underlying population. We indicate the true parameter value <span class="math inline">\(\theta\)</span> with the vertical dashed line. Counting from the bottom, we observe that the fourth and seventh intervals do not overlap the true value; if we were to claim that the probability that <span class="math inline">\(\theta\)</span> lies within each interval is, e.g., 0.95, then the probability that <span class="math inline">\(\theta\)</span> lies in either the fourth or seventh interval is 1.9which is clearly wrong. Thus the proper interpretation would be that 95 percent of evaluated intervals overlap the true value.
</p>
</div>
<p>As laid out in the last section above, to determine a
confidence interval bound, one would determine the root of the following
equation:
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta) - q &amp;= 0 \,.
\end{align*}\]</span>
There is a very important nuance here, however, that we ignored in the
last section, which is that it is not guaranteed that the value of the
statistic <span class="math inline">\(Y\)</span> that we adopt to construct the interval increases as
<span class="math inline">\(\theta\)</span> increases. What that means here is that if, e.g., <span class="math inline">\(q = \alpha/2\)</span>,
then the root <span class="math inline">\(\hat{\theta}\)</span>
might represent an upper bound on <span class="math inline">\(\theta\)</span>but it might represent a lower
bound instead. It all comes down to whether
the expected value <span class="math inline">\(E[Y]\)</span> increases as <span class="math inline">\(\theta\)</span> increases, or decreases
as <span class="math inline">\(\theta\)</span> increases, as we detail in the table below:</p>
<table>
<colgroup>
<col width="26%" />
<col width="26%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Interval</th>
<th align="right"><span class="math inline">\(E[Y]\)</span> Increases with <span class="math inline">\(\theta\)</span>?</th>
<th align="center"><span class="math inline">\(q\)</span> for Lower Bound</th>
<th align="center"><span class="math inline">\(q\)</span> for Upper Bound</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">two-sided</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(1-\alpha/2\)</span></td>
<td align="center"><span class="math inline">\(\alpha/2\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(\alpha/2\)</span></td>
<td align="center"><span class="math inline">\(1-\alpha/2\)</span></td>
</tr>
<tr class="odd">
<td align="right">one-sided lower</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(1-\alpha\)</span></td>
<td align="center"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td align="right">one-sided upper</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(-\)</span></td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(-\)</span></td>
<td align="center"><span class="math inline">\(1-\alpha\)</span></td>
</tr>
</tbody>
</table>
<p>This is the confidence interval reference table that we will refer back
to throughout the rest of book. To be clear: <em>this is not a table to
memorize</em>!</p>
<p>Before we turn to examples, it is important to put the methodology we
outline here for deriving interval bounds into historical context.</p>
<p>In traditional calculus-based probability and statistical inference classes,
the focus has been on introducing techniques for constructing confidence
intervals <em>analytically</em>, but they are really only analytical up to a point:
one always has to turn to statistical tables to derive final numerical answers.
An example of such a technique is the <em>pivotal method</em>, which for illustrative
purposes we discuss in an example in Chapter 2. However, the pivotal method
is not generally applicable across all families of distributions, and thus
other methods exist as well, methods
that are <em>not</em> as commonly seen in introductory classes (see, e.g.,
Chapter 9 of Casella and Berger 2002). Our methodology
is that described in section 9.2.3 of Casella and Berger, entitled
Pivoting the CDF, where it is noted that even if [the equations
<span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_{1-\alpha/2}) = 1 - \alpha/2\)</span> and
<span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_{\alpha/2}) = \alpha/2\)</span>] cannot be
solved analytically, we really only need to solve them numerically
since the proof that we have a <span class="math inline">\(1-\alpha\)</span> confidence interval [does]
not require an analytic solution. This is key: we can use this technique
to solve for intervals bounds by hand, or by <em>writing code</em> when we
cannot solve for the bounds by hand (as we will see
beginning in Chapter 2). Our methodology is thus generally applicable and
is thus the only one that a student ultimately needs to learn.</p>
<hr />
<div id="confidence-interval-where-ey-increases-with-theta" class="section level3 hasAnchor" number="1.16.1">
<h3><span class="header-section-number">1.16.1</span> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We have conducted an experiment in which we sample <span class="math inline">\(n=1\)</span> datum from
the following pdf:
<span class="math display">\[
f_X(x) = \theta x^{\theta-1} ~~~ x \in [0,1] \,.
\]</span>
The value we observe is <span class="math inline">\(x_{\rm obs}\)</span>. Below, we define
a two-sided confidence interval on <span class="math inline">\(\theta\)</span> assuming <span class="math inline">\(\alpha = 0.05\)</span>,
as well as show how we would define either a lower or an upper bound
on <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<blockquote>
<p>Before proceeding, we will make a notation change by saying our
statistic <span class="math inline">\(Y\)</span> equals <span class="math inline">\(X\)</span> (because, with a single datum, what else could
our statistic realistically be?), and thus the observed value is
<span class="math inline">\(y_{\rm obs} = x_{\rm obs}\)</span>. (This makes our random variable notation
consistent with that of the last section.) The sampling distribution pdf is
<span class="math inline">\(f_Y(y) = \theta y^{\theta-1}\)</span> and the cdf is
<span class="math display">\[
F_Y(y) = \int_0^y \theta z^{\theta-1} dz = \left. z^\theta \right|_0^y = y^\theta \,.
\]</span></p>
</blockquote>
<blockquote>
<p>Now, does <span class="math inline">\(E[Y]\)</span> increase with <span class="math inline">\(\theta\)</span>?
<span class="math display">\[\begin{align*}
E[Y] = \int_0^1 y f_Y(y) dy &amp;= \int_0^1 y \theta y^{\theta-1} dy \\
&amp;= \theta \int_0^1 y^{\theta} dy \\
&amp;= \frac{\theta}{\theta+1} \left. y^{\theta+1} \right|_0^1 = \frac{\theta}{\theta+1} \,.
\end{align*}\]</span>
As <span class="math inline">\(\theta \rightarrow \infty\)</span>, <span class="math inline">\(E[Y]\)</span> increases towards 1.
The confidence interval reference table thus tells us that for the lower
bound, <span class="math inline">\(q = 1-\alpha/2\)</span>, while for the upper bound, <span class="math inline">\(q = \alpha/2\)</span>.
For the lower bound, we find that
<span class="math display">\[\begin{align*}
y_{\rm obs}^{\hat{\theta}_L} &amp;= 1 - \frac{\alpha}{2} \\
\Rightarrow ~~~ \hat{\theta}_L \log y_{\rm obs} &amp;= \log \left( 1 - \frac{\alpha}{2} \right) \\
\Rightarrow ~~~ \hat{\theta}_L &amp;= \frac{\log \left( 1 - \frac{\alpha}{2} \right)}{\log y_{\rm obs}} \,,
\end{align*}\]</span>
while for the upper bound, we simply switch <span class="math inline">\(1-\alpha/2\)</span> to <span class="math inline">\(\alpha/2\)</span>:
<span class="math display">\[
\hat{\theta}_U = \frac{\log \left( \frac{\alpha}{2} \right)}{\log y_{\rm obs}} \,,
\]</span>
For instance, for <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(y_{\rm obs}\)</span> = 0.6, the
confidence interval would be <span class="math inline">\(\hat{\theta}_L = 0.05\)</span> to
<span class="math inline">\(\hat{\theta}_U = 7.22\)</span>. Not surprisingly, if we have but a single datum,
we cannot say that much about <span class="math inline">\(\theta\)</span>!</p>
</blockquote>
<blockquote>
<p>We can also solve for a 95% upper bound, wherein we use the same
upper bound equation as above but substitute <span class="math inline">\(\alpha\)</span> for <span class="math inline">\(\alpha/2\)</span>:
<span class="math display">\[
\hat{\theta}_U = \frac{\log \alpha}{\log y_{\rm obs}} \,,
\]</span>
The one-sided interval would be <span class="math inline">\((-\infty,5.86]\)</span> (although because <span class="math inline">\(\theta\)</span>
is limited to be <span class="math inline">\(&gt; 0\)</span>, we would actually write <span class="math inline">\((0,5.86]\)</span>). The
one-sided lower bound is then
<span class="math display">\[
\hat{\theta}_L = \frac{\log (1 - \alpha)}{\log y_{\rm obs}} \,,
\]</span>
which in our example evaluates to <span class="math inline">\([0.10,\infty)\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="confidence-interval-where-ey-decreases-with-theta" class="section level3 hasAnchor" number="1.16.2">
<h3><span class="header-section-number">1.16.2</span> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets switch things up a bit from the previous example, and assume that
instead of sampling a single datum from the pdf <span class="math inline">\(\theta x^{\theta-1}\)</span>, we
instead sample one from the pdf
<span class="math display">\[
f_X(x) = \theta (1-x)^{\theta-1} \,,
\]</span>
where <span class="math inline">\(x \in [0,1]\)</span> and the cdf is
<span class="math display">\[\begin{align*}
F_X(x) &amp;= \int_0^x \theta (1-y)^{\theta-1} dy \\
&amp;= \left. -(1-y)^\theta \right|_0^x = 1 - (1-x)^\theta \,.
\end{align*}\]</span>
How does this change our confidence interval computations?</p>
</blockquote>
<blockquote>
<p>We first compute the expected value (after changing the notation from
<span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> to be consistent about how we denote statistics).
This computation is a bit more complicated to
do, as it involves integration by parts, but it is still relatively
straightforward:
<span class="math display">\[\begin{align*}
E[Y] = \int_0^1 y f_Y(y) dy &amp;= \int_0^1 y \theta (1-y)^{\theta-1} dy \\
&amp;= \left. -y(1-y)^\theta \right|_0^1 + \int_0^1 (1-y)^\theta dy \\
&amp;= 0 - \frac{1}{\theta+1} \left. (1-y)^{\theta+1} \right|_0^1 = \frac{1}{\theta+1} \,.
\end{align*}\]</span>
As <span class="math inline">\(\theta\)</span> increases, <span class="math inline">\(E[Y]\)</span> <em>decreases</em>.
Thus, unlike in the last example, <span class="math inline">\(q = \alpha/2\)</span> will map to
the interval <em>lower bound</em>,
while <span class="math inline">\(q = 1-\alpha/2\)</span> will map to the upper bound.
The calculations then proceed in an analogous manner to those in the
last example:
<span class="math display">\[\begin{align*}
1-(1-y_{\rm obs})^{\hat{\theta}_L} &amp;= \frac{\alpha}{2} \\
\Rightarrow ~~~ (1-y_{\rm obs})^{\hat{\theta}_L} &amp;= 1 - \frac{\alpha}{2} \\
\Rightarrow ~~~ \hat{\theta}_L \log (1-y_{\rm obs}) &amp;= \log \left( 1 - \frac{\alpha}{2} \right) \\
\Rightarrow ~~~ \hat{\theta}_L &amp;= \frac{\log \left( 1 - \frac{\alpha}{2} \right)}{\log (1-y_{\rm obs})} \,,
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
1-(1-y_{\rm obs})^{\hat{\theta}_U} &amp;= 1 - \frac{\alpha}{2} \\
\Rightarrow ~~~ (1-y_{\rm obs})^{\hat{\theta}_U} &amp;= \frac{\alpha}{2} \\
\Rightarrow ~~~ \hat{\theta}_U \log (1-y_{\rm obs}) &amp;= \log \left( \frac{\alpha}{2} \right) \\
\Rightarrow ~~~ \hat{\theta}_U &amp;= \frac{\log \left( \frac{\alpha}{2} \right)}{\log (1-y_{\rm obs})} \,.
\end{align*}\]</span>
For instance, if <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(y_{\rm obs} = 0.6\)</span>, the interval
is <span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U] = [0.028,4.03]\)</span>.</p>
</blockquote>
<blockquote>
<p>We can also construct one-sided intervals, as we do above. We leave the
computation of these intervals to the reader, noting that the 95%
upper bound is 3.27 and the 95% lower bound is 0.056.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing" class="section level2 hasAnchor" number="1.17">
<h2><span class="header-section-number">1.17</span> Hypothesis Testing<a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hypothesis testing is, in short, an inference mechanism that takes a
pre-conceived notion about <span class="math inline">\(\theta\)</span> into account. This is in contrast to
both point estimation and interval estimation, where we derive <span class="math inline">\(\hat{\theta}\)</span>
or <span class="math inline">\((\hat{\theta}_L,\hat{\theta}_U)\)</span> without regard to what we
might think the true value of <span class="math inline">\(\theta\)</span> might be.
Perhaps we have randomly sampled
a set of students and recorded their weights. Point estimation provides
an estimate of the average student weight; interval estimation provides
an interval that has probability <span class="math inline">\(1-\alpha\)</span> of overlapping the true student
weight; but hypothesis testing allows one to ask questions like do my
data support my hypothesis that the true average weight of students on campus
is 140 pounds?</p>
<p>The steps of basic hypothesis testing are as follows.</p>
<ol style="list-style-type: decimal">
<li>We formulate a <em>null hypothesis</em>, denoted <span class="math inline">\(H_o\)</span> (H naught),
in which we specify the value we are testing. Here, that hypothesis
might be <span class="math inline">\(H_o: \mu = 140\)</span>.</li>
<li>We formulate an <em>alternate</em> or <em>alternative hypothesis</em>, denoted
<span class="math inline">\(H_a\)</span> (H a). This also takes our pre-conceived notions into account:
do we wish to test whether the average student weight is higher, lower,
or simply substantially different from <span class="math inline">\(H_o\)</span>? (So here, the possibilities
are <span class="math inline">\(H_a: \mu &lt; 140\)</span>, <span class="math inline">\(H_a: \mu &gt; 140\)</span>, or <span class="math inline">\(H_a: \mu \neq 140\)</span>.)</li>
<li>We choose a <em>test statistic</em> (e.g., <span class="math inline">\(\bar{X}\)</span>) whose sampling distribution
we can specify <em>assuming <span class="math inline">\(H_o\)</span> is true</em>.</li>
<li>We choose the <em>level of the test</em> <span class="math inline">\(\alpha\)</span>. Combined with the sampling
distribution
for the test statistic, this allows us to determine a <em>rejection region</em>: if
the test statistic falls into this region, we reject the null
hypothesisotherwise, we fail to reject the null.</li>
</ol>
<p>There is much to unpack here.</p>
<p>We will start by stating that it is imperative
that one specify <span class="math inline">\(H_o\)</span>, <span class="math inline">\(H_a\)</span>, and <span class="math inline">\(\alpha\)</span> <strong>before any data are
collected</strong> (or at the very least before any data are examined).
To look at the data first and
then formulate hypotheses and set levels acts to bias the process: it is a
manifestation of human nature that we might be tempted to define the test so
as to maximize our chances observing the result we desire to achieve.
In the end, testing is
about assessing <em>pre-conceived notions</em>; actively working to achieve
a particular result should never be our aim.</p>
<p>Another thing to keep in mind is that hypothesis testing generates
<em>decisions</em>, not proofs. When we formulate a test, we are defining a
decision boundary: we decide whether we have sufficient evidence to reject the
null. When we do not, we fail to reject the null, i.e.,
we do not say that we have proven that the null is correct,
but rather we have concluded that we
simply do not have enough data to convince ourselves that the null hypothesis
is wrong. And the decisions we make can be wrong! This leads to some more
important hypothesis-testing terminology:</p>
<ul>
<li><em>Type I error</em>: this is the probability that we will decide to
reject the null when
it is actually true. We set thisthis is <span class="math inline">\(\alpha\)</span> (the level of the test).
A conventional choice
for <span class="math inline">\(\alpha\)</span> is 0.05: if the null is true, we have a 1 in 20 chance of
erroneously rejecting it.</li>
<li><em>Type II error</em>: this is the probability that we would fail to reject the
null given an arbitrary value of <span class="math inline">\(\theta\)</span>;
it is denoted <span class="math inline">\(\beta\)</span> (or perhaps more clearly <span class="math inline">\(\beta(\theta,\alpha)\)</span>,
since the value is a function of both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\alpha\)</span>).</li>
</ul>
<p>Now that we have laid out the basic framework and terminology, we need to
illustrate how hypothesis testing works in practice.
But we have the same issue here that we have when constructing confidence
intervals in the last section:
we do not yet have the tools to derive the sampling distribution for the
test statistic. (We introduce these later, beginning in Chapter 2.)
So, for now, we illustrate hypothesis testing by assuming that
we sample a single datum <span class="math inline">\(X\)</span> from a pdf (which is, by definition,
the sampling distribution for our test statistic <span class="math inline">\(Y = X\)</span>).</p>
<p>Lets assume that our statistic <span class="math inline">\(Y\)</span> is drawn from the following pdf:
<span class="math display">\[
f_Y(y) = \frac{1}{\theta} \exp\left(-\frac{y}{\theta}\right) \,,
\]</span>
where <span class="math inline">\(\theta &gt; 0\)</span> and <span class="math inline">\(x \geq 0\)</span>. (See Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:rr">1.34</a>.)
Lets further assume that our null hypothesis is <span class="math inline">\(\theta_o = 2\)</span>.
To reiterate: if the null hypothesis holds, we expect to
observe a statistic <span class="math inline">\(y_{\rm obs}\)</span> that is consistent with this distribution.
The question
now is: how close or far from zero does <span class="math inline">\(y_{\rm obs}\)</span> have to be for us
to decide to reject the null?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rr"></span>
<img src="_main_files/figure-html/rr-1.png" alt="\label{fig:rr}Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for $\alpha = 0.1$. Each curve represents the sampling distribution for the hypothesis test statistic $Y$, given that the null hypothesis is true. If $y_{\rm obs}$ falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null." width="30%" /><img src="_main_files/figure-html/rr-2.png" alt="\label{fig:rr}Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for $\alpha = 0.1$. Each curve represents the sampling distribution for the hypothesis test statistic $Y$, given that the null hypothesis is true. If $y_{\rm obs}$ falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null." width="30%" /><img src="_main_files/figure-html/rr-3.png" alt="\label{fig:rr}Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for $\alpha = 0.1$. Each curve represents the sampling distribution for the hypothesis test statistic $Y$, given that the null hypothesis is true. If $y_{\rm obs}$ falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null." width="30%" />
<p class="caption">
Figure 1.34: Illustrations of rejection regions for upper-tail (left), lower-tail (center), and two-tail (right) hypothesis tests, for <span class="math inline">\(\alpha = 0.1\)</span>. Each curve represents the sampling distribution for the hypothesis test statistic <span class="math inline">\(Y\)</span>, given that the null hypothesis is true. If <span class="math inline">\(y_{\rm obs}\)</span> falls into the shaded region for a given test, we reject the null; otherwise we fail to reject the null.
</p>
</div>
<p>In the section before last, we discuss how we would answer this
question for a two-tail hypothesis test:
we determine the roots <span class="math inline">\(y_{\alpha/2}\)</span> and <span class="math inline">\(y_{1-\alpha/2}\)</span> that are solutions
to the following equations:
<span class="math display">\[\begin{align*}
F_Y(y_{\alpha/2} \vert \theta_o) - \frac{\alpha}{2} &amp;= 0 ~~\Rightarrow~~ y_{\alpha/2} = F_Y^{-1}(\alpha/2 \vert \theta_o) \\
F_Y(y_{1-\alpha/2} \vert \theta_o) - \left(1-\frac{\alpha}{2}\right) &amp;= 0 ~~\Rightarrow~~ y_{1-\alpha/2} = F_Y^{-1}(1-\alpha/2 \vert \theta_o)\,,
\end{align*}\]</span>
where <span class="math inline">\(F_Y^{-1}(\cdot)\)</span> is the inverse cdf function associated with the
sampling distribution. Both <span class="math inline">\(y_{\alpha/2}\)</span> and <span class="math inline">\(y_{1-\alpha/2}\)</span> are
rejection region boundaries: if <span class="math inline">\(y_{\rm obs} &lt; y_{\alpha/2}\)</span> or
<span class="math inline">\(y_{\rm obs} &gt; y_{1-\alpha/2}\)</span>, we make the decision to reject the null.
For the example we are working with here:
<span class="math display">\[
F_Y(y) = \int_0^y \frac{1}{\theta} \exp\left(-\frac{z}{\theta}\right) dz
= -\exp\left.\left(-\frac{z}{\theta}\right)\right|_0^y = 1 - \exp\left(-\frac{y}{\theta}\right) \,,
\]</span>
and so
<span class="math display">\[
1 - \exp\left(-\frac{y_{\alpha/2}}{\theta_o}\right) = \frac{\alpha}{2} ~~~ \Rightarrow ~~~ y_{\alpha/2} = -\theta_o \log\left(1-\frac{\alpha}{2}\right) \,,
\]</span>
and
<span class="math display">\[
1 - \exp\left(-\frac{y_{1-\alpha/2}}{\theta_o}\right) = 1-\frac{\alpha}{2} ~~~ \Rightarrow ~~~ y_{1-\alpha/2} = -\theta_o \log\left(\frac{\alpha}{2}\right) \,.
\]</span>
For <span class="math inline">\(\theta_o = 2\)</span> and <span class="math inline">\(\alpha = 0.05\)</span>, the rejection region bounds are
<span class="math inline">\(y_{\alpha/2} = -2\log(0.975) = 0.051\)</span> and
<span class="math inline">\(y_{1-\alpha/2} = -2\log(0.025) = 7.378\)</span>. In other words, if the value
we sample is <span class="math inline">\(&lt; 0.051\)</span> or <span class="math inline">\(&gt; 7.378\)</span>, we decide to reject the null
hypothesis that <span class="math inline">\(\theta = 2\)</span>.</p>
<p>If we wish to perform a lower- or upper-tail test instead, we need to tread
a bit more carefully, in that we need determine how <span class="math inline">\(E[Y]\)</span> varies with
<span class="math inline">\(\theta\)</span>: does it increase as <span class="math inline">\(\theta\)</span> increases, or does it decrease?
This dictates whether <span class="math inline">\(y_{\alpha}\)</span>, for instance, is the correct rejection
region bound (as opposed to <span class="math inline">\(y_{1-\alpha}\)</span>). For our current example,
<span class="math inline">\(E[Y] = \theta\)</span>, which obviously increases with <span class="math inline">\(\theta\)</span>and so
<span class="math inline">\(y_{\alpha}\)</span> is appropriate for lower-tail tests and <span class="math inline">\(y_{1-\alpha}\)</span> is
appropriate for upper-tail tests.
Our rejection regions are
<span class="math display">\[\begin{align*}
\mbox{lower-tail:} &amp;~~~ y_{\alpha} = -\theta_o \log(1-\alpha) = -2 \log(0.95) = 0.103 \\
\mbox{upper-tail:} &amp;~~~ y_{1-\alpha} = -\theta_o \log(\alpha) = -2 \log(0.05) = 5.991 \,.
\end{align*}\]</span></p>
<p>We summarize the evaluation of rejection region boundaries
in the table below. Note that as was the case with the confidence interval
reference table, the contents of this first hypothesis test reference table
are not to be memorized.</p>
<table>
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="34%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Type</th>
<th align="right"><span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span>?</th>
<th align="center">Rejection Region Boundary</th>
<th align="center">Reject If</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">two-tail</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(y_{\rm RR,lo} = F_Y^{-1}(\alpha/2 \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &lt; y_{\rm RR,lo}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="center"><span class="math inline">\(y_{\rm RR,hi} = F_Y^{-1}(1-\alpha/2 \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR,hi}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(y_{\rm RR,lo} = F_Y^{-1}(1-\alpha/2 \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &lt; y_{\rm RR,lo}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="center"><span class="math inline">\(y_{\rm RR,hi} = F_Y^{-1}(\alpha/2 \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR,hi}\)</span></td>
</tr>
<tr class="odd">
<td align="right">lower-tail</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(y_{\rm RR} = F_Y^{-1}(\alpha \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &lt; y_{\rm RR}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(y_{\rm RR} = F_Y^{-1}(1-\alpha \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR}\)</span></td>
</tr>
<tr class="odd">
<td align="right">upper-tail</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(y_{\rm RR} = F_Y^{-1}(1-\alpha \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(y_{\rm RR} = F_Y^{-1}(\alpha \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(y_{\rm obs} &lt; y_{\rm RR}\)</span></td>
</tr>
</tbody>
</table>
<p>Note that lower and upper refer to being less than <span class="math inline">\(\theta_o\)</span> or
greater than <span class="math inline">\(\theta_o\)</span>, and not necessarily to the tail of the sampling
distribution itself that contains the rejection region. (For instance,
if <span class="math inline">\(E[Y]\)</span> decreases with <span class="math inline">\(\theta\)</span>, then the upper tail for <span class="math inline">\(\theta\)</span> would
map to the lower tail for <span class="math inline">\(Y\)</span> itself.)
Also, we note the following.</p>
<ul>
<li>If we reject the null using
a one-tail test, for a given dataset, it is not guaranteed that we would have
rejected the null using a two-tail test. For instance, for our example,
a value <span class="math inline">\(y_{\rm obs} = 0.075\)</span> would lead us to reject the null for a
lower-tail test, but fail to reject the null for a two-tail test.</li>
<li>If we reject the null using
a two-tail test, it is also not guaranteed that we would have rejected the
null using a one-tail test. For instance, a value of <span class="math inline">\(y_{\rm obs} = 8\)</span> would
lead us to reject the null if we perform a two-tail test, but fail to reject
the null if we perform a <em>lower-tail</em> test.</li>
</ul>
<hr />
<div id="hypothesis-test-where-ey-increases-with-theta" class="section level3 hasAnchor" number="1.17.1">
<h3><span class="header-section-number">1.17.1</span> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We repeat the first example in the confidence interval section above,
in which <span class="math inline">\(f_X(x) = \theta x^{\theta-1}\)</span> for <span class="math inline">\(x \in [0,1]\)</span>, and in which
we sample a single datum (and thus assume that our test statistic
is <span class="math inline">\(Y = X\)</span>, with sampling distribution given by <span class="math inline">\(f_X(x)\)</span>). Lets
assume our null hypothesis is <span class="math inline">\(H_o : \theta = \theta_o = 1\)</span>, and
that we are testing this against the alternative <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>.
What is the rejection region for this test, assuming <span class="math inline">\(\alpha = 0.05\)</span>?</p>
</blockquote>
<blockquote>
<p>We start by reminding ourselves that <span class="math inline">\(E[Y] = \theta/(\theta+1)\)</span> and that
it increases with <span class="math inline">\(\theta\)</span>. Given this information, we examine the table
above and see that the rejection region boundary is
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(\alpha \vert \theta_o) \,.
\]</span>
Stated another way, the boundary is the solution to the equation
<span class="math display">\[
F_Y(y \vert \theta_o) - \alpha = 0 \,.
\]</span></p>
</blockquote>
<blockquote>
<p>It is important to point out here, before continuing,
that <em>the numerical evaluation of rejection region boundaries only
depends on <span class="math inline">\(\theta_o\)</span></em>, and specifically not on the observed statistic
value. (<em>Where</em> the regions are, and whether there are one or two,
is a function of the alternative hypothesis, but not the numerical
evaluation of the boundaries themselves.)</p>
</blockquote>
<blockquote>
<p>The cdf for <span class="math inline">\(Y\)</span> is <span class="math inline">\(F_Y(y) = y^\theta\)</span>. Hence
<span class="math display">\[\begin{align*}
F_Y(y_{\alpha} \vert \theta_o) - \alpha &amp;= 0 \\
\Rightarrow ~~~ y_{\alpha}^{\theta_o} &amp;= \alpha \\
\Rightarrow ~~~ y_{\alpha} &amp;= \alpha^{1/\theta_o} = \alpha \,.
\end{align*}\]</span>
Thus if we observe <span class="math inline">\(y_{\rm obs} = x &lt; \alpha = 0.05\)</span>, we reject the
null hypothesis.</p>
</blockquote>
<hr />
</div>
<div id="hypothesis-test-where-ey-decreases-with-theta" class="section level3 hasAnchor" number="1.17.2">
<h3><span class="header-section-number">1.17.2</span> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here we repeat the second example in the confidence interval section above,
in which <span class="math inline">\(f_X(x) = \theta (1-x)^{\theta - 1}\)</span> for <span class="math inline">\(x \in [0,1]\)</span>, and in which
we again sample a single datum. Here, we assume the null hypothesis
<span class="math inline">\(H_o : \theta = \theta_o = 2\)</span>, and will we test this against the
alternative <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>.</p>
</blockquote>
<blockquote>
<p>We know that <span class="math inline">\(E[Y] = 1/(\theta+1)\)</span>, which <em>decreases</em> with <span class="math inline">\(\theta\)</span>. Thus
we define the rejection region boundary as
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(1-\alpha \vert \theta_o) \,.
\]</span>
We know that <span class="math inline">\(F_Y(y) = 1 - (1-y)^\theta\)</span>. So
<span class="math display">\[\begin{align*}
F_Y(y_{1-\alpha} \vert \theta_o) - (1-\alpha) &amp;= 0 \\
\Rightarrow ~~~ 1 - (1-y_{1-\alpha})^{\theta_o} &amp;= 1-\alpha \\
\Rightarrow ~~~ (1-y_{1-\alpha})^{\theta_o} &amp;= \alpha \\
\Rightarrow ~~~ y_{1-\alpha} &amp;= 1 - \alpha^{1/\theta_o} = 1 - \alpha^{1/2} \,.
\end{align*}\]</span>
Thus if we observe <span class="math inline">\(y_{\rm obs} = x &gt; 1 - \alpha^{1/2} =
1 - 0.05^{1/2} = 0.776\)</span>, we reject the null hypothesis.</p>
</blockquote>
</div>
</div>
<div id="working-with-r-simulating-statistical-inference" class="section level2 hasAnchor" number="1.18">
<h2><span class="header-section-number">1.18</span> Working With R: Simulating Statistical Inference<a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We wrap up this chapter by discussing how one might simulate aspects
of statistical inference. For instance, we might want to know, e.g.,</p>
<ul>
<li>the empirical distribution of a maximum likelihood estimator
<span class="math inline">\(\hat{\theta}_{MLE}\)</span>, from which we can estimate the standard error
or the mean-squared error;</li>
<li>whether a proposed confidence interval has its advertised coverage; and</li>
<li>whether a proposed hypothesis test has its advertised Type I error, etc.</li>
</ul>
<p>All of these examples utilize the same basic algorithm, which we render in
pseudocode as</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-1" tabindex="-1"></a>set random number seed</span>
<span id="cb45-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-2" tabindex="-1"></a></span>
<span id="cb45-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-3" tabindex="-1"></a>initialize <span class="fu">vector</span>(s) <span class="cf">for</span> storing <span class="fu">result</span>(s)</span>
<span id="cb45-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-4" tabindex="-1"></a></span>
<span id="cb45-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-5" tabindex="-1"></a>begin <span class="cf">for</span><span class="sc">-</span>loop</span>
<span id="cb45-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-6" tabindex="-1"></a>  sample dataset</span>
<span id="cb45-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-7" tabindex="-1"></a>  compute statistic</span>
<span id="cb45-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-8" tabindex="-1"></a>  compute and store result</span>
<span id="cb45-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-9" tabindex="-1"></a>end <span class="cf">for</span><span class="sc">-</span>loop</span>
<span id="cb45-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-10" tabindex="-1"></a></span>
<span id="cb45-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-11" tabindex="-1"></a>optional<span class="sc">:</span> process result vector </span>
<span id="cb45-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-12" tabindex="-1"></a></span>
<span id="cb45-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb45-13" tabindex="-1"></a>display result</span></code></pre></div>
<p>This builds upon the last Working With <code>R</code> section, which describes
dataset sampling. Note that we dont always need
to follow this pseudocoded algorithm to the letter, as <code>R</code>s vectorization
facility plus its <code>apply()</code> function can help us both eliminate the
<code>for</code> loop. We illustrate these shortcuts in the examples below.</p>
<hr />
<div id="estimating-the-sampling-distribution-for-the-sample-median" class="section level3 hasAnchor" number="1.18.1">
<h3><span class="header-section-number">1.18.1</span> Estimating the Sampling Distribution for the Sample Median<a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The median of a data sample is defined as the middle value of the sorted
data: if we have <span class="math inline">\(n=5\)</span> data, for instance, the median is <span class="math inline">\(x_{(3)}\)</span>.
(Recall that the parentheses indicating that we are examining sorted
data, with <span class="math inline">\(x_{(1)}\)</span> and <span class="math inline">\(x_{(n)}\)</span> being the smallest and largest
observed values.)
Here, we show how to simulate the distribution of a sample median.
In Chapter 3,
we show how we can attempt to derive distributions related to sorted
data mathematically.</p>
</blockquote>
<blockquote>
<p>Lets assume that we draw data according to the following distribution:
<span class="math display">\[
f_X(x) = \left\{ \begin{array}{cl} \frac{1}{\pi} x \sin x &amp; 0 \leq x \leq \pi \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,,
\]</span>
To simulate the distribution of the sample median, for a given sample size
<span class="math inline">\(n\)</span>, we repeatedly sample datasets (here, we use our rejection sampling
code defined earlier in this chapter) and we record the median value for
each. In the code chunk below, we show the estimated mean and standard
error for our distribution, while in Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:sampmed">1.35</a> we show
the empirical distribution of the median values.</p>
</blockquote>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb46-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-2" tabindex="-1"></a></span>
<span id="cb46-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-3" tabindex="-1"></a><span class="co"># Let&#39;s put the rejection sampling code inside a function.</span></span>
<span id="cb46-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-4" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n,x.lo,x.hi,f.x.hi)</span>
<span id="cb46-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-5" tabindex="-1"></a>{</span>
<span id="cb46-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-6" tabindex="-1"></a>  X   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,n)</span>
<span id="cb46-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-7" tabindex="-1"></a>  ii  <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb46-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-8" tabindex="-1"></a>  <span class="cf">while</span> ( ii <span class="sc">&lt;</span> n ) {</span>
<span id="cb46-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-9" tabindex="-1"></a>    u <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>,<span class="at">min=</span>x.lo,<span class="at">max=</span>x.hi)</span>
<span id="cb46-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-10" tabindex="-1"></a>    v <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span>f.x.hi)</span>
<span id="cb46-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-11" tabindex="-1"></a>    <span class="cf">if</span> ( v <span class="sc">&lt;</span> u<span class="sc">*</span><span class="fu">sin</span>(u)<span class="sc">/</span>pi ) {</span>
<span id="cb46-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-12" tabindex="-1"></a>      ii    <span class="ot">&lt;-</span> ii<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb46-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-13" tabindex="-1"></a>      X[ii] <span class="ot">&lt;-</span> u</span>
<span id="cb46-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-14" tabindex="-1"></a>    }</span>
<span id="cb46-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-15" tabindex="-1"></a>  }</span>
<span id="cb46-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-16" tabindex="-1"></a>  <span class="fu">return</span>(X)</span>
<span id="cb46-17"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-17" tabindex="-1"></a>}</span>
<span id="cb46-18"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-18" tabindex="-1"></a></span>
<span id="cb46-19"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-19" tabindex="-1"></a>num.sim  <span class="ot">&lt;-</span> <span class="dv">10000</span>      <span class="co"># number of medians to record</span></span>
<span id="cb46-20"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-20" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">11</span></span>
<span id="cb46-21"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-21" tabindex="-1"></a>x.lo     <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb46-22"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-22" tabindex="-1"></a>x.hi     <span class="ot">&lt;-</span> pi</span>
<span id="cb46-23"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-23" tabindex="-1"></a>f.x.hi   <span class="ot">&lt;-</span> <span class="fl">0.58</span></span>
<span id="cb46-24"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-24" tabindex="-1"></a>X.median <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim) <span class="co"># initialize vector to store result</span></span>
<span id="cb46-25"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-25" tabindex="-1"></a></span>
<span id="cb46-26"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-26" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb46-27"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-27" tabindex="-1"></a>  X            <span class="ot">&lt;-</span> <span class="fu">sample_data</span>(n,x.lo,x.hi,f.x.hi)</span>
<span id="cb46-28"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-28" tabindex="-1"></a>  X.median[ii] <span class="ot">&lt;-</span> <span class="fu">median</span>(X)</span>
<span id="cb46-29"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-29" tabindex="-1"></a>}</span>
<span id="cb46-30"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-30" tabindex="-1"></a></span>
<span id="cb46-31"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-31" tabindex="-1"></a><span class="co"># What do we want to do with the result? Here...</span></span>
<span id="cb46-32"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-32" tabindex="-1"></a><span class="co">#   - we visualize the empirical distribution of medians with a histogram</span></span>
<span id="cb46-33"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-33" tabindex="-1"></a><span class="co">#   - we process the median vector to estimate the mean and standard error</span></span>
<span id="cb46-34"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-34" tabindex="-1"></a></span>
<span id="cb46-35"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-35" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">X.median=</span>X.median),<span class="fu">aes</span>(<span class="at">x=</span>X.median,<span class="at">y=</span><span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb46-36"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-36" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="fl">2.7</span>,<span class="at">by=</span><span class="fl">0.1</span>)) <span class="sc">+</span></span>
<span id="cb46-37"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-37" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;x&quot;</span>) <span class="sc">+</span></span>
<span id="cb46-38"><a href="the-basics-of-probability-and-statistical-inference.html#cb46-38" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampmed"></span>
<img src="_main_files/figure-html/sampmed-1.png" alt="\label{fig:sampmed}The empirical distribution of the sample median for the pdf $f_X(x) = (x \sin x)/\pi$, assuming a sample size of $n = 11$." width="50%" />
<p class="caption">
Figure 1.35: The empirical distribution of the sample median for the pdf <span class="math inline">\(f_X(x) = (x \sin x)/\pi\)</span>, assuming a sample size of <span class="math inline">\(n = 11\)</span>.
</p>
</div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb47-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical mean is           &quot;</span>,<span class="fu">round</span>(<span class="fu">mean</span>(X.median),<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical mean is            1.907</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb49-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical standard error is &quot;</span>,<span class="fu">round</span>(<span class="fu">sd</span>(X.median),<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical standard error is  0.255</code></pre>
<hr />
</div>
<div id="the-empirical-distribution-of-maximum-likelihood-estimates" class="section level3 hasAnchor" number="1.18.2">
<h3><span class="header-section-number">1.18.2</span> The Empirical Distribution of Maximum Likelihood Estimates<a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>To drive home the point that maximum likelihood estimates are random
variables, we simulate the process of estimating <span class="math inline">\(\theta\)</span> for the
pdf <span class="math inline">\(f_X(x) = \theta x^{\theta-1}\)</span>, where <span class="math inline">\(x \in [0,1]\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets write down the (log-)likelihood:
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) &amp;= \prod_{i=1}^n \theta x_i^{\theta-1} \\
&amp;= \theta^n \left( \prod_{i=1}^n x_i \right)^{\theta - 1} \\
\Rightarrow ~ \ell(\theta \vert \mathbf{x}) &amp;= n \log \theta + (\theta-1)\log \left( \prod_{i=1}^n x_i \right) \\
&amp;= n \log \theta + (\theta-1) \sum_{i=1}^n \log x_i \,,
\end{align*}\]</span>
and solve for the MLE:
<span class="math display">\[
\frac{d}{d\theta} \ell(\theta \vert \mathbf{x}) = \frac{n}{\theta} + \sum_{i=1}^n \log x_i ~ \Rightarrow ~ \hat{\theta}_{MLE} = -\frac{n}{\sum_{i=1}^n \log x_i} \,.
\]</span>
Given this expression, we can now determine the empirical distribution. See
Figure <a href="the-basics-of-probability-and-statistical-inference.html#fig:empmle">1.36</a>. We observe immediately that the distribution
is right-skewed. (As we will see in Chapter 2, as
<span class="math inline">\(n \rightarrow \infty\)</span>, the empirical distribution will tend more and more
to a bell-curve-like shape.)</p>
</blockquote>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb51-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-2" tabindex="-1"></a>theta  <span class="ot">&lt;-</span> <span class="fl">2.25</span></span>
<span id="cb51-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-3" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb51-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-4" tabindex="-1"></a></span>
<span id="cb51-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-5" tabindex="-1"></a><span class="co"># Here, we show an alternative means by which to sample data.</span></span>
<span id="cb51-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-6" tabindex="-1"></a><span class="co"># (This assumes that there is a data-sampling code that we can easily call.)</span></span>
<span id="cb51-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-7" tabindex="-1"></a></span>
<span id="cb51-8"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-8" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="cf">function</span>(n,theta)</span>
<span id="cb51-9"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-9" tabindex="-1"></a>{</span>
<span id="cb51-10"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-10" tabindex="-1"></a>  q <span class="ot">&lt;-</span> <span class="fu">runif</span>(n,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">1</span>)</span>
<span id="cb51-11"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-11" tabindex="-1"></a>  X <span class="ot">&lt;-</span> q<span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>theta)</span>
<span id="cb51-12"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-12" tabindex="-1"></a>}</span>
<span id="cb51-13"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-13" tabindex="-1"></a></span>
<span id="cb51-14"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-14" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb51-15"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-15" tabindex="-1"></a></span>
<span id="cb51-16"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-16" tabindex="-1"></a><span class="co"># first: create one long vector of data, of length num.sim * n</span></span>
<span id="cb51-17"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-17" tabindex="-1"></a>X.all   <span class="ot">&lt;-</span> <span class="fu">sample_data</span>(n<span class="sc">*</span>num.sim,theta)</span>
<span id="cb51-18"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-18" tabindex="-1"></a></span>
<span id="cb51-19"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-19" tabindex="-1"></a><span class="co"># second: &quot;fold&quot; the vector into a matrix with num.sim rows and n columns</span></span>
<span id="cb51-20"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-20" tabindex="-1"></a><span class="co">#         every row of X is thus a separate independent dataset</span></span>
<span id="cb51-21"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-21" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">matrix</span>(X.all,<span class="at">nrow=</span>num.sim)</span>
<span id="cb51-22"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-22" tabindex="-1"></a></span>
<span id="cb51-23"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-23" tabindex="-1"></a><span class="co"># third: use R&#39;s apply() function to compute MLE for each row/dataset</span></span>
<span id="cb51-24"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-24" tabindex="-1"></a><span class="co">#        the argument 1 means &quot;apply the following function to each row&quot;</span></span>
<span id="cb51-25"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-25" tabindex="-1"></a><span class="co">#        x in the third argument is the data row; the function returns the mle</span></span>
<span id="cb51-26"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-26" tabindex="-1"></a><span class="co">#        mle is thus a vector of maximum-likelihood estimates</span></span>
<span id="cb51-27"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-27" tabindex="-1"></a>mle     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,<span class="cf">function</span>(x){<span class="sc">-</span>n<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">log</span>(x))})</span>
<span id="cb51-28"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-28" tabindex="-1"></a></span>
<span id="cb51-29"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-29" tabindex="-1"></a><span class="co"># What do we want to do with the result? Here...</span></span>
<span id="cb51-30"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-30" tabindex="-1"></a><span class="co">#   - we visualize the empirical distribution of medians with a histogram</span></span>
<span id="cb51-31"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-31" tabindex="-1"></a><span class="co">#   - we process the median vector to estimate the mean and standard error</span></span>
<span id="cb51-32"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-32" tabindex="-1"></a></span>
<span id="cb51-33"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-33" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">mle=</span>mle),<span class="fu">aes</span>(<span class="at">x=</span>mle,<span class="at">y=</span><span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb51-34"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-34" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="fl">1.2</span>,<span class="fl">4.2</span>,<span class="at">by=</span><span class="fl">0.2</span>)) <span class="sc">+</span></span>
<span id="cb51-35"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-35" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="fl">2.25</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb51-36"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-36" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(theta)) <span class="sc">+</span></span>
<span id="cb51-37"><a href="the-basics-of-probability-and-statistical-inference.html#cb51-37" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:empmle"></span>
<img src="_main_files/figure-html/empmle-1.png" alt="\label{fig:empmle}The empirical distribution of maximum likelihood estimates for $\theta$ for the distribution $f_X(x) = \theta x^{\theta-1}$ ($x \in [0,1]$) with $\theta = 2.25$ (red line) and $n = 40$." width="50%" />
<p class="caption">
Figure 1.36: The empirical distribution of maximum likelihood estimates for <span class="math inline">\(\theta\)</span> for the distribution <span class="math inline">\(f_X(x) = \theta x^{\theta-1}\)</span> (<span class="math inline">\(x \in [0,1]\)</span>) with <span class="math inline">\(\theta = 2.25\)</span> (red line) and <span class="math inline">\(n = 40\)</span>.
</p>
</div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb52-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical mean is               &quot;</span>,<span class="fu">round</span>(<span class="fu">mean</span>(mle),<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical mean is                2.317</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb54-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical standard deviation is &quot;</span>,<span class="fu">round</span>(<span class="fu">sd</span>(mle),<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical standard deviation is  0.377</code></pre>
<hr />
</div>
<div id="empirically-verifying-the-confidence-coefficient-value" class="section level3 hasAnchor" number="1.18.3">
<h3><span class="header-section-number">1.18.3</span> Empirically Verifying the Confidence Coefficient Value<a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the example above, where
<span class="math display">\[
f_X(x) = \theta x^{\theta-1} \,,
\]</span>
with <span class="math inline">\(\theta &gt; 0\)</span> and <span class="math inline">\(x \in [0,1]\)</span>, we find the
two-sided confidence interval
<span class="math display">\[
\left[ -\frac{0.0253}{\log y_{\rm obs}} , -\frac{3.689}{\log y_{\rm obs}} \right] \,,
\]</span>
where we assume a confidence coefficient of <span class="math inline">\(\alpha = 0.05\)</span>.
A question that naturally arises is: does this confidence interval actually
overlap, or cover, the true value <span class="math inline">\(\theta\)</span> 95% of the time? We can check
this with a relatively simple simulation.</p>
</blockquote>
<blockquote>
<p>Lets assume that the true value of <span class="math inline">\(\theta\)</span> is 1. Lets also specify here
that <span class="math inline">\(f_X(x)\)</span> is part of the <em>beta</em> family of distributions, which we
officially introduce in Chapter 3. (This allows us to utilize the
<code>R</code> random variable sampling function <code>rbeta()</code> rather than having to
create our own inverse-transform data sampler.)</p>
</blockquote>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb56-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-2" tabindex="-1"></a>num.sim     <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb56-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-3" tabindex="-1"></a>theta       <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb56-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-4" tabindex="-1"></a>y.obs       <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(num.sim,theta,<span class="dv">1</span>) <span class="co"># R can do the sampling for us here</span></span>
<span id="cb56-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-5" tabindex="-1"></a>hat.theta.L <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.0253</span><span class="sc">/</span><span class="fu">log</span>(y.obs)</span>
<span id="cb56-6"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-6" tabindex="-1"></a>hat.theta.U <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">3.689</span><span class="sc">/</span><span class="fu">log</span>(y.obs)</span>
<span id="cb56-7"><a href="the-basics-of-probability-and-statistical-inference.html#cb56-7" tabindex="-1"></a>(coverage   <span class="ot">&lt;-</span> <span class="fu">sum</span>(theta<span class="sc">&gt;=</span>hat.theta.L <span class="sc">&amp;</span> theta<span class="sc">&lt;=</span>hat.theta.U)<span class="sc">/</span>num.sim)</span></code></pre></div>
<pre><code>## [1] 0.9523</code></pre>
<blockquote>
<p>Before discussing the result, lets talk about what is happening
in the last line of code above. First of all, by surrounding this line
with parentheses, we are telling <code>R</code> to not only assign
a value to the variable <code>coverage</code>, but to print the value of
<code>coverage</code> as well. As far as the argument passed to <code>sum()</code>:
<code>theta&gt;=hat.theta.L</code> is a logical comparison, an implicit function call
that returns <code>TRUE</code> if <span class="math inline">\(\theta \geq \hat{\theta}_L\)</span> and <code>FALSE</code> otherwise.
The ampersand <code>&amp;</code> is the logical <code>AND</code> operator that combines the
results of the comparison on the left and the one on the right:
<code>TRUE &amp; TRUE</code> returns <code>TRUE</code>, otherwise <code>FALSE</code> is returned. (This
stands in contrast to the logical <code>OR</code> operator, <code>|</code>, which only returns
<code>FALSE</code> for <code>FALSE | FALSE</code>.) So the long argument passed to <code>sum()</code>
turns into a logical vector of <code>TRUE</code>s (if the true value is inside
the bounds) and <code>FALSE</code>s (if the true value is outside the bounds).
What happens when we pass a logical vector to <code>sum()</code>? It treats <code>TRUE</code>
as 1 and <code>FALSE</code> as 0in other words, <code>sum()</code> returns the number
of <code>TRUE</code> values.</p>
</blockquote>
<blockquote>
<p>We find that 9,523 of the 10,000 intervals overlap <span class="math inline">\(\theta = 1\)</span>.
Because we do not perform an infinite number of simulations, the
coverage that we compute is a random variable: we do not expect the
value to match 9500 exactly. Thus what we observe is
very strong evidence that our interval construction algorithm has the
coverage that we defined it to have. (Once we introduce the binomial
distribution in Chapter 3, we can turn this strong evidence into
something more firmly quantitative!)</p>
</blockquote>
<hr />
</div>
<div id="empirically-verifying-the-type-i-error-alpha" class="section level3 hasAnchor" number="1.18.4">
<h3><span class="header-section-number">1.18.4</span> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the second example in the last section above, we determine if
we sample a statistic <span class="math inline">\(Y\)</span> from the distribution
<span class="math inline">\(f_Y(y) = \theta (1-y)^{\theta-1}\)</span>, with <span class="math inline">\(y \in [0,1]\)</span>, then if
<span class="math inline">\(y_{\rm obs} \geq 0.776\)</span>,
we reject the null hypothesis that <span class="math inline">\(\theta = 1\)</span>. This threshold is
based on our adoption of the Type I error value <span class="math inline">\(\alpha = 0.05\)</span>: if
the null is correct and we repeatedly randomly sample data from the
sampling distribution <span class="math inline">\(f_Y(y) = 2y\)</span>, we should observe values of
<span class="math inline">\(y\)</span> larger than 0.776 only five percent of the time. Is this what we
actually observe?</p>
</blockquote>
<blockquote>
<p>As a reminder, we have identified <span class="math inline">\(f_Y(y)\)</span> as being part of the
beta family of distributions, which we
officially introduce in Chapter 3; thus we can utilize <code>rbeta()</code> like
we do in the last example above.</p>
</blockquote>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="the-basics-of-probability-and-statistical-inference.html#cb58-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb58-2"><a href="the-basics-of-probability-and-statistical-inference.html#cb58-2" tabindex="-1"></a>theta       <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb58-3"><a href="the-basics-of-probability-and-statistical-inference.html#cb58-3" tabindex="-1"></a>num.sim     <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb58-4"><a href="the-basics-of-probability-and-statistical-inference.html#cb58-4" tabindex="-1"></a>y.obs       <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(num.sim,<span class="dv">1</span>,theta)</span>
<span id="cb58-5"><a href="the-basics-of-probability-and-statistical-inference.html#cb58-5" tabindex="-1"></a>(typeIerror <span class="ot">&lt;-</span> <span class="fu">sum</span>(y.obs<span class="sc">&gt;</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">sqrt</span>(<span class="fl">0.05</span>)))<span class="sc">/</span>num.sim)</span></code></pre></div>
<pre><code>## [1] 0.0508</code></pre>
<blockquote>
<p>We find that we make Type I errors 508 times, or that the empirical
Type I error rate is 508/10,000 = 0.0508.
Because we do not perform an infinite number of simulations, the
coverage that we compute is a random variable: we do not expect the
value to match 500 exactly. Thus what we observe is
very strong evidence that our Type I error rate is
what we expect it to be; in Chapter 3, we learn how to
turn this evidence into something more quantitative!)</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-normal-and-related-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

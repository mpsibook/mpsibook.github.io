<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 The Normal (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="2 The Normal (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="2 The Normal (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 The Normal (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-basics-of-probability-and-statistical-inference.html"/>
<link rel="next" href="the-binomial-and-related-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.3</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.5</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-normal-and-related-distributions" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> The Normal (and Related) Distributions<a href="the-normal-and-related-distributions.html#the-normal-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Motivation<a href="the-normal-and-related-distributions.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we illustrate probability and statistical inference
concepts utilizing the <em>normal distribution</em>. The normal, also known in
the physical sciences as the Gaussian distribution and, colloquially, as the
“bell curve,” is the most often utilized probability distribution in data
analyses, for a number of reasons.</p>
<ol style="list-style-type: decimal">
<li>We often observe that the data that we sample in many experiments
are at least approximately normally distributed. And while other, more general
families of distributions (such as the gamma distribution) might explain data
just as well as the normal, the normal has the advantage of having intuitively
easy-to-understand parameters: <span class="math inline">\(\mu\)</span> for the mean, and <span class="math inline">\(\sigma^2\)</span> for the
variance (meaning that <span class="math inline">\(\sigma\)</span> directly indicates the “width” of the region
on the real-number line over which <span class="math inline">\(f_X(x)\)</span> is effectively non-zero).</li>
<li>The normal distribution is the limiting distibution for many other
distributions (i.e., there are many families of distributions that, with the
right choice(s) for parameter value(s), can mimic the shape of the normal.</li>
<li>The normal distribution figures prominently in the Central Limit Theorem,
which states that the sample mean of a sufficiently large sample of data,
from any
distribution with finite variance, is approximately normally distributed.</li>
</ol>
</div>
<div id="probability-density-function" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Probability Density Function<a href="the-normal-and-related-distributions.html#probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a probability density function is one way to represent a continuous probablity distribution, and it has the properties (a) <span class="math inline">\(f_X(x) \geq 0\)</span> and (b) <span class="math inline">\(\int_x f_X(x) dx = 1\)</span>, where the integral is over all values of <span class="math inline">\(x\)</span> in the distribution’s domain.</em></p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable sampled from a normal distribution
with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span>.
The pdf for <span class="math inline">\(X\)</span> is
<span class="math display">\[
f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) ~~~~~~ x \in (-\infty,\infty) \,.
\]</span>
A first thing we notice about this pdf is that it is symmetric around <span class="math inline">\(\mu\)</span>.
A second thing that we notice is that the integral under this curve
approaches 1 over the range <span class="math inline">\(\mu - 3\sigma \leq x \leq \mu + 3\sigma\)</span>.
(The value of the integral of <span class="math inline">\(f_X(x)\)</span> over this range is 0.9973. This gives
rise to one aspect of the so-called <em>empirical rule</em>: if data are approximately
normally distributed, we expect all of the data to lie with three standard
deviations of the sample mean, with rare exceptions.) See Figure <a href="the-normal-and-related-distributions.html#fig:normpdf">2.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normpdf"></span>
<img src="_main_files/figure-html/normpdf-1.png" alt="\label{fig:normpdf}A normal probability density function with mean 0 and standard deviation 1." width="50%" />
<p class="caption">
Figure 2.1: A normal probability density function with mean 0 and standard deviation 1.
</p>
</div>
<p><strong>Recall:</strong> <em>the expected value of a continuously distributed random variable is</em>
<span class="math display">\[
E[X] = \int_x x f_X(x) dx\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(f_X(x)\)</span>.</em></p>
<p>The expected value of <span class="math inline">\(X\)</span> is
<span class="math display">\[
E[X] = \int_{-\infty}^\infty x \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) dx
\]</span>
We implement a variable substitution to evaluate this integral. Recall that
the three steps of variable substitution are…</p>
<ol style="list-style-type: decimal">
<li>to write down a viable substitution <span class="math inline">\(u = g(x)\)</span>;</li>
<li>to then derive <span class="math inline">\(du = h(u,x) dx\)</span>; and finally</li>
<li>to use <span class="math inline">\(u = g(x)\)</span> to transform the bounds of the integral.</li>
</ol>
<p>For this particular integral:
<span class="math display">\[
(1) ~~ u = \frac{x-\mu}{\sigma} ~~ \implies ~~ (2) ~~ du = \frac{1}{\sigma}dx
\]</span>
and
<span class="math display">\[
(3) ~~ x = -\infty ~\implies~ u = -\infty ~~~ \mbox{and} ~~~ x = \infty ~\implies~ u = \infty \,,
\]</span>
Thus
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_{-\infty}^\infty (\sigma u + \mu) \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{u^2}{2}\right) \sigma du \\
&amp;= \int_{-\infty}^\infty \frac{\sigma u}{\sqrt{2 \pi}} \exp\left(-\frac{u^2}{2}\right) du + \int_{-\infty}^\infty \frac{\mu}{\sqrt{2 \pi}} \exp\left(-\frac{u^2}{2}\right) du
\end{align*}\]</span>
The first integrand is the product of an odd function (<span class="math inline">\(u\)</span>) and an even
function (<span class="math inline">\(\exp(-u^2/2)\)</span>), and thus the first integral evaluates to zero.
The second integral is
<span class="math display">\[
E[X] = \mu \int_{-\infty}^\infty \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{u^2}{2}\right) du = \mu \,,
\]</span>
since the integrand is a normal pdf (with parameters
<span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2\)</span> = 1) and
the bounds of the integral match that of the domain of the normal pdf,
making the value of the integral 1.</p>
<hr />
<div id="variance-of-a-normal-probability-density-function" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Variance of a Normal Probability Density Function<a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the variance of a continuously distributed random variable is</em>
<span class="math display">\[
V[X] = \int_x (x-\mu)^2 f_X(x) dx = E[X^2] - (E[X])^2\,,
\]</span>
<em>where the integral is over all values of <span class="math inline">\(x\)</span> within the domain of the pdf <span class="math inline">\(f_X(x)\)</span>. The variance represents the square of the “width” of a probability density function, where by “width” we mean the range of values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(f_X(x)\)</span> is effectively non-zero.</em></p>
<blockquote>
<p>As seen above, we want to compute <span class="math inline">\(E[X^2] - (E[X])^2\)</span>. We have already
computed <span class="math inline">\(E[X]\)</span>: it is equal to <span class="math inline">\(\mu\)</span>. Here, we compute <span class="math inline">\(E[X^2]\)</span>.</p>
</blockquote>
<blockquote>
<p>The variable substitution setup is entirely the same, except that now
<span class="math display">\[\begin{align*}
E[X^2] &amp;= \int_{-\infty}^\infty (\sigma t + \mu)^2 \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{t^2}{2}\right) \sigma dt \\
&amp;= \int_{-\infty}^\infty \frac{\sigma^2 t^2}{\sqrt{2 \pi}} \exp\left(-\frac{t^2}{2}\right) dt + \int_{-\infty}^\infty \frac{2 \mu \sigma t}{\sqrt{2 \pi}} \exp\left(-\frac{t^2}{2}\right) dt + \int_{-\infty}^\infty \frac{\mu^2}{\sqrt{2 \pi}} \exp\left(-\frac{t^2}{2}\right) dt \,.
\end{align*}\]</span>
The second integral is that of an odd function and thus evaluates to zero, and
the third integral is, given results above, <span class="math inline">\(\mu^2\)</span>. This leaves
<span class="math display">\[
E[X^2] = \int_{-\infty}^\infty \frac{\sigma^2 t^2}{\sqrt{2 \pi}} \exp\left(-\frac{t^2}{2}\right) dt + \mu^2 \,.
\]</span>
We note that if we were to apply the shortcut formula, the <span class="math inline">\(\mu^2\)</span>
immediately above would cancel with <span class="math inline">\((E[X])^2 = \mu^2\)</span>, so now we can say
that
<span class="math display">\[
V[X] = \frac{\sigma^2}{\sqrt{2 \pi}} \int_{-\infty}^\infty t^2 \exp\left(-\frac{t^2}{2}\right) dt \,.
\]</span>
This requires integration by parts.
Setting aside the constants outside the integral, we have that
<span class="math display">\[
\begin{array}{ll} u = -t &amp; dv = -t \exp(-t^2/2) dt \\ du = -dt &amp; v = \exp(-t^2/2) \end{array} \,,
\]</span>
and so
<span class="math display">\[\begin{align*}
\int_{-\infty}^\infty t^2 \exp\left(-\frac{t^2}{2}\right) dt &amp;= \left. uv \right|_{-\infty}^{\infty} - \int_{-\infty}^\infty v du \\
&amp;= - \left. t \exp(-t^2/2) \right|_{-\infty}^{\infty} + \int_{-\infty}^\infty \exp(-t^2/2) dt \\
&amp;= 0 + \sqrt{2\pi} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} \exp(-t^2/2) dt \\
&amp;= \sqrt{2\pi} \,.
\end{align*}\]</span>
The first term evaluates to zero between for each bound, as <span class="math inline">\(e^{-t^2/2}\)</span> goes
to zero much faster than <span class="math inline">\(\vert t \vert\)</span> goes to infinity. As for the
second term: we recognize that the integrand is a normal pdf with
mean zero and variance one…so the integral by definition evaluates to 1.
In the end, after
restoring the constants we set aside above, we have that
<span class="math display">\[
V[X] = \frac{\sigma^2}{\sqrt{2 \pi}} \sqrt{2\pi} = \sigma^2 \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="skewness-of-the-normal-probability-density-function" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Skewness of the Normal Probability Density Function<a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The <em>skewness</em> of a pmf or pdf is a metric that indicates its level
of asymmetry. Fisher’s moment coefficient of skewness is
<span class="math display">\[
E\left[\left(\frac{X-\mu}{\sigma}\right)^3\right] \,.
\]</span>
which, expanded out, becomes
<span class="math display">\[
\frac{E[X^3] - 3 \mu \sigma^2 - \mu^3}{\sigma^3} \,.
\]</span>
What is the skewness of the normal distribution? At first, this appears
to require a long and involved series of integrations so as to solve
<span class="math inline">\(E[X^3]\)</span>. But let’s try to be more clever.</p>
</blockquote>
<blockquote>
<p>We know, from above, that the quantity <span class="math inline">\((\sigma t + \mu)^3\)</span> will appear
in the integral for <span class="math inline">\(E[X^3]\)</span>. Let’s expand this out:
<span class="math display">\[
\sigma^3 t^3 + 3 \sigma^2 \mu t^2 + 3 \sigma \mu^2 t + \mu^3 \,.
\]</span>
Each of these terms will appear separately in integrals of
<span class="math inline">\(\exp(-t^2/2)/\sqrt{2\pi}\)</span> over the domain <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>.
From above, what do we already know? First, we know that if <span class="math inline">\(t\)</span> is
raised to an odd power, the integral will be zero. This eliminates
<span class="math inline">\(\sigma^3 t^3\)</span> and <span class="math inline">\(3 \sigma \mu^2 t\)</span>. Second, we know that for
the <span class="math inline">\(t^0\)</span> term, the result will be <span class="math inline">\(\mu^3\)</span>, and we know that for
the <span class="math inline">\(t^2\)</span> term, the result will be <span class="math inline">\(3 \sigma^2 \mu\)</span>. Thus
<span class="math display">\[
E[X^3] = 3\sigma^2\mu + \mu^3
\]</span>
and the skewness is
<span class="math display">\[
\frac{3 \mu \sigma^2 + \mu^3 - 3 \mu \sigma^2 - \mu^3}{\sigma^3} = 0 \,.
\]</span>
The skewness is zero, meaning that a normal pdf is symmetric around
its mean <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="computing-probabilities" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Before diving into this example, we will be clear that this is <em>not</em>
the optimal way to compute probabilities associated with normal random
variables, as we should utilize <code>R</code>’s
<code>pnorm()</code> function, as shown in the next section.
However, showing how to utilize <code>integrate()</code> is
useful review. (We also show how to pass distribution parameters to
<code>integrate()</code>, which we did not do in the last chapter.)</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(10,4)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>To find this probability, we integrate over the normal pdf with mean
<span class="math inline">\(\mu = 10\)</span> and variance <span class="math inline">\(\sigma^2 = 4\)</span>. Visually, we are determining
the area of the red-shaded region in Figure <a href="the-normal-and-related-distributions.html#fig:pdfint">2.2</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdfint"></span>
<img src="_main_files/figure-html/pdfint-1.png" alt="\label{fig:pdfint}The probability $P(8 \leq X \leq 13.5)$ is the area of the red-shaded region, i.e., the integral of the normal probability density function from 8 to 13.5, assuming $\mu = 10$ and $\sigma^2 = 4$." width="50%" />
<p class="caption">
Figure 2.2: The probability <span class="math inline">\(P(8 \leq X \leq 13.5)\)</span> is the area of the red-shaded region, i.e., the integral of the normal probability density function from 8 to 13.5, assuming <span class="math inline">\(\mu = 10\)</span> and <span class="math inline">\(\sigma^2 = 4\)</span>.
</p>
</div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="the-normal-and-related-distributions.html#cb60-1" tabindex="-1"></a>integrand <span class="ot">&lt;-</span> <span class="cf">function</span>(x,mu,sigma2) {</span>
<span id="cb60-2"><a href="the-normal-and-related-distributions.html#cb60-2" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">exp</span>(<span class="sc">-</span>(x<span class="sc">-</span>mu)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span><span class="dv">2</span><span class="sc">/</span>sigma2)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>sigma2))</span>
<span id="cb60-3"><a href="the-normal-and-related-distributions.html#cb60-3" tabindex="-1"></a>}</span>
<span id="cb60-4"><a href="the-normal-and-related-distributions.html#cb60-4" tabindex="-1"></a><span class="fu">integrate</span>(integrand,<span class="dv">8</span>,<span class="fl">13.5</span>,<span class="at">mu=</span><span class="dv">10</span>,<span class="at">sigma2=</span><span class="dv">4</span>)<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.8012856</code></pre>
<blockquote>
<p>The result is 0.801. Note how the <code>integrand()</code> function requires extra
information above and beyond the value of <code>x</code>. This information is passed
in using the extra arguments <code>mu</code> and <code>sigma2</code>, the values of which
are set in the call to <code>integrate()</code>.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(13,5)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5 \vert X &lt; 14)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>Recall that
<span class="math inline">\(P(a \leq X \leq b \vert X \leq c) = P(a \leq X \leq b)/P(X \leq c)\)</span>,
assuming that <span class="math inline">\(a &lt; b &lt; c\)</span>. So this probability is, visually, the
ratio of the area of the brown-shaded region in Figure <a href="the-normal-and-related-distributions.html#fig:pdfcond">2.3</a> to
the area of the red-shaded region underlying it.
We can reuse the <code>integrand()</code> function from above, calling it twice:</p>
</blockquote>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="the-normal-and-related-distributions.html#cb62-1" tabindex="-1"></a><span class="fu">integrate</span>(integrand,<span class="dv">8</span>,<span class="fl">13.5</span>,<span class="at">mu=</span><span class="dv">13</span>,<span class="at">sigma2=</span><span class="dv">5</span>)<span class="sc">$</span>value <span class="sc">/</span> </span>
<span id="cb62-2"><a href="the-normal-and-related-distributions.html#cb62-2" tabindex="-1"></a>  <span class="fu">integrate</span>(integrand,<span class="sc">-</span><span class="cn">Inf</span>,<span class="dv">14</span>,<span class="at">mu=</span><span class="dv">13</span>,<span class="at">sigma2=</span><span class="dv">5</span>)<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.8560226</code></pre>
<blockquote>
<p>The result is 0.856. Note how we use <code>-Inf</code> in the second call to
<code>integrate()</code>: <code>Inf</code>, like <code>pi</code>,
is a reserved word in the <code>R</code> programming language.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdfcond"></span>
<img src="_main_files/figure-html/pdfcond-1.png" alt="\label{fig:pdfcond}The conditional probability $P(8 \leq X \leq 13.5 \vert X &lt; 14)$ is the ratio of the area of the brown-shaded region to the area of the red-shaded region underlying the brown-shaded region." width="50%" />
<p class="caption">
Figure 2.3: The conditional probability <span class="math inline">\(P(8 \leq X \leq 13.5 \vert X &lt; 14)\)</span> is the ratio of the area of the brown-shaded region to the area of the red-shaded region underlying the brown-shaded region.
</p>
</div>
</div>
</div>
<div id="cumulative-distribution-function" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Cumulative Distribution Function<a href="the-normal-and-related-distributions.html#cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a continuous distribution, it is defined as <span class="math inline">\(F_X(x) = \int_{y \leq x} f_Y(y) dy\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>The cdf for the normal distribution is the
“accumulated probability” between <span class="math inline">\(-\infty\)</span> and the functional input <span class="math inline">\(x\)</span>:
<span class="math display">\[
F_X(x) = \int_{-\infty}^x f_Y(y) dy = \int_{-\infty}^x \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right) dy \,.
\]</span>
(See Figure <a href="the-normal-and-related-distributions.html#fig:normcdf1">2.4</a>.)
Recall that because <span class="math inline">\(x\)</span> is the upper bound of the integral,
we have to replace <span class="math inline">\(x\)</span> with some other variable in the integrand itself.
(Here we choose <span class="math inline">\(y\)</span>. The choice is arbitrary.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normcdf1"></span>
<img src="_main_files/figure-html/normcdf1-1.png" alt="\label{fig:normcdf1}The cdf is the area of the red-shaded polygon." width="50%" />
<p class="caption">
Figure 2.4: The cdf is the area of the red-shaded polygon.
</p>
</div>
<p>To evaluate this integral, we
again implement a variable substitution strategy:
<span class="math display">\[
(1) ~~ t = \frac{(y-\mu)}{\sqrt{2}\sigma} ~~\implies~~ (2) ~~  dt = \frac{1}{\sqrt{2}\sigma}dy
\]</span>
and
<span class="math display">\[
(3) ~~ y = -\infty ~\implies~ t = -\infty ~~~ \mbox{and} ~~~ y = x ~\implies~ t = \frac{(x-\mu)}{\sqrt{2}\sigma} \,,
\]</span>
so
<span class="math display">\[\begin{align*}
F_X(x) &amp;= \int_{-\infty}^x \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right) dy \\
&amp;= \int_{-\infty}^{\frac{x-\mu}{\sqrt{2}\sigma}} \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-t^2) \sqrt{2}\sigma dt = \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\frac{x-\mu}{\sqrt{2}\sigma}} \exp(-t^2) dt \,.
\end{align*}\]</span>
The <em>error function</em> is defined as
<span class="math display">\[
\mbox{erf}(z) = \frac{2}{\sqrt{\pi}} \int_0^z \exp(-t^2)dt \,,
\]</span>
which is close to, but not quite, our expression for <span class="math inline">\(F_X(x)\)</span>. (The integrand is the same, but the bounds of integration differ.) To match the bounds:
<span class="math display">\[\begin{align*}
\frac{2}{\sqrt{\pi}} \int_{-\infty}^z \exp(-t^2)dt &amp;= \frac{2}{\sqrt{\pi}} \left( \int_{-\infty}^0 \exp(-t^2)dt + \int_0^z \exp(-t^2)dt \right) \\
&amp;= \frac{2}{\sqrt{\pi}} \left( -\int_0^{-\infty} \exp(-t^2)dt + \int_0^z \exp(-t^2)dt \right) \\
&amp;= -\mbox{erf}(-\infty) + \mbox{erf}(z) = \mbox{erf}(\infty) + \mbox{erf}(z) = 1 + \mbox{erf}(z) \,.
\end{align*}\]</span>
Here we make use of two properties of the error function: <span class="math inline">\(\mbox{erf}(-z) = -\mbox{erf}(z)\)</span>, and <span class="math inline">\(\mbox{erf}(\infty)=1\)</span>. Thus
<span class="math display">\[
\frac{1}{\sqrt{\pi}} \int_{-\infty}^z \exp(-t^2)dt = \frac{1}{2}[1 + \mbox{erf}(z)] \,.
\]</span>
By matching this expression with that given for <span class="math inline">\(F(x)\)</span> above, we see that
<span class="math display">\[
F_X(x) = \frac{1}{2}\left[1 + \mbox{erf}\left(\frac{x-\mu}{\sqrt{2}\sigma}\right)\right] \,.
\]</span>
In Figure <a href="the-normal-and-related-distributions.html#fig:normcdf2">2.5</a>, we plot <span class="math inline">\(F_X(x)\)</span>.
We note that while the error function is available to use directly in
some <code>R</code> packages, it is provided only indirectly in base-<code>R</code>,
via the <code>pnorm()</code> function. (In general, one computes cdf values for
probability distributions in <code>R</code> using functions prefixed with <code>p</code>:
<code>pnorm()</code>, <code>pbinom()</code>, <code>punif()</code>, etc.) Examining this figure,
we see that this cdf abides by the properties listed in the previous
chapter: <span class="math inline">\(F(-\infty) = 0\)</span> and <span class="math inline">\(F(\infty) = 1\)</span>, and it is (strictly)
monotonically increasing.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normcdf2"></span>
<img src="_main_files/figure-html/normcdf2-1.png" alt="\label{fig:normcdf2}The cdf for a normal distribution with mean 0 and standard deviation 1." width="50%" />
<p class="caption">
Figure 2.5: The cdf for a normal distribution with mean 0 and standard deviation 1.
</p>
</div>
<p>To compute the probability <span class="math inline">\(P(a &lt; X &lt; b)\)</span>, we make use of the cdf. (As
a reminder: if we have the cdf at our disposal and
need to compute a probability…we should use it!)
<span class="math display">\[\begin{align*}
P(a &lt; X &lt; b) &amp;= P(X &lt; b) - P(X &lt; a) \\
&amp;= F(b) - F(a) = \frac{1}{2}\left[ \mbox{erf}\left(\frac{b-\mu}{\sqrt{2}\sigma}\right) - \mbox{erf}\left(\frac{a-\mu}{\sqrt{2}\sigma}\right)\right] \,,
\end{align*}\]</span>
which is more simply rendered in <code>R</code> as</p>
<pre><code>pnorm(b,mean=mu,sd=sigma) - pnorm(a,mean=mu,sd=sigma)</code></pre>
<p>Let’s look again at Figure <a href="the-normal-and-related-distributions.html#fig:normcdf1">2.4</a>.
What if, instead of asking the question “what is the shaded area under the
curve,” which is answered by integrating the normal pdf from <span class="math inline">\(-\infty\)</span>
to a specified coordinate <span class="math inline">\(x\)</span>, we instead
ask the question “what value of <span class="math inline">\(x\)</span> is associated with a given area
under the curve”? This is the inverse problem, one that we can solve
so long as the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(F(x)\)</span> is bijective
(i.e., one-to-one): <span class="math inline">\(x = F_X^{-1}[F_X(x)]\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(q = F_X(x)\)</span>.</em></p>
<p>Let <span class="math inline">\(q = F_X(x)\)</span>. Then, for the case of the normal cdf, we can write that
<span class="math display">\[
x = \sqrt{2} \sigma~ \mbox{erf}^{-1}\left(2q-1\right) + \mu \,,
\]</span>
where <span class="math inline">\(\mbox{erf}^{-1}(\cdot)\)</span> is the inverse error function. Like the error function itself, the inverse error function is available for use in some <code>R</code> packages, but
it is most commonly accessed, indirectly via the base-<code>R</code> function <code>qnorm()</code>. (In general, one computes inverse cdf values for probability
distributions in <code>R</code> using functions prefixed with <code>q</code>: <code>qnorm()</code>, <code>qpois()</code>, <code>qbeta()</code>, etc.)</p>
<hr />
<div id="computing-probabilities-1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We utilize the two examples provided in the last section to show how
one would compute probabilities associated with the normal pdf by hand.
But we note that a computer is still needed to derive the final numbers!</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(10,4)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We have that
<span class="math display">\[
P(8 \leq X \leq 13.5) = P(X \leq 13.5) - P(X \leq 8) = F_X(13.5 \vert 10,4) - F_X(8 \vert 10,4) \,.
\]</span>
Well, this is where analytic computation ends, as we cannot evaluate the
cdfs using pen and paper. Instead, we utilize the function <code>pnorm()</code>:</p>
</blockquote>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="the-normal-and-related-distributions.html#cb65-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">13.5</span>,<span class="at">mean=</span><span class="dv">10</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">4</span>)) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">8</span>,<span class="at">mean=</span><span class="dv">10</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## [1] 0.8012856</code></pre>
<blockquote>
<p>We get the same result as we got in the last section: 0.801.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(13,5)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5 \vert X &lt; 14)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>Knowing that we cannot solve this by hand, we
go directly to <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="the-normal-and-related-distributions.html#cb67-1" tabindex="-1"></a>(<span class="fu">pnorm</span>(<span class="fl">13.5</span>,<span class="at">mean=</span><span class="dv">13</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">5</span>)) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">8</span>,<span class="at">mean=</span><span class="dv">13</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">5</span>))) <span class="sc">/</span> </span>
<span id="cb67-2"><a href="the-normal-and-related-distributions.html#cb67-2" tabindex="-1"></a>  <span class="fu">pnorm</span>(<span class="dv">14</span>,<span class="at">mean=</span><span class="dv">13</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">5</span>))</span></code></pre></div>
<pre><code>## [1] 0.8560226</code></pre>
<blockquote>
<p>The answer, as before, is 0.856.</p>
</blockquote>
<blockquote>
<p>But let’s go ahead and add some complexity here. How would we answer the
following question?</p>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>If <span class="math inline">\(\mu = 20\)</span> and <span class="math inline">\(\sigma^2 = 3\)</span>, what is the value of <span class="math inline">\(a\)</span>
such that <span class="math inline">\(P(20 - a \leq X \leq 20 + a) = 0.48\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>(The first thing to thing about here is: does the value of <span class="math inline">\(\mu\)</span> actually
matter? The answer here is no. Think about why that may be.)</p>
</blockquote>
<blockquote>
<p>We have that
<span class="math display">\[\begin{align*}
P(20-a \leq X \leq 20+a) = 0.48 &amp;= P(X \leq 20+a) - P(X \leq 20-a)\\
&amp;= F_X(20+a \vert 20,3) - F_X(20-a \vert 20,3) \,.
\end{align*}\]</span>
Hmm…we’re stuck. We cannot invert this equation so as to isolate <span class="math inline">\(a\)</span>…or
can we? Remember that a normal pdf is symmetric around the mean. Hence
<span class="math display">\[
P(X \leq 20+a) = 1 - P(X &gt; 20+a) = 1 - P(X \leq 20-a) \,.
\]</span>
The area under the normal pdf from <span class="math inline">\(20+a\)</span> to <span class="math inline">\(\infty\)</span> is the same
as the area from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(20-a\)</span>. So…
<span class="math display">\[\begin{align*}
P(20-a \leq X \leq 20+a) &amp;= 1 - F_X(20-a \vert 20,3) - F_X(20-a \vert 20,3)\\
&amp;= 1 - 2F_X(20-a \vert 20,3) \,,
\end{align*}\]</span>
or
<span class="math display">\[
F_X(20-a \vert 20,3) = \frac{1}{2}[1 - P(20-a \leq X \leq 20+a)] = \frac{1}{2} 0.52 = 0.26 \,.
\]</span>
Now, we invert the cdf and rearrange terms to get:
<span class="math display">\[
a = 20 - F_X^{-1}(0.26 \vert 20,3) \,,
\]</span>
and once again, we transition to <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="the-normal-and-related-distributions.html#cb69-1" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.26</span>,<span class="at">mean=</span><span class="dv">20</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">3</span>))</span></code></pre></div>
<blockquote>
<p>The result is <span class="math inline">\(a = 1.114\)</span>, i.e., <span class="math inline">\(P(18.886 \leq X \leq 21.114) = 0.48\)</span>.</p>
</blockquote>
<blockquote>
<p>The reader might quibble with our assertion that <span class="math inline">\(\mu\)</span> doesn’t
matter here, because, after all, the value of <span class="math inline">\(\mu\)</span> appears in the
final code. However, if we changed both values of 20 to some other,
arbitrary value, we would derive the same value for <span class="math inline">\(a\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="visualizing-a-cumulative-distribution-function" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Visualizing a Cumulative Distribution Function<a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s say we want to make a figure like that in Figure <a href="the-normal-and-related-distributions.html#fig:normcdf1">2.4</a>,
where we are given that our normal distribution has mean <span class="math inline">\(\mu = 10\)</span> and
variance <span class="math inline">\(\sigma^2 = 4\)</span>, and we want to overlay the shaded region associated
with <span class="math inline">\(F_X(9)\)</span>. How do we do this?</p>
</blockquote>
<blockquote>
<p>The first step is to determine the population standard deviation. Here,
that would be <span class="math inline">\(\sigma = \sqrt{4} = 2\)</span>.</p>
</blockquote>
<blockquote>
<p>The second step is to determine an appropriate range of values of <span class="math inline">\(x\)</span>
over which to plot the cdf. Here we
will assume that <span class="math inline">\(\mu - 4\sigma\)</span> to <span class="math inline">\(\mu + 4\sigma\)</span> is sufficient; this
corresponds to <span class="math inline">\(x = 2\)</span> to <span class="math inline">\(x = 18\)</span>.</p>
</blockquote>
<blockquote>
<p>The third step is to code. The result of our coding is shown in
Figure <a href="the-normal-and-related-distributions.html#fig:normviz">2.6</a>.</p>
</blockquote>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="the-normal-and-related-distributions.html#cb70-1" tabindex="-1"></a>x         <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">2</span>,<span class="dv">18</span>,<span class="at">by=</span><span class="fl">0.05</span>)       <span class="co"># vector with x = {2,2.05,2.1,...,18}</span></span>
<span id="cb70-2"><a href="the-normal-and-related-distributions.html#cb70-2" tabindex="-1"></a>f.x       <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x,<span class="at">mean=</span><span class="dv">10</span>,<span class="at">sd=</span><span class="dv">2</span>)   <span class="co"># compute f(x) for all x</span></span>
<span id="cb70-3"><a href="the-normal-and-related-distributions.html#cb70-3" tabindex="-1"></a>df        <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">f.x=</span>f.x) <span class="co"># define a dataframe with above data</span></span>
<span id="cb70-4"><a href="the-normal-and-related-distributions.html#cb70-4" tabindex="-1"></a>df.shaded <span class="ot">&lt;-</span> <span class="fu">subset</span>(df,x<span class="sc">&lt;=</span><span class="dv">9</span>)         <span class="co"># get subset of data with x values &lt;= 9</span></span>
<span id="cb70-5"><a href="the-normal-and-related-distributions.html#cb70-5" tabindex="-1"></a></span>
<span id="cb70-6"><a href="the-normal-and-related-distributions.html#cb70-6" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>f.x)) <span class="sc">+</span></span>
<span id="cb70-7"><a href="the-normal-and-related-distributions.html#cb70-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span>     </span>
<span id="cb70-8"><a href="the-normal-and-related-distributions.html#cb70-8" tabindex="-1"></a>  <span class="fu">geom_area</span>(<span class="at">data=</span>df.shaded,<span class="at">fill=</span><span class="st">&quot;red&quot;</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">outline.type=</span><span class="st">&quot;full&quot;</span>) <span class="sc">+</span></span>
<span id="cb70-9"><a href="the-normal-and-related-distributions.html#cb70-9" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb70-10"><a href="the-normal-and-related-distributions.html#cb70-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(f[X]<span class="sc">*</span><span class="st">&quot;(x)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb70-11"><a href="the-normal-and-related-distributions.html#cb70-11" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normviz"></span>
<img src="_main_files/figure-html/normviz-1.png" alt="\label{fig:normviz}The cumulative distribution function is the area of the red-shaded region." width="50%" />
<p class="caption">
Figure 2.6: The cumulative distribution function is the area of the red-shaded region.
</p>
</div>
<p>For the polygon, the first <span class="math inline">\((x,y)\)</span> pair is <span class="math inline">\((9,0)\)</span>, the second is <span class="math inline">\((2,0)\)</span>
(where <span class="math inline">\(x = 2\)</span> is the lower limit of the plot…there is no need to take
the polygon further “to the left”), and the next set are <span class="math inline">\((x,f_X(x))\)</span> for
all <span class="math inline">\(x\)</span> values <span class="math inline">\(\leq 9\)</span>. The last point is the first point: this closes the
polygon.</p>
</div>
</div>
<div id="moment-generating-functions" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Moment-Generating Functions<a href="the-normal-and-related-distributions.html#moment-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter, we learned that if we linearly transform a
random variable, i.e., if we define a new random variable
<span class="math inline">\(Y = \sum_{i=1}^n a_i X_i + b\)</span>, where <span class="math inline">\(\{a_1,\ldots,a_n\}\)</span> and <span class="math inline">\(b\)</span> are
constants, then
<span class="math display">\[
E[Y] = E\left[\sum_{i=1}^n a_i X_i + b\right] = b + \sum_{i=1}^n a_i E[X_i] \,.
\]</span>
Furthermore, if we assume that the <span class="math inline">\(X_i\)</span>’s are independent
random variables, then the variance of <span class="math inline">\(Y\)</span> is
<span class="math display">\[
V[Y] = V\left[\sum_{i=1}^n a_i X_i + b\right] = \sum_{i=1}^n a_i^2 V[X_i] \,.
\]</span>
(Because the <span class="math inline">\(X_i\)</span>’s are independent, we do not need to take into account
any covariance, or linear dependence, between the <span class="math inline">\(X_i\)</span>’s, simplifying
the equation for <span class="math inline">\(V[Y]\)</span>. We discuss how covariance is taken into account in
Chapter 6.)
So we know where <span class="math inline">\(Y\)</span> is centered and we know how “wide” it is. However,
we don’t yet know the shape of the distribution for <span class="math inline">\(Y\)</span>, i.e., we don’t
yet know <span class="math inline">\(f_Y(y)\)</span>. To show how we might derive the distribution,
we introduce a new concept, that of the <em>moment-generating function</em>, or
<em>mgf</em>.</p>
<p>In the previous chapter, we introduced the concept of distribution moments,
i.e., the expected values <span class="math inline">\(E[X^k]\)</span> and <span class="math inline">\(E[(X-\mu)^k]\)</span>.
The moments of a given probability distribution
are unique, and can often (but not always) be neatly encapsulated
in a single mathematical expression: the moment-generating function (or mgf).
To derive an mgf for a given distribution,
we invoke the Law of the Unconscious Statistician:
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \int_x e^{tX} f_X(x) \\
&amp;= \int_x \left[1 + tx + \frac{t^2}{2!}x^2 + \cdots \right] f_X(x) \\
&amp;= \int_x \left[ f_X(x) + t x f_X(x) + \frac{t^2}{2!} x^2 f_X(x) + \cdots \right] \\
&amp;= \int_x f_X(x) + t \int_x x f_X(x) + \frac{t^2}{2!} \int_x x^2 f_X(x) + \cdots \\
&amp;= 1 + t E[X] + \frac{t^2}{2!} E[X^2] + \cdots \,.
\end{align*}\]</span>
An mgf only exists for a particular distribution if there is a
constant <span class="math inline">\(b\)</span> such that <span class="math inline">\(m_X(t)\)</span> is finite for <span class="math inline">\(\vert t \vert &lt; b\)</span>. (This is
a detail that we will not concern ourselves with here, i.e., we will
assume that the expressions we derive for <span class="math inline">\(E[e^{tX}]\)</span> are valid expressions.)
An example of a distribution for which the mgf does not exist
is the Cauchy distribution.</p>
<p>Moment-generating functions are called such because,
as one might guess, they generate moments (via differentiation):
<span class="math display">\[
\left.\frac{d^k m_X(t)}{dt^k}\right|_{t=0} = \frac{d^k}{dt^k} \left. \left[1 + t E[X] + \frac{t^2}{2!} E[X^2] + \cdots \right]\right|_{t=0} = \left. \left[E[X^k] + tE[X^{k+1}] + \cdots\right] \right|_{t=0} = E[X^k] \,.
\]</span>
The <span class="math inline">\(k^{\rm th}\)</span> derivative of an mgf, with <span class="math inline">\(t\)</span> set to zero,
yields the <span class="math inline">\(k^{\rm th}\)</span> moment <span class="math inline">\(E[X^k]\)</span>.</p>
<p>But, the reader may ask, why are mgfs important here, in the context of
normal random variables? We already know all the moments of this distribution
that we care to know: they are written down.
The answer is that a remarkably
useful property of mgfs is the following:
if <span class="math inline">\(Y = b + \sum_{i=1}^n a_i X_i\)</span>, where the <span class="math inline">\(X_i\)</span>’s are independent
random variables sampled from a distribution whose mgf exists,
then
<span class="math display">\[
m_Y(t) = e^{bt} m_{X_1}(a_1 t) \cdot m_{X_2}(a_2 t) \cdots m_{X_n}(a_n t) = e^{bt} \prod_{i=1}^n m_{X_i}(a_i t) \,,
\]</span>
where <span class="math inline">\(m_{X_i}(\cdot)\)</span> is the moment-generating function for the random
variable <span class="math inline">\(X_i\)</span>. An mgf is typically written as a function of <span class="math inline">\(t\)</span>;
the notation <span class="math inline">\(m_{X_i}(a_it)\)</span> simply means that when we evaluate the
above equation, wherever there is a <span class="math inline">\(t\)</span>, we plug in <span class="math inline">\(a_it\)</span>.
Here’s the key point:
<em>if we recognize the form of <span class="math inline">\(m_Y(t)\)</span> as
matching that of the mgf for a given distribution, then we know
the distribution for <span class="math inline">\(Y\)</span></em>.</p>
<p>Below, we show how the method of moment-generating
functions allows us to
derive distributions for linearly transformed normal random variables.</p>
<hr />
<div id="moment-generating-function-for-a-probability-mass-function" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Moment-Generating Function for a Probability Mass Function<a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we sample data from the following pmf:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x\)</span></th>
<th align="center"><span class="math inline">\(p_X(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0.2</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">0.3</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">0.5</td>
</tr>
</tbody>
</table>
<blockquote>
<p>What is the mgf for this distribution? What is its expected value?</p>
</blockquote>
<blockquote>
<p>To derive the mgf, we compute <span class="math inline">\(E[e^{tX}]\)</span>:
<span class="math display">\[
m_X(t) = E[e^{tX}] = \sum_x e^{tx} p_X(x) = 1 \cdot 0.2 + e^t \cdot 0.3 + e^{2t} \cdot 0.5 = 0.2 + 0.3 e^t + 0.5 e^{2t} \,.
\]</span>
This cannot be simplified, and thus this is the final answer. As far as
the expected value is concerned: we could simply compute
<span class="math inline">\(E[X] = \sum_x x p_X(x)\)</span>, but since we have the mgf now, we can use it too:
<span class="math display">\[
E[X] = \left.\frac{d}{dt}m_X(t)\right|_{t=0} = \left. (0.3 e^t + e^{2t})\right|_{t=0} = 0.3 + 1 = 1.3 \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="moment-generating-function-for-a-probability-density-function" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Moment-Generating Function for a Probability Density Function<a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we now sample data from the following pdf:
<span class="math display">\[
f_X(x) = \frac{1}{\theta} e^{-x/\theta} \,,
\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. What is the mgf of this distribution?
<span class="math display">\[\begin{align*}
E[e^{tX}] &amp;= \int_0^\infty e^{tx} \frac{1}{\theta} e^{-x/\theta} dx = \frac{1}{\theta} \int_0^\infty e^{-x\left(\frac{1}{\theta}-t\right)} dx \\
&amp;= \frac{1}{\theta} \int_0^\infty e^{-x/\theta&#39;} dx = \frac{\theta&#39;}{\theta} = \frac{1}{\theta} \frac{1}{(1/\theta - t)} = \frac{1}{(1-\theta t)} = (1-\theta t)^{-1} \,.
\end{align*}\]</span>
The expected value of this distribution can be computed via the
integral <span class="math inline">\(\int_x x f_X(x) dx\)</span>, but again, as we now have the mgf,
<span class="math display">\[
E[X] = \left.\frac{d}{dt}m_X(t)\right|_{t=0} = \left. -(1-\theta t)^{-2} (-\theta)\right|_{t=0} = \theta \,.
\]</span></p>
</blockquote>
</div>
</div>
<div id="linear-functions-of-normal-random-variables" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Linear Functions of Normal Random Variables<a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s assume we are given <span class="math inline">\(n\)</span> normal random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>,
which are independent (but not necessarily identically distributed), and we
wish to determine the distribution of the sum
<span class="math inline">\(Y = b + \sum_{i=1}^n a_i X_i\)</span>. We recall that the expected value operator
<span class="math inline">\(E[X]\)</span> and the variance operator <span class="math inline">\(V[X]\)</span> are linear operators, thus we
can note immediately that…</p>
<ul>
<li>the expected value is
<span class="math display">\[
E[Y] = E\left[\sum_{i=1}^n a_i X_i\right] = \sum_{i=1}^n E[a_i X_i] = \sum_{i=1}^n a_i E[X_i] = \sum_{i=1}^n a_i \mu_i \,;
\]</span></li>
<li>and the variance is
<span class="math display">\[
V[Y] = V\left[\sum_{i=1}^n a_i X_i\right] = \sum_{i=1}^n V[a_i X_i] = \sum_{i=1}^n a_i^2 V[X_i] = \sum_{i=1}^n a_i^2 \sigma_i^2 \,.
\]</span></li>
</ul>
<p>As far as deriving the distribution: the mgf for the normal is
<span class="math display">\[
m_X(t) = \exp\left(\mu t + \sigma^2\frac{t^2}{2} \right) \,,
\]</span>
and thus if we have <span class="math inline">\(n\)</span> independent normal random variables, we find that
<span class="math display">\[\begin{align*}
m_Y(t) &amp;= \exp\left(\mu_1a_1t+a_1^2\sigma_1^2\frac{t^2}{2}\right) \cdot \cdots \cdot \exp\left(\mu_na_nt+a_n^2\sigma_n^2\frac{t^2}{2}\right) \\
&amp;= \exp\left[ (a_1\mu_1+\cdots+a_n\mu_n)t + \left(a_1^2\sigma_1^2 + \cdots + a_n^2\sigma_n^2\right)\frac{t^2}{2} \right] \\
&amp;= \exp\left[ \left(\sum_{i=1}^n a_i\mu_i\right)t + \left(\sum_{i=1}^n a_i^2\sigma_i^2\right)\frac{t^2}{2} \right] \,.
\end{align*}\]</span>
When we examine the result, we see immediately that
it retains the functional form of a normal mgf, and
thus we conclude that <span class="math inline">\(Y\)</span> itself is normally distributed,
with mean <span class="math inline">\(\sum_{i=1}^n a_i\mu_i\)</span> and
variance <span class="math inline">\(\sum_{i=1}^n a_i^2\sigma_i^2\)</span>, i.e.,
<span class="math display">\[
Y \sim \mathcal{N}\left(\sum_{i=1}^n a_i\mu_i,\sum_{i=1}^n a_i^2\sigma_i^2\right) \,.
\]</span></p>
<hr />
<div id="the-distribution-of-the-sample-mean-of-iid-normal-random-variables" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> The Distribution of the Sample Mean of iid Normal Random Variables<a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We have
previously seen that when have a sample of <span class="math inline">\(n\)</span> iid random variables,
<span class="math inline">\(E[\bar{X}] = \mu\)</span> and <span class="math inline">\(V[\bar{X}] = \sigma^2/n\)</span>. Now we want to determine
the <em>distribution</em> of <span class="math inline">\(\bar{X}\)</span>, not just its mean and variance. In general,
if <span class="math inline">\(\bar{X} = (1/n)\sum_{i=1}^n X_i\)</span>, then
<span class="math display">\[\begin{align*}
m_{\bar{X}}(t) &amp;= m_{X_1}(a_1 t) \cdot m_{X_2}(a_2 t) \cdots m_{X_n}(a_n t) \\
&amp;= m_{X_1}\left(\frac{t}{n}\right) \cdots m_{X_n}\left(\frac{t}{n}\right) = \prod_{i=1}^n m_{X_i}\left(\frac{t}{n}\right) \,,
\end{align*}\]</span>
and thus
<span class="math display">\[\begin{align*}
m_{\bar{X}}(t) &amp;= \exp\left[ \left(\sum_{i=1}^n \frac{\mu}{n}\right)t + \left(\sum_{i=1}^n \frac{\sigma^2}{n^2}\right)\frac{t^2}{2} \right] \\
&amp;= \exp\left( n \frac{\mu}{n} t + n \frac{\sigma^2}{n^2} \frac{t^2}{2} \right) = \exp\left(\mu t + \frac{\sigma^2}{n} \frac{t^2}{2} \right) \,.
\end{align*}\]</span>
We see that <span class="math inline">\(\bar{X} \sim \mathcal{N}(\mu,\sigma^2/n)\)</span>, i.e., that the sample
mean observed in any given experiment is sampled from a normal distribution
centered on the true mean, with a width that goes to zero as
<span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="using-variable-substitution-to-determine-distribution-of-y-ax-b" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Using Variable Substitution to Determine Distribution of Y = aX + b<a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We set <span class="math inline">\(y = ax+b\)</span> and note that <span class="math inline">\(dy = a dx\)</span> and that if <span class="math inline">\(x = \pm \infty\)</span>
then <span class="math inline">\(y\)</span> also equals <span class="math inline">\(\pm \infty\)</span>. Thus
<span class="math display">\[\begin{align*}
\int_{-\infty}^{\infty} \frac{1}{2\pi\sigma^2} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] dx &amp;= \int_{-\infty}^{\infty} \frac{1}{2\pi\sigma^2} \exp\left[-\frac{([y-b]/a-\mu)^2}{2\sigma^2}\right] \frac{dy}{a} \\
&amp;= \int_{-\infty}^{\infty} \frac{1}{2\pi a^2\sigma^2} \exp\left[-\frac{(y-b-a\mu)^2}{2a^2\sigma^2}\right] dy \\
&amp;= \int_{-\infty}^{\infty} \frac{1}{2\pi a^2\sigma^2} \exp\left[-\frac{(y-[a\mu+b])^2}{2a^2\sigma^2}\right] dy
\end{align*}\]</span>
The key point here is that <em>the integrand has the functional form of a
normal pdf and the integral bounds match the domain of a normal pdf</em>…hence
<span class="math inline">\(Y\)</span> is normally distributed, with mean <span class="math inline">\(a\mu+b\)</span> and variance <span class="math inline">\(a^2\sigma^2\)</span>.
(This key point holds in general…if we have a pmf or pdf whose
functional form and domain match that of a known, “named” family of
distributions, then the pmf or pdf belongs to that family.)</p>
</blockquote>
</div>
</div>
<div id="standardizing-a-normal-random-variable-with-known-variance" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Standardizing a Normal Random Variable with Known Variance<a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To standardize any random variable, we subtract the expected value and divide by the standard deviation, i.e., we set
<span class="math display">\[
Z = \frac{X - E[X]}{\sqrt{V[X]}} \,.
\]</span>
If <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span>, what are the mean and variance of
<span class="math inline">\(Z\)</span>, and can we derive <span class="math inline">\(f_Z(z)\)</span>?</p>
<ul>
<li><p><em>Expected Value</em>. If we write <span class="math inline">\(Z = aX+b\)</span>, where <span class="math inline">\(a\)</span> is <span class="math inline">\(1/\sigma\)</span> and
<span class="math inline">\(b = -\mu/\sigma\)</span>, then
<span class="math display">\[
E[Z] = E[aX+b] = \frac{1}{\sigma}E[X] - b = \frac{\mu}{\sigma} - \frac{\mu}{\sigma} = 0 \,.
\]</span></p></li>
<li><p><em>Variance</em>. The variance is
<span class="math display">\[
V[Z] = V[aX+b] = a^2V[X] = \frac{1}{\sigma^2} \sigma^2 = 1 \,.
\]</span></p></li>
</ul>
<p>OK…so far, so good: the distribution is centered at 0 and has variance 1.
To derive the distribution, we use the methods of mgfs.</p>
<p>The mgf for a normal random variable is
<span class="math display">\[
m_X(t) = \exp\left(\mu t + \frac{\sigma^2 t^2}{2}\right) \,.
\]</span>
Harkening back to the last section, the mgf for <span class="math inline">\(Z = aX+b\)</span> is
<span class="math display">\[\begin{align*}
m_Z(t) &amp;= e^{bt} m_X(at) \\
&amp;= \exp\left(-\frac{\mu t}{\sigma}\right) \exp\left(a \mu t + \frac{a^2 \sigma^2 t^2} {2}\right) \\
&amp;= \exp\left(-\frac{\mu t}{\sigma}\right) \exp\left(\frac{\mu t}{\sigma} + \frac{\sigma^2 t^2}{ 2 \sigma^2}\right) \\
&amp;= \exp\left(0 t + \frac{1^2 t^2}{2}\right) \,.
\end{align*}\]</span>
This mgf retains the functional form of a normal mgf, but with <span class="math inline">\(\mu = 0\)</span>
and <span class="math inline">\(\sigma = 1\)</span>. Thus <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>, and thus the pdf for <span class="math inline">\(Z\)</span> is
<span class="math display">\[
f_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}\right) ~~~~ z \in (-\infty,\infty) \,,
\]</span>
while the cdf for <span class="math inline">\(Z\)</span> is
<span class="math display">\[
F_Z(z) = \Phi(z) = \frac{1}{2}\left[1 + \mbox{erf}\left(\frac{z}{\sqrt{2}}\right)\right] \,.
\]</span>
<span class="math inline">\(Z\)</span> is a so-called <em>standard normal</em> random variable.
By historical convention, the cdf of the standard normal distribution
is denoted <span class="math inline">\(\Phi(z)\)</span> (“phi” of z, pronounced “fye”) rather than <span class="math inline">\(F_Z(z)\)</span>.</p>
<p>Statisticians often standardize normally distributed random variables
and perform probability calculations using the standard normal.
This is unnecessary in the age of computers, but in the pre-computer
era standardization greatly simplified calculations since all one needed
was a single table of values of <span class="math inline">\(\Phi(z)\)</span> to compute probabilities.
That said, standardization has its uses.
For instance, mentally computing
an approximate value for <span class="math inline">\(P(6 &lt; X &lt; 12)\)</span> when <span class="math inline">\(X \sim \mathcal{N}(9,9)\)</span>
can be quite a bit more taxing than if we were to write down the equivalent
expression <span class="math inline">\(P(-1 &lt; Z &lt; 1)\)</span> and then evaluate that.
(A skilled practitioner would know right away that the latter
expression evaluates to <span class="math inline">\(\approx\)</span> 0.68.)</p>
<table style="width:100%;">
<caption>Common <span class="math inline">\(Z\)</span>-Range/Probability Conversions</caption>
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\((z_L,z_U)\)</span></th>
<th><span class="math inline">\(\pm\)</span> 1</th>
<th><span class="math inline">\(\pm\)</span> 2</th>
<th><span class="math inline">\(\pm\)</span> 3</th>
<th><span class="math inline">\(\pm\)</span> 1.645</th>
<th><span class="math inline">\(\pm\)</span> 1.960</th>
<th><span class="math inline">\(\pm\)</span> 2.576</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>prob.</td>
<td>0.6837</td>
<td>0.9544</td>
<td>0.9973</td>
<td>0.90</td>
<td>0.95</td>
<td>0.99</td>
</tr>
</tbody>
</table>
<hr />
<div id="computing-probabilities-2" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here we will reexamine the three examples that we worked through above
in the section in which we introduce the normal cumulative distribution
function; here, we will utilize standardization. Note: the final results
will all be the same!</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(10,4)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5)\)</span>?</li>
</ol>
<p>We standardize <span class="math inline">\(X\)</span>: <span class="math inline">\(Z = (X-\mu)/\sigma = (X-10)/2\)</span>. Hence the bounds
of integration are <span class="math inline">\((8-10)/2 = -1\)</span> and <span class="math inline">\((13.5-10)/2 = 1.75\)</span>, and the
probability we seek is
<span class="math display">\[
P(-1 \leq Z \leq 1.75) = \Phi(1.75) - \Phi(-1) \,.
\]</span>
To compute the final number, we utilize <code>pnorm()</code> with its default values
of <code>mean=0</code> and <code>sd=1</code>:</p>
</blockquote>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="the-normal-and-related-distributions.html#cb71-1" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">1.75</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.8012856</code></pre>
<blockquote>
<p>The probability is 0.801.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim \mathcal{N}(13,5)\)</span>, what is <span class="math inline">\(P(8 \leq X \leq 13.5 \vert X &lt; 14)\)</span>?</li>
</ol>
<p>The integral bounds are <span class="math inline">\((8-13)/\sqrt{5} = -\sqrt{5}\)</span> and
<span class="math inline">\((13.5-13)/\sqrt{5} = \sqrt{5}/10\)</span> in the numerator, and
<span class="math inline">\(-\infty\)</span> and <span class="math inline">\((14-13/\sqrt{5} = \sqrt{5}/5\)</span> in the denominator. In <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="the-normal-and-related-distributions.html#cb73-1" tabindex="-1"></a>(<span class="fu">pnorm</span>(<span class="fu">sqrt</span>(<span class="dv">5</span>)<span class="sc">/</span><span class="dv">10</span>) <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="sc">-</span><span class="fu">sqrt</span>(<span class="dv">5</span>))) <span class="sc">/</span> <span class="fu">pnorm</span>(<span class="fu">sqrt</span>(<span class="dv">5</span>)<span class="sc">/</span><span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.8560226</code></pre>
<blockquote>
<p>The probability is 0.856.</p>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>If <span class="math inline">\(\mu = 20\)</span> and <span class="math inline">\(\sigma^2 = 3\)</span>, what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(20-a \leq X \leq 20+a) = 0.48\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>Here,
<span class="math display">\[
P(20-a \leq X \leq 20+a) = P\left( \frac{20-a-20}{\sqrt{3}} \leq \frac{X - 20}{\sqrt{3}} \leq \frac{20+a-20}{\sqrt{3}}\right) = P\left(-\frac{a}{\sqrt{3}} \leq Z \leq \frac{a}{\sqrt{3}} \right) \,,
\]</span>
and we utilize the symmetry of the standard normal around zero to write
<span class="math display">\[
P\left(-\frac{a}{\sqrt{3}} \leq Z \leq \frac{a}{\sqrt{3}} \right) = 1 - 2P\left(Z \leq -\frac{a}{\sqrt{3}}\right) = 1 - 2\Phi\left(-\frac{a}{\sqrt{3}}\right) \,.
\]</span>
Thus
<span class="math display">\[
1 - 2\Phi\left(-\frac{a}{\sqrt{3}}\right) = 0.48 ~\Rightarrow~ \Phi\left(-\frac{a}{\sqrt{3}}\right) = 0.26 ~\Rightarrow~ -\frac{a}{\sqrt{3}} = \Phi^{-1}(0.26) = -0.64 \,,
\]</span>
and thus <span class="math inline">\(a = 1.114\)</span>. (Note that we numerically
evaluate <span class="math inline">\(\Phi^{-1}(0.26)\)</span> using <code>qnorm(0.26)</code>).</p>
</blockquote>
</div>
</div>
<div id="general-transformations-of-a-single-random-variable" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> General Transformations of a Single Random Variable<a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thus far, when discussing transformations of a random variable, we have
limited ourselves to <em>linear</em> transformations of independent r.v.’s, i.e.,
<span class="math inline">\(Y = b + \sum_{i=1}^n a_i X_i\)</span>. What if instead we want to make a more
general transformation of a single random variable, e.g., <span class="math inline">\(Y = X^2 + 3\)</span>
or <span class="math inline">\(Y = \sin X\)</span>? We cannot use the method of moment-generating functions
here…we need a new method.</p>
<p>Let’s assume we have a random variable <span class="math inline">\(X\)</span>, and we
transform it according to a function <span class="math inline">\(g(\cdot)\)</span>: <span class="math inline">\(U = g(X)\)</span>. Then:</p>
<ol style="list-style-type: decimal">
<li>we identify the inverse function <span class="math inline">\(X = g^{-1}(U)\)</span>;</li>
<li>we derive the cdf of <span class="math inline">\(U\)</span>: <span class="math inline">\(F_U(u) = P(U \leq u) = P(g(X) \leq u) = P(X \leq g^{-1}(u))\)</span>; and last</li>
<li>we derive the pdf of <span class="math inline">\(U\)</span>: <span class="math inline">\(f_U(u) = dF_U(u)/du\)</span>.</li>
</ol>
<p><strong>Recall:</strong> <em>a continuous distribution’s pdf is the derivative of its cdf.</em></p>
<hr />
<div id="distribution-of-a-transformed-random-variable" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Distribution of a Transformed Random Variable<a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We are given the following pdf:
<span class="math display">\[
f_X(x) = 3x^2 \,,
\]</span>
where <span class="math inline">\(x \in [0,1]\)</span>.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is the distribution of <span class="math inline">\(U = X/3\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We follow the three steps outlined above. First, we note that <span class="math inline">\(U = X/3\)</span>
and thus <span class="math inline">\(X = 3U\)</span>. Next, we find
<span class="math display">\[
F_U(u) = P(U \leq u) = P\left(\frac{X}{3} \leq u\right) = P(X \leq 3u) = \int_0^{3u} 3x^2 dx = \left. x^3 \right|_0^{3u} = 27u^3 \,,
\]</span>
for <span class="math inline">\(u \in [0,1/3]\)</span>. (The bounds are determined by plugging <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1\)</span>
into <span class="math inline">\(u = x/3\)</span>.) Now we can derive the pdf for <span class="math inline">\(U\)</span>:
<span class="math display">\[
f_U(u) = \frac{d}{du} 27u^3 = 54u^2 \,,
\]</span>
for <span class="math inline">\(u \in [0,1/3]\)</span>.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>What is the distribution of <span class="math inline">\(U = -X\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We note that <span class="math inline">\(U = -X\)</span> and thus <span class="math inline">\(X = -U\)</span>. Next, we find
<span class="math display">\[
F_U(u) = P(U \leq u) = P(-X \leq u) = P(X \geq -u) = \int_{-u}^{1} 3x^2 dx = \left. x^3 \right|_{-u}^{1} = 1 - (-u)^3 = 1 + u^3 \,,
\]</span>
for <span class="math inline">\(u \in [-1,0]\)</span>. (Note that the direction of the inequality
changed in the probability statement because of the sign change from
<span class="math inline">\(-X\)</span> to <span class="math inline">\(X\)</span>, and the bounds are reversed to be in ascending order.)
Now we derive the pdf:
<span class="math display">\[
f_U(u) = \frac{d}{du} (1+u^3) = 3u^2 \,,
\]</span>
for <span class="math inline">\(u \in [-1,0]\)</span>.</p>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>What is the distribution of <span class="math inline">\(U = 2e^X\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We note that <span class="math inline">\(U = 2e^X\)</span> and thus <span class="math inline">\(X = \log(U/2)\)</span>. Next, we find
<span class="math display">\[\begin{align*}
F_U(u) = P(U \leq u) &amp;= P\left(2e^X \leq u\right) = P\left(X \leq \log\frac{u}{2}\right)\\
&amp;= \int_0^{\log(u/2)} 3x^2 dx = \left. x^3 \right|_0^{\log(u/2)} = \left(\log\frac{u}{2}\right)^3 \,,
\end{align*}\]</span>
for <span class="math inline">\(u \in [2,2e]\)</span>. The pdf for <span class="math inline">\(U\)</span> is thus
<span class="math display">\[
f_U(u) = \frac{d}{du} \left(\log\frac{u}{2}\right)^3 = 3\left(\log\frac{u}{2}\right)^2 \frac{1}{u} \,,
\]</span>
for <span class="math inline">\(u \in [2,2e]\)</span>. Hint: if we want to see if our derived
distribution is correct, we can code the pdf in <code>R</code> and
integrate over the inferred domain.</p>
</blockquote>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="the-normal-and-related-distributions.html#cb75-1" tabindex="-1"></a>integrand <span class="ot">&lt;-</span> <span class="cf">function</span>(u) {</span>
<span id="cb75-2"><a href="the-normal-and-related-distributions.html#cb75-2" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">3</span><span class="sc">*</span>(<span class="fu">log</span>(u<span class="sc">/</span><span class="dv">2</span>))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>u)</span>
<span id="cb75-3"><a href="the-normal-and-related-distributions.html#cb75-3" tabindex="-1"></a>}</span>
<span id="cb75-4"><a href="the-normal-and-related-distributions.html#cb75-4" tabindex="-1"></a><span class="fu">integrate</span>(integrand,<span class="dv">2</span>,<span class="dv">2</span><span class="sc">*</span><span class="fu">exp</span>(<span class="dv">1</span>))<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<blockquote>
<p>Our answer is 1, as it should be for a properly defined pdf.</p>
</blockquote>
</div>
</div>
<div id="squaring-standard-normal-random-variables" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Squaring Standard Normal Random Variables<a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The square of a standard normal random variable is an important quantity
that arises, e.g., in statistical model assessment (through the “sum of
squared errors” when the “error” is normally distributed) and in
hypothesis tests like the chi-square goodness-of-fit test. But for this
quantity to be useful in statistical inference, we need to know its
distribution.</p>
<p>In step (1) of the algorithm laid out in the last section, we identify the
inverse function: if <span class="math inline">\(U = Z^2\)</span>, with <span class="math inline">\(Z \in (-\infty,\infty)\)</span>,
then <span class="math inline">\(Z = \pm \sqrt{U}\)</span>. Then, following step (2), we state that
<span class="math display">\[
F_U(u) = P(U \leq u) = P(Z^2 \leq u) = P(-\sqrt{u} \leq Z \leq \sqrt{u}) = \Phi(\sqrt{u}) - \Phi(-\sqrt{u}) \,,
\]</span>
where, as we recall, <span class="math inline">\(\Phi(\cdot)\)</span> is the notation for the standard normal
cdf. Because of symmetry, we can simplify this expression:
<span class="math display">\[
F_U(u) = \Phi(\sqrt{u}) - [1 - \Phi(\sqrt{u})] = 2\Phi(\sqrt{u}) - 1 \,.
\]</span></p>
<p>To carry out step (3), we utilize the chain rule of differentiation
to determine that the pdf is
<span class="math display">\[\begin{align*}
f_U(u) = \frac{d}{du} F_U(u) = \frac{d}{du} [2\Phi(\sqrt{u}) - 1] &amp;= 2 \frac{d\Phi}{du}(\sqrt{u}) \cdot \frac{d}{du}\sqrt{u} \\
&amp;= 2 f_Z(\sqrt{u}) \cdot \frac{1}{2\sqrt{u}} \\
&amp;= \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{u}{2}\right) \cdot \frac{1}{\sqrt{u}} \\
&amp;= \frac{u^{-1/2}}{\sqrt{2\pi}} \exp(-\frac{u}{2}) \,,
\end{align*}\]</span>
with <span class="math inline">\(u \in [0,\infty)\)</span>. This is the
pdf for a <em>chi-square distribution</em> with one “degree of freedom”:
<span class="math display">\[
U \sim \chi_1^2 \,,
\]</span>
with the subscript “1” indicating the number of degrees of freedom.
In general, the number of degrees of freedom can be an arbitrary positive
integer, and it is conventionally denoted with the
Greek letter <span class="math inline">\(\nu\)</span> (nu, pronounced “noo”).</p>
<p>Let’s now look at a sample of <span class="math inline">\(n\)</span> independent standard-normal random variables
<span class="math inline">\(\{Z_1,\ldots,Z_n\}\)</span>. What is the distribution of <span class="math inline">\(W = \sum_{i=1}^n Z_i^2\)</span>?
This is a sum of independent random variables, so we <em>can</em> use mgfs to try
to answer this question.
The mgf for a chi-square random variable with one degree of freedom is
<span class="math display">\[
m_{Z^2}(t) = (1-2t)^{-\frac{1}{2}} \,,
\]</span>
and thus the mgf for the sum of independent chi-square-distributed
random variables will be
<span class="math display">\[
m_W(t) = \prod_{i=1}^n m_{Z_i^2}(t) = \prod_{i=1}^n (1-2t)^{-\frac{1}{2}} = (1-2t)^{-\frac{n}{2}} \,.
\]</span>
We identify <span class="math inline">\(m_W(t)\)</span> as the mgf for a chi-square distribution with
<span class="math inline">\(\nu = n\)</span> degrees of freedom. Thus: <em>if we sum chi-square-distributed
random variables, the summed quantity is itself chi-square distributed,
with <span class="math inline">\(\nu\)</span> being the sum of the numbers of degrees of
freedom for the original random variables.</em> (This result can be
generalized: if <span class="math inline">\(X_1 \sim \chi_a^2\)</span> and <span class="math inline">\(X_2 \sim \chi_b^2\)</span>,
then <span class="math inline">\(X_1 + X_2 = W \sim \chi_{a+b}^2\)</span>.)</p>
<p>For completeness, we write down the pdf for <span class="math inline">\(\chi_{\nu}^2\)</span>:
<span class="math display">\[
f_X(x) = \frac{x^{\nu/2-1}}{2^{\nu/2} \Gamma(\nu/2)} \exp\left(-\frac{x}{2}\right) \,,
\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\nu\)</span> is a positive integer; <span class="math inline">\(E[X] = \nu\)</span>
and <span class="math inline">\(V[X] = 2\nu\)</span>.
(As we will see in Chapter 4, the chi-square family of distributions
is a “sub-family”
of the more general gamma family of distributions.)
The chi-square distribution is highly skew for small
values of <span class="math inline">\(\nu\)</span>, while chi-square random variables converge in distribution
to normal random variables
as <span class="math inline">\(\nu \rightarrow \infty\)</span>. See Figure <a href="the-normal-and-related-distributions.html#fig:chi2">2.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2"></span>
<img src="_main_files/figure-html/chi2-1.png" alt="\label{fig:chi2}Chi-square distributions for $\nu = 3$ (left) and $\nu = 30$ (right) degrees of freedom. Chi-square random variables converge in distribution to normal random variables as $\nu \rightarrow \infty$." width="45%" /><img src="_main_files/figure-html/chi2-2.png" alt="\label{fig:chi2}Chi-square distributions for $\nu = 3$ (left) and $\nu = 30$ (right) degrees of freedom. Chi-square random variables converge in distribution to normal random variables as $\nu \rightarrow \infty$." width="45%" />
<p class="caption">
Figure 2.7: Chi-square distributions for <span class="math inline">\(\nu = 3\)</span> (left) and <span class="math inline">\(\nu = 30\)</span> (right) degrees of freedom. Chi-square random variables converge in distribution to normal random variables as <span class="math inline">\(\nu \rightarrow \infty\)</span>.
</p>
</div>
<hr />
<div id="the-expected-value-of-a-chi-square-random-variable" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> The Expected Value of a Chi-Square Random Variable<a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a chi-square distribution for one degree of freedom is
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_0^\infty x f_X(x) dx \\
&amp;= \int_0^\infty \frac{x^{1/2}}{\sqrt{2\pi}}\exp\left(-\frac{x}{2}\right) dx \,.
\end{align*}\]</span>
To find the value of this integral, we are going to utilize the
gamma function
<span class="math display">\[
\Gamma(t) = \int_0^\infty u^{t-1} \exp(-u) du \,,
\]</span>
which we first introduced in Chapter 1.
(Note that <span class="math inline">\(t &gt; 0\)</span>.) Recall that if <span class="math inline">\(t\)</span> is an integer, then
<span class="math inline">\(\Gamma(t) = (t-1)! = (t-1) \times (t-2) \times \cdots \times 1\)</span>, with
the exclamation point representing the factorial function. If <span class="math inline">\(t\)</span> is
a half-integer and small, one can look up the value of <span class="math inline">\(\Gamma(t)\)</span> online.</p>
</blockquote>
<blockquote>
<p>The integral we are trying to compute doesn’t quite match the form
of the gamma function integral…but as one should recognize by now, we
can attempt a variable substitution to change <span class="math inline">\(e^{-x/2}\)</span> to <span class="math inline">\(e^{-u}\)</span>:
<span class="math display">\[
u = x/2 ~,~ du = dx/2 ~,~ x = 0 \implies u = 0 ~,~ x = \infty \implies u = \infty
\]</span>
We thus rewrite our expected value as
<span class="math display">\[\begin{align*}
E[X] &amp;= \int_0^\infty \frac{\sqrt{2}u^{1/2}}{\sqrt{2\pi}} \exp(-u) (2 du) = \frac{2}{\sqrt{\pi}} \int_0^\infty u^{1/2} \exp(-u) du \\
&amp;= \frac{2}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) = \frac{2}{\sqrt{\pi}} \frac{\sqrt{\pi}}{2} = 1 \,.
\end{align*}\]</span>
As we saw above, the sum of <span class="math inline">\(n\)</span> chi-square random variables, each
distributed with 1 degree of freedom, is itself
chi-square-distributed for <span class="math inline">\(n\)</span> degrees of freedom. Hence, in general,
if <span class="math inline">\(W \sim \chi_{n}^2\)</span>, then <span class="math inline">\(E[W] = n\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="computing-probabilities-3" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume at first that we have a single random variable
<span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(Z^2 &gt; 1)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We know that <span class="math inline">\(Z^2\)</span> is sampled from a chi-square distribution for
1 degree of freedom. So
<span class="math display">\[
P(Z^2 &gt; 1) = 1 - P(Z^2 \leq 1) = 1 - F_{W(1)}(1) \,.
\]</span>
This is not easily computed by hand, so we utilize <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="the-normal-and-related-distributions.html#cb77-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="dv">1</span>,<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.3173105</code></pre>
<blockquote>
<p>The probability is 0.3173. With the benefit of hindsight,
we can see why this was going to
be the value all along: we know that <span class="math inline">\(P(-1 \leq Z \leq 1) = 0.6827\)</span>, and
thus <span class="math inline">\(P(\vert Z \vert &gt; 1) = 1-0.6827 = 0.3173\)</span>. If we square both sides
in this last probability statement, we see that <span class="math inline">\(P(\vert Z \vert &gt; 1) =
P(Z^2 &gt; 1)\)</span>.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>What is the value <span class="math inline">\(a\)</span> such that <span class="math inline">\(P(W &gt; a) = 0.9\)</span>, where
<span class="math inline">\(W = Z_1^2 + \cdots + Z_4^2\)</span> and <span class="math inline">\(\{Z_1,\ldots,Z_4\}\)</span> are iid standard
normal random variables?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we recognize that <span class="math inline">\(W \sim \chi_4^2\)</span>, i.e., <span class="math inline">\(W\)</span> is chi-square
distributed for 4 degrees of freedom.</p>
</blockquote>
<blockquote>
<p>Second, we re-express <span class="math inline">\(P(W &gt; a)\)</span> in terms of the cdf of the chi-square
distribution,
<span class="math display">\[
P(W &gt; a) = 1 - P(W \leq a) = 1 - F_{W(4)}(a) = 0.9 \,,
\]</span>
and we rearrange terms:
<span class="math display">\[
F_{W(4)}(a) = 0.1 \,.
\]</span>
To isolate <span class="math inline">\(a\)</span>, we use the inverse CDF function:
<span class="math display">\[
F_{W(4)}^{-1} [F_{W(4)}(a)] = a = F_{W(4)}^{-1}(0.1) \,.
\]</span>
To compute <span class="math inline">\(a\)</span>, we use the <code>R</code> function <code>qchisq()</code>:</p>
</blockquote>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="the-normal-and-related-distributions.html#cb79-1" tabindex="-1"></a>(<span class="at">a =</span> <span class="fu">qchisq</span>(<span class="fl">0.1</span>,<span class="dv">4</span>))</span></code></pre></div>
<pre><code>## [1] 1.063623</code></pre>
<blockquote>
<p>We find that <span class="math inline">\(a = 1.064\)</span>.</p>
</blockquote>
</div>
</div>
<div id="sample-variance-of-normal-random-variables" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Sample Variance of Normal Random Variables<a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall the definition of the sample variance:
<span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span>
The reason why we divide by <span class="math inline">\(n-1\)</span> instead of <span class="math inline">\(n\)</span> is that it makes
<span class="math inline">\(S^2\)</span> an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. We will illustrate this
point below when we cover point estimation.</p>
<p>Above, we showed how the mean for a sample of independent and
identically distributed normal random variables is itself normal,
with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>. What, on the other hand,
is the distribution of <span class="math inline">\(S^2\)</span>?</p>
<p>Let’s suppose that we have <span class="math inline">\(n\)</span> iid normal random variables. We can write
down that
<span class="math display">\[
W = \sum_{i=1}^n \left( \frac{X_i-\mu}{\sigma} \right)^2 \,,
\]</span>
and we know from results above that <span class="math inline">\(W \sim \chi_n^2\)</span>.
We can work with the expression to the right of the equals sign
now to determine the distribution not of <span class="math inline">\(S^2\)</span> itself, but
of <span class="math inline">\((n-1)S^2/\sigma^2\)</span>:
<span class="math display">\[\begin{align*}
\sum_{i=1}^n \left( \frac{X_i-\mu}{\sigma} \right)^2 &amp;= \sum_{i=1}^n \left( \frac{(X_i-\bar{X})+(\bar{X}-\mu)}{\sigma} \right)^2 \\
&amp;= \sum_{i=1}^n \left( \frac{X_i-\bar{X}}{\sigma}\right)^2 + \sum_{i=1}^n \left( \frac{\bar{X}-\mu}{\sigma}\right)^2 + \mbox{cross~term~equaling~zero} \\
&amp;= \frac{(n-1)S^2}{\sigma^2} + n\left(\frac{\bar{X}-\mu}{\sigma}\right)^2 = \frac{(n-1)S^2}{\sigma^2} + \left(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\right)^2 \,.
\end{align*}\]</span>
The expression farthest to the right,
<span class="math display">\[
Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \,,
\]</span>
is the standardization of <span class="math inline">\(\bar{X} \sim \mathcal{N}(\mu,\sigma^2/n)\)</span>,
and thus it is standard-normal distributed, and thus
<span class="math inline">\(Z^2 \sim \chi_1^2\)</span>. By utilizing the
general result about the sum of chi-square-distributed random variables
given at the end of the last section, we can immediately
see that
<span class="math display">\[
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2 \,.
\]</span>
We note that if we want to determine the distribution of <span class="math inline">\(S^2\)</span> itself,
all we would need to do is perform a general transformation, following
the steps outlined earlier.</p>
<hr />
<div id="computing-probabilities-4" class="section level3 hasAnchor" number="2.9.1">
<h3><span class="header-section-number">2.9.1</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s suppose we sample <span class="math inline">\(16\)</span> iid normal random variables, and we know
that <span class="math inline">\(\sigma^2 = 10\)</span>. What is the probability <span class="math inline">\(P(S^2 &gt; 12)\)</span>?</p>
</blockquote>
<blockquote>
<p>(We’ll stop for a moment to answer a question that might occur to the reader.
“Why are we doing a problem where we assume <span class="math inline">\(\sigma^2\)</span> is known? In real life,
it almost certainly won’t be.” This is an entirely fair question.
This example <em>is</em> contrived, but it builds towards a particular situation
where we <em>can</em> assume a value for <span class="math inline">\(\sigma^2\)</span>: hypothesis testing. The
calculation we will do below is analogous to calculations
we will do later when testing hypotheses about normal population variances.)</p>
</blockquote>
<blockquote>
<p>The first question that we should always ask ourselves is whether
we know the distribution of the quantity in the probability statement,
in this case <span class="math inline">\(S^2\)</span>. The answer is no. (As noted above, we can derive it,
but we have not explicitly done so.) However,
we <em>do</em> know the distribution of <span class="math inline">\((n-1)S^2/\sigma^2\)</span>.
Hence:
<span class="math display">\[\begin{align*}
P(S^2 &gt; 12) &amp;= P\left(\frac{(n-1)S^2}{\sigma^2} &gt; \frac{(n-1) \cdot 12}{\sigma^2}\right) \\
&amp;= P\left(W &gt; \frac{15 \cdot 12}{10}\right) = P(W &gt; 18) = 1 - P(W \leq 18) = 1 - F_{W(15)}(18) \,,
\end{align*}\]</span>
where <span class="math inline">\(W \sim \chi_{n-1}^2\)</span> and <span class="math inline">\(n-1 = 15\)</span>. To compute the probability,
we use <code>pchisq()</code>:</p>
</blockquote>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="the-normal-and-related-distributions.html#cb81-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(<span class="dv">18</span>,<span class="dv">15</span>)</span></code></pre></div>
<pre><code>## [1] 0.2626656</code></pre>
<blockquote>
<p>The probability is 0.263: there is only a 26.3% chance that we will
sample a value of <span class="math inline">\(S^2\)</span> greater than 12 when the true value is 10.</p>
</blockquote>
<blockquote>
<p>To estimate the probability via simulation, we can</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>repeatedly generate data samples of size <span class="math inline">\(n = 16\)</span> from a normal
distribution with some arbitrary mean (the value doesn’t matter) and
variance <span class="math inline">\(\sigma^2 = 10\)</span>;</li>
<li>compute and record <span class="math inline">\(S^2\)</span>; and</li>
<li>determine the proportion of our simulated <span class="math inline">\(S^2\)</span> values that are
greater than 12.</li>
</ol>
</blockquote>
<blockquote>
<p>Note that we record <span class="math inline">\(S^2\)</span> and not <span class="math inline">\((n-1)S^2/\sigma^2\)</span>; as we are
performing a simulation, we need not know the distribution of <span class="math inline">\(S^2\)</span>
(because…we are simulating it!).
All we need to know is the distribution from which the individual data
are sampled. Also note that
because we are not simulating an infinite number of data, the
proportion that we observe will itself be a random variable with
some mean, some variance, and some sampling distribution. The key
here is “some variance”: to generate a more accurate accounting of
the proportion of <span class="math inline">\(S^2\)</span> values greater than 12, we want to sample
as much data as we can (a) without having to wait too long
for the result, and (b) without causing memory allocation issues by
recording too many values of <span class="math inline">\(S^2\)</span>, if we choose to record them all.</p>
</blockquote>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="the-normal-and-related-distributions.html#cb83-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)       <span class="co"># set so that the same data are generated every time</span></span>
<span id="cb83-2"><a href="the-normal-and-related-distributions.html#cb83-2" tabindex="-1"></a>m      <span class="ot">&lt;-</span> <span class="dv">100000</span>    <span class="co"># the number of data samples</span></span>
<span id="cb83-3"><a href="the-normal-and-related-distributions.html#cb83-3" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">16</span>        <span class="co"># the size of each data sample</span></span>
<span id="cb83-4"><a href="the-normal-and-related-distributions.html#cb83-4" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">10</span>        <span class="co"># the true variance</span></span>
<span id="cb83-5"><a href="the-normal-and-related-distributions.html#cb83-5" tabindex="-1"></a>s2     <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,m) <span class="co"># allocated space for S^2</span></span>
<span id="cb83-6"><a href="the-normal-and-related-distributions.html#cb83-6" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m ) {</span>
<span id="cb83-7"><a href="the-normal-and-related-distributions.html#cb83-7" tabindex="-1"></a>  x      <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb83-8"><a href="the-normal-and-related-distributions.html#cb83-8" tabindex="-1"></a>  s2[ii] <span class="ot">&lt;-</span> <span class="fu">var</span>(x)</span>
<span id="cb83-9"><a href="the-normal-and-related-distributions.html#cb83-9" tabindex="-1"></a>}</span>
<span id="cb83-10"><a href="the-normal-and-related-distributions.html#cb83-10" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>(s2<span class="sc">&gt;</span><span class="dv">12</span>)<span class="sc">/</span>m,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.262</code></pre>
<blockquote>
<p>Another way to code this to circumvent memory allocation is</p>
</blockquote>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="the-normal-and-related-distributions.html#cb85-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)       <span class="co"># set so that the same data are generated every time</span></span>
<span id="cb85-2"><a href="the-normal-and-related-distributions.html#cb85-2" tabindex="-1"></a>m      <span class="ot">&lt;-</span> <span class="dv">100000</span>    <span class="co"># the number of data samples</span></span>
<span id="cb85-3"><a href="the-normal-and-related-distributions.html#cb85-3" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">16</span>        <span class="co"># the size of each data sample</span></span>
<span id="cb85-4"><a href="the-normal-and-related-distributions.html#cb85-4" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">10</span>        <span class="co"># the true variance</span></span>
<span id="cb85-5"><a href="the-normal-and-related-distributions.html#cb85-5" tabindex="-1"></a>s2true <span class="ot">&lt;-</span>  <span class="dv">0</span>        <span class="co"># a counter of the number of S^2 values &gt; 12</span></span>
<span id="cb85-6"><a href="the-normal-and-related-distributions.html#cb85-6" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m ) {</span>
<span id="cb85-7"><a href="the-normal-and-related-distributions.html#cb85-7" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb85-8"><a href="the-normal-and-related-distributions.html#cb85-8" tabindex="-1"></a>  <span class="cf">if</span> ( <span class="fu">var</span>(x) <span class="sc">&gt;</span> <span class="dv">12</span>) s2true <span class="ot">=</span> s2true<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb85-9"><a href="the-normal-and-related-distributions.html#cb85-9" tabindex="-1"></a>}</span>
<span id="cb85-10"><a href="the-normal-and-related-distributions.html#cb85-10" tabindex="-1"></a><span class="fu">round</span>(s2true<span class="sc">/</span>m,<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.262</code></pre>
<blockquote>
<p>The tradeoff is that while this code uses less memory, it takes
(slightly) longer to run.</p>
</blockquote>
<hr />
</div>
<div id="expected-value-of-the-sample-variance-and-standard-deviation" class="section level3 hasAnchor" number="2.9.2">
<h3><span class="header-section-number">2.9.2</span> Expected Value of the Sample Variance and Standard Deviation<a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>It is extremely straightforward to determine the expected value for
the sample variance:
<span class="math display">\[
E\left[ \frac{(n-1)S^2}{\sigma^2} \right] = n-1 ~~~ \Rightarrow ~~~ E[S^2] = \frac{(n-1)\sigma^2}{(n-1)} = \sigma^2 \,.
\]</span>
Here, we use the fact that <span class="math inline">\(W = (n-1)S^2/\sigma^2\)</span> is a chi-square-distributed
random variable, and that <span class="math inline">\(E[W]\)</span> equals the number of degrees of freedom,
which here is <span class="math inline">\(n-1\)</span>.</p>
</blockquote>
<blockquote>
<p>What is more difficult to determine is <span class="math inline">\(E[(S^2)^a]\)</span>, where <span class="math inline">\(a\)</span> is some
constant. For instance, <span class="math inline">\(E[S^4]\)</span> is <em>not</em> <span class="math inline">\(\sigma^4\)</span> (making the variance
of <span class="math inline">\(S^2\)</span> somewhat more difficult to compute than we might initiall
expect)…and <span class="math inline">\(E[S]\)</span> is <em>not</em> <span class="math inline">\(\sigma\)</span>. Let’s determine <span class="math inline">\(E[S]\)</span> here.</p>
</blockquote>
<blockquote>
<p>The way we do this is by performing a random variable transformation
(<span class="math inline">\(S = \sqrt{S^2}\)</span>) to determine the pdf <span class="math inline">\(f_S(s)\)</span>, and then
computing <span class="math inline">\(E[S] = \int s f_S(s) ds\)</span>.</p>
</blockquote>
<blockquote>
<p>We start by writing
<span class="math display">\[\begin{align*}
P(S \leq s) &amp;= P(\sqrt{S^2} \leq s) = P(S^2 \leq s^2) = P\left( \frac{(n-1)S^2}{\sigma^2} \leq \frac{(n-1)s^2}{\sigma^2} \right) \\
&amp;= P\left( W \leq \frac{(n-1)s^2}{\sigma^2} \right) = F_{W(n-1)}\left( \frac{(n-1)s^2}{\sigma^2} \right) \,.
\end{align*}\]</span>
We can then use the chain rule of differentiation to write that
<span class="math display">\[
f_S(s) = \frac{d}{ds}F_{W(n-1)}\left( \frac{(n-1)s^2}{\sigma^2} \right) = f_{W(n-1)}F_{W(n-1)}\left( \frac{(n-1)s^2}{\sigma^2} \right) \frac{2(n-1)s}{\sigma^2} \,.
\]</span>
We then substitute in the pdf:
<span class="math display">\[
f_S(s) = \frac{2(n-1)s}{\sigma^2} \frac{[(n-1)s^2/\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\Gamma((n-1)/2)} \exp\left(-\frac{(n-1)s^2}{2\sigma^2}\right) \,.
\]</span>
The expected value of <span class="math inline">\(S\)</span> is then
<span class="math display">\[
E[S] = \int_0^\infty s f_S(s) ds = \int_0^\infty \frac{2(n-1)s^2}{\sigma^2} \frac{[(n-1)s^2/\sigma^2]^{(n-3)/2}}{2^{(n-1)/2}\Gamma((n-1)/2)} \exp\left(-\frac{(n-1)s^2}{2\sigma^2}\right) ds \,.
\]</span>
OK…how should we pursue this? Let’s try a variable substitution approach,
with
<span class="math display">\[
x = \frac{(n-1)s^2}{2\sigma^2} ~~~ \Rightarrow ~~~ dx = \frac{(n-1)s}{\sigma^2}ds = \frac{n-1}{\sigma^2} \left(\frac{2 \sigma^2 x}{n-1}\right)^{1/2} ds = \left(\frac{ 2 (n-1) x }{\sigma^2} \right)^{1/2} ds \,.
\]</span>
We note that if <span class="math inline">\(s = 0\)</span>, <span class="math inline">\(x = 0\)</span>, and if <span class="math inline">\(s = \infty\)</span>, <span class="math inline">\(x = \infty\)</span>, so
the integral bounds are unchanged. So now we have that
<span class="math display">\[\begin{align*}
E[S] &amp;= \int_0^\infty 4x \frac{(2x)^{(n-3)/2}}{2^{(n-1)/2}\Gamma((n-1)/2)} \exp(-x) \frac{\sigma}{\sqrt{2(n-1)x}} dx \\
&amp;= \sqrt{2} \int_0^\infty \frac{x^{(n-2)/2}}{\Gamma((n-1)/2)} \exp(-x) \frac{\sigma}{\sqrt{n-1}} dx \\
&amp;= \sqrt{2} \frac{\sigma}{\sqrt{n-1}} \frac{1}{\Gamma((n-1)/2)} \int_0^\infty x^{n/2-1} \exp(-x) dx \\
&amp;= \sigma \sqrt{\frac{2}{n-1}} \frac{\Gamma(n/2)}{\Gamma((n-1)/2)} \,.
\end{align*}\]</span>
The integral in the second-to-last line is that which defines the
gamma function <span class="math inline">\(\Gamma(\cdot)\)</span>. So we see that while <span class="math inline">\(S^2\)</span> is an unbiased
estimator of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(S\)</span> is a <em>biased</em> estimator of <span class="math inline">\(\sigma\)</span>.
We note, without getting into details,
that as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(E[S] \rightarrow \sigma\)</span>, so <span class="math inline">\(S\)</span> is
asymptotically unbiased.</p>
</blockquote>
</div>
</div>
<div id="standardizing-a-normal-random-variable-with-unknown-variance" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Standardizing a Normal Random Variable with Unknown Variance<a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is generally the case in real-life that
when we assume our data are sampled from a normal distribution, both
the mean <em>and</em> the variance are unknown. This means that instead of this
<span class="math display">\[
Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1) \,,
\]</span>
we have
<span class="math display">\[
\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim \mbox{?} \,.
\]</span>
This last expression contains <em>two</em> random variables, one in the
numerator and one in the denominator, and each is independent of the other.
(We state this without proof…but we will appeal to intuition. For many
distributions, the expected value and variance are <em>both</em> functions of one
or more parameters <span class="math inline">\(\theta\)</span> [e.g., the exponential, binomial, and Poisson
distributions, etc., etc.]. However, for a normal distribution, the
expected value only depends on <span class="math inline">\(\mu\)</span> and the variance only depends on
<span class="math inline">\(\sigma^2\)</span>.) How would we determine the distribution of the ratio above?</p>
<p>There is no unique way by which to approach the derivation of the
distribution: we simply illustrate one way of doing so. First, we rewrite
the ratio above as
<span class="math display">\[
T = \frac{\bar{X}-\mu}{S/\sqrt{n}} = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} / \frac{S}{\sigma} = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} / \sqrt{\frac{S^2}{\sigma^2}} = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} / \left( \sqrt{\frac{\nu S^2}{\sigma^2}} \frac{1}{\sqrt{\nu}} \right) = Z / \left(\sqrt{\frac{W_{\nu}}{\nu}}\right) \,,
\]</span>
where <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span> and <span class="math inline">\(W_{\nu}\)</span> is sampled from
a chi-square distribution with <span class="math inline">\(\nu = n-1\)</span> degrees of freedom.
Our next step is to determine the distribution
of the random variable <span class="math inline">\(U = \sqrt{W_{\nu}/\nu}\)</span>.
We identify that <span class="math inline">\(V = \sqrt{W_{\nu}}\)</span> is sampled from a
<em>chi distribution</em> (note: no square!), whose pdf may be derived via
a general transformation (since <span class="math inline">\(V = \sqrt{\nu}S/\sigma\)</span> and since we have
written down <span class="math inline">\(f_S(s)\)</span> above), but can also simply be looked up:
<span class="math display">\[
f_V(v) = \frac{v^{\nu-1} \exp(-v^2/2)}{2^{\nu/2-1}\Gamma(\nu/2)} \,,
\]</span>
for <span class="math inline">\(v \geq 0\)</span> and <span class="math inline">\(\nu &gt; 0\)</span>. However, we <em>do</em> still have to apply a
general transformation to determine the pdf for <span class="math inline">\(U = V/\sqrt{\nu}\)</span>.
Doing so, we find that
<span class="math display">\[
f_U(u) = f_V(\sqrt{\nu}u) \sqrt{\nu} = \frac{\sqrt{\nu} (\sqrt{\nu}u)^{\nu-1} \exp(-(\sqrt{\nu}u)^2/2)}{2^{\nu/2-1}\Gamma(\nu/2)} \,.
\]</span>
(So, yes, we could have derived <span class="math inline">\(f_U(u)\)</span> directly from <span class="math inline">\(f_S(s)\)</span> via a general
transformation…but here we at least indicate to the reader that there is
such a thing as the chi distribution.)</p>
<p>At this point, we know the distributions for both the numerator and the denominator.
Now we write down a general result dubbed the <em>ratio distribution</em>.
For <span class="math inline">\(T = Z/U\)</span>, where <span class="math inline">\(Z\)</span> and <span class="math inline">\(U\)</span> are independent variables, that distribution is
<span class="math display">\[
f_T(t) = \int_{-\infty}^\infty \vert u \vert f_Z(tu) f_{U}(u) du \rightarrow \int_0^\infty u f_Z(tu) f_{U}(u) du \,,
\]</span>
where we make use of the fact that <span class="math inline">\(u &gt; 0\)</span>. Thus
<span class="math display">\[\begin{align*}
f_T(t) &amp;= \int_0^\infty u \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{t^2u^2}{2}\right) \frac{\sqrt{\nu} (\sqrt{\nu}u)^{\nu-1} \exp(-(\sqrt{\nu}u)^2/2)}{2^{\nu/2-1}\Gamma(\nu/2)} du \\
&amp;= \frac{1}{\sqrt{2\pi}} \frac{1}{2^{\nu/2-1}\Gamma(\nu/2)} \nu^{\nu/2} \int_0^\infty u^\nu \exp\left(-\frac{(t^2+\nu)u^2}{2}\right) du \,.
\end{align*}\]</span>
Given the form of the integral and the fact that it is evaluated
from zero to infinity, we will use a variable substitution approach and
eventually turn the integral into a gamma function:
<span class="math display">\[
x = \frac{(t^2+\nu)u^2}{2} ~~~ \Rightarrow ~~~ dx = (t^2+\nu)~u~du \,.
\]</span>
When <span class="math inline">\(u = 0\)</span>, <span class="math inline">\(x = 0\)</span>, and when <span class="math inline">\(u = \infty\)</span>, <span class="math inline">\(x = \infty\)</span>, so the integral
bounds are unchanged. Hence we can rewrite the integral above as
<span class="math display">\[\begin{align*}
f_T(t) &amp;= \frac{1}{\sqrt{2\pi}} \frac{1}{2^{\nu/2-1}\Gamma(\nu/2)} \nu^{\nu/2} \int_0^\infty \left(\sqrt{\frac{2x}{(t^2+\nu)}}\right)^\nu \exp\left(-x\right) \frac{dx}{\sqrt{2 x (t^2+\nu)}} \\
&amp;= \frac{1}{\sqrt{2\pi}} \frac{1}{2^{\nu/2-1}\Gamma(\nu/2)} \nu^{\nu/2} 2^{\nu/2} 2^{-1/2} \frac{1}{(t^2+\nu)^{(\nu+1)/2}} \int_0^\infty \frac{x^{\nu/2}}{x^{1/2}} \exp\left(-x\right) dx \\
&amp;= \frac{1}{\sqrt{\pi}} \frac{1}{\Gamma(\nu/2)} \nu^{\nu/2} \frac{1}{(t^2+\nu)^{(\nu+1)/2}} \int_0^\infty x^{\nu/2-1/2} \exp\left(-x\right) dx \\
&amp;= \frac{1}{\sqrt{\pi}} \frac{1}{\Gamma(\nu/2)} \nu^{\nu/2} \frac{1}{(t^2+\nu)^{(\nu+1)/2}} \Gamma((\nu+1)/2) \\
&amp;= \frac{1}{\sqrt{\pi}} \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)} \nu^{\nu/2} \frac{1}{\nu^{(\nu+1)/2}(t^2/\nu+1)^{(\nu+1)/2}} \\
&amp;= \frac{1}{\sqrt{\nu \pi}} \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)} \left(1+\frac{t^2}{\nu}\right)^{-(\nu+1)/2} \,.
\end{align*}\]</span>
<span class="math inline">\(T\)</span> is sampled from a <em>Student’s t distribution</em> for <span class="math inline">\(\nu\)</span> degrees of
freedom. Assuming that <span class="math inline">\(\nu\)</span> is integer-valued, the expected value of <span class="math inline">\(T\)</span>
is <span class="math inline">\(E[T] = 0\)</span> for <span class="math inline">\(\nu \geq 2\)</span> (and is undefined otherwise), while the variance
is <span class="math inline">\(\nu/(\nu-2)\)</span> for <span class="math inline">\(\nu \geq 3\)</span>, is infinite for <span class="math inline">\(\nu=2\)</span>, and is otherwise
undefined. Appealing to intuition, we can form a (symmetric)
<span class="math inline">\(t\)</span> distribution by taking
a standard normal <span class="math inline">\(\mathcal{N}(0,1)\)</span> and “pushing down” in the center, the
act of which transfers probability density equally to both the lower and
upper tails. In Figure <a href="the-normal-and-related-distributions.html#fig:t">2.8</a>, we see that the smaller the
number of degrees of freedom, the more density is transferred to the tails,
and that as <span class="math inline">\(\nu\)</span> increases, the more and more <span class="math inline">\(f_{T(\nu)}(t)\)</span> becomes
indistinguishable from a standard normal.
(The more technical way of stating this is that the random variable <span class="math inline">\(T\)</span>
converges in distribution to a standard normal random variable
as <span class="math inline">\(\nu \rightarrow \infty\)</span>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t"></span>
<img src="_main_files/figure-html/t-1.png" alt="\label{fig:t}The natural logarithm of the pdf for the standard normal (black) and for $t$ distributions with 3 (red dashed), 6 (green dotted), and 12 (blue dot-dashed) degrees of freedom. We observe that as $n$ decreases, there is more probability density in the lower and upper tails of the distributions." width="50%" />
<p class="caption">
Figure 2.8: The natural logarithm of the pdf for the standard normal (black) and for <span class="math inline">\(t\)</span> distributions with 3 (red dashed), 6 (green dotted), and 12 (blue dot-dashed) degrees of freedom. We observe that as <span class="math inline">\(n\)</span> decreases, there is more probability density in the lower and upper tails of the distributions.
</p>
</div>
<hr />
<div id="computing-probabilities-5" class="section level3 hasAnchor" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In a study, the diameters of 8 widgets are measured. It is known from
previous work that the widget diamaters are normally distributed, with
sample standard deviation <span class="math inline">\(S = 1\)</span> unit. What is the probability that the
the sample mean <span class="math inline">\(\bar{X}\)</span> observed in the current study will be within one
unit of the population mean <span class="math inline">\(\mu\)</span>?</p>
</blockquote>
<blockquote>
<p>The probability we seek is
<span class="math display">\[
P( \vert \bar{X} - \mu \vert &lt; 1 ) = P(\mu - 1 \leq \bar{X} \leq \mu + 1) \,.
\]</span>
The key here is that we don’t know <span class="math inline">\(\sigma\)</span>, so the probability can only
be determined if we manipulate this expression such that the random
variable inside it is <span class="math inline">\(t\)</span>-distributed…and <span class="math inline">\(\bar{X}\)</span> itself is <em>not</em>.
So the first step is to standardize:
<span class="math display">\[
P\left( \frac{\mu - 1 - \mu}{S/\sqrt{n}} \leq \frac{\bar{X} - \mu}{S/\sqrt{n}} \leq \frac{\mu + 1 - \mu}{S/\sqrt{n}}\right) = P\left( -\frac{\sqrt{n}}{S} \leq T \leq \frac{\sqrt{n}}{S}\right) \,.
\]</span>
We know that <span class="math inline">\(\sqrt{n}(\bar{X} - \mu)/S\)</span> is <span class="math inline">\(t\)</span>-distributed (with <span class="math inline">\(\nu = n-1\)</span>
degrees of freedom), so long as the individual data are normal
iid random variables. (If the individual data are not normally distributed,
this question might have to be solved via simulations…if we cannot
determine the distribution of <span class="math inline">\(\bar{X}\)</span> using, e.g., moment-generating
functions.)</p>
</blockquote>
<blockquote>
<p>The probability is the difference between two cdf values:
<span class="math display">\[
P\left( -\frac{\sqrt{n}}{S} \leq T \leq \frac{\sqrt{n}}{S}\right) = F_{T(7)}(\sqrt{8}) - F_{T(7)}(-\sqrt{8}) \,,
\]</span>
where <span class="math inline">\(n-1\)</span>, the number of degrees of freedom, is 7. This is the
end of the problem if we are using pen and paper. If we have <code>R</code> at our
disposal, we would code the following:</p>
</blockquote>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="the-normal-and-related-distributions.html#cb87-1" tabindex="-1"></a><span class="fu">pt</span>(<span class="fu">sqrt</span>(<span class="dv">8</span>),<span class="dv">7</span>) <span class="sc">-</span> <span class="fu">pt</span>(<span class="sc">-</span><span class="fu">sqrt</span>(<span class="dv">8</span>),<span class="dv">7</span>)</span></code></pre></div>
<pre><code>## [1] 0.9745364</code></pre>
<blockquote>
<p>The probability that <span class="math inline">\(\bar{X}\)</span> will be within one unit of <span class="math inline">\(\mu\)</span> is 0.975.</p>
</blockquote>
</div>
</div>
<div id="point-estimation-1" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Point Estimation<a href="the-normal-and-related-distributions.html#point-estimation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter, we introduced the concept of point estimation, using
statistics to make inferences about a population parameter <span class="math inline">\(\theta\)</span>. Recall
that a point estimator <span class="math inline">\(\hat{\theta}\)</span>,
being a statistic, is a random variable and has a
sampling distribution. One can
define point estimators arbitrarily, but in the end, we can choose the best
among a set of estimators by assessing properties of their sampling distributions:</p>
<ul>
<li>Bias: <span class="math inline">\(B[\hat{\theta}] = E[\hat{\theta}-\theta]\)</span></li>
<li>Variance: <span class="math inline">\(V[\hat{\theta}]\)</span></li>
</ul>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p>Assuming that our observed data are independent and identically distributed (or
iid), then computing each of these quantities is straightforward. Choosing a
best estimator based on these two quantities might not lead to a clear
answer, but we can overcome that obstacle by combining them into one metric,
the mean-squared error (or MSE):</p>
<ul>
<li>MSE: <span class="math inline">\(MSE[\hat{\theta}] = B[\hat{\theta}]^2 + V[\hat{\theta}]\)</span></li>
</ul>
<p>In the previous chapter, we also discussed how guessing the form of an
estimator is sub-optimal, and how there are various approaches to deriving
good estimators. The first one that we highlight is maximum likelihood
estimation (or MLE). We will apply the MLE here to derive an estimator for
the normal population mean.
Then we will introduce methodology by which to assess the
estimator in an absolute sense, and extend these results to write down the
asymptotic sampling distribution for the MLE, i.e., the sampling distribution
as the sample size <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>The setting, again, is that we have randomly sampled <span class="math inline">\(n\)</span> data from a
normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. This means that the
likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(\mu,\sigma \vert \mathbf{x}) &amp;= \prod_{i=1}^n f_X(x \vert \mu,\sigma) \\
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right) \\
&amp;= \frac{1}{(2 \pi)^{n/2}} \frac{1}{\sigma^n} \exp\left({-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2}\right) \,,
\end{align*}\]</span>
and the log-likelihood is
<span class="math display">\[
\ell(\mu,\sigma \vert \mathbf{x}) = -\frac{n}{2} \log (2 \pi) - n \log \sigma - \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \,.
\]</span>
Because there is more than one parameter, we take the partial derivative of
<span class="math inline">\(\ell\)</span> with respect to <span class="math inline">\(\mu\)</span>:
<span class="math display">\[\begin{align*}
\ell&#39;(\mu,\sigma \vert \mathbf{x}) &amp;= \frac{\partial}{\partial \mu} \left( -\frac{n}{2} \log (2 \pi) - n \log \sigma - \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right) \\
&amp;= -\frac{1}{\sigma^2} \sum_{i=1}^n 2(x_i - \mu)(-1) \\
&amp;= \frac{2}{\sigma^2} \sum_{i=1}^n (x_i - \mu) \,.
\end{align*}\]</span>
After setting this quantity to zero and dividing out the term <span class="math inline">\(2/\sigma^2\)</span>,
we find that
<span class="math display">\[
\sum_{i=1}^n x_i = \sum_{i=1}^n \mu ~~\Rightarrow~~ \sum_{i=1}^n x_i = n \mu ~~\Rightarrow~~ \frac{1}{n} \sum_{i=1}^n x_i = n \mu ~~\Rightarrow~~ \mu = \bar{x} \,,
\]</span>
and thus <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>. (Recall that as we go from a purely
mathematical derivation to a final definition of an estimator, we need to take
into account that the estimator is a random variable and thus we need to
change from lower-case to upper-case when writing out the variable.
Also recall that for this MLE to be valid, the second derivative of the
log-likelihood function <span class="math inline">\(\ell(\theta \vert \mathbf{x})\)</span> at <span class="math inline">\(x = \bar{x}\)</span>
has to be negative. We leave checking this as an exercise to the
reader.)</p>
<p>Because the estimator is <span class="math inline">\(\bar{X}\)</span>, we know immediately its expected value and
variance given previous results:</p>
<ul>
<li><span class="math inline">\(E[\hat{\mu}_{MLE}] = E[\bar{X}] = \mu\)</span> (the estimator is unbiased); and</li>
<li><span class="math inline">\(V[\hat{\mu}_{MLE}] = V[\bar{X}] = \sigma^2/n\)</span> (and thus <span class="math inline">\(MSE[\hat{\mu}_{MLE}] = \sigma^2/n\)</span>).</li>
</ul>
<p>While <span class="math inline">\(\hat{\mu}_{MLE}\)</span> is an unbiased estimator, recall that MLEs can
be biased…but if they are, they are always asymptotically unbiased, meaning
that the bias goes away as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Here, we will introduce one more estimator concept, that of <em>consistency</em>:
do both the bias and the variance of the estimator go to zero as the sample
size <span class="math inline">\(n\)</span> increases? If so, our estimator is said to be consistent.
If an estimator is not consistent, it will never converge to the true
value <span class="math inline">\(\theta\)</span>, regardless of sample size.
Here,
we need not worry about the bias (<span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span> in unbiased for
all <span class="math inline">\(n\)</span>), and we note that
<span class="math display">\[
\lim_{n \rightarrow \infty} \frac{\sigma^2}{n} = 0 \,.
\]</span>
Thus <span class="math inline">\(\hat{\mu}_{MLE}\)</span> is a consistent estimator.</p>
<p>The question that we are going to pose now refers to a point made obliquely
in the previous chapter: how do we assess <span class="math inline">\(V[\hat{\mu}_{MLE}]\)</span> in absolute
terms? Can we come up with an estimator that is more accurate for
any given value of <span class="math inline">\(n\)</span>?</p>
<p>The answer lies in the <em>Cramer-Rao inequality</em>, which specifies the
lower bound on the variance of an estimator (so long as the domain of
<span class="math inline">\(p_X(x \vert \theta)\)</span> or <span class="math inline">\(f_X(x \vert \theta)\)</span> does not depend on
<span class="math inline">\(\theta\)</span> itself…which is true for the normal). In this work, we focus
on determining lower bounds for <em>unbiased</em> estimators, but bounds can
be found for biased estimators as well, as shown
<a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">here</a>.</p>
<p>If we sample <span class="math inline">\(n\)</span> iid data,
then the <em>Cramer-Rao lower bound</em> (CRLB) on the
variance of an unbiased estimator <span class="math inline">\(\hat{\theta}\)</span> is
<span class="math display">\[
V[\hat{\theta}] = \frac{1}{I_n(\theta)} = \frac{1}{nI(\theta)} \,,
\]</span>
where <span class="math inline">\(I(\theta)\)</span> represents the <em>Fisher information</em> for <span class="math inline">\(\theta\)</span>
contained in a single random variable <span class="math inline">\(X\)</span>. Assuming we sample our data
from a pdf, the Fisher information is
<span class="math display">\[
I(\theta) = E\left[\left(\frac{\partial}{\partial \theta} \log f_X(x \vert \theta) \right)^2\right] ~~~\mbox{or}~~~ I(\theta) = -E\left[ \frac{\partial^2}{\partial \theta^2} \log f_X(x \vert \theta) \right] \,.
\]</span>
(The equation to the right is valid only under certain regularity
conditions, but those conditions are usually met in the context of
this book and thus this is the equation that we will use in general,
as it makes for more straightforward computation.)
Let’s step back for a second and think about what the Fisher information
represents. It is the average value of the square of the first derivative
of a pdf. If a pdf’s slope is relatively small (because the pdf is wide,
like a normal with a large <span class="math inline">\(\sigma\)</span> parameter), then the average value of
the slope, squared, is relatively small, i.e., the information that <span class="math inline">\(X\)</span>
provides about <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma\)</span> is relatively small. This makes
intuitive sense: a wide pdf means that, for instance, when we sample data
and try to subsequently estimate <span class="math inline">\(\theta\)</span>, we will be more uncertain about
its actual value.
On the other hand, if the pdf is highly concentrated, then the average slope
of the pdf is larger and…<span class="math inline">\(I(\theta)\)</span> is relatively large;
a sampled datum would contain more information
by which to constrain <span class="math inline">\(\theta\)</span>.
(See Figure <a href="the-normal-and-related-distributions.html#fig:fisher">2.9</a>, in which the pdf to the left
has less Fisher information content about <span class="math inline">\(\mu\)</span> than the pdf to the right.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fisher"></span>
<img src="_main_files/figure-html/fisher-1.png" alt="\label{fig:fisher}An illustration of Fisher information. For the pdf to the left, the slope changes slowly, and thus the rate of change of the slope is smaller than for the pdf to the right. Thus the pdf to the left contains less information about the population mean, i.e., its Fisher information value $I(\mu)$ is smaller." width="45%" /><img src="_main_files/figure-html/fisher-2.png" alt="\label{fig:fisher}An illustration of Fisher information. For the pdf to the left, the slope changes slowly, and thus the rate of change of the slope is smaller than for the pdf to the right. Thus the pdf to the left contains less information about the population mean, i.e., its Fisher information value $I(\mu)$ is smaller." width="45%" />
<p class="caption">
Figure 2.9: An illustration of Fisher information. For the pdf to the left, the slope changes slowly, and thus the rate of change of the slope is smaller than for the pdf to the right. Thus the pdf to the left contains less information about the population mean, i.e., its Fisher information value <span class="math inline">\(I(\mu)\)</span> is smaller.
</p>
</div>
<p>For the normal,
<span class="math display">\[\begin{align*}
\log f_X(X \vert \mu) &amp;= -\frac{1}{2} \log (2\pi\sigma^2) - \frac{(x - \mu)^2}{2\sigma^2} \\
\frac{\partial}{\partial \mu} \log f_X(X \vert \mu) &amp;= 0 - \frac{2(x-\mu)(-1)}{2\sigma^2} = \frac{(x-\mu)}{\sigma^2} \\
\frac{\partial^2}{\partial \mu^2} \log f_X(X \vert \mu) &amp;= -\frac{1}{\sigma^2} \,,
\end{align*}\]</span>
so
<span class="math display">\[
I(\mu) = -E\left[-\frac{1}{\sigma^2}\right] = \frac{1}{\sigma^2} \,,
\]</span>
and
<span class="math display">\[
V[\hat{\mu}] = \frac{1}{n (1/\sigma^2)} = \frac{\sigma^2}{n} \,.
\]</span>
We see that the variance of <span class="math inline">\(\hat{\mu} = \bar{X}\)</span> achieves the
CRLB, meaning that indeed we cannot propose a better
unbiased estimator for the normal population mean.</p>
<p>We conclude this section by using the Fisher information to state an
important result about maximum likelihood estimates. Recall that an
estimator is a statistic, and thus it is a random variable with a sampling
distribution. What do we know about this distribution? For arbitrary
values of <span class="math inline">\(n\)</span>, nothing, per se; we would have to run simulations to derive
an empirical sampling distribution. However, as <span class="math inline">\(n \rightarrow \infty\)</span>, the MLE
converges in distribution to a normal random variable:
<span class="math display">\[
\sqrt{n}(\hat{\theta}_{MLE}-\theta) \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(0,\frac{1}{I(\theta)}\right) ~\mbox{or}~ (\hat{\theta}_{MLE}-\theta) \stackrel{d}{\rightarrow} Y&#39; \sim \mathcal{N}\left(0,\frac{1}{nI(\theta)}\right) \,.
\]</span>
(As usual, some regularity conditions apply that do not concern us
at the current time.) We already knew about the mean zero part: we had
stated that the MLE is asymptotically unbiased. What’s new is the
variance, and the fact that the variance achieves the CRLB, and the
identification of the sampling distribution as being normal. (It turns
out that the normality of the distribution is related to the Central
Limit Theorem, the subject of the next section, and thus this is ultimately
not a surprising result.) A sketch of how one would prove the asymptotic
normality of the MLE is provided as optional material in Chapter 7.</p>
<hr />
<div id="maximum-likelihood-estimation-for-normal-variance" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> Maximum Likelihood Estimation for Normal Variance<a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As above, we will suppose that we are given a data sample
<span class="math inline">\(\{X_1,\ldots,X_n\} \stackrel{iid}{\sim} \mathcal{N}(\mu,\sigma^2)\)</span>,
but here, our desire is to estimate the population variance <span class="math inline">\(\sigma^2\)</span>
instead of the population mean.
Borrowing the form of the log-likelihood
<span class="math inline">\(\ell(\mu,\sigma^2 \vert \mathbf{x})\)</span> from above, we find that
<span class="math display">\[\begin{align*}
\frac{\partial \ell}{\partial \sigma^2} &amp;= -\frac{n}{2}\frac{1}{\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (x_i-\mu)^2 = 0 \\
~\implies~ \hat{\sigma^2}_{MLE} &amp;= \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2 = \hat{\sigma^2}_{MLE} = \frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2 \,,
\end{align*}\]</span>
where the last equality follows from the fact that <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>.
We can see immediately that
<span class="math display">\[
\hat{\sigma^2}_{MLE} = \frac{n-1}{n}S^2 \,.
\]</span>
Thus
<span class="math display">\[
E\left[\hat{\sigma^2}_{MLE}\right] = E\left[\frac{n-1}{n}S^2\right] = \frac{n-1}{n}\sigma^2 \,.
\]</span>
At this point, one might say,
“wait a minute…how do we know <span class="math inline">\(E[S^2] = \sigma^2\)</span>?
We know this because
<span class="math display">\[
E[S^2] = \frac{\sigma^2}{n-1} E\left[\frac{(n-1)S^2}{\sigma^2}\right] = \frac{\sigma^2}{n-1} E[W] = \frac{\sigma^2}{n-1} (n-1) = \sigma^2 \,,
\]</span>
where <span class="math inline">\(W\)</span> is a chi-square-distributed random variable for <span class="math inline">\(n-1\)</span> degrees
of freedom; <span class="math inline">\(E[W] = n-1\)</span>.
Now, to get back to the narrative at hand:
the MLE for <span class="math inline">\(\sigma^2\)</span> is a biased estimator, but it is
asymptotically unbiased:
<span class="math display">\[
\lim_{n \rightarrow \infty} \left( E\left[\hat{\sigma^2}_{MLE}\right] - \sigma^2 \right) = \lim_{n \rightarrow \infty} \left( \frac{n-1}{n}\sigma^2 - \sigma^2 \right) = 0 \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="asymptotic-normality-of-the-mle-for-the-normal-population-variance" class="section level3 hasAnchor" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Asymptotic Normality of the MLE for the Normal Population Variance<a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We will derive the Fisher information and use it to
to write down the Cramer-Rao lower bound
for the variances of estimates of the normal population variance.</p>
</blockquote>
<blockquote>
<p>We have that
<span class="math display">\[\begin{align*}
\log f_X(X \vert \sigma^2) &amp;= -\frac{1}{2} \log (2\pi\sigma^2) - \frac{(x - \mu)^2}{2\sigma^2} \\
\frac{\partial}{\partial \sigma^2} \log f_X(X \vert \sigma^2) &amp;= -\frac{1}{2}\frac{1}{2 \pi \sigma^2} 2 \pi + \frac{(x-\mu)^2}{2(\sigma^2)^2} = -\frac{1}{2 \sigma^2} + \frac{(x-\mu)^2}{2(\sigma^2)^2} \\
\frac{\partial^2}{\partial (\sigma^2)^2} \log f_X(X \vert \sigma^2) &amp;= \frac{1}{2 (\sigma^2)^2} - \frac{2(x-\mu)^2}{2(\sigma^2)^3} = \frac{1}{2(\sigma^2)^2} - \frac{(x-\mu)^2}{(\sigma^2)^3} \,,
\end{align*}\]</span>
The expected value is
<span class="math display">\[
E\left[ \frac{1}{2(\sigma^2)^2} - \frac{(x-\mu)^2}{(\sigma^2)^3} \right] = \frac{1}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} E[(x-\mu)^2] = \frac{1}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} \sigma^2 = -\frac{1}{2\sigma^4} \,,
\]</span>
and thus <span class="math inline">\(I(\sigma^2) = -E[-1/(2\sigma^4)] = 1/(2\sigma^4)\)</span>,
<span class="math inline">\(I_n(\sigma^2) = n/(2\sigma^4)\)</span>, and
<span class="math display">\[
\hat{\sigma^2} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(\sigma^2,\frac{2\sigma^4}{n}\right) \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance" class="section level3 hasAnchor" number="2.11.3">
<h3><span class="header-section-number">2.11.3</span> Simulating the Sampling Distribution of the MLE for the Normal Population Variance<a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The result that we derive immediately above is an asymptotic result…but
we never actually have an infinite sample size. If our sample size is low,
we should expect that the distribution of <span class="math inline">\(\hat{\sigma^2}\)</span> will deviate
substantially from the asymptotic expectation, but we cannot know by how
much unless we run simulations.</p>
<p>Below, we show how we might run a simulation if we assume that we
have <span class="math inline">\(n = 15\)</span> data drawn from a <span class="math inline">\(\mathcal{N}(0,4)\)</span> distribution. See
Figure <a href="the-normal-and-related-distributions.html#fig:sigma2hatmlefig">2.10</a>.</p>
</blockquote>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="the-normal-and-related-distributions.html#cb89-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb89-2"><a href="the-normal-and-related-distributions.html#cb89-2" tabindex="-1"></a>n          <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb89-3"><a href="the-normal-and-related-distributions.html#cb89-3" tabindex="-1"></a>sigma2     <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb89-4"><a href="the-normal-and-related-distributions.html#cb89-4" tabindex="-1"></a>k          <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb89-5"><a href="the-normal-and-related-distributions.html#cb89-5" tabindex="-1"></a>X          <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>k,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2)),<span class="at">nrow=</span>k)</span>
<span id="cb89-6"><a href="the-normal-and-related-distributions.html#cb89-6" tabindex="-1"></a>sigma2.hat <span class="ot">&lt;-</span> (n<span class="dv">-1</span>)<span class="sc">*</span><span class="fu">apply</span>(X,<span class="dv">1</span>,var)<span class="sc">/</span>n</span>
<span id="cb89-7"><a href="the-normal-and-related-distributions.html#cb89-7" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The sample mean for sigma2.hat               = &quot;</span>,<span class="fu">mean</span>(sigma2.hat),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The sample mean for sigma2.hat               =  3.679903</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="the-normal-and-related-distributions.html#cb91-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The sample standard deviation for sigma2.hat = &quot;</span>,<span class="fu">sd</span>(sigma2.hat),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The sample standard deviation for sigma2.hat =  1.362853</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="the-normal-and-related-distributions.html#cb93-1" tabindex="-1"></a>empirical.dist <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">sigma2.hat=</span>sigma2.hat)</span>
<span id="cb93-2"><a href="the-normal-and-related-distributions.html#cb93-2" tabindex="-1"></a>x              <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="dv">3</span><span class="sc">*</span>sigma2,<span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb93-3"><a href="the-normal-and-related-distributions.html#cb93-3" tabindex="-1"></a>y              <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x,<span class="at">mean=</span>sigma2,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">2</span>)<span class="sc">*</span>sigma2<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb93-4"><a href="the-normal-and-related-distributions.html#cb93-4" tabindex="-1"></a>asymptotic.pdf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)</span>
<span id="cb93-5"><a href="the-normal-and-related-distributions.html#cb93-5" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>empirical.dist,<span class="fu">aes</span>(<span class="at">x=</span>sigma2.hat,<span class="at">y=</span><span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb93-6"><a href="the-normal-and-related-distributions.html#cb93-6" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;blue&quot;</span>,<span class="at">col=</span><span class="st">&quot;black&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb93-7"><a href="the-normal-and-related-distributions.html#cb93-7" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>asymptotic.pdf,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb93-8"><a href="the-normal-and-related-distributions.html#cb93-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Estimate of sigma^2&quot;</span>) <span class="sc">+</span></span>
<span id="cb93-9"><a href="the-normal-and-related-distributions.html#cb93-9" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)) <span class="sc">+</span></span>
<span id="cb93-10"><a href="the-normal-and-related-distributions.html#cb93-10" tabindex="-1"></a>  base_theme</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sigma2hatmlefig"></span>
<img src="_main_files/figure-html/sigma2hatmlefig-1.png" alt="\label{fig:sigma2hatmlefig}The empirical pdf for $\hat{\sigma^2}_{MLE}$ given $n = 15$ and true variance $\sigma^2=4$. We overlay the asymptotic pdf in red." width="50%" />
<p class="caption">
Figure 2.10: The empirical pdf for <span class="math inline">\(\hat{\sigma^2}_{MLE}\)</span> given <span class="math inline">\(n = 15\)</span> and true variance <span class="math inline">\(\sigma^2=4\)</span>. We overlay the asymptotic pdf in red.
</p>
</div>
<blockquote>
<p>We see that if <span class="math inline">\(n\)</span> is small, the distribution of the MLE for
<span class="math inline">\(\hat{\sigma^2}\)</span> is definitely not normal, but right skew with a
mean smaller than the true mean (4) and standard deviation slightly
smaller than the true standard deviation (1.46).</p>
</blockquote>
</div>
</div>
<div id="the-central-limit-theorem" class="section level2 hasAnchor" number="2.12">
<h2><span class="header-section-number">2.12</span> The Central Limit Theorem<a href="the-normal-and-related-distributions.html#the-central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thus far in this chapter, we have assumed that we have sampled individual
data from normal distributions. While we have been able to illustrate
many concepts related to probability and statistical inference in this
setting, the reader may feel that this is unduly limiting: what if the data
we sample are not normally distributed? While we do examine the world beyond
normality in future chapters, there is one major concept we can discuss
now: the idea that <em>statistics</em> computed using non-normal data can themselves
have sampling distributions that are at least approximately normal. This is
big: it means that, e.g., we can utilize the same machinery for deriving
normal-based confidence intervals and for conducting normal-based
hypothesis tests, machinery that we outline below, to generate inferences
for these (approximately) normally distributed statistics.</p>
<p>The <em>central limit theorem</em>, or <em>CLT</em>, is one of the most important
probability theorems, if not the most important. It states that if
we have <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> with mean
<span class="math inline">\(E[X_i] = \mu\)</span> and finite variance <span class="math inline">\(V[X_i] = \sigma^2 &lt; \infty\)</span>, and
if <span class="math inline">\(n\)</span> is sufficiently large,
then <span class="math inline">\(\bar{X}\)</span> is approximately normally distributed:
<span class="math display">\[
\lim_{n \rightarrow \infty} P\left(\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \leq z \right) = \Phi(z) \,.
\]</span>
In other words,
<span class="math display">\[
{\bar X} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right) ~~\mbox{or, alternatively,}~~ X_+ = \sum_{i=1}^n X_i \stackrel{d}{\rightarrow} Y_+ \sim \mathcal{N}\left(n\mu,n\sigma^2\right) \,.
\]</span>
Note that above we added the caveat “if <span class="math inline">\(n\)</span> is sufficiently large.”
How large is sufficiently large? The historical rule of thumb is that
if <span class="math inline">\(n \gtrsim 30\)</span>, then we may utilize the CLT. However, the true answer
is that it depends on the distribution from which the data are sampled,
and thus that it never hurts to perform simulations to see if, e.g., fewer
(or more!) data are needed in a particular setting.</p>
<p>A proof of the CLT that utilizes moment-generating functions is given in
Chapter 7.</p>
<p>We note an apparent limitation of the CLT: here, we say that
<span class="math display">\[
\frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \stackrel{d}{\rightarrow} Z \sim \mathcal{N}(0,1) \,,
\]</span>
but in reality we rarely, if ever, know <span class="math inline">\(\sigma\)</span>. If we use the sample
standard deviation <span class="math inline">\(S\)</span> instead, how does that effect our use of the CLT?
The answer is some, but not much…effectively, it does not change the sample
size rule of thumb, but it does mean that we will need a few more samples
to achieve the same level of accuracy that we would achieve if we know
<span class="math inline">\(\sigma\)</span>. Refer to the proof in Chapter 7. We see that initially, we
standardize the random variables (i.e., transform <span class="math inline">\(X\)</span> to <span class="math inline">\(Z = (X-\mu)/\sigma\)</span>)
and declare that <span class="math inline">\(E[Z] = 0\)</span> and <span class="math inline">\(V[Z] = 1\)</span>. The latter equality no longer
holds if we use <span class="math inline">\(Z&#39; = (X-\mu)/S\)</span> instead! But <span class="math inline">\(V[Z&#39;]\)</span> does converge to 1
as <span class="math inline">\(n \rightarrow \infty\)</span>: <span class="math inline">\(S^2 \rightarrow \sigma^2\)</span> by the Law of Large
Numbers, and <span class="math inline">\(S \rightarrow \sigma\)</span> by the continuous mapping theorem. So
even if we do not know <span class="math inline">\(\sigma\)</span>, the CLT will “kick in” eventually.</p>
<p>Another question the reader may have is whether or not it is the case that,
since <span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/\sigma\)</span> converges in distribution to a
standard normal random variable, it would also be the case that
<span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/S\)</span> will converge in distribution to a <span class="math inline">\(t\)</span>-distributed
random variable.
The short answer: no. <span class="math inline">\(t\)</span>-distributed data are such because
when data are drawn from a normal distribution,
<span class="math inline">\(\bar{X}\)</span> is normally distributed <em>and</em> <span class="math inline">\((n-1)S^2/\sigma^2\)</span> is chi-square
distributed, and the ratio of <span class="math inline">\(\bar{X}-\mu\)</span> and <span class="math inline">\(S/\sqrt{n}\)</span>
is a <span class="math inline">\(t\)</span>-distributed random variable. If the
individual data are drawn from an arbitrary distribution, then
<span class="math inline">\((n-1)S^2/\sigma^2\)</span> will deviate, perhaps markedly, from being chi-square
distributed, thus <em>we cannot say anything about the distribution of</em>
<span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/S\)</span>…other than it eventually converges in
distribution to a standard normal random variable.</p>
<p>An important final note: if we have data drawn from a known, non-normal
distribution and we need to derive the distribution of
<span class="math inline">\(\bar{X}\)</span> in order to, e.g., construct confidence intervals or to perform
hypothesis tests, <em>we should not just default to utilizing the CLT</em>! We
might be able to compute the <em>exact</em> distribution for <span class="math inline">\(\bar{X}\)</span>, for
all sample sizes <span class="math inline">\(n\)</span>, via, e.g., the method of moment-generating functions.</p>
<hr />
<div id="computing-probabilities-6" class="section level3 hasAnchor" number="2.12.1">
<h3><span class="header-section-number">2.12.1</span> Computing Probabilities<a href="the-normal-and-related-distributions.html#computing-probabilities-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have a sample of <span class="math inline">\(n = 64\)</span> iid data drawn from
unknown distribution with mean <span class="math inline">\(\mu = 10\)</span> and finite variance
<span class="math inline">\(\sigma^2 = 9\)</span>. What is the probability that the observed sample
mean will be larger than 11?</p>
</blockquote>
<blockquote>
<p>This is a good example of a canonical CLT exercise: <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>
are given to us, but the distribution is left unstated…and <span class="math inline">\(n \geq 30\)</span>.
This is a very unrealistic: when will we know population parameters
exactly? But such an exercise has its place: it allows us to practice
the computation of probabilities.
<span class="math display">\[\begin{align*}
P\left( \bar{X} &gt; 11 \right) &amp;= 1 - P\left( \bar{X} \leq 11 \right) \\
&amp;= 1 - P\left( \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \leq \frac{\sqrt{n}(11-\mu)}{\sigma}\right) \\
&amp;\approx 1 - P\left( Z \leq \frac{8(11-10)}{3} \right) = 1 - \Phi\left(\frac{8}{3}\right) \,.
\end{align*}\]</span>
As we know by now, we cannot go any further by hand, so we call upon <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="the-normal-and-related-distributions.html#cb94-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">8</span><span class="sc">/</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.003830381</code></pre>
<blockquote>
<p>The probability that <span class="math inline">\(\bar{X}\)</span> is greater than 11 is small: 0.0038.</p>
</blockquote>
<blockquote>
<p>To reiterate a point made above: if we were given <span class="math inline">\(\mu\)</span> but not <span class="math inline">\(\sigma^2\)</span>,
we would plug in <span class="math inline">\(S\)</span> instead and expect that the distribution of
<span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/S\)</span> would be
not as close to a standard normal as the distribution of
<span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/\sigma\)</span>…it takes <span class="math inline">\(\sqrt{n}(\bar{X}-\mu)/S\)</span>
“longer” to converge in distribution to a standard normal as <span class="math inline">\(n\)</span> increases.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-1" class="section level2 hasAnchor" number="2.13">
<h2><span class="header-section-number">2.13</span> Confidence Intervals<a href="the-normal-and-related-distributions.html#confidence-intervals-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}\)</span> by solving the following equation:</em>
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span>, <span class="math inline">\(y_{\rm obs}\)</span> is the observed value of the statistic,
and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined using
the confidence interval reference table introduced in
section 16 of Chapter 1.</em></p>
<p>When we construct confidence intervals, we are allowed to utilize any
arbitrary statistic, so long as we know the sampling distribution of that
statistic (which links the statistic value to the underlying parameter of
interest). However, in the context of the normal distribution, there are
conventional choices:</p>
<ul>
<li><span class="math inline">\(\theta = \mu\)</span>, and the variance <span class="math inline">\(\sigma^2\)</span> is known: we use the sample mean <span class="math inline">\(\bar{X}\)</span></li>
<li><span class="math inline">\(\theta = \mu\)</span>, and the variance <span class="math inline">\(\sigma^2\)</span> is unknown: we still use <span class="math inline">\(\bar{X}\)</span></li>
<li><span class="math inline">\(\theta = \sigma^2\)</span>: we use the sample variance <span class="math inline">\(S^2\)</span></li>
</ul>
<p>These are conventional choices, but we will state here that there are
well-established reasons for using these statistics based on the idea of
<em>interval length</em>: if we examine the use of two separate statistics for
constructing a confidence interval for <span class="math inline">\(\theta\)</span>, we generally want to use
the one that generates <em>shorter confidence intervals</em> (i.e., the one that
indicates the least amount of uncertainty about <span class="math inline">\(\theta\)</span>). And <span class="math inline">\(\bar{X}\)</span> and
<span class="math inline">\(S^2\)</span> are generally the best statistics to use with normally distributed data.</p>
<p>(We keep saying “generally.” In an academic setting, where there are no
outlier data and the data-generating process is clean, <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span>
will be the best choices of statistics.
In a real-world setting, where data are messy,
there may be times where utilizing, e.g., the sample median is warranted.
Luckily for
us, here we exist in the world of pristine data…the real world can wait.)</p>
<p>Let’s assume that we have collected a sample of <span class="math inline">\(n\)</span> iid data, drawn from
a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and with
sample mean and variance <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span>, and let’s examine each of the
three situations outlined above.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\theta = \mu\)</span>, and the variance <span class="math inline">\(\sigma^2\)</span> is known.
We adopt <span class="math inline">\(Y = \bar{X}\)</span>, with the observed statistic being
<span class="math inline">\(y_{\rm obs} = \bar{x}\)</span>. By using the method
of moment-generating functions, we have found that
<span class="math display">\[
Y \sim \mathcal{N}(\mu,\sigma^2/n)
\]</span>
and thus that
<span class="math display">\[
F_Y(y) = \frac{1}{2}\left[ 1 + {\rm erf}\left(\frac{\sqrt{n}(y - \mu)}{\sqrt{2}\sigma}\right) \right] \,.
\]</span>
We can see immediately that we are <em>not</em> solving interval bounds by hand!
For instance,
<span class="math display">\[
\hat{\mu}_U = y_{\rm obs} - \frac{\sigma}{\sqrt{n}}{\rm erf}^{-1}(2q-1) \,,
\]</span>
where <span class="math inline">\(q = \alpha/2\)</span> and
where erf<span class="math inline">\(^{-1}(\cdot)\)</span> is the inverse error function, which is not
analytically tractable.
Thus we fall back upon
numerical methods, utilizing cdf-evaluation functions.
In an example below, we provide <code>R</code> code, utilizing <code>pnorm()</code>, for computing
confidence interval bounds.</p></li>
<li><p><span class="math inline">\(\theta = \mu\)</span>, and the variance <span class="math inline">\(\sigma^2\)</span> is unknown.
We adopt <span class="math inline">\(Y = \bar{X}\)</span>, with the observed statistic being
<span class="math inline">\(y_{\rm obs} = \bar{x}\)</span>. Earlier in this chapter, we show that
<span class="math display">\[
T = \frac{Y-\mu}{S/\sqrt{n}} \sim t(n-1) \,,
\]</span>
where <span class="math inline">\(t(n-1)\)</span> is the <span class="math inline">\(t\)</span>-distribution for <span class="math inline">\(n-1\)</span> degrees of freedom. We
immediately sense a problem here: we need to know <span class="math inline">\(F_Y(y)\)</span> for our
root-finding algorithm to work; <span class="math inline">\(F_{T(n-1)}(t)\)</span> will not help us! (<span class="math inline">\(T\)</span> subsumes the
parameter of interest <span class="math inline">\(\mu\)</span>, leaving us unable to solve for <span class="math inline">\(\mu\)</span>
when applying the root-finding algorithm.) However, we can determine
<span class="math inline">\(F_Y(y)\)</span> through a random-variable transformation. Let <span class="math inline">\(Y = aT+b\)</span>, where
<span class="math inline">\(a = s_{\rm obs}/\sqrt{n}\)</span> and <span class="math inline">\(b = \mu\)</span>. Then
<span class="math display">\[
F_Y(y) = P(Y \leq y) = P\left(T \leq \frac{y-b}{a}\right) = F_{T,n-1}\left(\frac{y-b}{a}\right) \,.
\]</span>
Thus, in place of, e.g., <span class="math inline">\(F_Y(y_{\rm obs} \vert \mu_{\alpha/2})\)</span>, we plug
into the root-finding equation the quantity
<span class="math inline">\(F_{T,n-1}((y_{\rm obs}-\mu_{\alpha/2})/(s_{\rm obs}/\sqrt{n}))\)</span>; the
solution in this case would be
<span class="math display">\[
\hat{\mu}_U = y_{\rm obs} - \frac{s_{\rm obs}}{\sqrt{n}}F_{T,n-1}^{-1}\left(\frac{\alpha}{2}\right) \,.
\]</span>
As we cannot work with <span class="math inline">\(F_{T(n-1)}^{-1}\left(\frac{\alpha}{2}\right)\)</span> by
band, we will again fall back on numerical methods, and we will show in an
example below how to compute intervals using <code>R</code> code.</p></li>
<li><p><span class="math inline">\(\theta = \sigma^2\)</span>. We adopt <span class="math inline">\(Y = S^2\)</span>, with the observed statistic
being <span class="math inline">\(y_{\rm obs} = s_{\rm obs}^2\)</span>. Earlier in this chapter, we show
that
<span class="math display">\[
W = \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2 \,,
\]</span>
where <span class="math inline">\(\chi_{n-1}^2\)</span> is the chi-square distribution for <span class="math inline">\(n-1\)</span> degrees of
freedom. Here, we face the same problem that we faced immediately above:
we want <span class="math inline">\(F_Y(y)\)</span>, but are given <span class="math inline">\(F_{W(n-1)}(w)\)</span>. We mitigate this issue in
the same manner as above, by employing a random-variable transformation.
<span class="math display">\[
F_Y(y) = P(Y \leq y) = P\left(W \leq \frac{(n-1)y_{\rm obs}}{\sigma^2}\right) = F_{W,n-1}\left(\frac{(n-1)y_{\rm obs}}{\sigma^2}\right)
\]</span>
As was the case above when we deal with the <span class="math inline">\(t\)</span> distribution, we end up
having inverse cdfs that we cannot evaluate by hand, such as in the expression
<span class="math display">\[
\widehat{\sigma^2}_U = \frac{(n-1)y_{\rm obs}}{F_{W,n-1}^{-1}(\alpha/2)} \,.
\]</span>
As has been the case thus far, the evaluation of bounds requires coding,
and we show an example of such an evaluation below.</p></li>
</ol>
<hr />
<p>As this point, the reader may say, “all this looks nothing like the confidence
interval stuff I learned in introductory statistics.”</p>
<p>And the reader would be correct.
The way in which the construction of confidence intervals is described above
and in Chapter 1 <em>is</em> different from the way in which it is described in
introductory statistics classes and as well as in traditional
calculus-based probability and statistical inference classes.
In short: we propose a modern approach that is appropriate in the age
of computers that yields the same results as traditional approaches.</p>
<p>In introductory statistics classes,
equations are generally all that are provided for confidence intervals, such as
the one for putting bounds on the normal mean <span class="math inline">\(\mu\)</span> when the variance is known:
<span class="math display">\[
\bar{x}_{\rm obs} \pm z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \,,
\]</span>
where <span class="math inline">\(z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)\)</span>.
(We note that historically
this equation has also been applied in situations where we have
sampled iid normal data but the variance is unknown,
if <span class="math inline">\(n \gtrsim 30\)</span>. This context generally falls under the textbook
heading of “large-sample confidence intervals.”
<em>This is just simply wrong!</em> The <em>only</em> reason this
is done is because textbook writers have not wanted to
include <span class="math inline">\(t\)</span> tables in their books that go beyond <span class="math inline">\(n = 30\)</span>.)
We would argue that such equations obscure the reality what is actually
happening when we construct confidence intervals:
“moving” the sampling distribution for a statistic <span class="math inline">\(Y\)</span> back and forth,
by changing the value of a parameter <span class="math inline">\(\theta\)</span>, until the value of
<span class="math inline">\(\theta\)</span> becomes either too low or too high for the observed value
<span class="math inline">\(y_{\rm obs}\)</span> to continue to be plausible.</p>
<hr />
<div id="confidence-interval-for-the-normal-mean-with-variance-known" class="section level3 hasAnchor" number="2.13.1">
<h3><span class="header-section-number">2.13.1</span> Confidence Interval for the Normal Mean With Variance Known<a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here we
construct a two-sided confidence interval for the normal
mean <span class="math inline">\(\mu\)</span> when the variance <span class="math inline">\(\sigma^2\)</span> is known. We assume
that we have collected 25 iid data and that <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="the-normal-and-related-distributions.html#cb96-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb96-2"><a href="the-normal-and-related-distributions.html#cb96-2" tabindex="-1"></a>alpha  <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb96-3"><a href="the-normal-and-related-distributions.html#cb96-3" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb96-4"><a href="the-normal-and-related-distributions.html#cb96-4" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb96-5"><a href="the-normal-and-related-distributions.html#cb96-5" tabindex="-1"></a>X      <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb96-6"><a href="the-normal-and-related-distributions.html#cb96-6" tabindex="-1"></a></span>
<span id="cb96-7"><a href="the-normal-and-related-distributions.html#cb96-7" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(mu,sigma2,n,y.obs,q)</span>
<span id="cb96-8"><a href="the-normal-and-related-distributions.html#cb96-8" tabindex="-1"></a>{</span>
<span id="cb96-9"><a href="the-normal-and-related-distributions.html#cb96-9" tabindex="-1"></a>  <span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))<span class="sc">-</span>q</span>
<span id="cb96-10"><a href="the-normal-and-related-distributions.html#cb96-10" tabindex="-1"></a>}</span>
<span id="cb96-11"><a href="the-normal-and-related-distributions.html#cb96-11" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">sigma2=</span>sigma2,<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] -0.4875524</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="the-normal-and-related-distributions.html#cb98-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">sigma2=</span>sigma2,<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.2964302</code></pre>
<blockquote>
<p>In this code above, we search
between <span class="math inline">\(\mu = -10,000\)</span> and <span class="math inline">\(\mu = 10,000\)</span> to see where the function
<span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_q) - q\)</span> crosses zero, where <span class="math inline">\(q\)</span> is
<span class="math inline">\(1 - \alpha/2\)</span> (lower bound) or <span class="math inline">\(\alpha/2\)</span> (upper bound).
(Making the range of possible <span class="math inline">\(\mu\)</span>
values large has no noticeable impact on computation time and
helps ensure that the root will not lie outside the specified interval.)
We test the code by sampling <span class="math inline">\(n = 25\)</span> data from a standard normal
distribution <span class="math inline">\(\mathcal{N}(0,1)\)</span>, and passing the statistic
<span class="math inline">\(y_{\rm obs} = \bar{x}_{\rm obs}\)</span> to <code>uniroot()</code>.
We find that the interval is <span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U] =
[-0.488,0.296]\)</span>, which overlaps the true value. (See Figure
<a href="the-normal-and-related-distributions.html#fig:mukvarci">2.11</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mukvarci"></span>
<img src="_main_files/figure-html/mukvarci-1.png" alt="\label{fig:mukvarci}Sampling distributions for $Y = \bar{X} = \sum_{i=1}^n X_i$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,1)$, and where (left) $\mu=-0.488$ and (right) $\mu=0.296$. We observe $y_{\rm obs} = -0.096$ and we want to construct a 95\% confidence interval. $\mu=-0.488$ is the smallest value of $\mu$ such that $F_Y^{-1}(0.975) = -0.096$, while $\mu=0.296$ is the largest value of $\mu$ such that $F_Y^{-1}(0.025) = -0.096$." width="45%" /><img src="_main_files/figure-html/mukvarci-2.png" alt="\label{fig:mukvarci}Sampling distributions for $Y = \bar{X} = \sum_{i=1}^n X_i$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,1)$, and where (left) $\mu=-0.488$ and (right) $\mu=0.296$. We observe $y_{\rm obs} = -0.096$ and we want to construct a 95\% confidence interval. $\mu=-0.488$ is the smallest value of $\mu$ such that $F_Y^{-1}(0.975) = -0.096$, while $\mu=0.296$ is the largest value of $\mu$ such that $F_Y^{-1}(0.025) = -0.096$." width="45%" />
<p class="caption">
Figure 2.11: Sampling distributions for <span class="math inline">\(Y = \bar{X} = \sum_{i=1}^n X_i\)</span>, where <span class="math inline">\(n = 25\)</span> and <span class="math inline">\(X_i \sim \mathcal{N}(\mu,1)\)</span>, and where (left) <span class="math inline">\(\mu=-0.488\)</span> and (right) <span class="math inline">\(\mu=0.296\)</span>. We observe <span class="math inline">\(y_{\rm obs} = -0.096\)</span> and we want to construct a 95% confidence interval. <span class="math inline">\(\mu=-0.488\)</span> is the smallest value of <span class="math inline">\(\mu\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = -0.096\)</span>, while <span class="math inline">\(\mu=0.296\)</span> is the largest value of <span class="math inline">\(\mu\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = -0.096\)</span>.
</p>
</div>
<blockquote>
<p>We note that to verify the “coverage” of intervals, i.e., the
proportion of the intervals that overlap the true value, we can simply
wrap the code provided above with a <code>for</code> loop,
and save the lower and upper bounds for each generated dataset.</p>
</blockquote>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="the-normal-and-related-distributions.html#cb100-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb100-2"><a href="the-normal-and-related-distributions.html#cb100-2" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb100-3"><a href="the-normal-and-related-distributions.html#cb100-3" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb100-4"><a href="the-normal-and-related-distributions.html#cb100-4" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb100-5"><a href="the-normal-and-related-distributions.html#cb100-5" tabindex="-1"></a>sigma2  <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb100-6"><a href="the-normal-and-related-distributions.html#cb100-6" tabindex="-1"></a>lower   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb100-7"><a href="the-normal-and-related-distributions.html#cb100-7" tabindex="-1"></a>upper   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb100-8"><a href="the-normal-and-related-distributions.html#cb100-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(mu,sigma2,n,y.obs,q)</span>
<span id="cb100-9"><a href="the-normal-and-related-distributions.html#cb100-9" tabindex="-1"></a>{</span>
<span id="cb100-10"><a href="the-normal-and-related-distributions.html#cb100-10" tabindex="-1"></a>  <span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))<span class="sc">-</span>q</span>
<span id="cb100-11"><a href="the-normal-and-related-distributions.html#cb100-11" tabindex="-1"></a>}</span>
<span id="cb100-12"><a href="the-normal-and-related-distributions.html#cb100-12" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb100-13"><a href="the-normal-and-related-distributions.html#cb100-13" tabindex="-1"></a>  X         <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2))</span>
<span id="cb100-14"><a href="the-normal-and-related-distributions.html#cb100-14" tabindex="-1"></a>  lower[ii] <span class="ot">&lt;-</span></span>
<span id="cb100-15"><a href="the-normal-and-related-distributions.html#cb100-15" tabindex="-1"></a>    <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">sigma2=</span>sigma2,<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb100-16"><a href="the-normal-and-related-distributions.html#cb100-16" tabindex="-1"></a>  upper[ii] <span class="ot">&lt;-</span></span>
<span id="cb100-17"><a href="the-normal-and-related-distributions.html#cb100-17" tabindex="-1"></a>    <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">sigma2=</span>sigma2,<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb100-18"><a href="the-normal-and-related-distributions.html#cb100-18" tabindex="-1"></a>}</span></code></pre></div>
<blockquote>
<p>We can then see how often the true value is within the bounds:</p>
</blockquote>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="the-normal-and-related-distributions.html#cb101-1" tabindex="-1"></a>truth    <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb101-2"><a href="the-normal-and-related-distributions.html#cb101-2" tabindex="-1"></a>in.bound <span class="ot">&lt;-</span> (lower <span class="sc">&lt;=</span> truth) <span class="sc">&amp;</span> (upper <span class="sc">&gt;=</span> truth)</span>
<span id="cb101-3"><a href="the-normal-and-related-distributions.html#cb101-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The empirical coverage is &quot;</span>,<span class="fu">sum</span>(in.bound)<span class="sc">/</span>num.sim,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The empirical coverage is  0.9518</code></pre>
<blockquote>
<p>Here, if <code>lower &lt;= truth</code>, then <code>TRUE</code> is returned, as is the
case if <code>upper &gt;= truth</code>. <code>&amp;</code> is the logical <code>and</code> operator, which
returns <code>TRUE</code> if the input is <code>TRUE &amp; TRUE</code> and returns <code>FALSE</code> otherwise.
<code>R</code> treats <code>TRUE</code> as equivalent to 1 (and <code>FALSE</code> as equivalent to 0),
so <code>sum(in.bound)</code> returns the number of final <code>TRUE</code> values, i.e.,
the number of evaluated intervals that overlap the true value.</p>
</blockquote>
<blockquote>
<p>Our estimated coverage is 0.9518. We do not expect a value of exactly
0.95 given that we only run a finite number of simulations and thus the
coverage will exhibit random variation; however, we can say that this
value is “close” to 0.95 and thus almost certainly compatible with 0.95.
Once we discuss the binomial distribution in Chapter 3, we will more easily
be able to quantify the notion of being “close” to the expected value.</p>
</blockquote>
<hr />
</div>
<div id="confidence-interval-for-the-normal-mean-with-variance-unknown" class="section level3 hasAnchor" number="2.13.2">
<h3><span class="header-section-number">2.13.2</span> Confidence Interval for the Normal Mean With Variance Unknown<a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We extend the previous example by assuming the variance is unknown.</p>
</blockquote>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="the-normal-and-related-distributions.html#cb103-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb103-2"><a href="the-normal-and-related-distributions.html#cb103-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb103-3"><a href="the-normal-and-related-distributions.html#cb103-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb103-4"><a href="the-normal-and-related-distributions.html#cb103-4" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>)</span>
<span id="cb103-5"><a href="the-normal-and-related-distributions.html#cb103-5" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(mu,s2,n,y.obs,q)</span>
<span id="cb103-6"><a href="the-normal-and-related-distributions.html#cb103-6" tabindex="-1"></a>{</span>
<span id="cb103-7"><a href="the-normal-and-related-distributions.html#cb103-7" tabindex="-1"></a>  <span class="fu">pt</span>((y.obs<span class="sc">-</span>mu)<span class="sc">/</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n),n<span class="dv">-1</span>)<span class="sc">-</span>q</span>
<span id="cb103-8"><a href="the-normal-and-related-distributions.html#cb103-8" tabindex="-1"></a>}</span>
<span id="cb103-9"><a href="the-normal-and-related-distributions.html#cb103-9" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] -0.4483761</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="the-normal-and-related-distributions.html#cb105-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">mean</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.2572646</code></pre>
<blockquote>
<p>The code above is largely equivalent to that in the previous example,
with the only changes being swapping in the call to <code>pt()</code> (and
altering the argument in that call) in place of the call to <code>pnorm()</code>,
and replacing the known value of <span class="math inline">\(\sigma^2\)</span> (<code>sigma2</code> in the previous
example) with <span class="math inline">\(s_{\rm obs}^2\)</span> (<code>s2</code>).
We find that the interval is <span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U] =
[-0.448,0.257]\)</span>, which is similar to the interval derived in the last
example and which still overlaps the true value. (See
Figure <a href="the-normal-and-related-distributions.html#fig:muuvarci">2.12</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:muuvarci"></span>
<img src="_main_files/figure-html/muuvarci-1.png" alt="\label{fig:muuvarci}Sampling distributions for $Y = \bar{X}$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,\sigma^2)$, and where (left) $\mu=-0.448$ and (right) $\mu=0.257$. We observe $y_{\rm obs} = -0.096$ and $s_{\rm obs}^2 = 0.855$ and we want to construct a 95\% confidence interval. $\mu=-0.448$ is the smallest value of $\mu$ such that $F_Y^{-1}(0.975) = -0.096$, while $\mu=0.257$ is the largest value of $\mu$ such that $F_Y^{-1}(0.025) = -0.096$." width="45%" /><img src="_main_files/figure-html/muuvarci-2.png" alt="\label{fig:muuvarci}Sampling distributions for $Y = \bar{X}$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,\sigma^2)$, and where (left) $\mu=-0.448$ and (right) $\mu=0.257$. We observe $y_{\rm obs} = -0.096$ and $s_{\rm obs}^2 = 0.855$ and we want to construct a 95\% confidence interval. $\mu=-0.448$ is the smallest value of $\mu$ such that $F_Y^{-1}(0.975) = -0.096$, while $\mu=0.257$ is the largest value of $\mu$ such that $F_Y^{-1}(0.025) = -0.096$." width="45%" />
<p class="caption">
Figure 2.12: Sampling distributions for <span class="math inline">\(Y = \bar{X}\)</span>, where <span class="math inline">\(n = 25\)</span> and <span class="math inline">\(X_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, and where (left) <span class="math inline">\(\mu=-0.448\)</span> and (right) <span class="math inline">\(\mu=0.257\)</span>. We observe <span class="math inline">\(y_{\rm obs} = -0.096\)</span> and <span class="math inline">\(s_{\rm obs}^2 = 0.855\)</span> and we want to construct a 95% confidence interval. <span class="math inline">\(\mu=-0.448\)</span> is the smallest value of <span class="math inline">\(\mu\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = -0.096\)</span>, while <span class="math inline">\(\mu=0.257\)</span> is the largest value of <span class="math inline">\(\mu\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = -0.096\)</span>.
</p>
</div>
<hr />
</div>
<div id="confidence-interval-for-the-normal-variance" class="section level3 hasAnchor" number="2.13.3">
<h3><span class="header-section-number">2.13.3</span> Confidence Interval for the Normal Variance<a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Below, we adapt our confidence-interval code to the problem of
estimating an interval for the variance <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="the-normal-and-related-distributions.html#cb107-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb107-2"><a href="the-normal-and-related-distributions.html#cb107-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb107-3"><a href="the-normal-and-related-distributions.html#cb107-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb107-4"><a href="the-normal-and-related-distributions.html#cb107-4" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>)</span>
<span id="cb107-5"><a href="the-normal-and-related-distributions.html#cb107-5" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(sigma2,n,y.obs,q)</span>
<span id="cb107-6"><a href="the-normal-and-related-distributions.html#cb107-6" tabindex="-1"></a>{ </span>
<span id="cb107-7"><a href="the-normal-and-related-distributions.html#cb107-7" tabindex="-1"></a>  <span class="fu">pchisq</span>(y.obs<span class="sc">*</span>(n<span class="dv">-1</span>)<span class="sc">/</span>sigma2,n<span class="dv">-1</span>)<span class="sc">-</span>q</span>
<span id="cb107-8"><a href="the-normal-and-related-distributions.html#cb107-8" tabindex="-1"></a>}</span>
<span id="cb107-9"><a href="the-normal-and-related-distributions.html#cb107-9" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">1</span>.e<span class="dv">-6</span>,<span class="dv">10000</span>),<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">var</span>(X),<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.4454088</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="the-normal-and-related-distributions.html#cb109-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">1</span>.e<span class="dv">-6</span>,<span class="dv">10000</span>),<span class="at">n=</span>n,<span class="at">y.obs=</span><span class="fu">var</span>(X),alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 1.413897</code></pre>
<blockquote>
<p>The changes made to the code above include swapping in <code>pchisq()</code>, removing
the variable <code>mean</code>, removing the variable <code>s2</code> (as <code>y.obs</code> itself is now
<span class="math inline">\(s_{\rm obs}^2\)</span>), and adjusting the lower bound on the interval (since
<span class="math inline">\(\sigma^2 &gt; 0\)</span>).
We find that the interval is <span class="math inline">\([\widehat{\sigma}_L^2,\widehat{\sigma}_U^2] =
[0.445,1.414]\)</span>, which overlaps the true value. (See Figure
<a href="the-normal-and-related-distributions.html#fig:sigma2ci">2.13</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sigma2ci"></span>
<img src="_main_files/figure-html/sigma2ci-1.png" alt="\label{fig:sigma2ci}Sampling distributions for $Y = S^2$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,\sigma^2)$, and where (left) $\sigma^2=0.445$ and (right) $\sigma^2=1.414$. We observe $y_{\rm obs} = s_{\rm obs}^2 = 0.731$ and we want to construct a 95\% confidence interval. $\sigma^2=0.445$ is the smallest value of $\sigma^2$ such that $F_Y^{-1}(0.975) = 0.731$, while $\mu=1.414$ is the largest value of $\sigma^2$ such that $F_Y^{-1}(0.025) = 0.731$." width="45%" /><img src="_main_files/figure-html/sigma2ci-2.png" alt="\label{fig:sigma2ci}Sampling distributions for $Y = S^2$, where $n = 25$ and $X_i \sim \mathcal{N}(\mu,\sigma^2)$, and where (left) $\sigma^2=0.445$ and (right) $\sigma^2=1.414$. We observe $y_{\rm obs} = s_{\rm obs}^2 = 0.731$ and we want to construct a 95\% confidence interval. $\sigma^2=0.445$ is the smallest value of $\sigma^2$ such that $F_Y^{-1}(0.975) = 0.731$, while $\mu=1.414$ is the largest value of $\sigma^2$ such that $F_Y^{-1}(0.025) = 0.731$." width="45%" />
<p class="caption">
Figure 2.13: Sampling distributions for <span class="math inline">\(Y = S^2\)</span>, where <span class="math inline">\(n = 25\)</span> and <span class="math inline">\(X_i \sim \mathcal{N}(\mu,\sigma^2)\)</span>, and where (left) <span class="math inline">\(\sigma^2=0.445\)</span> and (right) <span class="math inline">\(\sigma^2=1.414\)</span>. We observe <span class="math inline">\(y_{\rm obs} = s_{\rm obs}^2 = 0.731\)</span> and we want to construct a 95% confidence interval. <span class="math inline">\(\sigma^2=0.445\)</span> is the smallest value of <span class="math inline">\(\sigma^2\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 0.731\)</span>, while <span class="math inline">\(\mu=1.414\)</span> is the largest value of <span class="math inline">\(\sigma^2\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 0.731\)</span>.
</p>
</div>
<hr />
</div>
<div id="confidence-interval-using-the-clt" class="section level3 hasAnchor" number="2.13.4">
<h3><span class="header-section-number">2.13.4</span> Confidence Interval: Using the CLT<a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have a iid sample of <span class="math inline">\(n\)</span> data drawn from the distribution
<span class="math display">\[
f_X(x) = \theta x^{\theta-1} ~~~~ x \in [0,1] \,.
\]</span>
This distribution is decidedly <em>not</em> normal. Can we derive a confidence
interval for the mean of this distribution, where the mean is
<span class="math display">\[
E[X] = \int_0^1 \theta x x^{\theta-1} dx = \left. \frac{\theta}{\theta+1} x^{\theta+1} \right|_0^1 = \frac{\theta}{\theta+1} \,?
\]</span>
If we wanted to try to do this “exactly,” then one workflow would be (a) to
attempt to determine the sampling distribution of <span class="math inline">\(Y = \bar{X}\)</span>
(utilizing, e.g., moment-generating functions), and then
(b) code up a variant of the codes given in the examples above.
However, regarding (a): the mgf of the distribution is
<span class="math display">\[
m_X(t) = E[e^{tX}] = \theta \int_0^1 x^{\theta-1} e^{tx} dx \,.
\]</span>
This looks suspiciously like a gamma-function integral, except the bounds
here are 0 and 1, not 0 and <span class="math inline">\(\infty\)</span>…what we actually have (or, technically,
what we would have after an appropriate variable substitution) is an
<em>incomplete gamma function</em> integral. We cannot easily work with
this…and so we turn to the Central Limit Theorem.</p>
</blockquote>
<blockquote>
<p>If <span class="math inline">\(n \gtrsim 30\)</span>, we may write that
<span class="math display">\[
\bar{X} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(\mu,s_{\rm obs}^2/n) \,.
\]</span>
(Recall that the CLT assumes that we know <span class="math inline">\(\sigma^2\)</span>, but if we do not and
use <span class="math inline">\(S^2\)</span> instead, the result will still hold as <span class="math inline">\(n \rightarrow \infty\)</span>.)
Thus we would use the confidence interval approach that we describe in
the first example above, while swapping in <span class="math inline">\(s_{\rm obs}^2\)</span> for <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="the-normal-and-related-distributions.html#cb111-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb111-2"><a href="the-normal-and-related-distributions.html#cb111-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb111-3"><a href="the-normal-and-related-distributions.html#cb111-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb111-4"><a href="the-normal-and-related-distributions.html#cb111-4" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="dv">2</span>   <span class="co"># so the mean is 2/3</span></span>
<span id="cb111-5"><a href="the-normal-and-related-distributions.html#cb111-5" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n,theta,<span class="dv">1</span>)  <span class="co"># theta x^(theta-1) is a beta distribution</span></span>
<span id="cb111-6"><a href="the-normal-and-related-distributions.html#cb111-6" tabindex="-1"></a></span>
<span id="cb111-7"><a href="the-normal-and-related-distributions.html#cb111-7" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(mu,y.obs,s2,n,q)</span>
<span id="cb111-8"><a href="the-normal-and-related-distributions.html#cb111-8" tabindex="-1"></a>{</span>
<span id="cb111-9"><a href="the-normal-and-related-distributions.html#cb111-9" tabindex="-1"></a>  <span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n))<span class="sc">-</span>q</span>
<span id="cb111-10"><a href="the-normal-and-related-distributions.html#cb111-10" tabindex="-1"></a>}</span>
<span id="cb111-11"><a href="the-normal-and-related-distributions.html#cb111-11" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.5888778</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="the-normal-and-related-distributions.html#cb113-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.7532408</code></pre>
<blockquote>
<p>The evaluated confidence interval is <span class="math inline">\([0.589,0.753]\)</span> (which overlaps the
true value <span class="math inline">\(\mu = 2/3\)</span>). Note, however, that
because an approximate sampling distribution is being used, the coverage
can (and will) deviate from expectation (e.g., 0.95). Let’s see how
much the coverage deviates in this one case via simulation:</p>
</blockquote>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="the-normal-and-related-distributions.html#cb115-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb115-2"><a href="the-normal-and-related-distributions.html#cb115-2" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb115-3"><a href="the-normal-and-related-distributions.html#cb115-3" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb115-4"><a href="the-normal-and-related-distributions.html#cb115-4" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb115-5"><a href="the-normal-and-related-distributions.html#cb115-5" tabindex="-1"></a>theta   <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb115-6"><a href="the-normal-and-related-distributions.html#cb115-6" tabindex="-1"></a>lower   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb115-7"><a href="the-normal-and-related-distributions.html#cb115-7" tabindex="-1"></a>upper   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.sim)</span>
<span id="cb115-8"><a href="the-normal-and-related-distributions.html#cb115-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(mu,y.obs,s2,n,q)</span>
<span id="cb115-9"><a href="the-normal-and-related-distributions.html#cb115-9" tabindex="-1"></a>{</span>
<span id="cb115-10"><a href="the-normal-and-related-distributions.html#cb115-10" tabindex="-1"></a>  <span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n))<span class="sc">-</span>q</span>
<span id="cb115-11"><a href="the-normal-and-related-distributions.html#cb115-11" tabindex="-1"></a>}</span>
<span id="cb115-12"><a href="the-normal-and-related-distributions.html#cb115-12" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb115-13"><a href="the-normal-and-related-distributions.html#cb115-13" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n,theta,<span class="dv">1</span>)  </span>
<span id="cb115-14"><a href="the-normal-and-related-distributions.html#cb115-14" tabindex="-1"></a>  lower[ii] <span class="ot">&lt;-</span> </span>
<span id="cb115-15"><a href="the-normal-and-related-distributions.html#cb115-15" tabindex="-1"></a>    <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb115-16"><a href="the-normal-and-related-distributions.html#cb115-16" tabindex="-1"></a>  upper[ii] <span class="ot">&lt;-</span> </span>
<span id="cb115-17"><a href="the-normal-and-related-distributions.html#cb115-17" tabindex="-1"></a>    <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>,<span class="dv">100</span>),<span class="at">y.obs=</span><span class="fu">mean</span>(X),<span class="at">s2=</span><span class="fu">var</span>(X),<span class="at">n=</span>n,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb115-18"><a href="the-normal-and-related-distributions.html#cb115-18" tabindex="-1"></a>}</span>
<span id="cb115-19"><a href="the-normal-and-related-distributions.html#cb115-19" tabindex="-1"></a></span>
<span id="cb115-20"><a href="the-normal-and-related-distributions.html#cb115-20" tabindex="-1"></a>truth    <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">3</span></span>
<span id="cb115-21"><a href="the-normal-and-related-distributions.html#cb115-21" tabindex="-1"></a>in.bound <span class="ot">&lt;-</span> (lower <span class="sc">&lt;=</span> truth) <span class="sc">&amp;</span> (upper <span class="sc">&gt;=</span> truth)</span>
<span id="cb115-22"><a href="the-normal-and-related-distributions.html#cb115-22" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The estimated coverage is &quot;</span>,<span class="fu">sum</span>(in.bound)<span class="sc">/</span>num.sim,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The estimated coverage is  0.936</code></pre>
<blockquote>
<p>The estimated coverage is 0.936; it is not quite 0.95. Thus it appears
that are evaluated confidence intervals do not
overlap the true value as often as we would expect. Note that we would expect
that as <span class="math inline">\(n \rightarrow 0\)</span>, the approximation will become progressively
worse, and that as <span class="math inline">\(n \rightarrow \infty\)</span>, the coverage should asymptotically
approach <span class="math inline">\(1 - \alpha\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="historical-digression-the-pivotal-method" class="section level3 hasAnchor" number="2.13.5">
<h3><span class="header-section-number">2.13.5</span> Historical Digression: the Pivotal Method<a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The pivotal method is an often-used analytical method for constructing
confidence
intervals wherein one “reformats” the problem to solve in such a way
that one can ultimately
utilize statistical tables to determine interval bounds. In the age
of computers, the pivotal method has become unnecessary; the numerical
root-finding algorithm presented in this chapter
constructs the same intervals (and does so even in situations where the
pivotal method cannot be used).</p>
</blockquote>
<blockquote>
<p>The steps of the pivotal method are as follows:</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>Determine a pivotal quantity <span class="math inline">\(Y\)</span>, a statistic that is a function of both the observed data and the parameter <span class="math inline">\(\theta\)</span>, and that has a sampling distribution that does <em>not</em> depend on <span class="math inline">\(\theta\)</span>. (Note that, as hinted at above, there is no guarantee that a pivotal quantity exists in any given situation. But there may also be situations where we can define multiple pivotal quantities; if so, it does not matter which we adopt, as they all will help generate the same interval.)</li>
<li>Determine constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(P(a \leq Y \leq b) = 1-\alpha\)</span>. (Here, we are assuming that we are constructing a two-sided interval.)</li>
<li>Rearrange the terms within the probability statement such that <span class="math inline">\(\theta\)</span> is alone in the middle: the quantities on either end are the interval bounds.</li>
</ol>
</blockquote>
<blockquote>
<p>Let’s demonstrate how this plays out in the situation where we sample
<span class="math inline">\(n\)</span> iid data from a normal distribution with known variance, and wish
to construct an interval for <span class="math inline">\(\mu\)</span>. As we know, in this situation,
<span class="math display">\[
\bar{X} \sim \mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right) \,.
\]</span>
<span class="math inline">\(\bar{X}\)</span> is <em>not</em> a pivotal quantity: its sampling distribution depends
on <span class="math inline">\(\mu\)</span>. However…
<span class="math display">\[
Y = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)
\]</span>
<em>is</em> a pivotal quantity, as it depends on both <span class="math inline">\(\bar{X}\)</span> <em>and</em> <span class="math inline">\(\mu\)</span> and
has a sampling distribution that does <em>not</em> depend on <span class="math inline">\(\mu\)</span>. Thus step 1
is complete.</p>
</blockquote>
<blockquote>
<p>As for step 2:
<span class="math display">\[\begin{align*}
P(a \leq Y \leq b) = P(Y \leq b) - P(Y \leq a) &amp;= 1-\alpha \\
\Phi(b) - \Phi(a) &amp;= 1-\alpha \,.
\end{align*}\]</span>
OK…we appear stuck here, except that we can fall back upon the fact
that the standard normal distribution is symmetric around zero, and
thus we can say that <span class="math inline">\(a = -b\)</span>. That, and the fact that symmetry allows
us to write that <span class="math inline">\(\Phi(-b) = 1 - \Phi(b)\)</span>, allows us to continue:
<span class="math display">\[\begin{align*}
\Phi(b) - \Phi(a) &amp;= 1-\alpha \\
\Phi(b) - (1 - \Phi(b)) &amp;= 1-\alpha \\
2\Phi(b) - 1 &amp;= 1-\alpha \\
\Phi(b) &amp;= \frac{2-\alpha}{2} = 1 - \frac{\alpha}{2} \\
b &amp;= \Phi^{-1}\left(1 - \frac{\alpha}{2}\right) = z_{1-\alpha/2} \\
\end{align*}\]</span>
Historically, at this point, one would make use of a <span class="math inline">\(z\)</span> table to
determine that, e.g., <span class="math inline">\(b = z_{0.975} = 1.96\)</span>.</p>
</blockquote>
<blockquote>
<p>So now we move on to step 3:
<span class="math display">\[\begin{align*}
P\left(-z_{1-\alpha/2} \leq Y \leq z_{1-\alpha/2}\right) = P\left(-z_{1-\alpha/2} \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq z_{1-\alpha/2}\right) &amp;= 1-\alpha \\
\Rightarrow P\left(\bar{X} - z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}\right) &amp;= 1-\alpha \,.
\end{align*}\]</span>
Thus our confidence interval is
<span class="math display">\[
\bar{x}_{\rm obs} \pm z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}} \,.
\]</span></p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-testing-for-normality" class="section level2 hasAnchor" number="2.14">
<h2><span class="header-section-number">2.14</span> Hypothesis Testing: Testing for Normality<a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>In a conventional data analysis workflow, we might receive a sample
of <span class="math inline">\(n\)</span> iid data without any knowledge about the distribution from
which the individual data are drawn, and we might want to test a hypothesis
about, e.g., the population mean. There are potentially many hypothesis
tests from which to choose, but the ones that are generally the most powerful
(i.e., the ones that do the best at discriminating between a null hypothesis
and a given alternative hypothesis) are the ones that <em>assume</em> a functional form
for the distribution of the individual data. Below,
we provide details about tests of population means and variances that are
built upon the assumption that the individual data are iid draws from
a normal distribution…but before we talk about them, we will
introduce hypothesis tests that allow us to decide
<em>whether our data are plausibly normally distributed in the first place</em>.</p>
<p>To be clear about the analysis workflow…</p>
<ul>
<li>If we perform a hypothesis test for which the null hypothesis is that
the individual data are normally distributed, <em>and</em> we fail to reject this
null hypothesis, we can feel free to use the hypothesis tests in the next
two sections, regardless of the sample size <span class="math inline">\(n\)</span>.</li>
<li>If we reject the null hypothesis but the sample size is sufficiently
large (<span class="math inline">\(n \gtrsim 30\)</span>), <em>and</em> we wish to test hypotheses about the population
mean, we can fall back on the central limit theorem and construct a variant
of the tests presented in the next section.</li>
<li>If we reject the null hypothesis that the data are normally distributed,
<em>and</em> the sample size is small or if we
wish to test hypotheses about population variances (or both), we should
tread carefully if we decide to
use the hypothesis testing frameworks presented in the next two sections,
as the results we get might be very inaccurate(!).</li>
</ul>
<p>However, regarding the third point above,
we have to keep in mind that as <span class="math inline">\(n\)</span> gets larger and larger,
tests of normality become more and more prone to rejecting the null
even when the deviation of the true distribution from a normal is small,
meaning that the hypothesis tests detailed in
the next two sections might actually yield “useful” results!
Again, we would need to tread carefully.
Alternative approaches include implementing so-called nonparametric tests (which
we do not cover in this book), or trying to determine another distribution
with which the data are consistent and building a test utilizing its
properties, etc.</p>
<p>In the end: if in doubt, simulate. If we can make an assumption about the
distribution from which our <span class="math inline">\(n\)</span> iid data are sampled, we can always carry
out hypothesis tests numerically. The reader should always keep this in mind:
in the age of computers, one rarely if ever needs to “force” an assumption
of normality into the solution of a hypothesis testing problem.</p>
<hr />
<p>Let’s assume that we have collected
<span class="math inline">\(n\)</span> iid data from some unknown (and unassumed) distribution <span class="math inline">\(P\)</span>.
Could <span class="math inline">\(P\)</span> be the normal distribution?
There are several varieties of tests whose null hypotheses are
“the data are sampled from the [insert name here] distribution.”
The most well-known and often-used is the <em>Kolmogorov-Smirnov</em> test,
or <em>KS</em> test. (We note that it is not necessarily the most powerful
test among those that assess the consistency of data with named
distributions, but it is easy to implement and simple to understand.
The interested reader should investigate alternatives like, e.g., the
Anderson-Darling test and Cramer-von Mises test, and for the
specific case of testing for normality, the Shapiro-Wilk test, which
we discuss below in an example.)</p>
<p>The <em>empirical cumulative distribution function</em> (or <em>ecdf</em>) for a dataset
is defined as
<span class="math display">\[
F_{X,n}(x) = \frac{1}{n} \left(\mbox{number of observed data} \, \leq x\right) \,.
\]</span>
We see immediately that <span class="math inline">\(F_{X,n}(x)\)</span> behaves like a cdf, in the sense that
<span class="math inline">\(F_{X,n}(-\infty) = 0\)</span> and <span class="math inline">\(F_{X,n}(\infty) = 1\)</span>, etc. It is a monotonically
increasing step function, with steps of size <span class="math inline">\(1/n\)</span> taken at each datum’s
coordinate <span class="math inline">\(x_i\)</span>. If we assume a particular distribution as the null
distribution, with cdf <span class="math inline">\(F_X(x)\)</span>, then the KS test statistic is
<span class="math display">\[
D_n = \mbox{sup} \vert F_{X,n}(x) - F_X(x) \vert \,.
\]</span>
We have seen the abbrevation “inf” before, signaling that we are taking
the infimum, or smallest value, of a set; here, “sup” stands for
<em>supremum</em>, the largest value of a set. So for the KS test, the test
statistic is simply the largest distance observed between the empirical
and null cdfs; if the null is wrong, we expect this distance to be large.
Under the null, <span class="math inline">\(K = \sqrt{n}D_n\)</span> is assumed to be, at least approximately,
sampled from the Kolmogorov distribution, whose cdf is
<span class="math display">\[
P(K \leq k) = \frac{\sqrt{2\pi}}{k} \sum_{x=1}^\infty \exp\left(-\frac{(2x-1)^2\pi^2}{8k^2}\right) \,.
\]</span>
Having sampled <span class="math inline">\(k_{obs} = \sqrt{n}D_n\)</span>,
we can use this cdf to establish whether our not the test statistic
falls into the rejection region. If it does, we reject the null hypothesis
that the individual data are normally distributed.</p>
<hr />
<div id="the-kolmogorov-smirnov-test" class="section level3 hasAnchor" number="2.14.1">
<h3><span class="header-section-number">2.14.1</span> The Kolmogorov-Smirnov Test<a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s suppose we are given a dataset that has the
empirical distribution given in Figure <a href="the-normal-and-related-distributions.html#fig:ksex">2.14</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ksex"></span>
<img src="_main_files/figure-html/ksex-1.png" alt="\label{fig:ksex} Histogram of a dataset with sample size $n = 30$. Is it plausible that these data are normally distributed?" width="50%" />
<p class="caption">
Figure 2.14:  Histogram of a dataset with sample size <span class="math inline">\(n = 30\)</span>. Is it plausible that these data are normally distributed?
</p>
</div>
<blockquote>
<p>The sample statistics for these data are <span class="math inline">\(\bar{X} = 100.6\)</span> and
<span class="math inline">\(S = 29.8\)</span>. In Figure <a href="the-normal-and-related-distributions.html#fig:ksecdf">2.15</a>, we plot the empirical cdf for
these data, overlaying the cdf for a <span class="math inline">\(\mathcal{N}(100.6,29.8^2)\)</span> distribution.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ksecdf"></span>
<img src="_main_files/figure-html/ksecdf-1.png" alt="\label{fig:ksecdf} Empirical cumulative distribution function of our dataset with the cdf for a $\mathcal{N}(100.6,29.8^2)$ distribution overlaid as the red dashed line." width="50%" />
<p class="caption">
Figure 2.15:  Empirical cumulative distribution function of our dataset with the cdf for a <span class="math inline">\(\mathcal{N}(100.6,29.8^2)\)</span> distribution overlaid as the red dashed line.
</p>
</div>
<blockquote>
<p>The largest difference between the empirical cdf and the overlaid normal cdf
in Figure <a href="the-normal-and-related-distributions.html#fig:ksecdf">2.15</a> occurs at <span class="math inline">\(x \approx 93\)</span>. Is this difference
large enough for us to decide the data are not normally distributed with
the inferred parameters <span class="math inline">\(\mu = 100.6\)</span> and <span class="math inline">\(\sigma = 29.8\)</span>?</p>
</blockquote>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="the-normal-and-related-distributions.html#cb117-1" tabindex="-1"></a><span class="fu">ks.test</span>(x,<span class="st">&quot;pnorm&quot;</span>,<span class="fl">100.6</span>,<span class="fl">29.8</span>)</span></code></pre></div>
<pre><code>## 
##  Exact one-sample Kolmogorov-Smirnov test
## 
## data:  x
## D = 0.13858, p-value = 0.5649
## alternative hypothesis: two-sided</code></pre>
<blockquote>
<p>By default, <code>ks.test()</code> performs a two-sided test; type <code>?ks.test</code> in the
<code>R</code> console to see how to specify alternatives. According to this test,
the <span class="math inline">\(p\)</span>-value is 0.56. We formally introduce <span class="math inline">\(p\)</span>-values below, in the
next section; it suffices to say here that if <span class="math inline">\(p &lt; \alpha\)</span>,
where <span class="math inline">\(\alpha\)</span> is the user-specified Type I error (typically 0.05),
we reject
the null, so here we fail to reject the null hypothesis that the
data are normally distributed. In truth, the simulated data are sampled
from a gamma distribution (see Chapter 4); with a larger sample size, the
test becomes more better able to differentiate between normally
distributed and gamma-distributed data, so eventually we
<em>would</em> reject the null hypothesis.</p>
</blockquote>
<hr />
</div>
<div id="the-shapiro-wilk-test" class="section level3 hasAnchor" number="2.14.2">
<h3><span class="header-section-number">2.14.2</span> The Shapiro-Wilk Test<a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Shapiro-Wilk test has the null hypothesis that
our <span class="math inline">\(n\)</span> iid data are sampled from a normal distribution. The
test statistic <span class="math inline">\(W\)</span>, which utilizes order statistics (a concept we
introduce in Chapter 3), is complicated and will
not be reproduced here. It suffices for now for us to make three points.</p>
</blockquote>
<blockquote>
<ul>
<li>The Shapiro-Wilk test is better able to reject the null hypothesis that
our data are normally distributed than the KS test. (This makes sense
because the SW test is built to be more “specific”<span class="math inline">\(-\)</span>it is just used to
test for normality). In other words, given the same data, the SW test will
almost always output a smaller <span class="math inline">\(p\)</span>-value than the KS test.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>The sampling distribution for <span class="math inline">\(W\)</span>
is non-analytic and thus the rejection region is estimated numerically, and
thus the Shapiro-Wilk test cannot be used in <code>R</code> with samples of size
<span class="math inline">\(&gt;\)</span> 5000. If our sample size is larger, we should randomly sub-sample the
data before performing the SW test.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>The Shapiro-Wilk test will often reject the null hypothesis of normality
even when the data appear by eye to be very nearly normally distributed.
This goes back to one of the points made above: we
still might be able to glean “useful” results when performing normal-based
hypothesis tests using technically non-normal data. Tread carefully, and
simulate if necessary.</li>
</ul>
</blockquote>
<blockquote>
<p>Below, we demonstrate the use of the Shapiro-Wilk test with the
gamma-distributed data of the first example above.</p>
</blockquote>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="the-normal-and-related-distributions.html#cb119-1" tabindex="-1"></a><span class="fu">shapiro.test</span>(x)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.9508, p-value = 0.1776</code></pre>
<blockquote>
<p>We see that while the KS test returned a <span class="math inline">\(p\)</span>-value of 0.56, the
Shapiro-Wilk test returns the value 0.18. This value is still
larger than <span class="math inline">\(\alpha = 0.05\)</span>, so we still fail to reject the null.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-population-mean" class="section level2 hasAnchor" number="2.15">
<h2><span class="header-section-number">2.15</span> Hypothesis Testing: Population Mean<a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s begin by assuming we are given <span class="math inline">\(n\)</span> iid data
<span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> sampled from a normal distribution with unknown
mean <span class="math inline">\(\mu\)</span> and <em>known</em> variance <span class="math inline">\(\sigma^2\)</span>.
Because <span class="math inline">\(Y = \bar{X}\)</span> is the MLE for the normal mean, we
identify it as a plausible statistic for testing hypotheses about <span class="math inline">\(\mu\)</span>.
(We will justify this choice in
in Chapters 3 and 4 when we discuss
the Neyman-Pearson Lemma and the Likelihood Ratio Test.)
Refer back to the hypothesis test reference table in section 17 of Chapter 1.
We observe that <span class="math inline">\(E[Y] = E[\bar{X}] = \mu\)</span>, so <span class="math inline">\(E[Y]\)</span> increases
with <span class="math inline">\(\mu\)</span>…and thus the rejection regions for a two-tail test are
<span class="math display">\[
y_{\rm obs} &lt; y_{\rm RR,lo} ~~~ \mbox{and} ~~~ y_{\rm obs} &gt; y_{\rm RR,hi} \,,
\]</span>
where <span class="math inline">\(y_{\rm RR,lo}\)</span> and <span class="math inline">\(y_{\rm RR,hi}\)</span> are solutions to the equations
<span class="math display">\[
F_Y(y \vert \mu_o,\sigma^2) - q = 0 \,,
\]</span>
with <span class="math inline">\(q = \alpha/2\)</span> and <span class="math inline">\(q = 1-\alpha/2\)</span>, respectively.
Can we solve for rejection region boundaries analytically?
The answer is no: the pdf for our test statistic
is <span class="math inline">\(\mathcal{N}(\mu,\sigma^2/n)\)</span>, and its cdf contains the error
function; this is not a cdf we can work with by hand. But finding a
numerical solution is straightforward: for instance,
<span class="math display">\[
y_{\rm RR,lo} = F_Y^{-1}\left(\frac{\alpha}{2} \vert \mu_o,\sigma^2\right)
\]</span>
can be numerically evaluated using the <code>R</code> function <code>qnorm()</code>.</p>
<p>What happens if the variance is unknown? In this case, we would borrow
(and slightly amend)
a result from the confidence interval section above, namely that
<span class="math display">\[
F_Y(y) = F_{T(n-1)}\left( \frac{y-\mu_o}{s_{\rm obs}/\sqrt{n}} \right) \,,
\]</span>
which means that for instance,
<span class="math display">\[
y_{\rm RR,lo} = \mu_o + \frac{s_{\rm obs}}{\sqrt{n}} F_{T(n-1)}^{-1}\left(\frac{\alpha}{2}\right) \,,
\]</span>
which can be numerically evaluated using the <code>R</code> function <code>qt()</code>.
(See Figure <a href="the-normal-and-related-distributions.html#fig:normrr">2.16</a>.)</p>
<hr />
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normrr"></span>
<img src="_main_files/figure-html/normrr-1.png" alt="\label{fig:normrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with $\mu_o = 0$ and $s_{\rm obs}^2 = 1$), assuming $\alpha = 0.05$ and $n-1 = 5$ degrees of freedom." width="30%" /><img src="_main_files/figure-html/normrr-2.png" alt="\label{fig:normrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with $\mu_o = 0$ and $s_{\rm obs}^2 = 1$), assuming $\alpha = 0.05$ and $n-1 = 5$ degrees of freedom." width="30%" /><img src="_main_files/figure-html/normrr-3.png" alt="\label{fig:normrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with $\mu_o = 0$ and $s_{\rm obs}^2 = 1$), assuming $\alpha = 0.05$ and $n-1 = 5$ degrees of freedom." width="30%" />
<p class="caption">
Figure 2.16: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) population mean tests (with <span class="math inline">\(\mu_o = 0\)</span> and <span class="math inline">\(s_{\rm obs}^2 = 1\)</span>), assuming <span class="math inline">\(\alpha = 0.05\)</span> and <span class="math inline">\(n-1 = 5\)</span> degrees of freedom.
</p>
</div>
<p>There are two new hypothesis-test-related concepts that we introduce here.</p>
<p>The first is the concept of the <em>p-value</em>.
This is the probability
of observing a hypothesis test statistic value <span class="math inline">\(y_{\rm obs}\)</span> or a more
“extreme” value (i.e., a value even further from the null), <em>given
that the null hypothesis is true</em>. We reject the null when <span class="math inline">\(p &lt; \alpha\)</span>,
the user-specified Type I error. See below for more information
about <span class="math inline">\(p\)</span>-values. Here, it suffices to say that when we <em>can</em> compute
a <span class="math inline">\(p\)</span>-value, we should: it is more informative than simply saying that
the test statistic does, or does not, lie in the rejection region.</p>
<p>The second new concept that we introduce here is hypothesis test <em>power</em>.
In words, it is the probability of rejecting the null hypothesis given an
arbitrary parameter value <span class="math inline">\(\theta\)</span>:
<span class="math display">\[
\mbox{power}(\theta) = P(\mbox{reject}~H_o \vert \theta) \,.
\]</span>
The power is a function of <span class="math inline">\(\theta\)</span>, and its value is
<span class="math inline">\(1-\beta(\theta,\alpha)\)</span>, where <span class="math inline">\(\beta(\theta,\alpha)\)</span> is the Type II error.
We note that if we set <span class="math inline">\(\theta = \theta_o\)</span>, the null hypothesis value,
then the test power is by definition <span class="math inline">\(\alpha\)</span> (the probability of rejecting
the null if the null is correct).</p>
<p>Below we provide a second hypothesis test reference table, one that
contains details on the computations that one needs to carry out to compute
<span class="math inline">\(p\)</span>-values and test power. Like the first table, this is <em>not</em> a table to
memorize!</p>
<table>
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="34%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="right">Type</th>
<th align="right"><span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span>?</th>
<th align="center"><span class="math inline">\(p\)</span>-Value</th>
<th align="center">Test Power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">two-tail</td>
<td align="right">yes</td>
<td align="center">2 <span class="math inline">\(\times\)</span> min[<span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_o)\)</span>,</td>
<td align="center"><span class="math inline">\(F_Y(y_{\rm RR,lo} \vert \theta)\)</span> +</td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="center"><span class="math inline">\(1-F_Y(y_{\rm obs} \vert \theta_o)\)</span>]</td>
<td align="center"><span class="math inline">\(1 - F_Y(y_{\rm RR,hi} \vert \theta)\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="right">no</td>
<td align="center">same as above</td>
<td align="center">same as above</td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="right">lower-tail</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(F_Y(y_{\rm RR} \vert \theta)\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(1-F_Y(y_{\rm obs} \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(1-F_Y(y_{\rm RR} \vert \theta)\)</span></td>
</tr>
<tr class="odd">
<td align="right">upper-tail</td>
<td align="right">yes</td>
<td align="center"><span class="math inline">\(1-F_Y(y_{\rm obs} \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(1-F_Y(y_{\rm RR} \vert \theta)\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="right">no</td>
<td align="center"><span class="math inline">\(F_Y(y_{\rm obs} \vert \theta_o)\)</span></td>
<td align="center"><span class="math inline">\(F_Y(y_{\rm RR} \vert \theta)\)</span></td>
</tr>
</tbody>
</table>
<hr />
<div id="testing-a-hypothesis-about-the-normal-mean-with-variance-known" class="section level3 hasAnchor" number="2.15.1">
<h3><span class="header-section-number">2.15.1</span> Testing a Hypothesis About the Normal Mean with Variance Known<a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have sampled <span class="math inline">\(n = 25\)</span> iid data from a normal
distribution whose variance is <span class="math inline">\(\sigma^2 = 1\)</span>. We wish to test the
null hypothesis <span class="math inline">\(H_o : \mu = \mu_o = 2\)</span> versus the alternative
<span class="math inline">\(H_a : \mu \neq \mu_o\)</span>. What are the rejection regions for this test?
What is the <span class="math inline">\(p\)</span>-value if we observe <span class="math inline">\(\bar{x}_{\rm obs} = 1.404\)</span>?
Last, what is the power of this test for <span class="math inline">\(\mu = 1.8\)</span>? We assume
the level of the test is <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>To answer each of these questions, we will refer to the hypothesis test
reference tables given in Chapter 1 and above.</p>
</blockquote>
<blockquote>
<p>First we determine the rejection region boundaries:
<span class="math display">\[
y_{\rm RR,lo} = F_Y^{-1}\left(\frac{\alpha}{2} \vert \mu_o,\sigma^2\right) ~~~ \mbox{and} ~~~ y_{\rm RR,hi} = F_Y^{-1}\left(1-\frac{\alpha}{2} \vert \mu_o,\sigma^2\right) \,,
\]</span>
where <span class="math inline">\(Y = \bar{X}\)</span>.
The <code>R</code> function calls are</p>
</blockquote>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="the-normal-and-related-distributions.html#cb121-1" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb121-2"><a href="the-normal-and-related-distributions.html#cb121-2" tabindex="-1"></a>alpha    <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb121-3"><a href="the-normal-and-related-distributions.html#cb121-3" tabindex="-1"></a>mu.o     <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb121-4"><a href="the-normal-and-related-distributions.html#cb121-4" tabindex="-1"></a>sigma2   <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb121-5"><a href="the-normal-and-related-distributions.html#cb121-5" tabindex="-1"></a>(y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)))</span></code></pre></div>
<pre><code>## [1] 1.608007</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="the-normal-and-related-distributions.html#cb123-1" tabindex="-1"></a>(y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)))</span></code></pre></div>
<pre><code>## [1] 2.391993</code></pre>
<blockquote>
<p>We reject the null hypothesis if <span class="math inline">\(y_{\rm obs}\)</span> is less than 1.608 or
greater than 2.392. Note that these boundaries each lie the same distance
from <span class="math inline">\(\mu_o = 2\)</span>, due to the symmetry of the normal distribution from
which <span class="math inline">\(\bar{X}\)</span> is sampled. Also note that the placement of
these boundaries does <em>not</em> depend on the observed statistic
value <span class="math inline">\(y_{\rm obs} = 1.404\)</span>). See Figure <a href="the-normal-and-related-distributions.html#fig:normex1">2.17</a>, where
the rejection regions are displayed in red.</p>
</blockquote>
<blockquote>
<p>Next, we compute the <span class="math inline">\(p\)</span>-value, noting that we already know that is has
to be less than 0.05, since the observed value of 1.404 lies in the
rejection region. Using the table above, we infer the relevant code:</p>
</blockquote>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="the-normal-and-related-distributions.html#cb125-1" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb125-2"><a href="the-normal-and-related-distributions.html#cb125-2" tabindex="-1"></a>y.obs  <span class="ot">&lt;-</span> <span class="fl">1.404</span></span>
<span id="cb125-3"><a href="the-normal-and-related-distributions.html#cb125-3" tabindex="-1"></a>mu.o   <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb125-4"><a href="the-normal-and-related-distributions.html#cb125-4" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb125-5"><a href="the-normal-and-related-distributions.html#cb125-5" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(<span class="fu">c</span>(<span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)),</span>
<span id="cb125-6"><a href="the-normal-and-related-distributions.html#cb125-6" tabindex="-1"></a>        <span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(y.obs,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))))</span></code></pre></div>
<pre><code>## [1] 0.002882484</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.0029. This value is
to be interpreted as the probability that we would observe a value as far
or farther from 2 as 1.404 (and 2.596, by symmetry) is 0.29 percent, if
the null is correct. That is sufficiently small that we conclude that
the null hypothesis is incorrect.</p>
</blockquote>
<blockquote>
<p>Last, we look at the test power assuming <span class="math inline">\(\mu = 1.8\)</span>. Again, we use
the reference table above and infer the relevant code:</p>
</blockquote>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="the-normal-and-related-distributions.html#cb127-1" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb127-2"><a href="the-normal-and-related-distributions.html#cb127-2" tabindex="-1"></a>alpha  <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb127-3"><a href="the-normal-and-related-distributions.html#cb127-3" tabindex="-1"></a>mu.o   <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb127-4"><a href="the-normal-and-related-distributions.html#cb127-4" tabindex="-1"></a>mu     <span class="ot">&lt;-</span> <span class="fl">1.8</span></span>
<span id="cb127-5"><a href="the-normal-and-related-distributions.html#cb127-5" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb127-6"><a href="the-normal-and-related-distributions.html#cb127-6" tabindex="-1"></a><span class="fu">pnorm</span>(y.rr.lo,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)) <span class="sc">+</span> </span>
<span id="cb127-7"><a href="the-normal-and-related-distributions.html#cb127-7" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(y.rr.hi,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)))</span></code></pre></div>
<pre><code>## [1] 0.170075</code></pre>
<blockquote>
<p>The test power is 0.170: <em>if</em> <span class="math inline">\(\mu\)</span> is equal to 1.8, then 17.0 percent of
the time we would sample a value of our hypothesis test statistic that lies
in the rejection region. The farther <span class="math inline">\(\mu\)</span> is from <span class="math inline">\(\mu_o\)</span>, the greater
the power gets.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normex1"></span>
<img src="_main_files/figure-html/normex1-1.png" alt="\label{fig:normex1}The sampling distribution $f_Y(y)$ (blue curve), rejection regions (red polygons), and observed statistic value $y_{m obs}$ (vertical green line) for $Y = \bar{X}$ given $n = 25$ iid data that are assumed to be sampled from a $\mathcal{N}(2,1)$ distribution under the null hypothesis. We perform a two-tailed test, with $\alpha = 0.05$, and the variance $\sigma^2 = 1$ is assumed known. The value we observe is in the rejection region, thus we reject the null hypothesis and conclude that $\mu \neq 2$." width="50%" />
<p class="caption">
Figure 2.17: The sampling distribution <span class="math inline">\(f_Y(y)\)</span> (blue curve), rejection regions (red polygons), and observed statistic value <span class="math inline">\(y_{m obs}\)</span> (vertical green line) for <span class="math inline">\(Y = \bar{X}\)</span> given <span class="math inline">\(n = 25\)</span> iid data that are assumed to be sampled from a <span class="math inline">\(\mathcal{N}(2,1)\)</span> distribution under the null hypothesis. We perform a two-tailed test, with <span class="math inline">\(\alpha = 0.05\)</span>, and the variance <span class="math inline">\(\sigma^2 = 1\)</span> is assumed known. The value we observe is in the rejection region, thus we reject the null hypothesis and conclude that <span class="math inline">\(\mu \neq 2\)</span>.
</p>
</div>
<hr />
</div>
<div id="testing-a-hypothesis-about-the-normal-mean-with-variance-unknown" class="section level3 hasAnchor" number="2.15.2">
<h3><span class="header-section-number">2.15.2</span> Testing a Hypothesis About the Normal Mean with Variance Unknown<a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have sampled <span class="math inline">\(n = 15\)</span> iid data from a normal
distribution whose variance is unknown. We wish to test the
null hypothesis <span class="math inline">\(H_o : \mu = \mu_o = 4\)</span> versus the alternative
<span class="math inline">\(H_a : \mu &gt; \mu_o\)</span>. What is the rejection region for this test?
What is the <span class="math inline">\(p\)</span>-value if we observe <span class="math inline">\(\bar{x}_{\rm obs} = 4.5\)</span>?
Last, what is the power of this test for <span class="math inline">\(\mu = 4.5\)</span>? We assume
the level of the test is <span class="math inline">\(\alpha = 0.05\)</span> and that the sample
variance is <span class="math inline">\(s_{\rm obs}^2 = 3\)</span>.</p>
</blockquote>
<blockquote>
<p>To answer each of these questions, we will refer to the hypothesis test
reference tables given in Chapter 1 and above.</p>
</blockquote>
<blockquote>
<p>First we determine the rejection region boundary:
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}\left(1-\alpha \vert \mu_o,s_{\rm obs}^2\right) \,,
\]</span>
where <span class="math inline">\(Y = (\bar{X}-\mu_o)/\sqrt{s_{\rm obs}^2/n}\)</span> is a <span class="math inline">\(t\)</span>-distributed
random variable for <span class="math inline">\(n-1\)</span> degrees of freedom.
The <code>R</code> function call is</p>
</blockquote>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="the-normal-and-related-distributions.html#cb129-1" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb129-2"><a href="the-normal-and-related-distributions.html#cb129-2" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb129-3"><a href="the-normal-and-related-distributions.html#cb129-3" tabindex="-1"></a>(y.rr  <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="dv">1</span><span class="sc">-</span>alpha,n<span class="dv">-1</span>))</span></code></pre></div>
<pre><code>## [1] 1.76131</code></pre>
<blockquote>
<p>We reject the null hypothesis if <span class="math inline">\(y_{\rm obs}\)</span> is greater then 1.761.
See Figure <a href="the-normal-and-related-distributions.html#fig:varex">2.18</a>, in which the rejection region is displayed
in red.</p>
</blockquote>
<blockquote>
<p>Next, we compute the <span class="math inline">\(p\)</span>-value.
Using the table above, we infer the relevant code:</p>
</blockquote>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="the-normal-and-related-distributions.html#cb131-1" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="dv">15</span></span>
<span id="cb131-2"><a href="the-normal-and-related-distributions.html#cb131-2" tabindex="-1"></a>mu.o   <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb131-3"><a href="the-normal-and-related-distributions.html#cb131-3" tabindex="-1"></a>s2     <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb131-4"><a href="the-normal-and-related-distributions.html#cb131-4" tabindex="-1"></a>y.obs  <span class="ot">&lt;-</span> (<span class="fl">4.5</span><span class="sc">-</span>mu.o)<span class="sc">/</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n)</span>
<span id="cb131-5"><a href="the-normal-and-related-distributions.html#cb131-5" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(y.obs,n<span class="dv">-1</span>)</span></code></pre></div>
<pre><code>## [1] 0.1411855</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.141, which is <span class="math inline">\(&gt; \alpha\)</span>.
The probability that we would observe a value as far
or farther from 4 as 4.5 is 14.1 percent, if
the null is correct. We fail to
reject the null hypothesis; we cannot conclude that we have sufficient
data to reject the idea that <span class="math inline">\(\mu = 4\)</span>.</p>
</blockquote>
<blockquote>
<p>Last, we look at the test power assuming <span class="math inline">\(\mu = 4.5\)</span>.
Again, we use
the reference table above…but there is an issue: a <span class="math inline">\(t\)</span>-distribution
pdf is not a function of <span class="math inline">\(\mu\)</span>! Thus we have to add some extra computational
steps to complete the power calculation.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>We “map” <span class="math inline">\(y_{\rm RR}\)</span> to <span class="math inline">\(\bar{x}_{\rm RR}\)</span>:
<span class="math display">\[
y_{\rm RR} = \frac{\bar{x}_{\rm RR} - \mu_o}{s_{\rm obs}/\sqrt{n}} ~~~ \Rightarrow ~~~ \bar{x}_{\rm RR} = \mu_o + y_{\rm RR}\frac{s_{\rm obs}}{\sqrt{n}} \,.
\]</span></li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>We determine <span class="math inline">\(y_{\rm RR}&#39;\)</span>, the rejection-region boundary given the
alternative value <span class="math inline">\(\mu\)</span>:
<span class="math display">\[
y_{\rm RR}&#39; = \frac{\bar{x}_{\rm RR} - \mu}{s_{\rm obs}/\sqrt{n}} \,.
\]</span>
We plug this value of the rejection region boundary into the power
calculation. Here’s the relevant code:</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="the-normal-and-related-distributions.html#cb133-1" tabindex="-1"></a>mu         <span class="ot">&lt;-</span> <span class="fl">4.5</span></span>
<span id="cb133-2"><a href="the-normal-and-related-distributions.html#cb133-2" tabindex="-1"></a>x.bar.rr   <span class="ot">&lt;-</span> mu.o <span class="sc">+</span> y.rr<span class="sc">*</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n)</span>
<span id="cb133-3"><a href="the-normal-and-related-distributions.html#cb133-3" tabindex="-1"></a>y.rr.prime <span class="ot">&lt;-</span> (x.bar.rr <span class="sc">-</span> mu)<span class="sc">/</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n)</span>
<span id="cb133-4"><a href="the-normal-and-related-distributions.html#cb133-4" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(y.rr.prime,n<span class="dv">-1</span>)</span></code></pre></div>
<pre><code>## [1] 0.2652206</code></pre>
<blockquote>
<p>The test power is 0.265: <em>if</em> <span class="math inline">\(\mu\)</span> is equal to 4.5, then 26.5 percent of
the time we would sample a value of our hypothesis test statistic that lies
in the rejection region. The farther <span class="math inline">\(\mu\)</span> is from <span class="math inline">\(\mu_o\)</span> (in the positive
direction), the higher this percentage gets. See Figure <a href="the-normal-and-related-distributions.html#fig:varpowcurve">2.19</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varex"></span>
<img src="_main_files/figure-html/varex-1.png" alt="\label{fig:varex}The sampling distribution $f_Y(y)$ (blue curve), rejection region (red polygon), and observed statistic value $y_{m obs}$ (vertical green line) for $Y = \bar{X}$ given $n = 15$ iid data that are assumed to be sampled from a normal distribution with mean $\mu_o = 4$ and variance unknown under the null hypothesis. We perform an upper-tailed test, with $\alpha = 0.05$. The value we observe is *not* in the rejection region, thus we fail to reject the null hypothesis and conclude that $\mu = 4$ is a plausible value." width="50%" />
<p class="caption">
Figure 2.18: The sampling distribution <span class="math inline">\(f_Y(y)\)</span> (blue curve), rejection region (red polygon), and observed statistic value <span class="math inline">\(y_{m obs}\)</span> (vertical green line) for <span class="math inline">\(Y = \bar{X}\)</span> given <span class="math inline">\(n = 15\)</span> iid data that are assumed to be sampled from a normal distribution with mean <span class="math inline">\(\mu_o = 4\)</span> and variance unknown under the null hypothesis. We perform an upper-tailed test, with <span class="math inline">\(\alpha = 0.05\)</span>. The value we observe is <em>not</em> in the rejection region, thus we fail to reject the null hypothesis and conclude that <span class="math inline">\(\mu = 4\)</span> is a plausible value.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varpowcurve"></span>
<img src="_main_files/figure-html/varpowcurve-1.png" alt="\label{fig:varpowcurve}The power curve for the upper-tail test $H_o : \mu = \mu_o = 4$ versus $H_a : \mu &gt; \mu_o$. The red dot at $\mu = 4$ indicates the test power under the null: the power is $\alpha$, as it should be given the definition of test power. We see that $n = 15$ is a sufficient amount of data to clearly disambiguate between $\mu_o = 4$ and, e.g., $\mu \gtrsim 5$. We note that if the alternative hypothesis is $\mu &lt; \mu_o$, the curve would be reversed$-$it would rise towards the left$-$and if the alternative hypothesis is $\mu \neq \mu_o$, the power curve would have a U shape, with the minimum power being $\alpha$ at $\mu = \mu_o$." width="50%" />
<p class="caption">
Figure 2.19: The power curve for the upper-tail test <span class="math inline">\(H_o : \mu = \mu_o = 4\)</span> versus <span class="math inline">\(H_a : \mu &gt; \mu_o\)</span>. The red dot at <span class="math inline">\(\mu = 4\)</span> indicates the test power under the null: the power is <span class="math inline">\(\alpha\)</span>, as it should be given the definition of test power. We see that <span class="math inline">\(n = 15\)</span> is a sufficient amount of data to clearly disambiguate between <span class="math inline">\(\mu_o = 4\)</span> and, e.g., <span class="math inline">\(\mu \gtrsim 5\)</span>. We note that if the alternative hypothesis is <span class="math inline">\(\mu &lt; \mu_o\)</span>, the curve would be reversed<span class="math inline">\(-\)</span>it would rise towards the left<span class="math inline">\(-\)</span>and if the alternative hypothesis is <span class="math inline">\(\mu \neq \mu_o\)</span>, the power curve would have a U shape, with the minimum power being <span class="math inline">\(\alpha\)</span> at <span class="math inline">\(\mu = \mu_o\)</span>.
</p>
</div>
<hr />
</div>
<div id="hypothesis-testing-using-the-clt" class="section level3 hasAnchor" number="2.15.3">
<h3><span class="header-section-number">2.15.3</span> Hypothesis Testing: Using the CLT<a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s go back to the example in the confidence interval section above,
in which we
assumed that we sampled <span class="math inline">\(n\)</span> iid data from the
decidedly not normal distribution
<span class="math display">\[
f_X(x) = \theta x^{\theta-1} ~~ x \in [0,1] \,,
\]</span>
with <span class="math inline">\(\theta &gt; 0\)</span>, and for which <span class="math inline">\(E[X] = \mu = \theta/(\theta+1)\)</span>.
We found that because we cannot express the sampling distribution
of <span class="math inline">\(Y = \bar{X}\)</span> analytically, our only path to determining a confidence
interval was by making use of the Central Limit Theorem.</p>
</blockquote>
<blockquote>
<p>Confidence interval construction and the performance of
hypothesis tests are “two sides of the same coin”: in both we are
solving for roots of equations, and the only difference is in terms
of what is fixed (the observed statistic for confidence intervals and
the null hypothesis value <span class="math inline">\(\theta_o\)</span> for hypothesis tests) and what
we solve for
(the parameter value <span class="math inline">\(\theta\)</span> for confidence intervals and the
rejection-region boundaries
for hypothesis tests). Thus what “works” for confidence
intervals will work for hypothesis tests, meaning that we can utilize
the CLT to test the null hypothesis <span class="math inline">\(H_o : \theta = \theta_o\)</span>.
(So long as <span class="math inline">\(n \gtrsim 30\)</span>!)</p>
</blockquote>
<blockquote>
<p>To find the rejection region(s), <span class="math inline">\(p\)</span>-value, and power for tests of the
mean when the CLT is involved, we utilize the same codes as we utilize
for the “variance known” case (i.e., the ones based on <code>pnorm()</code> and
<code>qnorm()</code>), except that instead of plugging in <span class="math inline">\(\sigma^2\)</span> (i.e., <code>sigma2</code>),
we would plug in the sample variance <span class="math inline">\(s_{\rm obs}^2\)</span> (i.e., <code>s2</code>).
For instance:</p>
</blockquote>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="the-normal-and-related-distributions.html#cb135-1" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb135-2"><a href="the-normal-and-related-distributions.html#cb135-2" tabindex="-1"></a>alpha    <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb135-3"><a href="the-normal-and-related-distributions.html#cb135-3" tabindex="-1"></a>theta.o  <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb135-4"><a href="the-normal-and-related-distributions.html#cb135-4" tabindex="-1"></a>mu.o     <span class="ot">&lt;-</span> theta.o<span class="sc">/</span>(theta.o<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb135-5"><a href="the-normal-and-related-distributions.html#cb135-5" tabindex="-1"></a>s2       <span class="ot">&lt;-</span> <span class="fl">0.0440</span></span>
<span id="cb135-6"><a href="the-normal-and-related-distributions.html#cb135-6" tabindex="-1"></a>(y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(  alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n)))</span></code></pre></div>
<pre><code>## [1] 0.5844416</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="the-normal-and-related-distributions.html#cb137-1" tabindex="-1"></a>(y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n)))</span></code></pre></div>
<pre><code>## [1] 0.7488918</code></pre>
<blockquote>
<p>The rejection region boundaries are thus 0.5844 and 0.7489.</p>
</blockquote>
<blockquote>
<p>We note that, as is the case for confidence interval coverage, our
result is only approximate,
i.e., the Type I error rate is not exactly <span class="math inline">\(\alpha\)</span>, because
the distribution of <span class="math inline">\(\bar{X}\)</span> is not exactly normal. However, as we
do in the confidence interval case, we can perform simulations to assess
just how far off our actual Type I error rate <span class="math inline">\(\alpha\)</span> is for any
given analysis setting.</p>
</blockquote>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="the-normal-and-related-distributions.html#cb139-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb139-2"><a href="the-normal-and-related-distributions.html#cb139-2" tabindex="-1"></a>num.sim  <span class="ot">&lt;-</span> <span class="dv">10000</span>    <span class="co"># repeat the data-generating process 10,000 times</span></span>
<span id="cb139-3"><a href="the-normal-and-related-distributions.html#cb139-3" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb139-4"><a href="the-normal-and-related-distributions.html#cb139-4" tabindex="-1"></a>alpha    <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb139-5"><a href="the-normal-and-related-distributions.html#cb139-5" tabindex="-1"></a>theta.o  <span class="ot">&lt;-</span> <span class="dv">2</span>        <span class="co"># the mean is 3/(3+1) = 3/4</span></span>
<span id="cb139-6"><a href="the-normal-and-related-distributions.html#cb139-6" tabindex="-1"></a>mu.o     <span class="ot">&lt;-</span> theta.o<span class="sc">/</span>(theta.o<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb139-7"><a href="the-normal-and-related-distributions.html#cb139-7" tabindex="-1"></a>typeIerr <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb139-8"><a href="the-normal-and-related-distributions.html#cb139-8" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.sim ) {</span>
<span id="cb139-9"><a href="the-normal-and-related-distributions.html#cb139-9" tabindex="-1"></a>  X       <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n,theta.o,<span class="dv">1</span>)</span>
<span id="cb139-10"><a href="the-normal-and-related-distributions.html#cb139-10" tabindex="-1"></a>  y.obs   <span class="ot">&lt;-</span> <span class="fu">mean</span>(X)</span>
<span id="cb139-11"><a href="the-normal-and-related-distributions.html#cb139-11" tabindex="-1"></a>  s2      <span class="ot">&lt;-</span> <span class="fu">var</span>(X)</span>
<span id="cb139-12"><a href="the-normal-and-related-distributions.html#cb139-12" tabindex="-1"></a>  y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(  alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n))</span>
<span id="cb139-13"><a href="the-normal-and-related-distributions.html#cb139-13" tabindex="-1"></a>  y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(s2<span class="sc">/</span>n))</span>
<span id="cb139-14"><a href="the-normal-and-related-distributions.html#cb139-14" tabindex="-1"></a>  <span class="cf">if</span> ( y.obs <span class="sc">&lt;=</span> y.rr.lo <span class="sc">||</span> y.obs <span class="sc">&gt;=</span> y.rr.hi ) typeIerr <span class="ot">=</span> typeIerr<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb139-15"><a href="the-normal-and-related-distributions.html#cb139-15" tabindex="-1"></a>}</span>
<span id="cb139-16"><a href="the-normal-and-related-distributions.html#cb139-16" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The observed proportion of Type I errors =&quot;</span>,typeIerr<span class="sc">/</span>num.sim,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The observed proportion of Type I errors = 0.064</code></pre>
<blockquote>
<p>We observe an empirical Type I error rate of 0.064.
This is sufficiently far from the expected value of 0.05 that we
conclude that
the distribution of the sample mean is only approximately normal.</p>
</blockquote>
<hr />
</div>
<div id="testing-with-two-data-samples-the-t-test" class="section level3 hasAnchor" number="2.15.4">
<h3><span class="header-section-number">2.15.4</span> Testing With Two Data Samples: the t Test<a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we are given two independent samples of normal iid data:
<span class="math display">\[\begin{align*}
\{U_1,\ldots,U_{n_U}\} &amp;\sim \mathcal{N}(\mu_U,\sigma_U^2) \\
\{V_1,\ldots,V_{n_V}\} &amp;\sim \mathcal{N}(\mu_V,\sigma_V^2) \,.
\end{align*}\]</span>
The sample means are <span class="math inline">\(\bar{U}\)</span> and <span class="math inline">\(\bar{V}\)</span>, respectively, and we know that
<span class="math display">\[
\bar{U} \sim \mathcal{N}\left(\mu_U,\frac{\sigma_U^2}{n_U}\right) ~~\mbox{and}~~ \bar{V} \sim \mathcal{N}\left(\mu_V,\frac{\sigma_V^2}{n_V}\right) \,.
\]</span>
In the <em>two-sample t test</em>, the null hypothesis <span class="math inline">\(H_o\)</span> is that
<span class="math inline">\(\mu_U = \mu_V\)</span>. Thus a natural test statistic is <span class="math inline">\(\bar{U}-\bar{V}\)</span>,
which, as the reader may confirm using the method of moment-generating
functions, is normally distributed:
<span class="math display">\[
\bar{U} - \bar{V} \sim \mathcal{N}\left(\mu_U-\mu_V,\frac{\sigma_U^2}{n_U}+\frac{\sigma_V^2}{n_V}\right) \,.
\]</span>
However, when neither variance is known, we cannot
model the observed data with the normal distribution; instead, in
<em>Welch’s t</em> test, we assume the following:
<span class="math display">\[
T = \frac{\bar{U}-\bar{V}}{S_\Delta} = \frac{\bar{U}-\bar{V}}{\sqrt{\frac{S_U^2}{n_U}+\frac{S_V^2}{n_V}}} \sim t({\nu}) \,,
\]</span>
i.e., <span class="math inline">\(T\)</span> is sampled from a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(\nu\)</span> degrees of freedom,
where <span class="math inline">\(\nu\)</span> is estimated using the Welch-Satterthwaite equation:
<span class="math display">\[
\nu \approx \frac{S_\Delta^4}{\frac{S_U^4}{n_U^2(n_U-1)} + \frac{S_V^4}{n_V^2(n_V-1)}} \,.
\]</span>
The rule-of-thumb for this approximation to hold is that both
<span class="math inline">\(n_U\)</span> and <span class="math inline">\(n_V\)</span> are larger than 5. Note that in some applications, one
might see <span class="math inline">\(\nu\)</span> rounded off to an integer, but this is not necessary:
one can work with the <span class="math inline">\(t\)</span> distribution’s probability density function just
fine even if <span class="math inline">\(\nu\)</span> in not an integer! (Any rounding off is done to allow one
to use <span class="math inline">\(t\)</span> tables to estimate <span class="math inline">\(p\)</span>-values.)</p>
</blockquote>
<blockquote>
<p>Can we “reformat” this test so as to be consistent with how we perform
one-sample tests? The answer is yes.
Let the test statistic be <span class="math inline">\(Y = \bar{U}-\bar{V} = S_{\Delta} T\)</span>,
and let the null hypothesis be <span class="math inline">\(H_o : \mu_U - \mu_V = 0\)</span>.
Then
<span class="math display">\[
F_Y(y) = P(Y \leq y) = P(S_{\Delta} T \leq y) = P\left(T \leq \frac{y}{S_{\Delta}}\right) = F_{T(\nu)}\left(\frac{y}{S_{\Delta}}\right) \,,
\]</span>
and thus the rejection-region boundaries are given by
<span class="math display">\[
y_q = S_{\Delta} F_{T(\nu)}^{-1}(q) \,,
\]</span>
where <span class="math inline">\(q = \alpha/2\)</span> or <span class="math inline">\(1-\alpha/2\)</span>. In <code>R</code> code, these
boundaries are given by</p>
</blockquote>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="the-normal-and-related-distributions.html#cb141-1" tabindex="-1"></a>y.rr.lo <span class="ot">&lt;-</span> S.Delta <span class="sc">*</span> <span class="fu">qt</span>(alpha<span class="sc">/</span><span class="dv">2</span>,nu)</span>
<span id="cb141-2"><a href="the-normal-and-related-distributions.html#cb141-2" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> S.Delta <span class="sc">*</span> <span class="fu">qt</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,nu)</span></code></pre></div>
<blockquote>
<p>Additionally, the <span class="math inline">\(p\)</span>-values and power are given by</p>
</blockquote>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="the-normal-and-related-distributions.html#cb142-1" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(<span class="fu">c</span>(<span class="fu">pt</span>(y.obs<span class="sc">/</span>S.Delta,nu),<span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(y.obs<span class="sc">/</span>S.Delta,nu)))</span>
<span id="cb142-2"><a href="the-normal-and-related-distributions.html#cb142-2" tabindex="-1"></a>power <span class="ot">&lt;-</span> <span class="fu">pt</span>((y.rr.lo<span class="sc">-</span>mu.Delta)<span class="sc">/</span>S.Delta,nu) <span class="sc">+</span> </span>
<span id="cb142-3"><a href="the-normal-and-related-distributions.html#cb142-3" tabindex="-1"></a>         (<span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>((y.rr.hi<span class="sc">-</span>mu.Delta)<span class="sc">/</span>S.Delta,nu))</span></code></pre></div>
<blockquote>
<p>where <code>mu.Delta</code> is the specified alternative value for <span class="math inline">\(\mu_U-\mu_V\)</span>.
We leave it as an exercise to the reader to generalize the expressions
and code above for cases in which the null hypothesis value differs from
zero.</p>
</blockquote>
<blockquote>
<p>We note that the above code yields equivalent results to
the <code>R</code> function <code>t.test()</code>.</p>
</blockquote>
<blockquote>
<p>As a last point: the reader may ask “what do I do if I have three or more
samples and I want to test whether all of them have the same mean, with
the alternative being that at least one of the means is different?”
In such a situation, one would employ a one-way analysis of variance test,
or an ANOVA test; we introduce this test in the final section of this
chapter.</p>
</blockquote>
<hr />
</div>
<div id="more-about-p-values" class="section level3 hasAnchor" number="2.15.5">
<h3><span class="header-section-number">2.15.5</span> More About p-Values<a href="the-normal-and-related-distributions.html#more-about-p-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As described above, the <span class="math inline">\(p\)</span>-value is the probability of observing a
hypothesis test statistic value <span class="math inline">\(y_{\rm obs}\)</span> or a more extreme value,
i.e., one further from the null value. In mathematical terms,
for a continuous sampling distribution, a <span class="math inline">\(p\)</span>-value is
<span class="math display">\[\begin{align*}
p = 2 \times \mbox{min}\left[ \int_{-\infty}^{y_{\rm obs}} f_Y(y \vert \theta_o) dy , \int_{y_{\rm obs}}^\infty f_Y(y \vert \theta_o) dy \right] ~~~&amp; \mbox{(two-tail)} \\
p = \int_{-\infty}^{y_{\rm obs}} f_Y(y \vert \theta_o) dy ~~~&amp; \mbox{(lower-tail)} \\
p = \int_{y_{\rm obs}}^\infty f_Y(y \vert \theta_o) dy ~~~&amp; \mbox{(upper-tail)} \,.
\end{align*}\]</span>
If the null hypothesis is correct, then <span class="math inline">\(p\)</span> is sampled uniformly between
0 and 1, i.e., 0.83 is just as likely to be observed as 0.14 and as 0.23,
etc. This makes sense: the probability that <span class="math inline">\(p &lt; \alpha\)</span> is exactly <span class="math inline">\(\alpha\)</span>,
the Type I error rate. If we set <span class="math inline">\(\alpha\)</span> to 0.05, then when the null
is true, <span class="math inline">\(p\)</span> will be less than <span class="math inline">\(\alpha\)</span> five percent of the time, i.e., we will
wrongly make the decision to reject a true null on average one time
in every 20 hypothesis tests we perform. (See Figure <a href="the-normal-and-related-distributions.html#fig:pval">2.20</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pval"></span>
<img src="_main_files/figure-html/pval-1.png" alt="\label{fig:pval}The empirical distribution of $p$-values under the null hypothesis, for a lower-tail test with $n = 20$, $\mu_o = 3$, and $\sigma^2 = 1$. We can see that the $p$-values are uniformly distributed between 0 and 1 (with some random variation due to the fact that our simulation contains a finite sample of data). When the null is true, the probability of sampling a $p$-value less than $\alpha$ is thus exactly $\alpha$." width="50%" />
<p class="caption">
Figure 2.20: The empirical distribution of <span class="math inline">\(p\)</span>-values under the null hypothesis, for a lower-tail test with <span class="math inline">\(n = 20\)</span>, <span class="math inline">\(\mu_o = 3\)</span>, and <span class="math inline">\(\sigma^2 = 1\)</span>. We can see that the <span class="math inline">\(p\)</span>-values are uniformly distributed between 0 and 1 (with some random variation due to the fact that our simulation contains a finite sample of data). When the null is true, the probability of sampling a <span class="math inline">\(p\)</span>-value less than <span class="math inline">\(\alpha\)</span> is thus exactly <span class="math inline">\(\alpha\)</span>.
</p>
</div>
<blockquote>
<p>Because we are performing a simulation, the number of observations falling
into each histogram bin is a random variable, and thus we observe
some amount of “noise” in Figure <a href="the-normal-and-related-distributions.html#fig:pval">2.20</a> (i.e., some amount of
variation around the dashed red line)…but nevertheless we can visually
infer that the sampling distribution for <span class="math inline">\(p\)</span> is uniform (i.e., flat).
(For completeness, the number
of counts in each bin are collectively sampled from a multinomial
distribution, where the probability of a count being recorded in any
given bin being the reciprocal of the number of bins. We discuss
the multinomial distribution in Chapter 3.)</p>
</blockquote>
<blockquote>
<p>Now, what if the null hypothesis is not correct? What will happen to the
distribution of <span class="math inline">\(p\)</span>-values? The answer depends on the circumstance: if
we are performing an lower-tail test (with <span class="math inline">\(E[Y]\)</span> increasing with <span class="math inline">\(\theta\)</span>)
and the true mean is <em>larger</em> than the
hypothesized mean, then the <span class="math inline">\(p\)</span>-values will skew towards 1: we are less
likely to reject the null when its value is closer to the tail of interest
than the true value is.
If the true mean is smaller than the hypothesized mean, the <span class="math inline">\(p\)</span>-values will
skew towards 0. See Figure <a href="the-normal-and-related-distributions.html#fig:pvalskew">2.21</a> for a demonstration of the
latter point.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pvalskew"></span>
<img src="_main_files/figure-html/pvalskew-1.png" alt="\label{fig:pvalskew}The empirical distribution of $p$-values when the stated alternative hypothesis is consistent with the truth. We can see that the $p$-values skew towards 0, meaning that when the alternative hypothesis is true, the probability of sampling a $p$-value less than $\alpha$ can be $\gg \alpha$." width="50%" />
<p class="caption">
Figure 2.21: The empirical distribution of <span class="math inline">\(p\)</span>-values when the stated alternative hypothesis is consistent with the truth. We can see that the <span class="math inline">\(p\)</span>-values skew towards 0, meaning that when the alternative hypothesis is true, the probability of sampling a <span class="math inline">\(p\)</span>-value less than <span class="math inline">\(\alpha\)</span> can be <span class="math inline">\(\gg \alpha\)</span>.
</p>
</div>
<pre><code>## The proportion of p-values &lt; 0.05 is  0.29707</code></pre>
<blockquote>
<p>In Figure <a href="the-normal-and-related-distributions.html#fig:pvalskew">2.21</a>, 29.71% of the counts are observed as having
value less than <span class="math inline">\(\alpha = 0.05\)</span>…so the test power is 0.2971. As the
true mean gets smaller and smaller than <span class="math inline">\(\mu_o = 3\)</span>,
the number of counts in the first bins
increase…which is equivalent to saying that the test power increases.</p>
</blockquote>
<blockquote>
<p>We will make three important statements about <span class="math inline">\(p\)</span>-values here, ones
that are good for all readers to remember.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li><strong>A p-value is not the probability that the null hypothesis is correct!</strong>
A hypothesis is <em>not</em> a random variable, and there is thus no probability
associated with it. A <span class="math inline">\(p\)</span>-value is the probability of observing a more
extreme hypothesis test statistic than what we actually observe, conditioned
on the null hypothesis being correct.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li><strong>We cannot select the Type I error rate <span class="math inline">\(\alpha\)</span> after computing the p-value.</strong>
We specify the Type I error rate <span class="math inline">\(\alpha\)</span> <em>before</em> data are analyzed;
specifying it afterwards opens up the analysis procedure to bias: “my
<span class="math inline">\(p\)</span>-value is 0.09…so I’ll set <span class="math inline">\(\alpha = 0.1\)</span> so that I can reject the
null.”</li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li><strong>We cannot perform multiple hypothesis tests without correcting our
result.</strong> Suppose that we perform 100 independent hypothesis tests, each
with <span class="math inline">\(\alpha = 0.05\)</span>. Even if the null is true for every test, we will
end up rejecting the null approximately five times. (Not necessarily
exactly five times: the
number of rejections we observe is a random variable with mean of five.)
That we will reject an increasing number of null hypotheses with an
increasing number of tests is inevitable: this is a feature of hypothesis
testing, not a bug. When performing multiple tests, we need to apply a
correction factor to each observed <span class="math inline">\(p\)</span>-value, such as a Bonferroni
correction (e.g., if <span class="math inline">\(m\)</span> is the number of conducted tests, reset
<span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\alpha/m\)</span>) or the less conservative false-discovery rate
(FDR) correction. Otherwise, we are engaging in <em>p-hacking</em> and
<em>data dredging</em>. We discuss correction factors in more detail in Chapter 5.</li>
</ol>
</blockquote>
<hr />
</div>
<div id="test-power-sample-size-computation" class="section level3 hasAnchor" number="2.15.6">
<h3><span class="header-section-number">2.15.6</span> Test Power: Sample-Size Computation<a href="the-normal-and-related-distributions.html#test-power-sample-size-computation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We can phrase a typical experimental design exercise as follows.
“We will collect data that we assume are iid from a normal distribution
with specified mean <span class="math inline">\(\mu\)</span> and specified variance <span class="math inline">\(\sigma^2\)</span>.” (Alternatively,
it could be a specification of <span class="math inline">\(s_{\rm obs}^2\)</span>, motivating the use of
<span class="math inline">\(t\)</span> distribution.) “How many data do we need to collect to achieve a
test power of 0.8, assuming <span class="math inline">\(\alpha = 0.05\)</span> and assuming that the
null hypothesis is <span class="math inline">\(H_o : \mu = \mu_o\)</span>, but the true value is
<span class="math inline">\(\mu &gt; \mu_o\)</span>?”</p>
</blockquote>
<blockquote>
<p>We will begin answering this question using math, and then we will transition
to code.</p>
</blockquote>
<blockquote>
<p>We refer back to the hypothesis test reference tables
and note that for an upper-tail
test where <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span>,
<span class="math display">\[
\mbox{power}(\theta) = 1 - F_Y(y_{\rm RR} \vert \mu_o,\sigma^2/n) \,.
\]</span>
We substitute in our target power value and solve for <span class="math inline">\(n\)</span>:
<span class="math display">\[\begin{align*}
1 - F_Y(y_{\rm RR} \vert \mu_o,\sigma^2/n) &amp;= 0.8 \\
\Rightarrow ~~~ F_Y(y_{\rm RR} \vert \mu_o,\sigma^2/n) &amp;= 0.2 \\
\Rightarrow ~~~ &amp;\mbox{...?} \,.
\end{align*}\]</span>
The issue is two-fold, as it turns out: first, <span class="math inline">\(n\)</span> appears to the right
of the condition symbol, which means that we will not be solving this
by hand but rather numerically via root-finding. But there’s another
less-readily apparent issue as well: <span class="math inline">\(y_{\rm RR}\)</span> <em>itself depends
on</em> <span class="math inline">\(n\)</span>! Can we still pursue root-finding? Yes…we simply will have a
more complicated expression.</p>
</blockquote>
<blockquote>
<p>We transition here to code. We refer back to the rejection-region table
and note that for an upper-tail normal mean test, <span class="math inline">\(y_{\rm RR}\)</span> is</p>
</blockquote>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="the-normal-and-related-distributions.html#cb144-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))   <span class="co"># the null enters here</span></span></code></pre></div>
<blockquote>
<p>The analogous code for test power is</p>
</blockquote>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="the-normal-and-related-distributions.html#cb145-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(y.hi,<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n))   <span class="co"># the alternative enters here</span></span></code></pre></div>
<blockquote>
<p>Combining the two expressions, we get</p>
</blockquote>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="the-normal-and-related-distributions.html#cb146-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">mean=</span>mu.o,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)),<span class="at">mean=</span>mu,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2<span class="sc">/</span>n)) </span></code></pre></div>
<blockquote>
<p>To solve the problem at hand, we would subtract 0.8 from this expression,
set the result equal to zero, and utilize <code>uniroot()</code> to solve for <span class="math inline">\(n\)</span>:</p>
</blockquote>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="the-normal-and-related-distributions.html#cb147-1" tabindex="-1"></a><span class="co"># Assume alpha &lt;- 0.05 ; mu.o &lt;- 8 ; mu &lt;- 9 ; sigma2 &lt;- 6</span></span>
<span id="cb147-2"><a href="the-normal-and-related-distributions.html#cb147-2" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(n)</span>
<span id="cb147-3"><a href="the-normal-and-related-distributions.html#cb147-3" tabindex="-1"></a>{</span>
<span id="cb147-4"><a href="the-normal-and-related-distributions.html#cb147-4" tabindex="-1"></a>  <span class="fu">pnorm</span>(<span class="fu">qnorm</span>(<span class="fl">0.975</span>,<span class="at">mean=</span><span class="dv">8</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">6</span><span class="sc">/</span>n)),<span class="at">mean=</span><span class="dv">9</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(<span class="dv">6</span><span class="sc">/</span>n)) <span class="sc">-</span> <span class="fl">0.2</span></span>
<span id="cb147-5"><a href="the-normal-and-related-distributions.html#cb147-5" tabindex="-1"></a>}</span>
<span id="cb147-6"><a href="the-normal-and-related-distributions.html#cb147-6" tabindex="-1"></a><span class="fu">ceiling</span>(<span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1000</span>))<span class="sc">$</span>root)</span></code></pre></div>
<pre><code>## [1] 48</code></pre>
<blockquote>
<p>Note how we use the function <code>ceiling()</code> here, which rounds up the
result to the next higher integer. We do this because the sample size
should be an integer, and because we want the power to be <em>at least</em> 0.8;
if we round our result down (via the <code>floor()</code> function), the power will
be less than 0.8. <em>In sample size-determination exercises, always round
the final result up!</em></p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-population-variance" class="section level2 hasAnchor" number="2.16">
<h2><span class="header-section-number">2.16</span> Hypothesis Testing: Population Variance<a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When performing hypothesis tests for the population mean <span class="math inline">\(\mu\)</span>, above, we
adopt <span class="math inline">\(\bar{X}\)</span> as our hypothesis test statistic
because it is the MLE for <span class="math inline">\(\mu\)</span>; we
do not theoretically justify the choice (by showing, e.g., that its use leads
us to defining the most powerful test out of a set of possible alternatives).
Here, we follow a similar logic: <span class="math inline">\(S^2\)</span> is an unbiased estimator for <span class="math inline">\(\sigma^2\)</span>,
so we will adopt <span class="math inline">\(Y = S^2\)</span> as our test statistic when testing hypotheses about
<span class="math inline">\(\sigma^2\)</span>.</p>
<p>Assume that we are given <span class="math inline">\(n\)</span> iid data <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>
sampled from a normal distribution with (unknown)
mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Refer back to the table in
the hypothesis testing section of Chapter 1. We observe that
<span class="math inline">\(E[Y] = E[S^2] = \sigma^2\)</span>, so <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\sigma^2\)</span>. Thus
the rejection region boundaries for, e.g., a two-tail test are
found by solving
<span class="math display">\[\begin{align*}
F_Y(y_{\rm RR,lo} \vert \sigma_o^2) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{\rm RR,hi} \vert \sigma_o^2) - \left(1 - \frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
As is the case for the population mean, we have to borrow (and amend) a
result from the confidence interval section to proceed here, namely that
<span class="math display">\[
F_Y(y) = F_{W(n-1)}\left(\frac{(n-1)y}{\sigma_o^2}\right) \,,
\]</span>
where <span class="math inline">\(F_{W(n-1)}\)</span> is the cdf for a chi-square distribution for <span class="math inline">\(n-1\)</span>
degrees of freedom. So, for instance,
<span class="math display">\[
y_{\rm RR,hi} = \frac{\sigma_o^2}{n-1}F_{W(n-1)}^{-1}\left(1-\frac{\alpha}{2}\right) \,,
\]</span>
or, in code,</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="the-normal-and-related-distributions.html#cb149-1" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> sigma2.o<span class="sc">*</span><span class="fu">qchisq</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,n<span class="dv">-1</span>)<span class="sc">/</span>(n<span class="dv">-1</span>)</span></code></pre></div>
<p>(See Figure <a href="the-normal-and-related-distributions.html#fig:varrr">2.22</a>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varrr"></span>
<img src="_main_files/figure-html/varrr-1.png" alt="\label{fig:varrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming $\alpha = 0.1$ and $n-1 = 3$ degrees of freedom." width="30%" /><img src="_main_files/figure-html/varrr-2.png" alt="\label{fig:varrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming $\alpha = 0.1$ and $n-1 = 3$ degrees of freedom." width="30%" /><img src="_main_files/figure-html/varrr-3.png" alt="\label{fig:varrr}Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming $\alpha = 0.1$ and $n-1 = 3$ degrees of freedom." width="30%" />
<p class="caption">
Figure 2.22: Illustration of rejection regions (shaded in red) for two-tail (left), lower-tail (center), and upper-tail (right) tests of population variance, assuming <span class="math inline">\(\alpha = 0.1\)</span> and <span class="math inline">\(n-1 = 3\)</span> degrees of freedom.
</p>
</div>
<hr />
<div id="testing-a-hypothesis-about-the-normal-population-variance" class="section level3 hasAnchor" number="2.16.1">
<h3><span class="header-section-number">2.16.1</span> Testing a Hypothesis About the Normal Population Variance<a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have sampled <span class="math inline">\(n = 14\)</span> iid data from a normal
distribution of unknown mean and variance. We wish to test the null
hypothesis <span class="math inline">\(H_o : \sigma^2 = \sigma_o^2 = 4\)</span> versus the alternative
<span class="math inline">\(H_a : \sigma^2 &gt; \sigma_o^2\)</span>. What are the rejection regions for this
test? What is the <span class="math inline">\(p\)</span>-value if we observe <span class="math inline">\(s_{\rm obs}^2 = 5.5\)</span>? Last,
what is the power of this test for <span class="math inline">\(\sigma^2 = 8\)</span>? We assume the level of
the test is <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>To answer each of these questions, we utilize the hypothesis test
reference tables.
For the rejection region, the <code>R</code> function call is</p>
</blockquote>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="the-normal-and-related-distributions.html#cb150-1" tabindex="-1"></a>alpha    <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb150-2"><a href="the-normal-and-related-distributions.html#cb150-2" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">14</span></span>
<span id="cb150-3"><a href="the-normal-and-related-distributions.html#cb150-3" tabindex="-1"></a>sigma2.o <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb150-4"><a href="the-normal-and-related-distributions.html#cb150-4" tabindex="-1"></a>(y.rr <span class="ot">&lt;-</span> (sigma2.o<span class="sc">/</span>(n<span class="dv">-1</span>))<span class="sc">*</span><span class="fu">qchisq</span>(<span class="dv">1</span><span class="sc">-</span>alpha,n<span class="dv">-1</span>))</span></code></pre></div>
<pre><code>## [1] 6.880625</code></pre>
<blockquote>
<p>The rejection region boundary is 6.881: we reject the null hypothesis
if <span class="math inline">\(y_{\rm obs} = s_{\rm obs}^2 &gt; 6.881\)</span>.</p>
</blockquote>
<blockquote>
<p>Next, we compute the <span class="math inline">\(p\)</span>-value, noting that it has
to be greater than 0.05, since the observed value of 5.5 does not lie
in the rejection region. The relevant code is</p>
</blockquote>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="the-normal-and-related-distributions.html#cb152-1" tabindex="-1"></a>y.obs    <span class="ot">&lt;-</span> <span class="fl">5.5</span></span>
<span id="cb152-2"><a href="the-normal-and-related-distributions.html#cb152-2" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">pchisq</span>((n<span class="dv">-1</span>)<span class="sc">*</span>y.obs<span class="sc">/</span>sigma2.o,n<span class="dv">-1</span>)</span></code></pre></div>
<pre><code>## [1] 0.1623236</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.162, which is indeed <span class="math inline">\(&gt; \alpha\)</span>. There is thus a
16.2 percent chance that we would sample a variance value of 5.5 or
greater if the null is correct. This is too high a percentage for
us to confidently reject the idea that <span class="math inline">\(\sigma^2 = 4\)</span>.</p>
</blockquote>
<blockquote>
<p>Last, we look at the test power assuming <span class="math inline">\(\sigma^2 = 8\)</span>:</p>
</blockquote>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="the-normal-and-related-distributions.html#cb154-1" tabindex="-1"></a>sigma2   <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb154-2"><a href="the-normal-and-related-distributions.html#cb154-2" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">pchisq</span>((n<span class="dv">-1</span>)<span class="sc">*</span>y.rr<span class="sc">/</span>sigma2,n<span class="dv">-1</span>)</span></code></pre></div>
<pre><code>## [1] 0.5956563</code></pre>
<blockquote>
<p>The test power is 0.596: <em>if</em> <span class="math inline">\(\sigma^2\)</span> is equal to 8, then 59.6 percent
of the time we would sample a value of <span class="math inline">\(y_{\rm obs}\)</span> that lies in the
rejection region. This is not a particularly high value; if being able
to differentiate between <span class="math inline">\(\sigma_o^2 = 4\)</span> and <span class="math inline">\(\sigma^2 = 8\)</span> is an
important research goal, we would want to collect more data!</p>
</blockquote>
<blockquote>
<p>In the previous section, we show in an example how to determine the
sample size that is needed to achieve a particular power. Here we will
repeat that exercise, but with a twist: we will visualize the power of
the test as a function of <span class="math inline">\(n\)</span>. Because we are not solving for <span class="math inline">\(n\)</span> for
a particular power value, there is no need to utilize <code>uniroot()</code> here;
we simply need to loop over calls to <code>qchisq()</code> and <code>pchisq()</code>. (But
note that we don’t actually “loop,” as <code>R</code>’s vectorization capabilities
allow us to simply pass vectors of sample-size values into the
function calls, with vectors of results being returned.) See Figure
<a href="the-normal-and-related-distributions.html#fig:varpow2">2.23</a>.</p>
</blockquote>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="the-normal-and-related-distributions.html#cb156-1" tabindex="-1"></a>alpha    <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb156-2"><a href="the-normal-and-related-distributions.html#cb156-2" tabindex="-1"></a>sigma2.o <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb156-3"><a href="the-normal-and-related-distributions.html#cb156-3" tabindex="-1"></a>sigma2   <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb156-4"><a href="the-normal-and-related-distributions.html#cb156-4" tabindex="-1"></a>n        <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb156-5"><a href="the-normal-and-related-distributions.html#cb156-5" tabindex="-1"></a>y.rr     <span class="ot">&lt;-</span> (sigma2.o<span class="sc">/</span>(n<span class="dv">-1</span>))<span class="sc">*</span><span class="fu">qchisq</span>(<span class="dv">1</span><span class="sc">-</span>alpha,n<span class="dv">-1</span>)</span>
<span id="cb156-6"><a href="the-normal-and-related-distributions.html#cb156-6" tabindex="-1"></a>power    <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pchisq</span>((n<span class="dv">-1</span>)<span class="sc">*</span>y.rr<span class="sc">/</span>sigma2,n<span class="dv">-1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="the-normal-and-related-distributions.html#cb157-1" tabindex="-1"></a>df       <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(n,power)</span>
<span id="cb157-2"><a href="the-normal-and-related-distributions.html#cb157-2" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>n,<span class="at">y=</span>power)) <span class="sc">+</span></span>
<span id="cb157-3"><a href="the-normal-and-related-distributions.html#cb157-3" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">lwd=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb157-4"><a href="the-normal-and-related-distributions.html#cb157-4" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">x=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">14</span>),<span class="at">y=</span><span class="fu">c</span>(<span class="fl">0.596</span>,<span class="fl">0.596</span>,<span class="dv">0</span>,<span class="fl">0.596</span>)),</span>
<span id="cb157-5"><a href="the-normal-and-related-distributions.html#cb157-5" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y),<span class="at">col=</span><span class="st">&quot;red&quot;</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="fl">0.85</span>) <span class="sc">+</span></span>
<span id="cb157-6"><a href="the-normal-and-related-distributions.html#cb157-6" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">x=</span><span class="dv">14</span>,<span class="at">y=</span><span class="fl">0.596</span>,<span class="at">size=</span><span class="dv">4</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb157-7"><a href="the-normal-and-related-distributions.html#cb157-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb157-8"><a href="the-normal-and-related-distributions.html#cb157-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb157-9"><a href="the-normal-and-related-distributions.html#cb157-9" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;n&quot;</span>,<span class="at">y=</span><span class="st">&quot;Test Power&quot;</span>) <span class="sc">+</span></span>
<span id="cb157-10"><a href="the-normal-and-related-distributions.html#cb157-10" tabindex="-1"></a>  base_theme</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varpow2"></span>
<img src="_main_files/figure-html/varpow2-1.png" alt="\label{fig:varpow2}The test power as a function of sample size $n$ assuming $\sigma_o^2 = 4$, $\sigma^2 = 8$, and $\alpha = 0.05$. The red dot indicates the test power for $n = 14$, which as shown in the text is 0.596." width="50%" />
<p class="caption">
Figure 2.23: The test power as a function of sample size <span class="math inline">\(n\)</span> assuming <span class="math inline">\(\sigma_o^2 = 4\)</span>, <span class="math inline">\(\sigma^2 = 8\)</span>, and <span class="math inline">\(\alpha = 0.05\)</span>. The red dot indicates the test power for <span class="math inline">\(n = 14\)</span>, which as shown in the text is 0.596.
</p>
</div>
<hr />
</div>
<div id="testing-with-two-data-samples-the-f-test" class="section level3 hasAnchor" number="2.16.2">
<h3><span class="header-section-number">2.16.2</span> Testing With Two Data Samples: the F Test<a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As was the case above for the two-sample <span class="math inline">\(t\)</span> test, described above,
we assume we are given two independent samples of iid data:
<span class="math display">\[\begin{align*}
\{U_1,\ldots,U_{n_U}\} &amp;\sim \mathcal{N}(\mu_U,\sigma_U^2) \\
\{V_1,\ldots,V_{n_V}\} &amp;\sim \mathcal{N}(\mu_V,\sigma_V^2) \,,
\end{align*}\]</span>
with sample variances <span class="math inline">\(S_U^2\)</span> and <span class="math inline">\(S_V^2\)</span>, respectively. Given what we
know about the distribution of sample variance values, we can write that
<span class="math display">\[
W_U = \frac{(n_U-1)S_U^2}{\sigma_U^2} \sim \chi_{n_U-1}^2 ~~\mbox{and}~~ W_V = \frac{(n_V-1)S_V^2}{\sigma_V^2} \sim \chi_{n_V-1}^2 \,.
\]</span>
The ratio of <span class="math inline">\(W_U/(n_U-1)\)</span> to <span class="math inline">\(W_V/(n_V-1)\)</span> is
<span class="math display">\[
F = \left. \left(\frac{\frac{(n_U-1)S_U^2}{\sigma_U^2}}{n_U-1}\right) \right/ \left(\frac{\frac{(n_V-1)S_U^2}{\sigma_V^2}}{n_V-1}\right) = \frac{S_U^2/\sigma_U^2}{S_V^2/\sigma_V^2} \sim F_{n_U-1,n_V-1} \,,
\]</span>
where <span class="math inline">\(F_{n_U-1,n_V-1}\)</span> is the <em>F distribution</em> for <span class="math inline">\(n_U-1\)</span> numerator,
and <span class="math inline">\(n_V-1\)</span> denominator, degrees of freedom. Examining the equation above,
we see that <span class="math inline">\(F\)</span> is an appropriate statistic for testing hypotheses
about the relationship between the variances of both samples.</p>
</blockquote>
<blockquote>
<p>Let the test statistic be
<span class="math display">\[
Y = \frac{s_U^2}{s_V^2} \,.
\]</span>
(Note that it doesn’t matter which sample is identified as <span class="math inline">\(U\)</span> and which
is identified as <span class="math inline">\(V\)</span> so long as care is taken to not accidentally
“reverse” the samples
when coding the test.) As was the case with the two-sample <span class="math inline">\(t\)</span> test, we
observe <span class="math inline">\(Y\)</span> but only know the sampling distribution for <span class="math inline">\(F\)</span>, so we
need to enact a variable transformation:
<span class="math display">\[
F_Y(y) = P(Y \leq y) = P\left(\frac{\sigma_U^2}{\sigma_V^2}F \leq y \right) = P\left( F \leq \frac{\sigma_V^2}{\sigma_U^2} y \right) = F_{F(n_U-1,n_V-1)}\left(\frac{\sigma_V^2}{\sigma_U^2} y\right) \,.
\]</span>
Thus the rejection-region boundaries for a two-tail test are given by</p>
</blockquote>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="the-normal-and-related-distributions.html#cb158-1" tabindex="-1"></a>y.rr.lo <span class="ot">&lt;-</span> (sigmaU2.o<span class="sc">/</span>sigmaV2.o) <span class="sc">*</span> <span class="fu">qf</span>(alpha<span class="sc">/</span><span class="dv">2</span>,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>)</span>
<span id="cb158-2"><a href="the-normal-and-related-distributions.html#cb158-2" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> (sigmaU2.o<span class="sc">/</span>sigmaV2.o) <span class="sc">*</span> <span class="fu">qf</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>)</span></code></pre></div>
<blockquote>
<p>with the <span class="math inline">\(p\)</span>-values and power given by</p>
</blockquote>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="the-normal-and-related-distributions.html#cb159-1" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">min</span>(<span class="fu">c</span>(<span class="fu">pf</span>(sigmaV2.o<span class="sc">*</span>y.obs<span class="sc">/</span>sigmaU2.o,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>),</span>
<span id="cb159-2"><a href="the-normal-and-related-distributions.html#cb159-2" tabindex="-1"></a>               <span class="dv">1</span><span class="sc">-</span><span class="fu">pf</span>(sigmaV2.o<span class="sc">*</span>y.obs<span class="sc">/</span>sigmaU2.o,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>)))</span>
<span id="cb159-3"><a href="the-normal-and-related-distributions.html#cb159-3" tabindex="-1"></a>power <span class="ot">&lt;-</span> <span class="fu">pf</span>(sigmaV2<span class="sc">*</span>y.rr.lo<span class="sc">/</span>sigmaU2,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>) <span class="sc">+</span> </span>
<span id="cb159-4"><a href="the-normal-and-related-distributions.html#cb159-4" tabindex="-1"></a>         (<span class="dv">1</span><span class="sc">-</span><span class="fu">pf</span>(sigmaV2<span class="sc">*</span>y.rr.hi<span class="sc">/</span>sigmaU2,n.U<span class="dv">-1</span>,n.V<span class="dv">-1</span>))</span></code></pre></div>
<blockquote>
<p>where <span class="math inline">\(\sigma_U^2/\sigma_V^2\)</span> is the specified alternative
variance ratio. We leave it as an exercise to the reader to
derive appropriate expressions for one-tail tests.</p>
</blockquote>
<blockquote>
<p>We note that the above code yields equivalent results to the <code>R</code> function
<code>var.test()</code>.</p>
</blockquote>
</div>
</div>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="2.17">
<h2><span class="header-section-number">2.17</span> Simple Linear Regression<a href="the-normal-and-related-distributions.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Thus far in this chapter, we have assumed that we have been
given a sample of iid data <span class="math inline">\(\{X_1,\ldots,X_n\} \sim
\mathcal{N}(\mu,\sigma^2)\)</span>, or, sometimes, two samples that are
independent of one another. Here we will move beyond this assumption to the
case where our data consists of tuples <span class="math inline">\(\{(x_1,Y_1),\ldots,(x_n,Y_n)\}\)</span> and
where we want to determine (or <em>learn</em>) the association between the variables
<span class="math inline">\(x_i\)</span> and <span class="math inline">\(Y_i\)</span>. (Assuming there is one! See Figure <a href="the-normal-and-related-distributions.html#fig:linreg">2.24</a>. Also,
note the use of the term “association”: a relationship might exist, but
it is not necessarily the case that <span class="math inline">\(x\)</span> “causes” <span class="math inline">\(Y\)</span>; to determine
causation, one would utilize methods of <em>causal inference</em>, which are
beyond the scope of this book.)
In other words, we want to <em>regress</em> <span class="math inline">\(\mathbf{Y}\)</span> upon <span class="math inline">\(\mathbf{x}\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbf{x} = \{x_1,\ldots,x_n\}\)</span>: the <em>predictor variable</em> (or <em>independent variable</em> or <em>covariate</em> or <em>feature</em>)</li>
<li><span class="math inline">\(\mathbf{Y} = \{Y_1,\ldots,Y_n\}\)</span>: the <em>response variable</em> (or the <em>dependent variable</em> or <em>target</em>)</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linreg"></span>
<img src="_main_files/figure-html/linreg-1.png" alt="\label{fig:linreg}Illustration of the setting for simple linear regression. The data $Y \vert x$ (blue points) are randomly distributed around the true regression line $y \vert x = 4 + x/2$ (red dashed line), with the error term $\epsilon_i \sim \mathcal{N}(y \vert 0,1)$." width="50%" />
<p class="caption">
Figure 2.24: Illustration of the setting for simple linear regression. The data <span class="math inline">\(Y \vert x\)</span> (blue points) are randomly distributed around the true regression line <span class="math inline">\(y \vert x = 4 + x/2\)</span> (red dashed line), with the error term <span class="math inline">\(\epsilon_i \sim \mathcal{N}(y \vert 0,1)\)</span>.
</p>
</div>
<p>If the relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> is <em>deterministic</em>,
then there exists a function <span class="math inline">\(f\)</span> such that <span class="math inline">\(y_i = f(x_i)\)</span> for all <span class="math inline">\(i\)</span>.
As statisticians, we are more interested in learning non-deterministic
associations (so-called <em>stochastic</em>, <em>probabilistic</em>, or <em>statistical</em>
relationships) between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>. One standard model
for the data-generating process is
<span class="math display">\[
Y_i \vert x_i = f(x_i) + \epsilon_i \,,
\]</span>
where <span class="math inline">\(\{\epsilon_1,\ldots,\epsilon_n\}\)</span> are random variables with
<span class="math inline">\(E[\epsilon_i] = 0\)</span> and uncorrelated variances <span class="math inline">\(V[\epsilon_i] = \sigma^2\)</span>
(i.e., the variances are the same for all <span class="math inline">\(i\)</span>…they are <em>homoscedastic</em>).
(Note that we are not saying anything about the distribution of
<span class="math inline">\(\epsilon_i\)</span> yet; we are just stating assumptions about its expected value
and variance.) Now we can say why the predictor variable is represented
in lower case while the response variable is in upper case: we consider
the predictor variable values to be fixed, while the response variable values
are random variables because the <span class="math inline">\(\epsilon_i\)</span>’s are random variables.</p>
<p>We now suppose that there is a linear relationship between <span class="math inline">\(\mathbf{x}\)</span>
and <span class="math inline">\(\mathbf{Y}\)</span> such that
<span class="math display">\[\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i \\
E[Y_i] &amp;= E[\beta_0 + \beta_1 x_i + \epsilon_i] = \beta_0 + \beta_1 x_i + E[\epsilon_i] = \beta_0 + \beta_1 x_i \\
V[Y_i] &amp;= V[\beta_0 + \beta_1 x_i + \epsilon_i] = V[\epsilon_i] = \sigma^2 \,.
\end{align*}\]</span>
This is a <em>simple linear regression</em> model, where
the word “simple” follows from the fact that
there is only one predictor variable. (Note that this is
a linear model because it is linear in the parameters; e.g.,
<span class="math inline">\(\beta_0 + \beta_1x_i^2\)</span> is also a linear model.) <span class="math inline">\(\beta_0\)</span> is dubbed
the <em>intercept</em> and <span class="math inline">\(\beta_1\)</span> is the <em>slope</em>. In analyses, we are
generally interested in estimating <span class="math inline">\(\beta_1\)</span>, as it tells us the
average change in <span class="math inline">\(Y\)</span> given a one-unit change in <span class="math inline">\(x\)</span>.</p>
<p>Once a simple linear regression model is learned, we can make predictions
<span class="math display">\[
\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</span>
and we can compute <em>residuals</em>
<span class="math display">\[
e_i = Y_i - \hat{Y}_i
\]</span>
The goal in the estimation process is to make the residuals as small as
possible. Since negative values of <span class="math inline">\(e_i\)</span> are as “bad” as
positive values, we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by minimizing the
<em>sum of squared errors</em> or <em>SSE</em>: <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span>.
(Note that this term is often used interchangeably with the
term <em>residual sum of squares</em>, or <em>RSS</em>.)
This is the so-called <em>ordinary least squares</em> estimator, or OLS. Our
use of the OLS estimator for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> (as opposed to,
e.g., <span class="math inline">\(\sum_{i=1}^n \vert e_i \vert\)</span>) is motivated by the
<em>Gauss-Markov theorem</em>, which states that the unbiased OLS estimator
is the best estimator
if the <span class="math inline">\(\epsilon_i\)</span>’s are uncorrelated, have equal variances,
and expected values of zero (hence motivating the assumptions stated above).
Note that we have still not said anything about the distribution of the
<span class="math inline">\(\epsilon_i\)</span>’s: the Gauss-Markov theorem does not mandate that they have
a particular distribution!</p>
<p>(As an aside: what happens if, e.g., we have data for which the variances are
unequal, or <em>heteroscedastic</em>? We
lose the ability to make statements like “the OLS estimator is the
best linear unbiased estimator.” We can always learn the simple linear model
we define above…a computer cannot stop us from doing so. It just may not
be the best model choice…for instance, <em>weighted OLS regression</em>, where
the weighting given each datum is related to how uncertain its value is,
may provide better estimates.)</p>
<p>To estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we take partial derivatives of
<span class="math display">\[
\sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\]</span>
with respect to <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, set the results to
zero, and solve for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.
Here, we will quote results, and provide the estimate of <span class="math inline">\(\sigma^2\)</span>,
while leaving the derivations as exercises to the reader:</p>
<table>
<colgroup>
<col width="17%" />
<col width="34%" />
<col width="17%" />
<col width="31%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Parameter</th>
<th align="center">Estimate</th>
<th align="center">Expected Value</th>
<th align="center">Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\beta_0\)</span></td>
<td align="center"><span class="math inline">\(\bar{Y}-\hat{\beta}_1\bar{x}\)</span></td>
<td align="center"><span class="math inline">\(\beta_0\)</span></td>
<td align="center"><span class="math inline">\(\sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\beta_1\)</span></td>
<td align="center"><span class="math inline">\(\frac{S_{xY}}{S_{xx}}\)</span></td>
<td align="center"><span class="math inline">\(\beta_1\)</span></td>
<td align="center"><span class="math inline">\(\frac{\sigma^2}{S_{xx}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\sigma^2\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n-2}\)</span> <span class="math inline">\(\sum_{i=1}^n\)</span> <span class="math inline">\(e_i^2\)</span> <span class="math inline">\(=\frac{SSE}{n-2}\)</span></td>
<td align="center"><span class="math inline">\(\sigma^2\)</span></td>
<td align="center"><span class="math inline">\(\frac{2\sigma^4}{n-2}\)</span></td>
</tr>
</tbody>
</table>
<p>The quantities <span class="math inline">\(S_{xx}\)</span> and <span class="math inline">\(S_{xY}\)</span> (and <span class="math inline">\(S_{YY}\)</span>) are
shorthand for the following:
<span class="math display">\[\begin{align*}
S_{xx} &amp;= \sum_{i=1}^n (x_i-\bar{x})^2 = \left(\sum_{i=1}^n x_i^2\right) - n\bar{x}^2 \\
S_{xY} &amp;= \sum_{i=1}^n (x_i-\bar{x})(Y_i-\bar{Y}) = \left(\sum_{i=1}^n x_iY_i\right) - n\bar{x}\bar{Y} \\
S_{YY} &amp;= \sum_{i=1}^n (Y_i-\bar{Y})^2 = \left(\sum_{i=1}^n Y_i^2\right) - n\bar{Y}^2 \,.
\end{align*}\]</span></p>
<p>In Figure <a href="the-normal-and-related-distributions.html#fig:linregest">2.25</a>, we display the estimated regression
line for the data shown above in Figure <a href="the-normal-and-related-distributions.html#fig:linreg">2.24</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linregest"></span>
<img src="_main_files/figure-html/linregest-1.png" alt="\label{fig:linregest}Same as the previous figure, with the estimated regression line $\hat{y} = 4.52 + 0.46 x$ overlaid (solid red line)." width="50%" />
<p class="caption">
Figure 2.25: Same as the previous figure, with the estimated regression line <span class="math inline">\(\hat{y} = 4.52 + 0.46 x\)</span> overlaid (solid red line).
</p>
</div>
<p>Ultimately, we are interested in determining if there is a statistically
significant relationship between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>, a question
that we can answer using a hypothesis test:
<span class="math display">\[\begin{align*}
H_o&amp;: \beta_1 = 0  \implies Y_i = \beta_0 + \epsilon_i \\
H_a&amp;: \beta_1 \neq 0 \implies Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{align*}\]</span>
In order to carry out hypothesis testing, however, we have to now make a
distributional assumption regarding the <span class="math inline">\(\epsilon_i\)</span>’s.
The standard assumption is that
<span class="math display">\[
\epsilon_i \sim \mathcal{N}(0,\sigma^2) \,,
\]</span>
which means that
<span class="math display">\[
Y_i \vert x_i \sim \mathcal{N}(\beta_0+\beta_1x_i,\sigma^2)
\]</span>
Adding this assumption to the model does not affect estimation; if, for
instance, we were to derive the MLE estimators for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>,
we would find that they are identical to the OLS estimators given above.</p>
<p>Does our distributional assumption for the <span class="math inline">\(\epsilon_i\)</span>’s allow us to
specify the distribution of, e.g., <span class="math inline">\(\hat{\beta}_1\)</span>? The answer is yes.
We begin by showing that <span class="math inline">\(\hat{\beta}_1\)</span> can be
written as a linear function of the response variable values:
<span class="math display">\[\begin{align*}
\hat{\beta}_1 &amp;= \frac{\left(\sum_{i=1}^n Y_i x_i\right) - n \bar{x}\bar{Y}}{\left(\sum_{i=1}^n x_i^2\right) - n\bar{x}^2} \\
              &amp;= \frac{1}{k} \left[ \left(\sum_{i=1}^n Y_i x_i\right) - n \bar{x}\bar{Y} \right] \\
              &amp;= \frac{1}{k} \left[ \left(\sum_{i=1}^n Y_i x_i\right) - \sum_{i=1}^n \bar{x} Y_i \right] \\
              &amp;= \frac{1}{k} \sum_{i=1}^n (x_i - \bar{x}) Y_i = \sum_{i=1}^n \left( \frac{x_i-\bar{x}}{k}\right) Y_i = \sum_{i=1}^n a_i Y_i \,.
\end{align*}\]</span>
Because the <span class="math inline">\(Y_i\)</span>’s are normally distributed, we know (via the method of
moment-generating functions) that <span class="math inline">\(\hat{\beta}_1\)</span> is also normally distributed,
with mean
<span class="math display">\[
E[\hat{\beta}_1] = \sum_{i=1}^n a_i E[Y_i] = \cdots = \beta_1
\]</span>
and variance
<span class="math display">\[
V[\hat{\beta}_1] = \sum_{i=1}^n a_i^2 V[Y_i] = \sum_{i=1}^n a_i^2 \sigma^2 = \cdots = \frac{\sigma^2}{\left(\sum_{i=1}^n x_i^2\right) - n\bar{x}^2} \,.
\]</span>
We can finally perform the hypothesis test.
If <span class="math inline">\(\sigma^2\)</span> is unknown, the standardized slope is assumed to
be <span class="math inline">\(t\)</span>-distributed for <span class="math inline">\(n-2\)</span> degrees of freedom:
<span class="math display">\[
\frac{\hat{\beta}_1 - \beta_1}{se({\hat{\beta}_1})} = \frac{\hat{\beta}_1 - \beta_1}{\sqrt{V[{\hat{\beta}_1}]}} \sim t_{n-2} \,.
\]</span></p>
<p>To reiterate:</p>
<ol style="list-style-type: decimal">
<li>The assumption that <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span> allows us
to test hypotheses about, e.g., <span class="math inline">\(\beta_1\)</span>. If this assumption is violated,
then OLS regression is still the best linear unbiased estimator.</li>
<li>If, in addition, the assumption that <span class="math inline">\(\epsilon_i\)</span>’s are homoscedastic
and uncorrelated is violated, the OLS regression will no longer be the
best linear unbiased estimator, and we would need to seek alternatives,
such as weighted OLS regression.</li>
<li>Last…if, in addition, the assumption of <span class="math inline">\(E[\epsilon_i] = 0\)</span> is
violated, then using OLS will be a biased estimator. At this point, we
would generally conclude that there is a better, nonlinear representation
of the data-generating process that we should use.</li>
</ol>
<hr />
<div id="correlation-the-strength-of-linear-association" class="section level3 hasAnchor" number="2.17.1">
<h3><span class="header-section-number">2.17.1</span> Correlation: the Strength of Linear Association<a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We can measure the strength of a linear association
via the metric of <em>correlation</em>. We define the
correlation between a pair of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as
<span class="math display">\[
\rho_{XY} = \frac{E[XY] - E[X]E[Y]}{\sqrt{V[X]V[Y]}}\,,
\]</span>
with <span class="math inline">\(\vert \rho_{XY} \vert \leq 1\)</span>. (We will discuss correlation and
the related concept of covariance more in depth in Chapter 6. For now,
just treat the equation above as a definition. The main thing to
keep in mind is that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables,
<span class="math inline">\(\rho_{XY} = 0\)</span>, while if <span class="math inline">\(X = Y\)</span> or <span class="math inline">\(X = -Y\)</span>, <span class="math inline">\(\rho_{XY} = 1\)</span> and <span class="math inline">\(-1\)</span>
respectively.) If instead of a pair of random variables
we have a set of tuples, we can still define a correlation, which we can
estimate using the <em>Pearson correlation coefficient</em>:
<span class="math display">\[
R = \frac{\sum_{i=1}^n (x_i-\bar{x})(Y_i-\bar{Y})}{\left[\sum_{i=1}^n (x_i-\bar{x})^2\right]\left[\sum_{i=1}^n (Y_i-\bar{Y})^2\right]} = \frac{S_{xY}}{\sqrt{S_{xx}S_{YY}}} = \hat{\beta}_1 \sqrt{\frac{S_{xx}}{S_{YY}}} \,.
\]</span>
The last equality follows from the fact that <span class="math inline">\(\hat{\beta}_1 = S_{xY}/S_{xx}\)</span>.
It follows from this expression that <span class="math inline">\(R\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> have the same
sign.</p>
</blockquote>
<blockquote>
<p>Related to the correlation is the <em>coefficient of determination</em>, or
<span class="math inline">\(R^2\)</span>, a quantity ranging from 0 to 1 that is interpreted as the proportion
of the variation in the response variable values explained by
the predictor variable values. The closer <span class="math inline">\(R^2\)</span> is to 1, the more strictly
linear is the association between <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>.</p>
</blockquote>
<blockquote>
<p>For completeness, we mention “adjusted <span class="math inline">\(R^2\)</span>,” as this is an oft-quoted
output from <code>R</code>’s linear model function, <code>lm()</code>. (See below for example
usage of <code>lm()</code>.) Adjusted <span class="math inline">\(R^2\)</span> attempts to correct for the tendency
of <span class="math inline">\(R^2\)</span> to over-optimistically indicate the quality of fit of a
linear model. There are several formulae for computing adjusted <span class="math inline">\(R^2\)</span>;
<code>R</code> uses Wherry’s formula:
<span class="math display">\[
\mbox{Adj.}~R^2 = 1 - (1-R^2)\frac{n-1}{n-p-1} \,,
\]</span>
where <span class="math inline">\(p\)</span> is the number of predictor variables (with <span class="math inline">\(p = 1\)</span> for simple
linear regression).</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-of-the-sum-of-squared-errors-sse" class="section level3 hasAnchor" number="2.17.2">
<h3><span class="header-section-number">2.17.2</span> The Expected Value of the Sum of Squared Errors (SSE)<a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here we show that <span class="math inline">\(SSE/(n-2)\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. This
is a non-trivial algebraic exercise, and thus we will leave the
calculation of <span class="math inline">\(V[\hat{\sigma^2}] = V[SSE/(n-2)] =
2\sigma^4/(n-2)\)</span> as an exercise to the (masochistic) reader.</p>
</blockquote>
<blockquote>
<p>We start by writing that
<span class="math display">\[\begin{align*}
SSE = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 &amp;= \sum_{i=1}^n [Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)]^2 \\
&amp;= \sum_{i=1}^n [Y_i - (\bar{Y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i)]^2 \\
&amp;= \sum_{i=1}^n [(Y_i - \bar{Y}) - \hat{\beta}_1 (x_i-\bar{x})]^2 \\
&amp;= \sum_{i=1}^n [(Y_i - \bar{Y})^2 - 2 \hat{\beta}_1 (x_i-\bar{x})(Y_i - \bar{Y}) + \hat{\beta}_1^2(x_i-\bar{x})^2] \\
&amp;= S_{YY} - 2 \hat{\beta}_1 S_{xY} + \hat{\beta}_1^2 S_{xx} \\
&amp;= S_{YY} - 2 \hat{\beta}_1 (S_{xx} \hat{\beta}_1) + \hat{\beta}_1^2 S_{xx} \\
&amp;= S_{YY} - \hat{\beta}_1^2 S_{xx} \,.
\end{align*}\]</span>
To find the expected value of this difference, we look first at
<span class="math inline">\(S_{YY}\)</span> and then at <span class="math inline">\(\hat{\beta}_1^2 S_{xx}\)</span>.
<span class="math display">\[\begin{align*}
E[S_{YY}] &amp;= E\left[ \sum_{i=1}^n (Y_i - \bar{Y})^2 \right] \\
&amp;= E\left[ \sum_{i=1}^n \left(\beta_0 + \beta_1x_i + \epsilon_i - \beta_0 - \beta_1 \bar{x} - \bar{\epsilon} \right)^2 \right] \\
&amp;= E\left[ \sum_{i=1}^n \left(\beta_1(x_i -\bar{x}) + (\epsilon_i - \bar{\epsilon}) \right)^2 \right] \\
&amp;= E\left[ \sum_{i=1}^n \beta_1^2(x_i -\bar{x})^2 + \sum_{i=1}^n 2 \beta_1 (x_i - \bar{x})(\epsilon_i - \bar{\epsilon}) + \sum_{i=1}^n (\epsilon_i - \bar{\epsilon})^2 \right] \\
&amp;= E\left[ \beta_1^2 S_{xx} + 2 \beta_1 \sum_{i=1}^n (x_i - \bar{x})(\epsilon_i - \bar{\epsilon}) + \sum_{i=1}^n (\epsilon_i - \bar{\epsilon})^2 \right] \\
&amp;= \beta_1^2 S_{xx} + 2 \beta_1 \sum_{i=1}^n (x_i - \bar{x}) E\left[ \epsilon_i - \bar{\epsilon} \right] + \sum_{i=1}^n E\left[ (\epsilon_i - \bar{\epsilon})^2 \right] \,.
\end{align*}\]</span>
Since <span class="math inline">\(E[\epsilon_i] = E[\bar{\epsilon}] = 0\)</span>, the middle term vanishes.
As for the right-most term:
<span class="math display">\[\begin{align*}
\sum_{i=1}^n E\left[ (\epsilon_i - \bar{\epsilon})^2 \right] &amp;= \sum_{i=1}^n E\left[ \epsilon_i^2 - 2\epsilon_i \bar{\epsilon} + \bar{\epsilon}^2 \right] \\
&amp;= \sum_{i=1}^n E\left[ \epsilon_i^2 \right] - 2 \sum_{i=1}^n E\left[\epsilon_i \bar{\epsilon}\right] + \sum_{i=1}^n E\left[\bar{\epsilon}^2\right] \\
&amp;= \sum_{i=1}^n E\left[ \epsilon_i^2 \right] - 2 E\left[ \bar{\epsilon} \sum_{i=1}^n \epsilon_i \right] + E\left[ \sum_{i=1}^n \bar{\epsilon}^2\right] \\
&amp;= \sum_{i=1}^n V[\epsilon_i] + \sum_{i=1}^n \left(E[\epsilon_i]\right)^2 - 2 n E\left[ \bar{\epsilon}^2 \right] + n E\left[ \bar{\epsilon}^2\right] \\
&amp;= \sum_{i=1}^n \sigma^2 - n E\left[ \bar{\epsilon}^2 \right] \\
&amp;= n \sigma^2 - n \frac{\sigma^2}{n} = (n-1)\sigma^2 \,.
\end{align*}\]</span>
So at this point, we have that
<span class="math display">\[
E[S_{YY}] = S_{xx} \beta_1^2 + (n-1)\sigma^2 \,.
\]</span>
Now let’s look at <span class="math inline">\(E[\hat{\beta}_1^2 S_{xx}]\)</span>:
<span class="math display">\[\begin{align*}
E\left[\hat{\beta}_1^2 S_{xx}\right] &amp;= S_{xx} E\left[\hat{\beta}_1^2\right] \\
&amp;= S_{xx} \left( V\left[ \hat{\beta}_1 \right] + \left( E\left[ \hat{\beta}_1 \right] \right)^2 \right) \\
&amp;= S_{xx} \left( \frac{\sigma^2}{S_{xx}} + \beta_1^2 \right) \\
&amp;= \sigma^2 + S_{xx} \beta_1^2 \,.
\end{align*}\]</span>
So
<span class="math display">\[
E[SSE] = S_{xx} \beta_1^2 + (n-1)\sigma^2 - \sigma^2 - S_{xx} \beta_1^2 = (n-2)\sigma^2 \,.
\]</span>
Thus <span class="math inline">\(SSE/(n-2)\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="linear-regression-in-r" class="section level3 hasAnchor" number="2.17.3">
<h3><span class="header-section-number">2.17.3</span> Linear Regression in R<a href="the-normal-and-related-distributions.html#linear-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Below, we show the results of learning a linear regression model using <code>R</code>.
The data are the same as used to produce Figure <a href="the-normal-and-related-distributions.html#fig:linregest">2.25</a>, with
the parameter estimates output by <code>lm()</code> mapping directly to the solid
red line in that figure.</p>
</blockquote>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="the-normal-and-related-distributions.html#cb160-1" tabindex="-1"></a><span class="co"># lm(): &quot;linear model&quot;</span></span>
<span id="cb160-2"><a href="the-normal-and-related-distributions.html#cb160-2" tabindex="-1"></a><span class="co"># y~x : this is a &quot;model formula&quot; that in words translates as</span></span>
<span id="cb160-3"><a href="the-normal-and-related-distributions.html#cb160-3" tabindex="-1"></a><span class="co">#       &quot;regress the response data y upon the predictor data x&quot;</span></span>
<span id="cb160-4"><a href="the-normal-and-related-distributions.html#cb160-4" tabindex="-1"></a><span class="co">#       or &quot;estimate beta_0 and beta_1 for the model E[Y] = beta_0 + beta_1 x&quot;</span></span>
<span id="cb160-5"><a href="the-normal-and-related-distributions.html#cb160-5" tabindex="-1"></a>lm.out <span class="ot">=</span> <span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb160-6"><a href="the-normal-and-related-distributions.html#cb160-6" tabindex="-1"></a></span>
<span id="cb160-7"><a href="the-normal-and-related-distributions.html#cb160-7" tabindex="-1"></a><span class="co"># show a summary of the model</span></span>
<span id="cb160-8"><a href="the-normal-and-related-distributions.html#cb160-8" tabindex="-1"></a><span class="fu">summary</span>(lm.out)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.00437 -0.53068  0.04523  0.40338  2.47660 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.5231     0.3216  14.063  &lt; 2e-16 ***
## x             0.4605     0.0586   7.859 1.75e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9792 on 38 degrees of freedom
## Multiple R-squared:  0.6191, Adjusted R-squared:  0.609 
## F-statistic: 61.76 on 1 and 38 DF,  p-value: 1.749e-09</code></pre>
<blockquote>
<ol style="list-style-type: decimal">
<li>The model residuals <span class="math inline">\(Y_i - \hat{Y}_i\)</span>
are summarized via five numbers (minimum, maximum,
and median, along with the 25<span class="math inline">\(^{\rm th}\)</span> and 75<span class="math inline">\(^{\rm th}\)</span> percentile values).
If, e.g., the median is substantially different from zero,
it is possible that the assumption that <span class="math inline">\(Y \vert x\)</span> is normally
distributed is violated…and it is possible that the
assumption <span class="math inline">\(E[\epsilon_i] = 0\)</span> is violated as well.</li>
<li>Under <code>Coefficients</code>, there are four numerical columns. The first
shows <span class="math inline">\(\hat{\beta}_0\)</span> (<code>(Intercept)</code>) and <span class="math inline">\(\hat{\beta}_1\)</span> (<code>x</code>), the
second shows the estimated standard errors for these statistics, the
third is simply the ratio of the values in the first and second columns,
and the fourth is the <span class="math inline">\(p\)</span>-value. The third and fourth columns are
distribution-specific: the hypothesis test being carried out has
a null of zero, and the <code>t value</code> is assumed to be <span class="math inline">\(t\)</span>-distributed for
<span class="math inline">\(n-2\)</span> degrees of freedom.
The <span class="math inline">\(p\)</span>-values in the fourth column are <span class="math inline">\(\ll \alpha = 0.05\)</span>, which lead
us to decide that both the intercept and the slope are truly non-zero.
(In our simulation, they are: <span class="math inline">\(\beta_0 = 4\)</span> and <span class="math inline">\(\beta_1 = 0.5\)</span>.)</li>
<li>The <code>Residual standard error</code> shows the value of
<span class="math inline">\(\hat{\sigma} = \sqrt{SSE/(n-2)}\)</span>, while
the “38 degrees of freedom” indicates that <span class="math inline">\(n = 40\)</span>. We note that in
this simulation, <span class="math inline">\(\sigma^2 = 1\)</span>.</li>
<li>The meanings behind the <code>R-squared</code> values are explained above in
the correlation example. In words, approximately 60% of the variation
exhibited by the data is accounted for with the linear model.</li>
<li>Last, the line with the phrase <code>F-statistic</code> shows the result of
a hypothesis test in which (in general) the null is that all the
<span class="math inline">\(\beta_i\)</span>’s (excluding <span class="math inline">\(\beta_0\)</span>) are zero and
the alternative is that at least one of the <span class="math inline">\(\beta_i\)</span>’s
is non-zero. Here, since there is only one non-intercept coefficient,
the <span class="math inline">\(p\)</span>-value is equivalent to the one printed under
<code>Coefficients</code>. We will discuss where the <code>F-statistic</code> and <code>DF</code> numbers
come from below in the next section.</li>
</ol>
</blockquote>
</div>
</div>
<div id="one-way-analysis-of-variance" class="section level2 hasAnchor" number="2.18">
<h2><span class="header-section-number">2.18</span> One-Way Analysis of Variance<a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the simple linear regression setting, our data consists of a continuously
distributed predictor variable <span class="math inline">\(\mathbf{x}\)</span> and response variable
<span class="math inline">\(\mathbf{Y}\)</span>. What if, instead, the predictor variable values come in <em>groups</em>?
For instance, we might wish to determine the time a person needs to
accomplish a task after eating either chocolate or vanilla ice cream.
To do this, we might map the eating of chocolate to a particular value of <span class="math inline">\(x\)</span>,
say <span class="math inline">\(x = 0\)</span>, and the eating of vanilla to another value
of <span class="math inline">\(x\)</span>, say <span class="math inline">\(x = 1\)</span>. Then, if we continue to adopt a simple linear regression
framework, we have that
<span class="math display">\[\begin{align*}
Y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i \\
E[Y_i \vert x_i = 0] &amp;= \beta_0 \\
E[Y_i \vert x_i = 1] &amp;= \beta_0 + \beta_1 \,.
\end{align*}\]</span>
To see if there is a difference in task-accomplishment time between groups,
we would test the hypothesis <span class="math inline">\(H_o: \beta_1 = 0\)</span> versus <span class="math inline">\(H_a: \beta_1 \neq 0\)</span>.
As the value of the slope is meaningless, beyond whether or not it is zero
(because the mapping from group characteristics to values of <span class="math inline">\(x\)</span> is arbitrary),
we would not run a conventional linear regression analysis; rather, if we
assume that <span class="math inline">\(\bar{Y} \vert x_i\)</span> is normally distributed, we would run a
two-sample <span class="math inline">\(t\)</span> test like the Welch’s <span class="math inline">\(t\)</span> test described earlier in this
chapter to determine whether <span class="math inline">\(\bar{Y} \vert x_i=0\)</span> is drawn from a distribution
with the same population mean as <span class="math inline">\(\bar{Y} \vert x_i=1\)</span>.</p>
<p>What if there are more than two groups (or <em>treatments</em>)? For instance,
what if we redo the experiment but add strawberry ice cream? When the
number of groups is greater than two, we move from the two-sample <span class="math inline">\(t\)</span> test
to <em>one-way analysis of variance</em> (or one-way <em>ANOVA</em>; the “one-way”
indicates that there is a single predictor variable,
which in our example is ice cream flavor). See Figure <a href="the-normal-and-related-distributions.html#fig:anova">2.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anova"></span>
<img src="_main_files/figure-html/anova-1.png" alt="\label{fig:anova}Illustration of the setting for a one-way analysis of variance. To the left, the spread of the data within each group is large compared to the differences in means between each group, so an ANOVA is less likely to reject the null hypothesis that $\mu_0 = \mu_1 = \mu_2$. To the right, the spread of data within groups is small relative to the differences in means between groups, so an ANOVA is more likely to reject the null hypothesis." width="45%" /><img src="_main_files/figure-html/anova-2.png" alt="\label{fig:anova}Illustration of the setting for a one-way analysis of variance. To the left, the spread of the data within each group is large compared to the differences in means between each group, so an ANOVA is less likely to reject the null hypothesis that $\mu_0 = \mu_1 = \mu_2$. To the right, the spread of data within groups is small relative to the differences in means between groups, so an ANOVA is more likely to reject the null hypothesis." width="45%" />
<p class="caption">
Figure 2.26: Illustration of the setting for a one-way analysis of variance. To the left, the spread of the data within each group is large compared to the differences in means between each group, so an ANOVA is less likely to reject the null hypothesis that <span class="math inline">\(\mu_0 = \mu_1 = \mu_2\)</span>. To the right, the spread of data within groups is small relative to the differences in means between groups, so an ANOVA is more likely to reject the null hypothesis.
</p>
</div>
<p>Let the index <span class="math inline">\(i\)</span> denote a particular group (chocolate ice-cream eaters, etc.)
and let the index <span class="math inline">\(j\)</span> denote different data within the same group. Let <span class="math inline">\(n_i\)</span>
denote the sample size within each group, and let <span class="math inline">\(k\)</span> denote the total number
of groups. The total sum of squares is defined as the sum of squared
differences between <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\bar{Y}\)</span>, which in this setting can be
expressed via a double summation, one over data within a group, and
another over groups:
<span class="math display">\[
\mbox{Total SS} = \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y})^2 = \sum_{i=1}^k \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y}_{i\bullet})^2 + \sum_{i=1}^k n_i(\bar{Y}_{i\bullet}-\bar{Y})^2 = SSE + SST \,,
\]</span>
where <span class="math inline">\(\bar{Y}_{i\bullet}\)</span> is the average value for group <span class="math inline">\(i\)</span>, <span class="math inline">\(SSE\)</span> has its
usual meaning as the sum of squared errors (although here
<span class="math inline">\(\hat{\beta}_0+\hat{\beta}_1x_i\)</span> is replaced with the estimated
group mean <span class="math inline">\(\bar{Y}_{i\bullet}\)</span>), and <span class="math inline">\(SST\)</span> is the sum of squared average
treatment effects.
If the <span class="math inline">\(SSE\)</span> value is large relative to the <span class="math inline">\(SST\)</span> value, then we are in
a situation where the data are spread widely within
each group relative to the spread in values of the group means.
In such a situation, it would be difficult to
detect a statistically significant difference between the group means.
(See the left panel of Figure <a href="the-normal-and-related-distributions.html#fig:anova">2.26</a>.)
On the other hand, if the <span class="math inline">\(SST\)</span> value is large relative to the <span class="math inline">\(SSE\)</span> value,
then the group means have a wide spread compared to the data in each group.
In such a situation, it would be much easier to
detect a statistically significant difference between the group means.
(See the right panel of Figure <a href="the-normal-and-related-distributions.html#fig:anova">2.26</a>.)
This reasoning motivates using the ratio
<span class="math display">\[
\frac{SST}{SSE}
\]</span>
as the hypothesis test statistic.
However, we do not know its distribution…or at least, not yet.
We do know is that under the null,
<span class="math display">\[\begin{align*}
W_T = \frac{SST}{\sigma^2} &amp;\sim \chi_{k-1}^2 \\
W_E = \frac{SSE}{\sigma^2} &amp;\sim \chi_{n-k}^2 \,.
\end{align*}\]</span>
And we know that
<span class="math display">\[
F = \frac{W_1/\nu_1}{W_2/\nu_2} \sim F_{\nu_1,\nu_2} \,,
\]</span>
i.e., the ratio of chi-square distributed random variables,
each divided by their respective number of degrees of freedom,
is <span class="math inline">\(F\)</span>-distributed for <span class="math inline">\(\nu_1\)</span> numerator and
<span class="math inline">\(\nu_2\)</span> denominator degrees of freedom. Thus
<span class="math display">\[
F = \frac{SST/(k-1)}{SSE/(n-k)} = \frac{MST}{MSE} \sim F_{k-1,n-k} \,.
\]</span>
In the ANOVA hypothesis test, the null hypothesis <span class="math inline">\(H_o\)</span> is that
<span class="math inline">\(E[Y_1] = E[Y_2] = \cdots = E[Y_k]\)</span>, and the alternative hypothesis
is that at least one mean is different from the others. If we test at
level <span class="math inline">\(\alpha\)</span>, we reject the null hypothesis if <span class="math inline">\(F &gt; F_{1-\alpha,k-1,n-k}\)</span>.
In <code>R</code>, the boundary of the rejection region would be given by
<code>qf(1-alpha,k-1,n-k)</code> while the <span class="math inline">\(p\)</span>-value would be given by <code>1-pf(F.obs,k-1,n-k)</code>.</p>
<hr />
<div id="one-way-anova-the-statistical-model" class="section level3 hasAnchor" number="2.18.1">
<h3><span class="header-section-number">2.18.1</span> One-Way ANOVA: the Statistical Model<a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the notes above, we build up the test statistic for the one-way
ANOVA, but we do not discuss the underlying statistical model. We can
write down the model as
<span class="math display">\[
Y_{ij} = \mu + \tau_i + \epsilon_{ij} \,,
\]</span>
where <span class="math inline">\(i\)</span> denotes the treatment group and <span class="math inline">\(j\)</span> denotes an observed datum
within group <span class="math inline">\(i\)</span>. <span class="math inline">\(\mu\)</span> is the overall mean response, while <span class="math inline">\(\tau_i\)</span> is
the <em>deterministic</em> effect of treatment in group <span class="math inline">\(i\)</span>. (<span class="math inline">\(\tau\)</span> is the Greek
letter tau, which is pronounced “tao” [rhyming with “ow,” an expression of
pain]. Recall that by “deterministic” we mean that <span class="math inline">\(\tau_i\)</span> is <em>not random</em>.)
The error terms <span class="math inline">\(\epsilon_{ij}\)</span> are independent, normally distributed,
and homoscedastic with variance <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<blockquote>
<p>Recall that <span class="math inline">\(\bar{Y}_{i\bullet}\)</span> is the sample mean within group <span class="math inline">\(i\)</span>.
We know it is distributed normally (by assumption) but what are its
expected value and variance?</p>
</blockquote>
<blockquote>
<p>The expected value is
<span class="math display">\[\begin{align*}
E[\bar{Y}_{i\bullet}] &amp;= E\left[\frac{1}{n_i}\sum_{j=1}^{n_i} Y_{ij}\right] = \frac{1}{n_i}\sum_{j=1}^{n_i} E[Y_{ij}] \\
&amp;= \frac{1}{n_i}\sum_{j=1}^{n_i} E[\mu + \tau_i + \epsilon_{ij}] = \mu + \tau_i + E[\epsilon_{ij}] = \mu+\tau_i \,,
\end{align*}\]</span>
while the variance is
<span class="math display">\[
V[\bar{Y}_{i\bullet}] = V\left[\frac{1}{n_i}\sum_{j=1}^{n_i} Y_{ij}\right] = \frac{1}{n_i}\sum_{j=1}^{n_i^2} V[Y_{ij}] = \frac{1}{n_i^2}\sum_{j=1}^{n_i} V[\mu + \tau_i + \epsilon_{ij}] = \frac{n_i V[\epsilon_{ij}]}{n_i^2} = \frac{\sigma^2}{n_i} \,.
\]</span>
Thus <span class="math inline">\(\bar{Y}_{i\bullet} \sim \mathcal{N}(\mu+\tau_i,\sigma^2/n_i)\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="linear-regression-in-r-redux" class="section level3 hasAnchor" number="2.18.2">
<h3><span class="header-section-number">2.18.2</span> Linear Regression in R: Redux<a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The last example of the last section showed the output from <code>lm()</code>, <code>R</code>’s
linear model function:</p>
</blockquote>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="the-normal-and-related-distributions.html#cb162-1" tabindex="-1"></a><span class="fu">summary</span>(lm.out)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.00437 -0.53068  0.04523  0.40338  2.47660 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.5231     0.3216  14.063  &lt; 2e-16 ***
## x             0.4605     0.0586   7.859 1.75e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9792 on 38 degrees of freedom
## Multiple R-squared:  0.6191, Adjusted R-squared:  0.609 
## F-statistic: 61.76 on 1 and 38 DF,  p-value: 1.749e-09</code></pre>
<blockquote>
<p>In that example, we indicated that the numbers on the line beginning
<code>F-statistic</code> would make more sense after we covered one-way analysis
of variance.</p>
</blockquote>
<blockquote>
<p>There are two coefficients in this model, which is analogous to two
“treatment groups.” Hence <span class="math inline">\(k = 2\)</span>,
<span class="math inline">\(SST/\sigma^2 \sim \chi_{k-1=1}^2\)</span>, and
<span class="math inline">\(SSE/\sigma^2 \sim \chi_{n-k=38}^2\)</span>. This explains <code>on 1 and 38 DF</code>.
The <span class="math inline">\(F\)</span> statistic is <span class="math inline">\(MST/MSE = 61.76\)</span>, which under the null is
sampled from an <span class="math inline">\(F\)</span> distribution with 1 numerator and 38 denominator
degrees of freedom. This is an extreme value under the null, as indicated
by the <span class="math inline">\(p\)</span>-value 1.749 <span class="math inline">\(\times 10^{-9}\)</span>…so we conclude that
the true value of the slope <span class="math inline">\(\beta_1\)</span> is not zero.</p>
</blockquote>
<blockquote>
<p>But there’s more. To access more information about the <span class="math inline">\(F\)</span> test that
is carried out, we can call the <code>anova()</code> function.</p>
</blockquote>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="the-normal-and-related-distributions.html#cb164-1" tabindex="-1"></a><span class="fu">anova</span>(lm.out)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## x          1 59.208  59.208  61.756 1.749e-09 ***
## Residuals 38 36.432   0.959                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<blockquote>
<p>This function call outputs information we could not see above: the
<span class="math inline">\(SST\)</span> (the first row of <code>Sum Sq</code>), <span class="math inline">\(SSE\)</span> (second row),
<span class="math inline">\(MST\)</span> (the first row of <code>Mean Sq</code>), and <span class="math inline">\(MSE\)</span> (second row). The
<code>F value</code> is the ratio <span class="math inline">\(MST/MSE\)</span>; here, that is 59.208/0.959 = 61.756.</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-basics-of-probability-and-statistical-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-binomial-and-related-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

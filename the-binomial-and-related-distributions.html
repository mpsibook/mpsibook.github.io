<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-normal-and-related-distributions.html"/>
<link rel="next" href="the-poisson-and-related-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-sampling"><i class="fa fa-check"></i><b>1.11</b> Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-basic-algorithm"><i class="fa fa-check"></i><b>1.12.2</b> Simulation: Basic Algorithm</a></li>
<li class="chapter" data-level="1.12.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-the-sampling-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.3</b> Simulation: the Sampling Distribution of the Sample Mean</a></li>
<li class="chapter" data-level="1.12.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-the-sampling-distribution-of-the-sample-median"><i class="fa fa-check"></i><b>1.12.4</b> Simulation: the Sampling Distribution of the Sample Median</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
<li class="chapter" data-level="1.14.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-sampling-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.14.3</b> Simulation: Sampling Distribution of Maximum Likelihood Estimates</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-confidence-interval-coverage"><i class="fa fa-check"></i><b>1.16.3</b> Simulation: Confidence Interval Coverage</a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-about-p-values"><i class="fa fa-check"></i><b>1.17.3</b> More About p-Values</a></li>
<li class="chapter" data-level="1.17.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#simulation-estimating-a-p-value"><i class="fa fa-check"></i><b>1.17.4</b> Simulation: Estimating a p-Value</a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#exercises"><i class="fa fa-check"></i><b>1.18</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-distribution-probability-density-function"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution: Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-and-variance"><i class="fa fa-check"></i><b>2.1.1</b> Expected Value and Variance</a></li>
<li class="chapter" data-level="2.1.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness"><i class="fa fa-check"></i><b>2.1.2</b> Skewness</a></li>
<li class="chapter" data-level="2.1.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.1.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-distribution-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.2</b> Normal Distribution: Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.2.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualization"><i class="fa fa-check"></i><b>2.2.2</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#method-of-moment-generating-functions"><i class="fa fa-check"></i><b>2.3</b> Method of Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#mgf-probability-mass-function"><i class="fa fa-check"></i><b>2.3.1</b> MGF: Probability Mass Function</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#mgf-probability-density-function"><i class="fa fa-check"></i><b>2.3.2</b> MGF: Probability Density Function</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#mgf-normal-distribution"><i class="fa fa-check"></i><b>2.3.3</b> MGF: Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-distribution-sample-mean"><i class="fa fa-check"></i><b>2.4</b> Normal Distribution: Sample Mean</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.4.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-sum"><i class="fa fa-check"></i><b>2.4.2</b> Sample Sum</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-distribution-standardization"><i class="fa fa-check"></i><b>2.5</b> Normal Distribution: Standardization</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.5.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.6</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#transformation-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> Transformation of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#continuous-transformation-injective-function"><i class="fa fa-check"></i><b>2.7.1</b> Continuous Transformation: Injective Function</a></li>
<li class="chapter" data-level="2.7.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#continuous-transformation-non-injective-function"><i class="fa fa-check"></i><b>2.7.2</b> Continuous Transformation: Non-Injective Function</a></li>
<li class="chapter" data-level="2.7.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#discrete-transformation"><i class="fa fa-check"></i><b>2.7.3</b> Discrete Transformation</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-related-distribution-chi-square"><i class="fa fa-check"></i><b>2.8</b> Normal-Related Distribution: Chi-Square</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#chi-square-distribution-moment-generating-function"><i class="fa fa-check"></i><b>2.8.1</b> Chi-Square Distribution: Moment-Generating Function</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#chi-square-distribution-expected-value"><i class="fa fa-check"></i><b>2.8.2</b> Chi-Square Distribution: Expected Value</a></li>
<li class="chapter" data-level="2.8.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.8.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-distribution-sample-variance"><i class="fa fa-check"></i><b>2.9</b> Normal Distribution: Sample Variance</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-related-distribution-t"><i class="fa fa-check"></i><b>2.10</b> Normal-Related Distribution: t</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.12</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-variance-unknown"><i class="fa fa-check"></i><b>2.12.1</b> Normal Mean: Variance Unknown</a></li>
<li class="chapter" data-level="2.12.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-variance"><i class="fa fa-check"></i><b>2.12.2</b> Normal Variance</a></li>
<li class="chapter" data-level="2.12.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-leveraging-the-clt"><i class="fa fa-check"></i><b>2.12.3</b> Normal Mean: Leveraging the CLT</a></li>
<li class="chapter" data-level="2.12.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-pivotal-method"><i class="fa fa-check"></i><b>2.12.4</b> The Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>2.13</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Normal Mean: Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-estimating-sample-size-with-variance-known"><i class="fa fa-check"></i><b>2.13.2</b> Normal Mean: Estimating Sample Size with Variance Known</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-variance-unknown-1"><i class="fa fa-check"></i><b>2.13.3</b> Normal Mean: Variance Unknown</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-variance-1"><i class="fa fa-check"></i><b>2.13.4</b> Normal Variance</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-central-limit-theorem"><i class="fa fa-check"></i><b>2.13.5</b> Normal Mean: Central Limit Theorem</a></li>
<li class="chapter" data-level="2.13.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-mean-two-sample-testing"><i class="fa fa-check"></i><b>2.13.6</b> Normal Mean: Two-Sample Testing</a></li>
<li class="chapter" data-level="2.13.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#normal-variance-two-sample-testing"><i class="fa fa-check"></i><b>2.13.7</b> Normal Variance: Two-Sample Testing</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-testing"><i class="fa fa-check"></i><b>2.14</b> Distribution Testing</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.15</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.15.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.15.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.15.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.16</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.16.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.16.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-exponential-family-of-distributions"><i class="fa fa-check"></i><b>2.17</b> The Exponential Family of Distributions</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exponential-family-with-a-vector-of-parameters"><i class="fa fa-check"></i><b>2.17.1</b> Exponential Family with a Vector of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#exercises-1"><i class="fa fa-check"></i><b>2.18</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.2.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-binomial-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>3.2.5</b> The Binomial Distribution as Part of the Exponential Family</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sufficiency-principle-examples-of-when-we-cannot-reduce-data"><i class="fa fa-check"></i><b>3.6.3</b> The Sufficiency Principle: Examples of When We Cannot Reduce Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.5</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation"><i class="fa fa-check"></i><b>3.8.5</b> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation</a></li>
<li class="chapter" data-level="3.8.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#utilizing-simulations-when-data-reduction-is-not-possible"><i class="fa fa-check"></i><b>3.8.6</b> Utilizing Simulations When Data Reduction is Not Possible</a></li>
<li class="chapter" data-level="3.8.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.7</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a>
<ul>
<li class="chapter" data-level="3.12.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#performing-a-multinomial-test"><i class="fa fa-check"></i><b>3.12.1</b> Performing a Multinomial Test</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-average-p-value-difference-between-the-multinomial-and-chi-square-gof-tests"><i class="fa fa-check"></i><b>3.13.2</b> The Average p-Value Difference Between the Multinomial and Chi-Square GoF Tests</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.14" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#exercises-2"><i class="fa fa-check"></i><b>3.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-poisson-distribution-as-part-of-the-exponential-family"><i class="fa fa-check"></i><b>4.2.1</b> The Poisson Distribution as Part of the Exponential Family</a></li>
<li class="chapter" data-level="4.2.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.2</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-10"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#maximum-likelihood-estimation-via-numerical-optimization"><i class="fa fa-check"></i><b>4.5.5</b> Maximum Likelihood Estimation via Numerical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
<li class="chapter" data-level="4.6.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-estimation-via-simulation"><i class="fa fa-check"></i><b>4.6.5</b> Confidence Interval Estimation via Simulation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#exercises-3"><i class="fa fa-check"></i><b>4.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.3.1</b> Sufficient Statistics for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-pareto-domain-parameter"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Pareto Domain Parameter</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-using-an-order-statistic"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Using an Order Statistic</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-4"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-test-for-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> Hypothesis Test for the Uniform Distribution Upper Bound</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#using-numerical-integration-to-characterize-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.3</b> Using Numerical Integration to Characterize a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.4</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
<li class="chapter" data-level="6.4.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#mutual-information"><i class="fa fa-check"></i><b>6.4.3</b> Mutual Information</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#exercises-5"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#multiple-comparisons"><i class="fa fa-check"></i><b>7.6</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>7.6.1</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><a href="appendix-b-confidence-interval-and-hypothesis-test-reference-tables.html"><i class="fa fa-check"></i>Appendix B: Confidence Interval and Hypothesis Test Reference Tables</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html"><i class="fa fa-check"></i>Chapter Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-1"><i class="fa fa-check"></i>Chapter 1</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-2"><i class="fa fa-check"></i>Chapter 2</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-3"><i class="fa fa-check"></i>Chapter 3</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-4"><i class="fa fa-check"></i>Chapter 4</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-5"><i class="fa fa-check"></i>Chapter 5</a></li>
<li class="chapter" data-level="" data-path="chapter-exercises-solutions.html"><a href="chapter-exercises-solutions.html#chapter-6"><i class="fa fa-check"></i>Chapter 6</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-binomial-and-related-distributions" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> The Binomial (and Related) Distributions<a href="the-binomial-and-related-distributions.html#the-binomial-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Motivation<a href="the-binomial-and-related-distributions.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are holding a coin. It may be a fair coin
(meaning that the probabilities of observing heads or tails in a
given flip of the coin are each 0.5)or perhaps it is not. We decide that we are
going to flip the coin some fixed number of times <span class="math inline">\(k\)</span>, and we will record
the outcome of each flip:
<span class="math display">\[
\mathbf{Y} = \{Y_1,Y_2,\ldots,Y_k\} \,,
\]</span>
where, e.g., <span class="math inline">\(Y_1 = 1\)</span> if we observe heads or <span class="math inline">\(0\)</span> if we observe tails.
This is an example of a <em>Bernoulli process</em>, where process denotes a
sequence of observations, and Bernoulli indicates that there are two
possible discrete outcomes for each observation.</p>
<p>A <em>binomial experiment</em> is one that generates Bernoulli process data through
the running of <span class="math inline">\(k\)</span> trials (e.g., <span class="math inline">\(k\)</span> separate coin flips).
The properties of such an experiment are that:</p>
<ol style="list-style-type: decimal">
<li>The number of trials <span class="math inline">\(k\)</span> is fixed in advance.</li>
<li>Each trial has two possible outcomes, generically denoted as
<span class="math inline">\(S\)</span> (success) or <span class="math inline">\(F\)</span> (failure).</li>
<li>The probability of success remains <span class="math inline">\(p\)</span> throughout the experiment.</li>
<li>The outcome of any one trial is independent of the outcomes of the others.</li>
</ol>
<p>The random variable of interest for a binomial experiment is the number
of observed successes. A closely related alternative to a binomial experiment
is a <em>negative binomial experiment</em>, where the number of successes <span class="math inline">\(s\)</span>
is fixed in advance, instead of the number of trials <span class="math inline">\(k\)</span>,
and the random variable
of interest is the number of failures that we observe
before achieving <span class="math inline">\(s\)</span> successes.
A simple example would
be flipping a coin until <span class="math inline">\(s\)</span> heads are observed and recording the overall
number of tails that we observe.</p>
<p>As a side note to the third point above, about the probability of success
remaining <span class="math inline">\(p\)</span> throughout the experiment: binomial and negative binomial
experiments rely on <em>sampling with replacement</em>if we observe a head
for a given coin flip, we can observe heads again in the future.
In the real world, however, the reader will observe instances where, e.g.,
a binomial distribution is used to model experiments featuring <em>sampling
without replacement</em>: we have <span class="math inline">\(K = 100\)</span> widgets, of which ten are defective;
we check one to see if it is defective (with probability <span class="math inline">\(p = 0.1\)</span>) and
set it aside, then check another (with probability either 10/99 or 9/99,
depending on the outcome of the first trial), etc. The convention for
using the binomial distribution to model data in such a situation is that
it is fine to do so if the number of trials <span class="math inline">\(k \lesssim K/10\)</span>.
However, in the age of computers, there is no reason to apply the binomial
distribution when we can apply the hypergeometric distribution instead.</p>
<p>And, as a side note to the fourth point above, about the outcome of each trial
being independent of the outcomes of the others: in a general process,
each datum can be dependent on the data observed previously. How each datum
is dependent on previous data defines the type of process that is observed:
a Markov process, a Gaussian process, etc.
A Bernoulli process is termed a <em>memoryless</em> process because
it is comprised of independent (and identically distributed) data.</p>
</div>
<div id="probability-mass-function" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Probability Mass Function<a href="the-binomial-and-related-distributions.html#probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets focus first on the outcome of a binomial experiment,
with the random variable <span class="math inline">\(X\)</span> being the number of observed successes
in <span class="math inline">\(k\)</span> trials. What is the probability
of observing <span class="math inline">\(X=x\)</span> successes, if the probability of observing a success
in any one trial is <span class="math inline">\(p\)</span>?
<span class="math display">\[\begin{align*}
\mbox{$x$ successes}&amp;: p^x \\
\mbox{$k-x$ failures}&amp;: (1-p)^{k-x} \,.
\end{align*}\]</span>
So <span class="math inline">\(P(X=x) = p^x (1-p)^{k-x}\)</span>but, no, this isnt right. Lets
start again and think this
through. Assume <span class="math inline">\(k = 2\)</span>. The sample space of possible experimental outcomes is
<span class="math display">\[
\Omega = \{ SS, SF, FS, FF \} \,.
\]</span>
If <span class="math inline">\(p\)</span> = 0.5, then we can see that the probability of observing one
success in two trials is 0.5but our proposed probability mass
function tells us that
<span class="math inline">\(P(X=1) = (0.5)^1 (1-0.5)^1 = 0.25\)</span>. What are we missing? We are missing
that there are two ways of observing a single successand we need to count
both. Because we ultimately do not
care about the order in which successes and failures are observed,
we utilize counting via combination:
<span class="math display">\[
\binom{k}{x} = \frac{k!}{x! (k-x)!} \,,
\]</span>
where the exclamation point represents the factorial function
<span class="math inline">\(x! = x(x-1)(x-2)\cdots 1\)</span>. So now we can correctly write down the
binomial probability mass function:
<span class="math display">\[
P(X=x) = p_X(x) = \binom{k}{x} p^x (1-p)^{k-x} ~~~ x \in \{0,\ldots,k\} \,.
\]</span>
We denote the distribution
of the random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Note that
when <span class="math inline">\(k = 1\)</span>, we have a <em>Bernoulli distribution</em>.</p>
<p><strong>Recall</strong>: <em>a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) <span class="math inline">\(0 \leq p_X(x) \leq 1\)</span> and (b) <span class="math inline">\(\sum_x p_X(x) = 1\)</span>, where the sum is over all values of <span class="math inline">\(x\)</span> in the distributions domain.</em></p>
<p>(The reader should note
that the number of trials is commonly denoted as <span class="math inline">\(n\)</span>, not as
<span class="math inline">\(k\)</span>. However, since <span class="math inline">\(n\)</span> is conventionally used to denote the sample size in
an experiment, to avoid confusion we use <span class="math inline">\(k\)</span> to denote the number
of trials in this text.)</p>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:bpmf">3.1</a>, we display three binomial pmfs,
one each for probabilities of success 0.1 (red, to the left), 0.5 (green,
to the center), and 0.8 (blue, to the right). This figure indicates an
important aspect of the binomial pmf, namely that it <em>can</em> attain a shape
akin to that of a normal distribution, if <span class="math inline">\(p\)</span> is such that any truncation
observed at the values <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=k\)</span> is minimal. In fact, a binomial
random variable converges in distribution to a normal random variable
in certain limiting situations, as we show in an example below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpmf"></span>
<img src="_main_files/figure-html/bpmf-1.png" alt="\label{fig:bpmf}Binomial probability mass functions for number of trials $k = 10$ and success probabilities $p = 0.1$ (red squares), 0.5 (green triangles), and 0.8 (blue circles)." width="50%" />
<p class="caption">
Figure 3.1: Binomial probability mass functions for number of trials <span class="math inline">\(k = 10\)</span> and success probabilities <span class="math inline">\(p = 0.1\)</span> (red squares), 0.5 (green triangles), and 0.8 (blue circles).
</p>
</div>
<p><strong>Recall:</strong> <em>the expected value of a discretely distributed random variable is</em>
<span class="math display">\[
E[X] = \sum_x x p_X(x) \,,
\]</span>
<em>where the sum is over all values of <span class="math inline">\(x\)</span> within the domain of the pmf p_X(x). The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(p_X(x)\)</span>.</em></p>
<p>For the binomial distribution, the expected value is
<span class="math display">\[
E[X] = \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \,.
\]</span>
At first, this does not appear to be easy to evaluate.
One trick in our arsenal
is to pull constants out of the summation such that whatever is left
as the summand is a pmf (and thus sums to 1). Lets try this here:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k x \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k \frac{k!}{(x-1)!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \,.
\end{align*}\]</span>
The summation appears almost like that of a binomial random variable.
Lets set <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!(k-(y+1))!} p^y (1-p)^{k-(y+1)} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!((k-1)-y)!} p^y (1-p)^{(k-1)-y} \,.
\end{align*}\]</span>
The summand is now the pmf for the random variable
<span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-1\)</span>,<span class="math inline">\(p\)</span>), summed over all values
of <span class="math inline">\(y\)</span> in the domain of the distribution. Thus the
summation evaluates to 1: <span class="math inline">\(E[X] = kp\)</span>. In an example below,
we use a similar strategy to determine the variance <span class="math inline">\(V[X] = kp(1-p)\)</span>.</p>
<p>A negative binomial experiment is governed by the <em>negative binomial
distribution</em>, whose pmf is
<span class="math display">\[
p_X(x) = \binom{x+s-1}{x} p^s (1-p)^x ~~~ x \in \{0,1,\ldots,\infty\}
\]</span>
The form of this pmf follows from the fact that the underlying Bernoulli
process would consist of <span class="math inline">\(x+s\)</span> data, <em>with the last datum being the observed
success that ends the experiment</em>. The first <span class="math inline">\(x+s-1\)</span> data
would feature <span class="math inline">\(s-1\)</span> successes and <span class="math inline">\(x\)</span> failures, with the order of
success and failure not matteringso we can view these data
as being binomially distributed (albeit with <span class="math inline">\(x\)</span> representing
failureshence the negative in negative binomial!):
<span class="math display">\[
p_X(x) = \underbrace{\binom{x+s-1}{x} p^{s-1} (1-p)^x}_{\mbox{first $x+s-1$ trials}} \cdot \underbrace{p}_{\mbox{last trial}} \,.
\]</span>
Note that when <span class="math inline">\(s = 1\)</span>, the resulting distribution is called
the <em>geometric distribution</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbpmf"></span>
<img src="_main_files/figure-html/nbpmf-1.png" alt="\label{fig:nbpmf}Negative binomial probability mass functions for the number of successes $s = 2$ and success probabilities $p = 0.7$ (red squares), 0.5 (green triangles), and 0.2 (blue circles)." width="50%" />
<p class="caption">
Figure 3.2: Negative binomial probability mass functions for the number of successes <span class="math inline">\(s = 2\)</span> and success probabilities <span class="math inline">\(p = 0.7\)</span> (red squares), 0.5 (green triangles), and 0.2 (blue circles).
</p>
</div>
<p>In an example below, we show how one would derive the expected value
of a negative binomial random variable, <span class="math inline">\(E[X] = s(1-p)/p\)</span>. (We can enact
a similar calculation to show that the variance is <span class="math inline">\(V[X] = s(1-p)/p^2\)</span>.)</p>
<hr />
<div id="variance-of-a-binomial-random-variable" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Variance of a Binomial Random Variable<a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the variance of a discretely distributed random variable is</em>
<span class="math display">\[
V[X] = \sum_x (x-\mu)^2 p_X(x) = E[X^2] - (E[X])^2\,,
\]</span>
<em>where the sum is over all values of <span class="math inline">\(x\)</span> in the domain of the pmf <span class="math inline">\(p_X(x)\)</span>. The variance represents the square of the width of a probability mass function, where by width we mean the range of values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(p_X(x)\)</span> is effectively non-zero.</em></p>
<blockquote>
<p>The variance of a random variable is given by the shortcut formula that
we have been using since Chapter 1: <span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>.
So we would expect that we would need to compute <span class="math inline">\(E[X^2]\)</span> here, since
we already know that <span class="math inline">\(E[X] = kp\)</span>. But for reasons that will become
apparent below, it is actually far easier for us to compute <span class="math inline">\(E[X(X-1)]\)</span>,
and to work with that to eventually derive the variance:
<span class="math display">\[\begin{align*}
E[X(X-1)] &amp;= \sum_{x=0}^k x(x-1) \binom{k}{x} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=0}^k x(x-1) \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=2}^k \frac{k!}{(x-2)!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= k(k-1) p^2 \sum_{x=2}^k \frac{(k-2)!}{(x-2)!(k-x)!} p^{x-2} (1-p)^{k-x} \,.
\end{align*}\]</span>
The advantage to using <span class="math inline">\(x(x-1)\)</span> was that it matches the first two terms
of <span class="math inline">\(x! = x(x-1)\cdots(1)\)</span>, allowing easy cancellation.
If we set <span class="math inline">\(y = x-2\)</span>, we find that the summand above will, in a similar
manner as in the calculation of <span class="math inline">\(E[X]\)</span>, become the pmf for the
random variable <span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-2,p\)</span>)and thus the summation will
evaluate to 1.</p>
</blockquote>
<blockquote>
<p>So <span class="math inline">\(E[X(X-1)] = E[X^2] - E[X] = k(k-1)p^2\)</span>, and
<span class="math inline">\(E[X^2] = k^2p^2-kp^2 + kp = V[X] + (E[X])^2\)</span>, and
<span class="math inline">\(V[X] = k^2p^2-kp^2+kp-k^2p^2 = kp-kp^2 = kp(1-p)\)</span>. Done.</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-of-a-negative-binomial-random-variable" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> The Expected Value of a Negative Binomial Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The calculation for the expected value <span class="math inline">\(E[X]\)</span> for a negative binomial
random variable is similar to that for a binomial random variable:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^{\infty} x \binom{x+s-1}{x} p^s (1-p)^x \\
&amp;= \sum_{x=0}^{\infty} x \frac{(x+s-1)!}{(s-1)!x!} p^s (1-p)^x \\
&amp;= \sum_{x=1}^{\infty} \frac{(x+s-1)!}{(s-1)!(x-1)!} p^s (1-p)^x \,.
\end{align*}\]</span>
Let <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{y=0}^{\infty} \frac{(y+s)!}{(s-1)!y!} p^s (1-p)^{y+1} \\
&amp;= \sum_{y=0}^{\infty} s(1-p) \frac{(y+s)!}{s!y!} p^s (1-p)^y \\
&amp;= \sum_{y=0}^{\infty} \frac{s(1-p)}{p} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \sum_{y=0}^{\infty} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \,.
\end{align*}\]</span>
The summand is that of a negative binomial distribution for <span class="math inline">\(s+1\)</span> successes,
hence the summation is 1, and thus <span class="math inline">\(E[X] = s(1-p)/p\)</span>.</p>
</blockquote>
<blockquote>
<p>We can use a similar calculation in which we evaluate <span class="math inline">\(E[X(X-1)]\)</span> in order
to derive the variance of a negative binomial random variable: <span class="math inline">\(V[X] = s(1-p)/p^2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="binomial-distribution-normal-approximation" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Binomial Distribution: Normal Approximation<a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In certain limiting situations, a binomial random variable converges in
distribution to a normal random variable. In other words, if
<span class="math display">\[
P\left(\frac{X-\mu}{\sigma} &lt; a \right) = P\left(\frac{X-kp}{\sqrt{kp(1-p)}} &lt; a \right) \approx P(Z &lt; a) = \Phi(a) \,,
\]</span>
then we can state that
<span class="math inline">\(X \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(kp,kp(1-p))\)</span>, or that
<span class="math inline">\(X\)</span> converges in distribution to a normal random variable <span class="math inline">\(Y\)</span>.
Now, what do we mean by certain
limiting situations? For instance, if <span class="math inline">\(p\)</span> is close to zero or one,
then the binomial distribution is truncated at 0 or at <span class="math inline">\(k\)</span>,
and the shape of the pmf does <em>not</em> appear to be like that of a normal pdf.
One convention is that the normal approximation is adequate if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,.
\]</span>
The reader might question why we would mention this approximation at all:
if we have binomially distributed data and a computer, then we need not
ever utilize such an approximation to, e.g., compute probabilities. This
point is correct (and is the reason why, for instance, we do not mention
the so-called <em>continuity correction</em> here; our goal is not to compute
probabilities). The reason we mention this is that this
approximation underlies a commonly used hypothesis test framework,
the <em>Wald interval</em>, that we will mention later in the chapter.</p>
</blockquote>
<hr />
</div>
<div id="computing-probabilities-8" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Computing Probabilities<a href="the-binomial-and-related-distributions.html#computing-probabilities-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be a random variable sampled from a binomial distribution with
number of trials <span class="math inline">\(k = 6\)</span> and success probability <span class="math inline">\(p = 0.4\)</span>, and let
<span class="math inline">\(Y\)</span> be a random variable sampled from a negative binomial distribution
with number of successes <span class="math inline">\(s = 3\)</span> and success probability <span class="math inline">\(p = 0.6\)</span>.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(2 \leq X &lt; 4)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>To find this probability, we sum over the binomial probability mass
function for values <span class="math inline">\(x = 2\)</span> and <span class="math inline">\(x = 3\)</span>. (The form of the
inequalities matter for discrete distributions!) We can perform this
computation analytically:
<span class="math display">\[\begin{align*}
P(2 \leq X &lt; 4) &amp;= \binom{6}{2} (0.4)^2 (1-0.4)^4 + \binom{6}{3} (0.4)^3 (1-0.4)^3 \\
&amp;= \frac{6!}{2!4!} \cdot 0.16 \cdot 0.1296 + \frac{6!}{3!3!} \cdot 0.064 \cdot 0.216 = 15 \cdot 0.0207 + 20 \cdot 0.0138 = 0.5875 \,.
\end{align*}\]</span>
There is a 58.75% chance that the next time we sample data according to
this distribution, we will observe a value of 2 or 3.</p>
</blockquote>
<blockquote>
<p>It is ultimately simpler to use <code>R</code> to perform this calculation. While we
can compute the probability as the difference of two cumulative distribution
function values, for discrete distributions it is often more straightforward
to sum over the relevant probability masses directly, since then we need
not worry about whether the input to the cdf is correct given the form of
the inequality:</p>
</blockquote>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="the-binomial-and-related-distributions.html#cb157-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">size=</span><span class="dv">6</span>,<span class="at">prob=</span><span class="fl">0.4</span>))</span></code></pre></div>
<pre><code>## [1] 0.58752</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(Y &gt; 1)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>If we were to determine this probability by hand, the first thing we would
do is specify that we will compute <span class="math inline">\(1 - P(Y \leq 3)\)</span>, as this has a finite
(and small!) number of terms:
<span class="math display">\[\begin{align*}
P(Y &gt; 1) &amp;= 1 - \binom{0+3-1}{0} (0.6)^3 (1-0.6)^0 - \binom{1+3-1}{1} (0.6)^3 (1-0.6)^1 \\
&amp;= 1 - 1 \cdot 0.216 \cdot 1 - 3 \cdot 0.216 \cdot 0.4 = 0.5248 \,.
\end{align*}\]</span>
There is a 52.48% chance that we would observe more than one failure the
next time we sample data according to this distribution.</p>
</blockquote>
<blockquote>
<p>Like before, it is ultimately simpler to use <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="the-binomial-and-related-distributions.html#cb159-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dnbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>,<span class="at">size=</span><span class="dv">3</span>,<span class="at">prob=</span><span class="fl">0.6</span>))</span></code></pre></div>
<pre><code>## [1] 0.5248</code></pre>
<hr />
</div>
<div id="the-binomial-distribution-as-part-of-the-exponential-family" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> The Binomial Distribution as Part of the Exponential Family<a href="the-binomial-and-related-distributions.html#the-binomial-distribution-as-part-of-the-exponential-family" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that the exponential family of distributions, introduced in the
last chapter, comprises distributions whose probability mass or density
functions can be written in the form
<span class="math display">\[\begin{align*}
h(x) \exp\left( \eta(\theta)T(x) - A(\theta) \right) \,.
\end{align*}\]</span>
Is the binomial distribution a member of the larger exponential family
of distributions? It would initially appear that the answer is no (as
there are no exponential functions in its pmf), but if we note that
<span class="math display">\[\begin{align*}
p^x = \exp\left(\log p^x\right) = \exp\left(x \log p\right)
\end{align*}\]</span>
and that
<span class="math display">\[\begin{align*}
(1-p)^{k-x} = \exp\left(\log (1-p)^{k-x}\right) = \exp\left((k-x) \log (1-p)\right) \,,
\end{align*}\]</span>
we can see that
<span class="math display">\[\begin{align*}
p_X(x \vert p) = \binom{k}{x} \exp\left( x [ \log(p) - \log(1-p) ] + k \log (1-p) \right)
\end{align*}\]</span>
and thus that
<span class="math display">\[\begin{align*}
h(x) &amp;= \binom{k}{x} \\
\eta(p) &amp;= \log(p) - \log(1-p) = \log \frac{p}{1-p} \\
T(x) &amp;= x \\
A(p) &amp;= -k \log(1-p) \,.
\end{align*}\]</span>
The binomial distribution is indeed a member of the exponential family.
We will leave it as an exercise to the reader to show that the
negative binomial distribution lies within the exponential family as well.</p>
</blockquote>
</div>
</div>
<div id="cumulative-distribution-function" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Cumulative Distribution Function<a href="the-binomial-and-related-distributions.html#cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as <span class="math inline">\(F_X(x) = \sum_{y\leq x} p_Y(y)\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>For the binomial distribution, the cdf is
<span class="math display">\[
F_X(x) = \sum_{y=0}^{\lfloor x \rfloor} p_Y(y) = \sum_{y=0}^{\lfloor x \rfloor} \binom{k}{y} p^y (1-p)^{k-y} \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the <em>floor function</em>, which
returns the largest integer that is less than or equal
to <span class="math inline">\(x\)</span> (e.g., if <span class="math inline">\(x\)</span> = 6.75, <span class="math inline">\(\lfloor x \rfloor\)</span> = 6).
(In closed form, we can represent this cdf with a regularized incomplete
beta function, which is not analytically easy to work with.)
Also, because a pmf is defined at discrete values of <span class="math inline">\(x\)</span>, its associated
cdf is a step function, as illustrated in the left panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>.
As we can see in this figure, the cdf steps up at each value of <span class="math inline">\(x\)</span>
in the domain of <span class="math inline">\(p_X(x)\)</span>, and unlike the case for continuous distributions,
the form of the inequalities in a probabilistic statement matter:
<span class="math inline">\(P(X &lt; x)\)</span> and <span class="math inline">\(P(X \leq x)\)</span> will not be the same, if <span class="math inline">\(x\)</span>
is an integer with value <span class="math inline">\(\{0,1,2,\ldots,k\}\)</span>.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(x = F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span>.
A discrete distribution has no unique inverse cdf; it is convention to
utilize the generalized inverse cdf,</em>
<span class="math inline">\(x = \mbox{inf}\{x : F_X(x) \geq q\}\)</span>,
<em>where inf indicates that the function is to return
the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q\)</span>.</em></p>
<p>In the right panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>, we display the inverse cdf
for the same distribution used to generate the figure in the left panel
(<span class="math inline">\(k=4\)</span> and <span class="math inline">\(p=0.5\)</span>). Like the cdf, the inverse cdf for a discrete distribution
is a step function. Below, in an example, we show how we adapt the inverse
transform sampler algorithm of Chapter 1 to accommodate the step-function
nature of an inverse cdf.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bincdf"></span>
<img src="_main_files/figure-html/bincdf-1.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" /><img src="_main_files/figure-html/bincdf-2.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" />
<p class="caption">
Figure 3.3: Illustration of the cumulative distribution function <span class="math inline">\(F_X(x)\)</span> (left) and inverse cumulative distribution function <span class="math inline">\(F_X^{-1}(q)\)</span> (right) for a binomial distribution with number of trials <span class="math inline">\(k = 4\)</span> and probability of success <span class="math inline">\(p=0.5\)</span>.
</p>
</div>
<hr />
<div id="computing-probabilities-9" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Computing Probabilities<a href="the-binomial-and-related-distributions.html#computing-probabilities-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Because computing the binomial pmf for a range of values of <span class="math inline">\(x\)</span> can
be laborious, we typically utilize <code>R</code> functions
when computing probabilities.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Binomial(10,0.6), which is <span class="math inline">\(P(4 \leq X &lt; 6)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We first note that due to the form of the inequality, we do <em>not</em> include <span class="math inline">\(X=6\)</span>
in the computation. Thus <span class="math inline">\(P(4 \leq X &lt; 6) = p_X(4) + p_X(5)\)</span>, which equals
<span class="math display">\[
\binom{10}{4} (0.6)^4 (1-0.6)^6 + \binom{10}{5} (0.6)^5 (1-0.6)^5 \,.
\]</span>
Even computing this is unnecessarily laborious; instead, we call on <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="the-binomial-and-related-distributions.html#cb161-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>))</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<p>(This utilizes <code>R</code>s vectorization feature: we need not explicitly define
a <code>for</code>-loop to evaluate <code>dbinom()</code> for <span class="math inline">\(x=4\)</span> and then at <span class="math inline">\(x=5\)</span>.)
We can also utilize cdf functions here: <span class="math inline">\(P(4 \leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4)
= P(X \leq 5) - P(X \leq 3) = F_X(5) - F_X(3)\)</span>, which in <code>R</code> is computed via</p>
</blockquote>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="the-binomial-and-related-distributions.html#cb163-1" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>) <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="dv">3</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<p>As we can see, the direct summation approach is the more straightforward one.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(X \sim\)</span> Binomial(10,0.6), what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(X \leq a) = 0.9\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we set up the inverse cdf formula:
<span class="math display">\[
P(X \leq a) = F_X(a) = 0.9 ~~ \Rightarrow ~~ a = F_X^{-1}(0.9)
\]</span>
Note that we didnt do anything differently here than we would have done
in a continuous distribution settingand we can proceed directly to
<code>R</code> because it utilizes the generalized inverse cdf algorithm.</p>
</blockquote>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="the-binomial-and-related-distributions.html#cb165-1" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fl">0.9</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<blockquote>
<p>We can see immediately how the cdf for a discrete distribution is not
a one-to-one function, as if we plug <span class="math inline">\(x = 8\)</span> into the cdf, we will not
recover the initial value <span class="math inline">\(q = 0.9\)</span>:</p>
</blockquote>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="the-binomial-and-related-distributions.html#cb167-1" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">8</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.9536426</code></pre>
<hr />
</div>
<div id="sampling-data-from-an-arbitrary-probability-mass-function" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Sampling Data From an Arbitrary Probability Mass Function<a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>While we would always utilize <code>R</code> shortcut functions like <code>rbinom()</code> when they exist,
there may be instances when we need to code our own functions for sampling data from
discrete distributions. The code below shows such a function for an arbitrary probability
mass function.</p>
</blockquote>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="the-binomial-and-related-distributions.html#cb169-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb169-2"><a href="the-binomial-and-related-distributions.html#cb169-2" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>)              <span class="co"># domain of x</span></span>
<span id="cb169-3"><a href="the-binomial-and-related-distributions.html#cb169-3" tabindex="-1"></a>p.x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>,<span class="fl">0.35</span>,<span class="fl">0.15</span>,<span class="fl">0.3</span>)    <span class="co"># p_X(x)</span></span>
<span id="cb169-4"><a href="the-binomial-and-related-distributions.html#cb169-4" tabindex="-1"></a>F.x <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(p.x)             <span class="co"># cumulative sum -&gt; produces F_X(x)</span></span>
<span id="cb169-5"><a href="the-binomial-and-related-distributions.html#cb169-5" tabindex="-1"></a>n   <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb169-6"><a href="the-binomial-and-related-distributions.html#cb169-6" tabindex="-1"></a>q   <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)                <span class="co"># we still ultimately need runif!</span></span>
<span id="cb169-7"><a href="the-binomial-and-related-distributions.html#cb169-7" tabindex="-1"></a>i   <span class="ot">&lt;-</span> <span class="fu">findInterval</span>(q,F.x)<span class="sc">+</span><span class="dv">1</span>   <span class="co"># the output are bin numbers [0,3], and not [1,4]</span></span>
<span id="cb169-8"><a href="the-binomial-and-related-distributions.html#cb169-8" tabindex="-1"></a>                               <span class="co"># hence we add 1</span></span>
<span id="cb169-9"><a href="the-binomial-and-related-distributions.html#cb169-9" tabindex="-1"></a>                               <span class="co"># 1 means q is between 0 and F.x[1], etc.</span></span>
<span id="cb169-10"><a href="the-binomial-and-related-distributions.html#cb169-10" tabindex="-1"></a>x.sample <span class="ot">&lt;-</span> x[i]</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pmfsamp"></span>
<img src="_main_files/figure-html/pmfsamp-1.png" alt="\label{fig:pmfsamp}Histogram of $n = 100$ iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of $x$." width="50%" />
<p class="caption">
Figure 3.4: Histogram of <span class="math inline">\(n = 100\)</span> iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of <span class="math inline">\(x\)</span>.
</p>
</div>
</div>
</div>
<div id="linear-functions-of-random-variables" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Linear Functions of Random Variables<a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Can we determine the distribution of
the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>? Yes, we canvia the method of moment-generating functions.</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(m_X(t) = E[e^{tX}]\)</span>. Also, if <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> as the mgf for a known family of distributions, then we can immediately identify the distribution for <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for the binomial distribution is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^k e^{tx} \binom{k}{x} p^x (1-p)^{k-x} \\
                   &amp;= \sum_{x=0}^k \binom{k}{x} (pe^t)^x (1-p)^{k-x} \,.
\end{align*}\]</span>
We utilize the binomial theorem
<span class="math display">\[
(x+y)^k = \sum_{i=0}^k \binom{k}{i} x^i y^{k-i}
\]</span>
to re-express <span class="math inline">\(m_X(t)\)</span>:
<span class="math display">\[
m_X(t) = [pe^t + (1-p)]^k \,.
\]</span>
Note that one may see this written as <span class="math inline">\((pe^t+q)^k\)</span>, where <span class="math inline">\(q = 1-p\)</span>.</p>
<p>The mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is thus
<span class="math display">\[
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = [m_X(t)]^n = [pe^t + (1-p)]^{nk} \,.
\]</span>
We can see that this has the form of a binomial mgf:
<span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>),
with expected value <span class="math inline">\(E[Y] = nkp\)</span> and variance <span class="math inline">\(V[Y] = nkp(1-p)\)</span>.
This makes sense, as the act of summing binomial data is equivalent to concatenating
<span class="math inline">\(n\)</span> separate Bernoulli processes into one longer Bernoulli processwhose
data can subsequently be modeled using a binomial distribution.</p>
<p>While we can identify the distribution of the sum by name, we cannot
say the same about the sample mean.
We know that the expected value is <span class="math inline">\(E[\bar{X}] = \mu = kp\)</span>
and that the variance is <span class="math inline">\(V[\bar{X}] = \sigma^2/n = kp(1-p)/n\)</span>,
but when we attempt to use the mgf method with
<span class="math inline">\(a_i = 1/n\)</span> instead of <span class="math inline">\(a_i = 1\)</span>, we find that
<span class="math display">\[
m_{\bar{X}}(t) = [pe^{t/n} + (1-p)]^{nk} \,.
\]</span>
Changing <span class="math inline">\(t\)</span> to <span class="math inline">\(t/n\)</span> has the effect of creating an mgf that does not
have the form of any known mgf.
However, we <em>do</em> know the distribution: it has a pmf that is identical
in form to that of the binomial distribution, but has the domain
<span class="math inline">\(\{0,1/n,2/n,...,k\}\)</span>.
(We can derive this result mathematically by making
the transformation <span class="math inline">\(\sum_{i=1}^n X_i \rightarrow (\sum_{i=1}^n X_i)/n\)</span>,
as we see below in an example.)
We could define the pmf ourselves
using our own <code>R</code> function, but there is no real need to: as we will see,
if we wish to construct a confidence interval for <span class="math inline">\(p\)</span>, we can just use the sum
<span class="math inline">\(\sum_{i=1}^n X_i\)</span> as our statistic.
(We could also, in theory, utilize the Central Limit Theorem if
<span class="math inline">\(n \gtrsim 30\)</span>, but there is absolutely no reason to do that to make
inferences about <span class="math inline">\(p\)</span>: we know the distribution of the sum of the data
exactly, and thus there is no need to fall back upon approximations.)</p>
<hr />
<div id="the-mgf-for-a-geometric-random-variable" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The MGF for a Geometric Random Variable<a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that a geometric distribution is equivalent to a
negative binomial distribution
with number of successes <span class="math inline">\(s = 1\)</span>; its probability mass function is
<span class="math display">\[
p_X(x) = p (1-p)^x \,,
\]</span>
with <span class="math inline">\(x = \{0,1,\ldots\}\)</span> and <span class="math inline">\(p \in [0,1]\)</span>.
The moment-generating function for a geometric random variable is thus
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^{\infty} e^{tx} p (1-p)^x = p \sum_{x=0}^\infty e^{tx} (1-p)^x \\
&amp;= p \sum_{x=0}^\infty [e^t(1-p)]^x = \frac{p}{1-e^t(1-p)} \,.
\end{align*}\]</span>
The last equality utilizes the
formula for the sum of an infinite geometric series:
<span class="math inline">\(\sum_{i=0}^\infty x^i = (1-x)^{-1}\)</span>, when <span class="math inline">\(\vert x \vert &lt; 1\)</span>.
(If <span class="math inline">\(t &lt; 0\)</span> and <span class="math inline">\(p &gt; 0\)</span>, then the condition that
<span class="math inline">\(\vert e^t(1-p) \vert &lt; 1\)</span> holds.)</p>
</blockquote>
<blockquote>
<p>The sum of <span class="math inline">\(s\)</span> geometric random variables has the moment-generating
function
<span class="math display">\[
m_Y(t) = \prod_{i=1}^s m_{X_i}(t) = \left[\frac{p}{1-e^t(1-p)}\right]^s \,.
\]</span>
This is the mgf for a negative binomial distribution for <span class="math inline">\(s\)</span> successes.
In the same way that the sum of <span class="math inline">\(k\)</span> Bernoulli random variables is a
binomially distributed random variable, the sum of <span class="math inline">\(s\)</span> geometric random
variables is a negative binomially distributed random variable.</p>
</blockquote>
<hr />
</div>
<div id="the-pmf-for-the-sample-mean" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> The PMF for the Sample Mean<a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). As we observe above, the
distribution of the sum <span class="math inline">\(Y = \sum_{i=1}^k X_i\)</span> is binomial with mean
<span class="math inline">\(nkp\)</span> and variance <span class="math inline">\(nkp(1-p)\)</span>.</p>
</blockquote>
<blockquote>
<p>The sample mean is <span class="math inline">\(\bar{X} = Y/n\)</span>, and so
<span class="math display">\[
F_{\bar{X}}(\bar{x}) = P(\bar{X} \leq \bar{x}) = P(Y \leq n\bar{x}) = \sum_{y=0}^{n\bar{x}} p_Y(y) \,.
\]</span>
(We note that <span class="math inline">\(n\bar{x}\)</span> is integer-valued by definition; we do not need
to round down here.) Because we are dealing with a pmf, we cannot
simply take the derivative of <span class="math inline">\(F_{\bar{X}}(\bar{x})\)</span> to find
<span class="math inline">\(f_{\bar{X}}(\bar{x})\)</span>but what we can do is assess the jump
in the cumulative distribution function at each step, because that
<em>is</em> the pmf. In other words, we can compute
<span class="math display">\[
f_{\bar{X}}(\bar{x}) = P(Y \leq n\bar{x}) - P(Y \leq n\bar{x}-1)
\]</span>
and store this as a numerically expressed pmf for <span class="math inline">\(\bar{X}\)</span>.
See Figure <a href="the-binomial-and-related-distributions.html#fig:xbarpmf">3.5</a>.</p>
</blockquote>
<blockquote>
<p>But it turns out we can say more about this pmf, by looking at the problem
in a different way.
We know that <span class="math inline">\(Y \sim\)</span> Binomial<span class="math inline">\((nkp,nkp(1-p))\)</span> and
thus that <span class="math inline">\(Y \in [0,1,\ldots,nk]\)</span>.
When we compute the quotient <span class="math inline">\(\bar{X} = Y/n\)</span>, <em>all we are doing is redefining the domain of the pmf</em>
from being <span class="math inline">\([0,1,\ldots,nk]\)</span> to being
<span class="math inline">\([0,1/n,2/n,\ldots,k]\)</span>. We do not actually change the probability masses!
So we can write
<span class="math display">\[
p_{\bar{X}}(\bar{x}) = \binom{nk}{n\bar{x}} p^{n\bar{x}} (1-p)^{nk-n\bar{x}} ~~ \bar{x} \in [0,1/n,2/n,\ldots,k] \,.
\]</span>
This pmf has the functional form of a binomial pmfbut <em>not</em> the domain
of a binomial pmf. For that reason, we cannot say that <span class="math inline">\(\bar{X}\)</span> is
binomially distributed. The pmf has a functional form, it has a domain,
but it has no known name and thus no associated <code>R</code> functions that
we can utilize when performing statistical inference.
(This is why we will utilize the sum of the data instead:
<code>R</code> functions for its distribution exist!)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xbarpmf"></span>
<img src="_main_files/figure-html/xbarpmf-1.png" alt="\label{fig:xbarpmf}Probability mass function for the sample mean of $n = 10$ iid binomial random variables, for $k = 10$ and $p = 0.6$." width="50%" />
<p class="caption">
Figure 3.5: Probability mass function for the sample mean of <span class="math inline">\(n = 10\)</span> iid binomial random variables, for <span class="math inline">\(k = 10\)</span> and <span class="math inline">\(p = 0.6\)</span>.
</p>
</div>
</div>
</div>
<div id="order-statistics" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Order Statistics<a href="the-binomial-and-related-distributions.html#order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets suppose that we have sampled <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> from
some arbitrary distribution. Previously, we
have summarized such data with the sample mean and the sample variance.
However, there are other summary statistics, some of which
are only calculable if we sort the data into ascending order:
<span class="math inline">\(\{X_{(1)},\ldots,X_{(n)}\}\)</span>. These are dubbed <em>order statistics</em> and
the <span class="math inline">\(j^{th}\)</span> order statistic is the samples <span class="math inline">\(j^{th}\)</span> smallest value
(i.e., the smallest-valued datum in the sample
is <span class="math inline">\(X_{(1)}\)</span> and the largest-valued datum is <span class="math inline">\(X_{(n)}\)</span>). Examples of
statistics based on ordering include
<span class="math display">\[\begin{align*}
\mbox{Range:}&amp; ~~X_{(n)} - X_{(1)} \\
\mbox{Median:}&amp; ~~X_{(n+1)/2} ~ \mbox{if $n$ is odd} \\
&amp; ~~(X_{n/2}+X_{(n+1)/2})/2 ~ \mbox{if $n$ is even} \,.
\end{align*}\]</span>
The most important point to keep in mind is that the probability mass
and density functions for order statistics differ from
the pmfs and pdfs for their constituent iid data. For instance, if we
sample <span class="math inline">\(n\)</span> data from a <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution, we
would not expect the minimum value to be distributed the same way;
if anything, the mean should take on larger and larger negative values,
and the variance on those values should decrease, as <span class="math inline">\(n\)</span> increases.</p>
<p>So: why are we discussing order statistics here, in the middle of
a discussion of the binomial distribution? It is because
we can derive, e.g., the pdf for an order statistic of a
continuous distribution using the binomial pmf. (Note that order statistics
exist for discretely valued data, but the probability mass functions for
them are not easily derived and thus we will only consider
order statistics for continuously valued data here.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:order"></span>
<img src="figures/order.png" alt="\label{fig:order}If we have, e.g., a probability density function $f_X(x)$ whose domain is $[a,b]$, and we view success as sampling a datum less than a given value $x$, then when we sample $n$ data, the number that have values $\leq x$ is a binomial random variable with $k=n$ and $p = F_X(x)$." width="60%" />
<p class="caption">
Figure 3.6: If we have, e.g., a probability density function <span class="math inline">\(f_X(x)\)</span> whose domain is <span class="math inline">\([a,b]\)</span>, and we view success as sampling a datum less than a given value <span class="math inline">\(x\)</span>, then when we sample <span class="math inline">\(n\)</span> data, the number that have values <span class="math inline">\(\leq x\)</span> is a binomial random variable with <span class="math inline">\(k=n\)</span> and <span class="math inline">\(p = F_X(x)\)</span>.
</p>
</div>
<p>See Figure <a href="the-binomial-and-related-distributions.html#fig:order">3.6</a>. Without loss of generality, we can assume that
<span class="math inline">\(f_X(x) &gt; 0\)</span> for <span class="math inline">\(x \in [a,b]\)</span> and that we sample <span class="math inline">\(n\)</span> data from this distribution.
The number of data <span class="math inline">\(X\)</span> that have value less than some arbitrarily chosen <span class="math inline">\(x\)</span> is a binomial random variable:
<span class="math display">\[
Y \sim \mbox{Binomial}(n,p=F_X(x))
\]</span>
What is the probability that the <span class="math inline">\(j^{th}\)</span> ordered datum has a value <span class="math inline">\(\leq x\)</span>?
Thats equivalent to asking for the probability that <span class="math inline">\(Y \geq j\)</span>, i.e., did we
see at least <span class="math inline">\(j\)</span> successes in <span class="math inline">\(n\)</span> trials?
<span class="math display">\[
F_{(j)}(x) = P(X_{(j)} \leq x) = P(Y \geq j) = \sum_{i=j}^n \binom{n}{i} [F_X(x)]^i [1 - F_X(x)]^{n-i} \,.
\]</span>
<span class="math inline">\(F_{(j)}(x)\)</span> is the cdf for the <span class="math inline">\(j^{th}\)</span> ordered datum.</p>
<p><strong>Recall:</strong> <em>a continuous distributions pdf is the derivative of its cdf.</em></p>
<p>Leaving aside algebraic details, we can write down the pdf for <span class="math inline">\(X_{(j)}\)</span>:
<span class="math display">\[
f_{(j)}(x) = \frac{d}{dx}F_{(j)}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j} \,,
\]</span>
and write down simplified expressions for the pdfs for the minimum and maximum data values:
<span class="math display">\[
f_{(1)}(x) = n f_X(x) [1 - F_X(x)]^{n-1} ~~\mbox{and}~~ f_{(n)}(x) = n f_X(x) [F_X(x)]^{n-1} \,.
\]</span></p>
<hr />
<div id="distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Distribution of the Minimum Value Sampled from an Exponential Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for an exponential random variable is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and the expected value of <span class="math inline">\(X\)</span> is
<span class="math inline">\(E[X] = \theta\)</span>. What is the pdf for the smallest value among
<span class="math inline">\(n\)</span> iid data sampled from an exponential distribution? What is the expected value
for the smallest value?</p>
</blockquote>
<blockquote>
<p>First, if we do not immediately recall the cumulative distribution function
<span class="math inline">\(F_X(x)\)</span>, we can easily derive it:
<span class="math display">\[
F_X(x) = \int_0^x \frac{1}{\theta} e^{-y/\theta} dy = 1 - e^{-x/\theta} \,.
\]</span>
We plug <span class="math inline">\(F_X(x)\)</span> into the expression of the pdf of the minimum datum given
above:
<span class="math display">\[\begin{align*}
f_{(1)}(x) &amp;= n \frac{1}{\theta} e^{-x/\theta} \left[ 1 - (1-e^{-x/\theta}) \right]^{n-1} \\
&amp;= n \frac{1}{\theta} e^{-x/\theta} e^{-(n-1)x/\theta} \\
&amp;= \frac{n}{\theta} e^{-nx/\theta} \,.
\end{align*}\]</span>
<span class="math inline">\(X_{(1)}\)</span> is thus an exponentially distributed random variable with
parameter <span class="math inline">\(\theta/n\)</span> and expected value <span class="math inline">\(\theta/n\)</span>. We can derive this
result as follows.
<span class="math display">\[
E[X_{(1)}] = \int_0^\infty x \frac{n}{\theta} e^{-nx/\theta} dx \,.
\]</span>
We recognize this as <em>almost</em> having the form of a gamma-function integral:
<span class="math display">\[
\Gamma(u) = \int_0^\infty x^{u-1} e^{-x} dx \,.
\]</span>
We affect a variable transformation <span class="math inline">\(y = nx/\theta\)</span>; for this transformation,
<span class="math inline">\(dy = (n/\theta)dx\)</span>,
and if <span class="math inline">\(x = 0\)</span> or <span class="math inline">\(\infty\)</span>, <span class="math inline">\(y = 0\)</span> or <span class="math inline">\(\infty\)</span> (meaning the integral bounds
are unchanged). Our new integral is
<span class="math display">\[
E[X] = \int_0^\infty \frac{\theta y}{n} \frac{n}{\theta} e^{-y} \frac{\theta}{n} dy = \frac{\theta}{n} \int_0^\infty y e^{-y} dy = \frac{\theta}{n} \Gamma(2) = \frac{\theta}{n} 1! = \frac{\theta}{n} = \frac{E[X]}{n} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for a Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. The cdf for this distribution is thus
<span class="math display">\[
F_X(x) = \int_0^x 1 dy = x \,.
\]</span>
Lets assume that we sample <span class="math inline">\(n\)</span> iid data from this distribution, where <span class="math inline">\(n\)</span> is an
odd number. The index of the median value is thus <span class="math inline">\((n+1)/2\)</span>, and if we plug into
the general expression for the pdf of the <span class="math inline">\(m^{th}\)</span> ordered datum, we find that
<span class="math display">\[\begin{align*}
f_{(n+1)/2} &amp;= \frac{n!}{\left(\frac{n+1}{2}-1\right)!\left(n - \frac{n+1}{2}\right)!} \cdot 1 \cdot x^{\left(\frac{n+1}{2}\right)-1} \cdot (1-x)^{n - \left(\frac{n+1}{2}\right)} \\
&amp;= \frac{n!}{2\left(\frac{n-1}{2}\right)!} x^{\left(\frac{n-1}{2}\right)} (1-x)^{\left(\frac{n-1}{2}\right)} \,.
\end{align*}\]</span>
As we will see later,
this is a beta distribution with parameters <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>. The median value has
expected value 1/2 and a variance that shrinks with <span class="math inline">\(n\)</span>.</p>
</blockquote>
</div>
</div>
<div id="point-estimation-2" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Point Estimation<a href="the-binomial-and-related-distributions.html#point-estimation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the first two chapters, we introduce a number of concepts related to point estimation, the
act of using statistics to make inferences about a population parameter <span class="math inline">\(\theta\)</span>. We</p>
<ul>
<li>assess point estimators using the metrics of bias, variance, mean-squared error, and consistency;</li>
<li>utilize the Fisher information metric to determine the lower bound on the variance
for unbiased estimators (the Cramer-Rao Lower Bound, or CRLB); and</li>
<li>define estimators via the maximum likelihood algorithm, which generates estimators
that are at least asymptotically unbiased and at least asymptotically reach the CRLB, and
which converge in distribution to normal random variables.</li>
</ul>
<p>We will review these concepts in the context of estimating population quantities for
binomial distributions below, in the body of the text and in examples. For now</p>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>Here we will introduce another means by which to define an estimator. The
<em>minimum variance unbiased estimator</em> (or <em>MVUE</em>) is the one
that has the smallest variance among all unbiased estimators of <span class="math inline">\(\theta\)</span>.
The readers first thought might be well, why didnt we use this estimator
in the first placeafter all, the MLE is not guaranteed to yield an
unbiased estimator, so why have we put off discussing the MVUE?
The primary reasons are that MVUEs are sometimes not definable (i.e., we
can reach insurmountable roadblocks when trying to derive them),
and unlike MLEs, they do not exhibit the invariance property. (For
instance, if
<span class="math inline">\(\hat{\theta}_{MLE} = \bar{X}\)</span>, then <span class="math inline">\(\hat{\theta^2}_{MLE} = \bar{X}^2\)</span>, but
if <span class="math inline">\(\hat{\theta}_{MVUE} = \bar{X}\)</span>, it is not necessarily the case that
<span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.) <em>However, we should always at least
try to define the MVUE, because if we can, it will be at least equal the
performance of, if not do better than, the MLE, in terms of bias and/or
variance.</em></p>
<p>There are two steps to carry out when deriving the MVUE:</p>
<ol style="list-style-type: decimal">
<li>determining a sufficient statistic for <span class="math inline">\(\theta\)</span>; and</li>
<li>correcting any bias that is observed when we utilize that sufficient
statistic as our initial estimator.</li>
</ol>
<p>A <em>sufficient statistic</em> for a parameter <span class="math inline">\(\theta\)</span>
captures all information about <span class="math inline">\(\theta\)</span> contained in the sample.
The sufficiency principle holds that if, e.g., <span class="math inline">\(Y\)</span> is
a sufficient statistic for <span class="math inline">\(\theta\)</span>, and we collect two datasets
<span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> such that <span class="math inline">\(Y(\mathbf{U}) = Y(\mathbf{V})\)</span>,
then the inferences we make about <span class="math inline">\(\theta\)</span> given that we observe <span class="math inline">\(\mathbf{U}\)</span>
will be exactly the same as those we would make if we were to
observe <span class="math inline">\(\mathbf{V}\)</span> instead.
Using any statistic beyond a sufficient
statistic will not lead to improved inferences about <span class="math inline">\(\theta\)</span>. This
means that, for instance, combining <span class="math inline">\(\bar{X}\)</span> (a sufficient statistic
for the normal mean <span class="math inline">\(\mu\)</span> when the variance is known) with, say,
the sample median will not reduce the length of confidence intervals for <span class="math inline">\(\mu\)</span>
or change the power of hypothesis tests about <span class="math inline">\(\mu\)</span>, relative to what we
would derive using <span class="math inline">\(\bar{X}\)</span> alone.
Note that utilizing the sufficiency principle is but one way by which
statisticians can attempt <em>data reduction</em> prior to inference; two others,
which are beyond the scope of this book, are the likelihood and
equivalence principles. For a deeper treatment of sufficiency and of data
reduction than we provide here, the interested reader should consult
Chapter 6 of Casella and Berger (2002).</p>
<p>A statistic <span class="math inline">\(Y(\mathbf{X})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if
the ratio
<span class="math display">\[\begin{align*}
\frac{f_X(\mathbf{x} \vert \theta)}{f_Y(y \vert \theta)} \mathrel{{iid}\rightarrow} \frac{\prod_{i=1}^n f_X(x_i \vert \theta)}{f_Y(y \vert \theta)} \,,
\end{align*}\]</span>
where <span class="math inline">\(f_Y(y \vert \theta)\)</span> is the probability density function of the
sampling distribution for <span class="math inline">\(Y\)</span>, is constant as a function of <span class="math inline">\(\theta\)</span>.
We note that by this definition, <span class="math inline">\(\mathbf{X}\)</span> itself comprises a
sufficient statistic for <span class="math inline">\(\theta\)</span>, as does the full set of order statistics
<span class="math inline">\(\{X_{(1)},\ldots,X_{(n)}\}\)</span>. The relevant questions are: can we
define a sufficient statistic that actually reduces the data? and if so,
how can we identify it?</p>
<p>Lets answer the second question first.
The simplest way by which to identify a sufficient statistic is
to write down the likelihood function and then to factorize it into two
separate functions, one of which depends <em>only</em> on the observed data and the
other of which depends on both the observed data and the parameter of
interest:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = g(\mathbf{x},\theta) \cdot h(\mathbf{x}) \,.
\]</span>
This is the so-called <em>factorization criterion</em>.
In the expression <span class="math inline">\(g(\mathbf{x},\theta)\)</span>, the data will appear within, e.g., a
summation (e.g., <span class="math inline">\(\sum_{i=1}^n x_i\)</span>) or a product
(e.g., <span class="math inline">\(\prod_{i=1}^n x_i\)</span>), and
we would identify that summation or product as a sufficient statistic for
<span class="math inline">\(\theta\)</span>. For instance, for the binomial distribution,
the factorized likelihood (given <span class="math inline">\(n\)</span> iid data) is
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n \binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \underbrace{\left[ \prod_{i=1}^n \binom{k}{x_i} \right]}_{h(\mathbf{x})} \underbrace{p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i}}_{g(\sum_{i=1}^n x_i,p)} \,.
\]</span>
By inspecting the function <span class="math inline">\(g(\mathbf{x},p)\)</span>, we immediately determine that
a sufficient statistic for <span class="math inline">\(p\)</span> is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>.
We say a sufficient statistic because
sufficient statistics are not unique: <em>any one-to-one function
of a sufficient statistic is also a sufficient statistic</em>.
For instance, if <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is a sufficient
statistic for <span class="math inline">\(\theta\)</span>, so is <span class="math inline">\(Y = \bar{X}\)</span>, etc.</p>
<p>Now we return to the first question above: can we define a sufficient
statistic that actually reduces the data? The answer is not always.
In fact, across all
possible distributions, it is relatively rare that we can do this.
The Pitman-Koopman-Darmois theorem holds that, among families of
distributions that do not have domain-specifying parameters, only the
exponential family of distributions have sufficient statistics whose
dimension does not increase as the sample size increases.
(This theorem motivates, in large part,
our introduction of the exponential family in Chapter 2.)
Recall that the exponential
family of distributions comprises a set of distributions whose probability
mass or density functions can be written as
<span class="math display">\[\begin{align*}
h(x) \exp\left( \eta(\theta) T(x) - A(\theta) \right) \,.
\end{align*}\]</span>
(Here we assume that the distribution has just one free parameter <span class="math inline">\(\theta\)</span>.)
In this expression, <span class="math inline">\(T(x)\)</span> <em>is</em> the sufficient statistic;
as seen in a previous example,
for the binomial distribution <span class="math inline">\(T(X) = X\)</span>. If we collect <span class="math inline">\(n\)</span> iid data,
then the sufficient statistic is <span class="math inline">\(T(\mathbf{X}) = \sum_{i=1}^n X_i\)</span>, and
as we can see it is still
a single number<span class="math inline">\(-\)</span>i.e., it has a dimension of one<span class="math inline">\(-\)</span>regardless of the value
of <span class="math inline">\(n\)</span>. Now, restricting ourselves to exponential family distributions in
inferential situations would appear to limit the practical utility of
the sufficiency principleexcept it is the case that <em>many</em> (if not <em>most</em>)
of the distributions that we
utilize in inference are indeed exponential-family distributions,
including all the ones that we focus on in this book.</p>
<p>(We note here for completeness that once we identify a sufficient statistic,
we are technically required to demonstrate that it is
both <em>minimally sufficient</em> and <em>complete</em>, as
it needs to be both so that we can use it, e.g., to determine the
minimum variance unbiased estimator. It suffices
to say here that the sufficient statistics that we identify for
exponential family distributions via likelihood factorization
are minimally sufficient and complete.
See Chapter 7 for more details on minimal sufficiency
and completeness.)</p>
<!--
TBD: add example that describes the concept of ancillary statistics, and
mention Basu's theorem and the result that, e.g., barX and S^2 are 
independent r.v.'s
-->
<p>If <span class="math inline">\(Y\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>, and there is a
function <span class="math inline">\(h(Y)\)</span> that is an unbiased estimator for <span class="math inline">\(\theta\)</span> and that depends
on the data only through <span class="math inline">\(Y\)</span>, then <span class="math inline">\(h(Y)\)</span> is the MVUE for
<span class="math inline">\(\theta\)</span>. (Recall from above that one-to-one functions of sufficient
statistics are themselves sufficient statistics, hence <span class="math inline">\(h(Y)\)</span> is sufficient
for <span class="math inline">\(\theta\)</span>.) Here, given <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>, we need to find a function
<span class="math inline">\(h(\cdot)\)</span> such that <span class="math inline">\(E[h(Y)] = p\)</span>. Earlier in this chapter, we determined
that the distribution for the sum of iid binomial random variables is
Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>), and thus we
know that this distribution has expected value <span class="math inline">\(nkp\)</span>. Thus
<span class="math display">\[
E\left[Y\right] = nkp ~\implies~ E\left[\frac{Y}{nk}\right] = p ~\implies~ h(Y) = \frac{Y}{nk} = \frac{\bar{X}}{k} ~\mbox{is the MVUE for}~p \,.
\]</span>
The variance of <span class="math inline">\(\hat{p}\)</span> is
<span class="math display">\[
V[\hat{p}] = V\left[\frac{\bar{X}}{k}\right] = \frac{1}{k^2}V[\bar{X}] = \frac{1}{k^2} \frac{V[X]}{n} = \frac{1}{k^2}\frac{kp(1-p)}{n} = \frac{p(1-p)}{nk} \,.
\]</span>
We know that this variance abides by the restriction
<span class="math display">\[
V[\hat{p}] \geq \frac{1}{nI(p)} = -\frac{1}{nE\left[\frac{d^2}{dp^2} \log p_X(X \vert p) \right]} \,.
\]</span>
But is it equivalent to the lower bound for unbiased estimators, the CRLB? (Note that in particular
situations, the MVUE <em>may</em> have a variance larger than the CRLB; when this is the case,
unbiased estimators that achieve the CRLB simply do not exist.) For the binomial distribution,
<span class="math display">\[\begin{align*}
p_{X}(x) &amp;= \binom{k}{x} p^{x} (1-p)^{k-x} \\
\log p_{X}(x) &amp;= \log \binom{k}{x} + x \log p + (k-x) \log (1-p) \\
\frac{d}{dp} \log p_{X}(x) &amp;= 0 + \frac{x}{p} - \frac{k-x}{(1-p)} \\
\frac{d^2}{dp^2} \log p_{X}(x) &amp;= -\frac{x}{p^2} - \frac{k-x}{(1-p)^2} \\
E\left[\frac{d^2}{dp^2} \log p_{X}(X)\right] &amp;= -\frac{1}{p^2}E[X] - \frac{1}{(1-p)^2}E[k-X] \\
&amp;= -\frac{kp}{p^2}-\frac{k-kp}{(1-p)^2} \\
&amp;= -\frac{k}{p}-\frac{k}{1-p} = -\frac{k}{p(1-p)} \,.
\end{align*}\]</span>
The lower bound on the variance is thus <span class="math inline">\(p(1-p)/(nk)\)</span>, and so
the MVUE <em>does</em> achieve the CRLB.
We cannot define a better unbiased estimator for <span class="math inline">\(p\)</span> than <span class="math inline">\(\bar{X}/k\)</span>.</p>
<hr />
<div id="the-mle-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> The MLE for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.</em></p>
<blockquote>
<p>Above, we determined that
the likelihood function for <span class="math inline">\(n\)</span> iid binomial random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> is
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \left[\prod_{i=1}^n \binom{k}{x_i} \right] p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i} \,.
\]</span>
Recall that the value <span class="math inline">\(\hat{p}_{MLE}\)</span> that maximizes <span class="math inline">\(\mathcal{L}(p \vert x)\)</span> also maximizes
<span class="math inline">\(\ell(p \vert x) = \log \mathcal{L}(p \vert x)\)</span>, which is considerably easier to work with:
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) &amp;= \left(\sum_{i=1}^n x_i\right) \log p + \left(nk - \sum_{i=1}^n x_i\right) \log (1-p) \\
\frac{d}{dp} \ell(p \vert \mathbf{x}) &amp;= \frac{1}{p} \sum_{i=1}^n x_i - \frac{1}{1-p} \left(nk - \sum_{i=1}^n x_i\right) = 0 \,.
\end{align*}\]</span>
(Here, we drop the binomial coefficient, which does not depend on
<span class="math inline">\(p\)</span> and thus differentiates to zero.)
After rearranging terms, we find that
<span class="math display">\[
p = \frac{1}{nk}\sum_{i=1}^n x_i ~\implies~ \hat{p}_{MLE} = \frac{\bar{X}}{k} \,.
\]</span>
The MLE matches the MVUE, thus we know that the MLE is unbiased and we know that it achieves the CRLB.</p>
</blockquote>
<blockquote>
<p>A useful property of MLEs is the invariance property, whereby the MLE for a function of <span class="math inline">\(\theta\)</span>
is given by applying the same function to the MLE itself. Thus</p>
</blockquote>
<blockquote>
<ul>
<li>the MLE for the population mean <span class="math inline">\(E[X] = \mu = kp\)</span> is <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>; and</li>
<li>the MLE for the population variance <span class="math inline">\(V[X] = \sigma^2 = kp(1-p)\)</span> is <span class="math inline">\(\widehat{\sigma^2}_{MLE} = \bar{X}(1-\bar{X}/k)\)</span>.</li>
</ul>
</blockquote>
<blockquote>
<p>Last, note that asymptotically, <span class="math inline">\(\hat{p}_{MLE}\)</span> converges in distribution to a normal random variable:
<span class="math display">\[
\hat{p}_{MLE} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(p,\frac{1}{nI(p)} = \frac{p(1-p)}{nk}\right) \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="sufficient-statistics-for-the-normal-distribution" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Sufficient Statistics for the Normal Distribution<a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>If we have <span class="math inline">\(n\)</span> iid data drawn from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and
unknown variance <span class="math inline">\(\sigma^2\)</span>, then the factorized likelihood is
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i^2, \sum x_i,\mu,\sigma)} \cdot \underbrace{1}_{h(\mathbf{x})} \,.
\]</span>
Here, we identify <span class="math inline">\(\sum x_i^2\)</span> and <span class="math inline">\(\sum x_i\)</span> as <em>joint</em> sufficient statistics: we
need two pieces of information to jointly estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. (To be
clear: it is not necessarily the case that one of the parameters matches up to
one of the statisticsrather, the two statistics are jointly
sufficient for estimation.) We thus cannot proceed further to define an MVUE for
<span class="math inline">\(\mu\)</span> or for <span class="math inline">\(\sigma^2\)</span>, without knowing the joint bivariate probability
density function for <span class="math inline">\(Y_1 = \sum_{i=1}^n X_i^2\)</span> and <span class="math inline">\(Y_2 = \sum_{i=1}^n X_i\)</span>.</p>
</blockquote>
<blockquote>
<p>(Note that we <em>can</em> proceed if we happen to know either <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma^2\)</span>;
if one
of these values is fixed, then there will only be one sufficient statistic
and we can determine the MVUE for the other, freely varying parameter.)</p>
</blockquote>
<hr />
</div>
<div id="the-sufficiency-principle-examples-of-when-we-cannot-reduce-data" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> The Sufficiency Principle: Examples of When We Cannot Reduce Data<a href="the-binomial-and-related-distributions.html#the-sufficiency-principle-examples-of-when-we-cannot-reduce-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As stated above, it is often impossible to reduce data to, e.g.,
a single-number summary. The following examples, utilizing distributions
that are <em>not</em> exponential-family distributions, illustrate this.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>A Laplace distribution with scale parameter 1 has the
probability density function
<span class="math display">\[\begin{align*}
f_X(x \vert \theta) &amp;= \frac12 e^{-\vert x - \theta \vert} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \in (-\infty,\infty)\)</span> and
<span class="math inline">\(\theta \in (-\infty,\infty)\)</span>. If we observe <span class="math inline">\(n\)</span> iid data, then
likelihood factorization yields
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \frac{1}{2^n} e^{-\sum_{i=1}^n \vert x_i - \theta \vert} \,.
\end{align*}\]</span>
<span class="math inline">\(Y = \sum_{i=1}^n \vert X_i - \theta \vert\)</span> is not a sufficient statistic
as it contains the (unknown) parameter: a sufficient statistic
for <span class="math inline">\(\theta\)</span> cannot contain <span class="math inline">\(\theta\)</span> itself! We cannot isolate a function
of <span class="math inline">\(\mathbf{X}\)</span> alone, and thus no data reduction is possible.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>A Cauchy distribution with scale parameter 1 has the probability density
function
<span class="math display">\[\begin{align*}
f_X(x \vert \theta) = \frac{1}{\pi (x-\theta)^2} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \in (-\infty,\infty)\)</span> and
<span class="math inline">\(\theta \in (-\infty,\infty)\)</span>. If we observe <span class="math inline">\(n\)</span> iid data, then
likelihood factorization yields
<span class="math display">\[\begin{align*}
\mathcal{L}(\theta \vert \mathbf{x}) = \frac{1}{\pi^n} \frac{1}{\prod_{i=1}^n (x_i-\theta)^2} \,.
\end{align*}\]</span>
Again, we cannot isolate a function of the <span class="math inline">\(X_i\)</span>s alone: no data reduction
is possible.</li>
</ol>
</blockquote>
<hr />
</div>
<div id="the-mvue-for-the-exponential-mean" class="section level3 hasAnchor" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> The MVUE for the Exponential Mean<a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The exponential distribution is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and where <span class="math inline">\(E[X] = \theta\)</span> and <span class="math inline">\(V[X] = \theta^2\)</span>.
Lets assume that we have <span class="math inline">\(n\)</span> iid data drawn from this distribution.
Can we define the MVUE for <span class="math inline">\(\theta\)</span>? For <span class="math inline">\(\theta^2\)</span>?</p>
</blockquote>
<blockquote>
<p>The likelihood function is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \frac{1}{\theta^n}\exp\left(-\frac{1}{\theta}\sum_{i=1}^n x_i \right) = h(\mathbf{x}) \cdot g(\theta,\mathbf{x}) \,.
\]</span>
Here, there are no terms that are functions of only the data, so <span class="math inline">\(h(\mathbf{x}) = 1\)</span> and
a sufficient statistic is <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>. We compute the expected value
of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
E[Y] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \theta = n\theta \,.
\]</span>
The expected value of <span class="math inline">\(Y\)</span> is not <span class="math inline">\(\theta\)</span>, so <span class="math inline">\(Y\)</span> is not unbiasedbut we can see
immediately that <span class="math inline">\(E[Y/n] = \theta\)</span>, so that <span class="math inline">\(Y/n\)</span> is unbiased. Thus the MVUE for <span class="math inline">\(\theta\)</span>
is thus <span class="math inline">\(\hat{\theta}_{MVUE} = Y/n = \bar{X}\)</span>.</p>
</blockquote>
<blockquote>
<p>Note that the MVUE does not possess the invariance propertyit is not necessarily the
case that <span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets propose a function of <span class="math inline">\(Y\)</span> and see if we can use that to define <span class="math inline">\(\hat{\theta^2}_{MVUE}\)</span>:
<span class="math inline">\(h(Y) = Y^2/n^2 = \bar{X}^2\)</span>. (To be clear, we are simply proposing a function and seeing if
it helps us define what we are looking for. It might not. If not, we can try again with
another function of <span class="math inline">\(Y\)</span>.) Utilizing what we know about the sample mean, we can write down that
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{V[X]}{n} + (E[X])^2 = \frac{\theta^2}{n}+\theta^2 = \theta^2\left(\frac{1}{n} + 1\right) \,.
\]</span>
So <span class="math inline">\(\bar{X}^2\)</span> itself is <em>not</em> an unbiased estimator of <span class="math inline">\(\theta^2\)</span>but we can see that
<span class="math inline">\(\bar{X}^2/(1/n+1)\)</span> is. Hence
<span class="math display">\[
\hat{\theta^2}_{MVUE} = \frac{\bar{X}^2}{\left(\frac{1}{n}+1\right)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-mvue-for-the-geometric-distribution" class="section level3 hasAnchor" number="3.6.5">
<h3><span class="header-section-number">3.6.5</span> The MVUE for the Geometric Distribution<a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that the geometric distribution is a negative binomial distribution
with <span class="math inline">\(s = 1\)</span>:
<span class="math display">\[\begin{align*}
p_X(x) = \binom{x + s - 1}{x} p^s (1-p)^x ~ \rightarrow ~ p(1-p)^x \,,
\end{align*}\]</span>
where <span class="math inline">\(p \in (0,1]\)</span> and where <span class="math inline">\(E[X] = (1-p)/p\)</span> and <span class="math inline">\(V[X] = (1-p)/p^2\)</span>.
Lets assume that we sample <span class="math inline">\(n\)</span> iid data from this distribution.
Can we define the MVUE for <span class="math inline">\(p\)</span>? For <span class="math inline">\(1/p\)</span>?</p>
</blockquote>
<blockquote>
<p>The likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n p_X(x_i \vert p) = p^n (1-p)^{\sum_{i=1}^n x_i} = 1 \cdot g(p,\mathbf{x}) \,.
\end{align*}\]</span>
We identify a sufficient statistic as <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>; the expected
value of <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{align*}
E[Y] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \frac{1-p}{p} = n\left(\frac{1}{p}-1\right) \,.
\end{align*}\]</span>
We cannot debias this expression to find the MVUE for <span class="math inline">\(p\)</span>:
<span class="math display">\[\begin{align*}
E\left[\frac{Y}{n}+1\right] = \frac{1}{p} \,.
\end{align*}\]</span>
Specifically,
<span class="math display">\[\begin{align*}
E\left[\frac{1}{Y/n+1}\right] \neq 1/E\left[\frac{Y}{n}+1\right] = p \,.
\end{align*}\]</span>
However, we did determine the MVUE for <span class="math inline">\(1/p\)</span>: <span class="math inline">\(Y/n+1 = \bar{X}+1\)</span>.
But as there is no invariance property for the MVUE, we cannot use this
expression to write down an MVUE for <span class="math inline">\(p\)</span>.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-2" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Confidence Intervals<a href="the-binomial-and-related-distributions.html#confidence-intervals-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}\)</span> by solving the following equation:</em>
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span>, <span class="math inline">\(y_{\rm obs}\)</span> is the observed value of the statistic,
and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined using
the confidence interval reference table introduced in
section 16 of Chapter 1.</em></p>
<p>A concept that we have not explicitly
discussed up until now is the duality between
confidence intervals and hypothesis tests: as they are mathematically
related, one can, in theory, invert hypothesis tests to derive confidence
intervals and vice-versa. We illustrate this duality
in Figure <a href="the-binomial-and-related-distributions.html#fig:contcdfci">3.7</a>, in which we assume that the distribution from
which we are to sample data is
a normal distribution with mean <span class="math inline">\(\mu\)</span> and <em>known</em> variance <span class="math inline">\(\sigma^2\)</span>. The
parallel green lines in the figure represent lower and upper rejection
region boundaries as a function of (the null hypothesis value) <span class="math inline">\(\mu\)</span>.
Lets say that we pick a specific value of <span class="math inline">\(\mu\)</span>,
say <span class="math inline">\(\mu = \mu_o = 1\)</span>, and draw a vertical line at that coordinate.
The part of that line that lies between the parallel green lines
(indicated in solid red) indicates the range of observed statistic
values <span class="math inline">\(y_{\rm obs}\)</span> for which we would <em>fail to reject</em> the null, and the
parts above and below the parallel lines (shown in dashed red)
would indicate <span class="math inline">\(y_{\rm obs}\)</span> values for which we would <em>reject</em> the null.
(These are the acceptance and rejection regions, respectively; acceptance
region is a term that we have not used up until now, but it simply denotes
the range of statistic values for which we would <em>fail</em> to reject a null
hypothesis, when it is indeed correct.)
Confidence intervals, on the other hand, are ranges of values of <span class="math inline">\(\mu\)</span>
for which a given value of <span class="math inline">\(y_{\rm obs}\)</span> lies in the acceptance region.
(For instance, the dashed horizontal blue line in the figure
shows the range of values associated with
a two-sided confidence interval for <span class="math inline">\(\mu\)</span> given <span class="math inline">\(y_{\rm obs} = 0.5\)</span>.)
We can see that for a given value of <span class="math inline">\(\mu\)</span>, if we were to sample
a value of <span class="math inline">\(y_{\rm obs}\)</span> between the two green lines (with probability
<span class="math inline">\(1-\alpha\)</span>), then the associated confidence
interval will overlap <span class="math inline">\(\mu\)</span>, and if we sample a value of <span class="math inline">\(y_{\rm obs}\)</span> outside
the green lines (with probability
<span class="math inline">\(\alpha\)</span>), the confidence interval will <em>not</em> overlap <span class="math inline">\(\mu\)</span>. The confidence
interval coverage is thus exactly <span class="math inline">\(100(1-\alpha)\)</span> percent.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contcdfci"></span>
<img src="_main_files/figure-html/contcdfci-1.png" alt="\label{fig:contcdfci}An illustration of the relationship between hypothesis testing and confidence interval estimation for a continuous sampling distribution (here, a normal distribution with known variance). The two parallel green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value $\mu$. Assume $\mu = 1$: the dashed red lines indicate the values of $y_{\rm obs}$ in the rejection region, while the solid red line indicates the values of $y_{\rm obs}$ in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of $y_{\rm obs}$. Given $\mu$, the probability of sampling a statistic in the acceptance region is exactly $1-\alpha$, and thus the coverage of confidence intervals will also be exactly $1-\alpha$." width="50%" />
<p class="caption">
Figure 3.7: An illustration of the relationship between hypothesis testing and confidence interval estimation for a continuous sampling distribution (here, a normal distribution with known variance). The two parallel green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value <span class="math inline">\(\mu\)</span>. Assume <span class="math inline">\(\mu = 1\)</span>: the dashed red lines indicate the values of <span class="math inline">\(y_{\rm obs}\)</span> in the rejection region, while the solid red line indicates the values of <span class="math inline">\(y_{\rm obs}\)</span> in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of <span class="math inline">\(y_{\rm obs}\)</span>. Given <span class="math inline">\(\mu\)</span>, the probability of sampling a statistic in the acceptance region is exactly <span class="math inline">\(1-\alpha\)</span>, and thus the coverage of confidence intervals will also be exactly <span class="math inline">\(1-\alpha\)</span>.
</p>
</div>
<p>When we work with discrete sampling distributions, the overall picture is
similar, but we sometimes need to make what we will dub discreteness
corrections so that the codes we use generate the correct results.
Figure <a href="the-binomial-and-related-distributions.html#fig:disccicover">3.8</a> is an analogue to Figure <a href="the-binomial-and-related-distributions.html#fig:contcdfci">3.7</a>
in which we illustrate the relationship between confidence
intervals and hypothesis tests for a situation in which we conduct
<span class="math inline">\(k = 10\)</span> binomial trials. Because the statistic is discretely valued, the
acceptance-region boundaries are step functions, but this is not the
only change introduced with discrete distributions:</p>
<ol style="list-style-type: decimal">
<li><p>In the continuous case, the probability of sampling a datum that lies
within the acceptance region for a given value of <span class="math inline">\(\theta\)</span>
is exactly <span class="math inline">\(1 - \alpha\)</span>. In the discrete case,
that probability is <span class="math inline">\(\geq 1 - \alpha\)</span>. (And it will rarely, if ever, be the
case that the probability masses within the acceptance region will sum to
exactly <span class="math inline">\(1 - \alpha\)</span>although that sum will trend towards <span class="math inline">\(1 - \alpha\)</span> as
the sample size and/or the number of trials increases. This motivates
our use of the word level, as in we conduct a level-<span class="math inline">\(\alpha\)</span> test,
rather than size. For a size-<span class="math inline">\(\alpha\)</span> test, the probability of
sampling a statistic value in the acceptance region is, by definition,
exactly <span class="math inline">\(1-\alpha\)</span>, while for a level-<span class="math inline">\(\alpha\)</span> test, it is <span class="math inline">\(\geq 1 - \alpha\)</span>.)</p></li>
<li><p>Because the probability of sampling a datum in the acceptance region is
equal to the confidence interval coverage, the coverage will also be
<span class="math inline">\(\geq 1 - \alpha\)</span>.</p></li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:disccicover"></span>
<img src="_main_files/figure-html/disccicover-1.png" alt="\label{fig:disccicover}An illustration of the relationship between hypothesis testing and confidence interval estimation for a discrete sampling distribution (here, a binomial distribution with $k=10$). The two green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value $p$. Assume $p = 0.45$: the red dots indicates the values of $y_{\rm obs}$ in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of $y_{\rm obs}$. Given $p$, the probability of sampling a statistic in the acceptance region is generally greater than $1-\alpha$, and thus the coverage of confidence intervals, which has the same value, will also be generally greater than $1-\alpha$." width="50%" />
<p class="caption">
Figure 3.8: An illustration of the relationship between hypothesis testing and confidence interval estimation for a discrete sampling distribution (here, a binomial distribution with <span class="math inline">\(k=10\)</span>). The two green lines define rejection-region boundaries for a two-sided test with null hypothesis parameter value <span class="math inline">\(p\)</span>. Assume <span class="math inline">\(p = 0.45\)</span>: the red dots indicates the values of <span class="math inline">\(y_{\rm obs}\)</span> in the acceptance region (where we fail to reject the null). The horizontal dashed blue line shows the confidence interval constructed for a given value of <span class="math inline">\(y_{\rm obs}\)</span>. Given <span class="math inline">\(p\)</span>, the probability of sampling a statistic in the acceptance region is generally greater than <span class="math inline">\(1-\alpha\)</span>, and thus the coverage of confidence intervals, which has the same value, will also be generally greater than <span class="math inline">\(1-\alpha\)</span>.
</p>
</div>
<p>The blue dashed line in Figure <a href="the-binomial-and-related-distributions.html#fig:disccicover">3.8</a> is an example of
a confidence interval (specifically for <span class="math inline">\(y_{\rm obs} = 4\)</span>). In order to
construct an interval like this, we would use a <code>uniroot()</code>-style code
as we have before, but with the following changes.</p>
<ul>
<li>If we are determining a <em>lower</em> bound <span class="math inline">\(\hat{\theta}_L\)</span> in a situation where <span class="math inline">\(E[Y]\)</span> <em>increases</em> with <span class="math inline">\(\theta\)</span>, we need to replace <span class="math inline">\(y_{\rm obs}\)</span> in the input to <code>uniroot()</code> with the next smaller value in the distributions domain (e.g., <span class="math inline">\(y_{\rm obs} - 1\)</span> for a binomially distributed statistic).</li>
<li>If we are determining an <em>upper</em> bound <span class="math inline">\(\hat{\theta}_U\)</span> in a situation where <span class="math inline">\(E[Y]\)</span> <em>decreases</em> with <span class="math inline">\(\theta\)</span>, we need to replace <span class="math inline">\(y_{\rm obs}\)</span> in the input to <code>uniroot()</code> with the next smaller value in the distributions domain (e.g., <span class="math inline">\(y_{\rm obs} - 1\)</span> for a negative binomially distributed statistic).</li>
</ul>
<p>We note that
for the specific case of the binomial distribution, the confidence intervals
that we construct are dubbed <em>exact</em>, or <em>Clopper-Pearson</em>, intervals.
Because binomial distributions have
historically been difficult to work with analytically,
a number of algorithms have been developed through the years for
constructing approximate confidence intervals for binomial probabilities.
(See, e.g., <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">this Wikipedia page</a> for examples.)
It is our opinion that there is no reason to utilize <em>any</em> of
these algorithms when one can compute exact intervals, particularly since
the coverages of exact intervals are easily derived for any given value <span class="math inline">\(p\)</span>.
However, for completeness, we illustrate how one would construct
the most-commonly used approximating interval, the <em>Wald interval</em>,
in an example below.</p>
<hr />
<div id="confidence-interval-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Confidence Interval for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume that we sample <span class="math inline">\(n\)</span> iid data from
a binomial distribution with number of trials <span class="math inline">\(k\)</span> and probability <span class="math inline">\(p\)</span>.
Then, as shown above, <span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Binom(<span class="math inline">\(nk,p\)</span>);
our observed test statistic is <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i\)</span>.
For this statistic, <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, so
<span class="math inline">\(q = 1-\alpha/2\)</span> maps to the lower bound, while <span class="math inline">\(q = \alpha/2\)</span> maps to
the upper bound.</p>
</blockquote>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="the-binomial-and-related-distributions.html#cb170-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb170-2"><a href="the-binomial-and-related-distributions.html#cb170-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb170-3"><a href="the-binomial-and-related-distributions.html#cb170-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb170-4"><a href="the-binomial-and-related-distributions.html#cb170-4" tabindex="-1"></a>k     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb170-5"><a href="the-binomial-and-related-distributions.html#cb170-5" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb170-6"><a href="the-binomial-and-related-distributions.html#cb170-6" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="at">size=</span>k,<span class="at">prob=</span>p)</span>
<span id="cb170-7"><a href="the-binomial-and-related-distributions.html#cb170-7" tabindex="-1"></a></span>
<span id="cb170-8"><a href="the-binomial-and-related-distributions.html#cb170-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(p,y.obs,n,k,q)</span>
<span id="cb170-9"><a href="the-binomial-and-related-distributions.html#cb170-9" tabindex="-1"></a>{</span>
<span id="cb170-10"><a href="the-binomial-and-related-distributions.html#cb170-10" tabindex="-1"></a>  <span class="fu">pbinom</span>(y.obs,<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p)<span class="sc">-</span>q</span>
<span id="cb170-11"><a href="the-binomial-and-related-distributions.html#cb170-11" tabindex="-1"></a>}</span>
<span id="cb170-12"><a href="the-binomial-and-related-distributions.html#cb170-12" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X)<span class="sc">-</span><span class="dv">1</span>,n,k,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root <span class="co"># note correction</span></span></code></pre></div>
<pre><code>## [1] 0.2459379</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="the-binomial-and-related-distributions.html#cb172-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,k,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.5010387</code></pre>
<blockquote>
<p>We find that the interval is <span class="math inline">\([\hat{p}_L,\hat{p}_U] =
[0.246,0.501]\)</span>, which overlaps the true value of 0.4.
(See Figure <a href="the-binomial-and-related-distributions.html#fig:binci">3.9</a>.)
Note that, unlike in Chapter 2, the interval over which we
search for the root is [0,1], which is the range of possible
values for <span class="math inline">\(p\)</span>.</p>
</blockquote>
<blockquote>
<p>We can compute the coverage of this interval following the
prescription given above:</p>
</blockquote>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="the-binomial-and-related-distributions.html#cb174-1" tabindex="-1"></a>y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(alpha<span class="sc">/</span><span class="dv">2</span>,n<span class="sc">*</span>k,p)</span>
<span id="cb174-2"><a href="the-binomial-and-related-distributions.html#cb174-2" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,n<span class="sc">*</span>k,p)</span>
<span id="cb174-3"><a href="the-binomial-and-related-distributions.html#cb174-3" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(y.rr.lo<span class="sc">:</span>y.rr.hi,n<span class="sc">*</span>k,p))</span></code></pre></div>
<pre><code>## [1] 0.9646363</code></pre>
<blockquote>
<p>Due to the discreteness of the binomial distribution, the true coverage
of our two-sided intervals, in this particular circumstance,
is 96.5%, not 95%.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binci"></span>
<img src="_main_files/figure-html/binci-1.png" alt="\label{fig:binci}Probability mass functions for binomial distributions for which $n \cdot k= 12 \cdot 5 = 60$ and (left) $p=0.246$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.246$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" /><img src="_main_files/figure-html/binci-2.png" alt="\label{fig:binci}Probability mass functions for binomial distributions for which $n \cdot k= 12 \cdot 5 = 60$ and (left) $p=0.246$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.246$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" />
<p class="caption">
Figure 3.9: Probability mass functions for binomial distributions for which <span class="math inline">\(n \cdot k= 12 \cdot 5 = 60\)</span> and (left) <span class="math inline">\(p=0.246\)</span> and (right) <span class="math inline">\(p=0.501\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 22\)</span> successes and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.246\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 22\)</span>, while <span class="math inline">\(p=0.501\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 22\)</span>.
</p>
</div>
<hr />
</div>
<div id="confidence-interval-for-the-negative-binomial-success-probability" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Confidence Interval for the Negative Binomial Success Probability<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have performed <span class="math inline">\(n = 10\)</span> separate negative binomial
trials, each with a target number of successes <span class="math inline">\(s\)</span>,
and recorded the number of failures <span class="math inline">\(X_1,\ldots,X_{n}\)</span> for
each. Further, assume that the probability of success is <span class="math inline">\(p\)</span>. Below we will
show how to compute the confidence interval for <span class="math inline">\(p\)</span>, but before we start,
we recall that <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is a negative binomially distributed
random variable for <span class="math inline">\(ns\)</span> successes and probability of success <span class="math inline">\(p\)</span>.
Here, <span class="math inline">\(E[Y] = s(1-p)/p\)</span>as <span class="math inline">\(p\)</span> increases, <span class="math inline">\(E[Y]\)</span> <em>decreases</em>. Thus
when we adapt the confidence interval code we use for binomially
distributed data, we need to switch the mapping of <span class="math inline">\(q = 1-\alpha/2\)</span> and
<span class="math inline">\(q = \alpha/2\)</span> to the <em>upper</em> and <em>lower</em> bounds, respectively.</p>
</blockquote>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="the-binomial-and-related-distributions.html#cb176-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb176-2"><a href="the-binomial-and-related-distributions.html#cb176-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb176-3"><a href="the-binomial-and-related-distributions.html#cb176-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb176-4"><a href="the-binomial-and-related-distributions.html#cb176-4" tabindex="-1"></a>s     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb176-5"><a href="the-binomial-and-related-distributions.html#cb176-5" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb176-6"><a href="the-binomial-and-related-distributions.html#cb176-6" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rnbinom</span>(n,<span class="at">size=</span>s,<span class="at">prob=</span>p)</span>
<span id="cb176-7"><a href="the-binomial-and-related-distributions.html#cb176-7" tabindex="-1"></a></span>
<span id="cb176-8"><a href="the-binomial-and-related-distributions.html#cb176-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(p,y.obs,n,s,q)</span>
<span id="cb176-9"><a href="the-binomial-and-related-distributions.html#cb176-9" tabindex="-1"></a>{     </span>
<span id="cb176-10"><a href="the-binomial-and-related-distributions.html#cb176-10" tabindex="-1"></a>  <span class="fu">pnbinom</span>(y.obs,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p)<span class="sc">-</span>q</span>
<span id="cb176-11"><a href="the-binomial-and-related-distributions.html#cb176-11" tabindex="-1"></a>}     </span>
<span id="cb176-12"><a href="the-binomial-and-related-distributions.html#cb176-12" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,s,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.3255305</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="the-binomial-and-related-distributions.html#cb178-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X)<span class="sc">-</span><span class="dv">1</span>,n,s,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root <span class="co"># note correction</span></span></code></pre></div>
<pre><code>## [1] 0.4853274</code></pre>
<blockquote>
<p>The confidence interval is <span class="math inline">\([\hat{p}_L,\hat{p}_U] = [0.326,0.485]\)</span>,
which overlaps the true value 0.4. We note that in the code, we change
the lower bound on the interval from 0 (in the binomial case) to
0.0001 (something suitably small but non-zero): a probability of success
of 0 maps to an infinite number of failures, which <code>R</code> cannot tolerate!</p>
</blockquote>
<blockquote>
<p>We can compute the coverage of this interval following the
prescription given above:</p>
</blockquote>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="the-binomial-and-related-distributions.html#cb180-1" tabindex="-1"></a>y.rr.lo <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,n<span class="sc">*</span>s,p)</span>
<span id="cb180-2"><a href="the-binomial-and-related-distributions.html#cb180-2" tabindex="-1"></a>y.rr.hi <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(alpha<span class="sc">/</span><span class="dv">2</span>,n<span class="sc">*</span>s,p)</span>
<span id="cb180-3"><a href="the-binomial-and-related-distributions.html#cb180-3" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dnbinom</span>(y.rr.lo<span class="sc">:</span>y.rr.hi,n<span class="sc">*</span>s,p))</span></code></pre></div>
<pre><code>## [1] 0.9510741</code></pre>
<blockquote>
<p>Due to the discreteness of the binomial distribution, the true coverage
of our two-sided intervals is 95.1%, not 95%.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbinci"></span>
<img src="_main_files/figure-html/nbinci-1.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions for which $n \cdot s = 12 \cdot 5 = 60$ and (left) $p=0.326$ and (right) $p=0.485$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.485$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" /><img src="_main_files/figure-html/nbinci-2.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions for which $n \cdot s = 12 \cdot 5 = 60$ and (left) $p=0.326$ and (right) $p=0.485$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.485$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" />
<p class="caption">
Figure 3.10: Probability mass functions for negative binomial distributions for which <span class="math inline">\(n \cdot s = 12 \cdot 5 = 60\)</span> and (left) <span class="math inline">\(p=0.326\)</span> and (right) <span class="math inline">\(p=0.485\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 88\)</span> failures and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.326\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 88\)</span>, while <span class="math inline">\(p=0.485\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 88\)</span>.
</p>
</div>
<hr />
</div>
<div id="wald-interval-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Wald Interval for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have sampled <span class="math inline">\(n\)</span> iid binomial variables with number
of trials <span class="math inline">\(k\)</span> and probability of success <span class="math inline">\(p\)</span>.
When <span class="math inline">\(k\)</span> is sufficiently large and <span class="math inline">\(p\)</span> is sufficiently
far from 0 or 1, we can assume that
<span class="math inline">\(\bar{X}\)</span> has a distribution whose shape is approximately
that of a normal distribution, with mean <span class="math inline">\(E[\bar{X}] = kp\)</span> and with
variance and standard error
<span class="math display">\[
V[\bar{X}] = \frac{kp(1-p)}{n} ~~~ \mbox{and} ~~~ se(\bar{X}) = \sqrt{V[\bar{X}]} = \sqrt{\frac{kp(1-p)}{n}} \,.
\]</span>
Furthermore, we can assume
that <span class="math inline">\(\hat{p} = \bar{X}/k\)</span> is approximately normally distributed, with
mean <span class="math inline">\(p\)</span>, variance <span class="math inline">\(V[\hat{p}] = V[\bar{X}]/k^2 = p(1-p)/nk\)</span>, and
standard error <span class="math inline">\(\sqrt{p(1-p)/nk}\)</span>. Given this information, it is simple to
express, e.g., an approximate two-sided <span class="math inline">\(100(1-\alpha)\)</span>% confidence
interval for <span class="math inline">\(p\)</span>:
<span class="math display">\[
\hat{p} \pm z_{1-\alpha/2} se(\hat{p}) ~~ \Rightarrow ~~ \left[ \hat{p} - z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \, , \, \hat{p} + z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \right] \,,
\]</span>
where <span class="math inline">\(z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)\)</span>. However, we dont
actually know the true value of <span class="math inline">\(p\)</span>so we plug in <span class="math inline">\(p = \hat{p}\)</span>.</p>
</blockquote>
<blockquote>
<p>This is the so-called <em>Wald interval</em>
that is typically provided to students in introductory statistics courses,
although it is typically provided assuming <span class="math inline">\(n = 1\)</span> and assuming that
<span class="math inline">\(\alpha = 0.05\)</span>:
<span class="math display">\[
\left[ \hat{p} - 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \, , \, \hat{p} + 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \right] \,,
\]</span>
where in this case <span class="math inline">\(\hat{p} = X/k\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the Wald interval for the situation provided in the first example
above?</p>
</blockquote>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="the-binomial-and-related-distributions.html#cb182-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb182-2"><a href="the-binomial-and-related-distributions.html#cb182-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb182-3"><a href="the-binomial-and-related-distributions.html#cb182-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb182-4"><a href="the-binomial-and-related-distributions.html#cb182-4" tabindex="-1"></a>k     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb182-5"><a href="the-binomial-and-related-distributions.html#cb182-5" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb182-6"><a href="the-binomial-and-related-distributions.html#cb182-6" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="at">size=</span>k,<span class="at">prob=</span>p)</span>
<span id="cb182-7"><a href="the-binomial-and-related-distributions.html#cb182-7" tabindex="-1"></a></span>
<span id="cb182-8"><a href="the-binomial-and-related-distributions.html#cb182-8" tabindex="-1"></a>z     <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb182-9"><a href="the-binomial-and-related-distributions.html#cb182-9" tabindex="-1"></a>p.hat <span class="ot">&lt;-</span> <span class="fu">sum</span>(X)<span class="sc">/</span>(n<span class="sc">*</span>k)</span>
<span id="cb182-10"><a href="the-binomial-and-related-distributions.html#cb182-10" tabindex="-1"></a></span>
<span id="cb182-11"><a href="the-binomial-and-related-distributions.html#cb182-11" tabindex="-1"></a>p.hat <span class="sc">-</span> z<span class="sc">*</span><span class="fu">sqrt</span>(p.hat<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p.hat)<span class="sc">/</span>(n<span class="sc">*</span>k))</span></code></pre></div>
<pre><code>## [1] 0.2447328</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="the-binomial-and-related-distributions.html#cb184-1" tabindex="-1"></a>p.hat <span class="sc">+</span> z<span class="sc">*</span><span class="fu">sqrt</span>(p.hat<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>p.hat)<span class="sc">/</span>(n<span class="sc">*</span>k))</span></code></pre></div>
<pre><code>## [1] 0.4886005</code></pre>
<blockquote>
<p>We can compare these values to those generated in
the first example: <span class="math inline">\([0.236,0.501]\)</span>. Here, the interval is smaller
relative to the Clopper-Pearson interval, and thus the coverage for
<span class="math inline">\(p = 0.4\)</span> (which we would have to estimate via simulation)
will be smaller as well.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-2" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Hypothesis Testing<a href="the-binomial-and-related-distributions.html#hypothesis-testing-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>In the previous chapter, we utilized <span class="math inline">\(\bar{X}\)</span> when testing hypotheses about the
normal mean <span class="math inline">\(\mu\)</span>. This is a principled choice for a test statistic<span class="math inline">\(-\)</span>after all,
<span class="math inline">\(\bar{X}\)</span> is the MLE for <span class="math inline">\(\mu-\)</span>but we do not yet know whether or not we can
choose a better one. Can we differentiate hypotheses more easily if we use a
test statistic other than <span class="math inline">\(\bar{X}\)</span>?</p>
<p>To help answer this question, we now introduce a method for
defining the <em>most powerful test</em> of a <em>simple</em> null hypothesis versus
a <em>simple</em> alternative hypothesis:
<span class="math display">\[
H_o : \theta = \theta_o ~~\mbox{and}~~ H_a : \theta = \theta_a \,.
\]</span>
Note that the word simple has a precise meaning here: it means that
when we set <span class="math inline">\(\theta\)</span> to a particular value,
we are completely fixing the shape and location of the
pmf or pdf from which data are sampled. If, for instance, we are dealing
with a normal distribution with unknown variance <span class="math inline">\(\sigma^2\)</span>, the hypothesis
<span class="math inline">\(\mu = \mu_o\)</span> would not be simple, since the width of the pdf
can still vary: the shape is not completely fixed.
(The hypothesis <span class="math inline">\(\mu = \mu_o\)</span> with variance unknown is dubbed a <em>composite</em>
hypothesis. We will examine how to work with composite hypotheses in the next chapter.)
For a given test level <span class="math inline">\(\alpha\)</span>, the <em>Neyman-Pearson lemma</em> states that the
test that maximizes the power has a rejection region of the form
<span class="math display">\[
\lambda_{NP} = \frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} &lt; c(\alpha) \,,
\]</span>
where <span class="math inline">\(c\)</span> is a constant whose value depends on <span class="math inline">\(\alpha\)</span> that we have to
determine. While this formulation initially
appears straightforward, it is in fact not necessarily clear how to
derive <span class="math inline">\(c(\alpha)\)</span>. However: when we work with distributions
that belong to the exponential family,
<em>we dont need to</em>! The NP lemma utilizes the likelihood function,
so in this situation it is implicitly telling us that <em>the best statistic for
differentiating between two simple hypotheses is a sufficient statistic</em>.
We simply determine a sufficient statistic and
and use its sampling distribution to define a hypothesis
test via the procedure we have laid out in previous chapters. Full stop.
(What if the distribution we are working with is not a member of
the exponential family? We would need to determine the sampling
distribution of <span class="math inline">\(\lambda_{NP}\)</span> itself, something most easily
done via the use of simulations. We illustrate this in an example below.)</p>
<p>For instance, when we draw <span class="math inline">\(n\)</span> iid data from a binomial distribution,
the sufficient statistic <span class="math inline">\(\sum_{i=1}^n X_i\)</span> has a known and easily
utilized sampling distribution<span class="math inline">\(-\)</span>namely, Binom(<span class="math inline">\(nk,p\)</span>)<span class="math inline">\(-\)</span>while
<span class="math inline">\(\bar{X} = (\sum_{i=1}^n X_i)/n\)</span> does not. So we would utilize
<span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>. (Note that it ultimately doesnt matter which
function of a sufficient statistic we use: if we can carry out the math,
we will end up with tests that have the same power and result in
the same <span class="math inline">\(p\)</span>-values. It would be <em>very</em> problematic if this wasnt
the case: wed have to fish around to determine which function of
a sufficient statistic would give us the best test, which clearly would
not be a good situation to find ourselves in.)</p>
<p>Lets assume that we use <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> to define, e.g.,
a lower-tail test <span class="math inline">\(H_o : p = p_o\)</span> versus <span class="math inline">\(H_a : p = p_a &lt; p_o\)</span>.
Since <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, we can go to the hypothesis test
reference tables and write (in code) that</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="the-binomial-and-related-distributions.html#cb186-1" tabindex="-1"></a>u.rr <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span>k,<span class="at">prob=</span>p.o)</span></code></pre></div>
<p>We see that our rejection region boundary depends on the value of <span class="math inline">\(p_o\)</span>,
but <em>not</em> on the value of <span class="math inline">\(p_a\)</span>. This means that the test we define above
is the most-powerful test regardless of the value <span class="math inline">\(p_a &lt; p_o\)</span>. We have
thus constructed a <em>uniformly most powerful</em> (or <em>UMP</em>) test for
disambiguating the simple hypotheses <span class="math inline">\(H_o : p = p_o\)</span> and <span class="math inline">\(H_a : p = p_a &lt; p_o\)</span>.
It is typically the case that when we use the NP lemma to define a most
powerful test for <span class="math inline">\(\theta_o\)</span> versus <span class="math inline">\(\theta_a\)</span>, we end up defining a
UMP test as well. (That is because the families of distributions that we
work with typically exhibit so-called <em>monotone likelihood ratios</em> [or MLRs].
For more information on MLRs and UMPs, the interested reader should
look at the Karlin-Rubin theorem.)</p>
<p>We note that we cannot use the NP lemma to construct most powerful
<em>two-tail</em> hypothesis tests. When we construct a two-tail test, it is
convention to define one rejection region boundary assuming <span class="math inline">\(q = \alpha/2\)</span>
and the other assuming <span class="math inline">\(q = 1 - \alpha/2\)</span>. But that is just convention;
we could put <span class="math inline">\(\alpha/10\)</span> on one side and <span class="math inline">\(1-9\alpha/10\)</span> on the other,
etc., and in general we cannot guarantee that any
one way of splitting <span class="math inline">\(\alpha\)</span> will yield a more powerful test in a given
situation than any other possible split.</p>
<hr />
<p>We conclude our present discussion of hypothesis tests with an overview of
how working with a discrete sampling distribution affects the computation of
<span class="math inline">\(p\)</span>-values and test power.</p>
<p>In the last section, we introduced the idea of a discreteness correction;
for confidence interval calculations, this involved changing the
observed statistic value to the next lower value in the domain of the
sampling distribution when estimating a lower interval bound (with
<span class="math inline">\(E[Y]\)</span> increasing with <span class="math inline">\(\theta\)</span>) or when estimating an upper interval bound
(with <span class="math inline">\(E[Y]\)</span> decreasing with <span class="math inline">\(\theta\)</span>). (For the binomial and negative
binomial distributions, this means changing <span class="math inline">\(y_{\rm obs}\)</span> to <span class="math inline">\(y_{\rm obs}-1\)</span>.)
What are the discreteness corrections that we need to make when performing
hypothesis tests? First, there are none when computing rejection-region
boundaries. Second, when computing <span class="math inline">\(p\)</span>-values, we change the
observed statistic value to the next lower value in the domain of the
sampling distribution when performing</p>
<ul>
<li>upper-tail tests where <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span> and</li>
<li>lower-tail tests where <span class="math inline">\(E[Y]\)</span> decreases with <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Last, when computing test power, we change the derived rejection-region
boundary to the next lower value in the domain of the
sampling distribution when performing</p>
<ul>
<li>upper-tail tests where <span class="math inline">\(E[Y]\)</span> decreases with <span class="math inline">\(\theta\)</span> and</li>
<li>lower-tail tests where <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span>.</li>
</ul>
<hr />
<div id="ump-test-exponential-distribution" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> UMP Test: Exponential Distribution<a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose we sample <span class="math inline">\(n = 3\)</span> iid data from the exponential distribution
<span class="math display">\[
f_X(x) = \frac{1}{\theta} e^{-x/\theta} \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and we wish to define the most powerful
test of the simple hypotheses <span class="math inline">\(H_o : \theta_o = 2\)</span> and
<span class="math inline">\(H_a : \theta_a = 1\)</span>. We observe the values <span class="math inline">\(\mathbf{x}_{\rm obs} =
\{0.215,1.131,2.064\}\)</span>, and we assume <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>We begin by determining a sufficient statistic:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^3 \frac{1}{\theta} e^{-x_i/\theta} = \frac{1}{\theta^3} e^{(\sum_{i=1}^3 x_i)/\theta} \,.
\]</span>
We identify <span class="math inline">\(Y = \sum_{i=1}^3 X_i\)</span> as a sufficient statistic. The next
question is whether we can determine the sampling distribution for <span class="math inline">\(Y\)</span>.
The moment-generating function for each of the <span class="math inline">\(X_i\)</span>s is
<span class="math display">\[
m_{X_i}(t) = (1 - \theta t)^{-1} \,,
\]</span>
and so the mgf for <span class="math inline">\(Y\)</span> is
<span class="math display">\[
m_Y(t) = \prod_{i=1}^3 m_{X_i}(t) = (1 - \theta t)^{-3} \,,
\]</span>
We (might!) recognize
this as the mgf for a Gamma(3,<span class="math inline">\(\theta\)</span>) distribution. (The gamma distribution
will be officially introduced in Chapter 4.) So: we know the sampling
distribution for <span class="math inline">\(Y\)</span>, and we can use it to define the most powerful test.
To reiterate: the only thing that the NP lemma is doing for us here is
guiding our selection of a test statistic. Beyond that, we construct the
test using the framework we already learned in Chapters 1 and 2.</p>
</blockquote>
<blockquote>
<p>Because <span class="math inline">\(\theta_a &lt; \theta_o\)</span>, we define a lower-tail test. And since
<span class="math inline">\(E[Y] = 3\theta\)</span> increases with <span class="math inline">\(\theta\)</span>, we utilize the formulae from the
hypothesis test reference tables that are on the yes line. The
rejection-region boundary is thus
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(\alpha \vert \theta_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="the-binomial-and-related-distributions.html#cb187-1" tabindex="-1"></a>x.obs   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.215</span>,<span class="fl">1.131</span>,<span class="fl">2.064</span>)</span>
<span id="cb187-2"><a href="the-binomial-and-related-distributions.html#cb187-2" tabindex="-1"></a>(y.obs  <span class="ot">&lt;-</span> <span class="fu">sum</span>(x.obs))</span></code></pre></div>
<pre><code>## [1] 3.41</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="the-binomial-and-related-distributions.html#cb189-1" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb189-2"><a href="the-binomial-and-related-distributions.html#cb189-2" tabindex="-1"></a>theta.o <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb189-3"><a href="the-binomial-and-related-distributions.html#cb189-3" tabindex="-1"></a>(y.rr <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(alpha,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta.o))</span></code></pre></div>
<pre><code>## [1] 1.635383</code></pre>
<blockquote>
<p>Our observed statistic is <span class="math inline">\(y_{\rm obs} = 3.410\)</span> and the rejection-region
boundary is 1.635: we fail to reject the null and conclude that
<span class="math inline">\(\theta_o = 2\)</span> is a plausible value of <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<blockquote>
<p>We note that because <span class="math inline">\(y_{\rm RR}\)</span> is not a function of <span class="math inline">\(\theta_a\)</span>, we have
not only defined the most powerful test, but we have also defined a
uniformly most powerful test for all alternative hypotheses
<span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the <span class="math inline">\(p\)</span>-value, and what is the power of the test if <span class="math inline">\(\theta = 1.5\)</span>?</p>
</blockquote>
<blockquote>
<p>According to the hypothesis test reference tables, the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[
p = F_Y(y_{\rm obs} \vert \theta_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="the-binomial-and-related-distributions.html#cb191-1" tabindex="-1"></a><span class="fu">pgamma</span>(y.obs,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta.o)</span></code></pre></div>
<pre><code>## [1] 0.2440973</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.244, which is greater than <span class="math inline">\(\alpha\)</span>, as we expect.</p>
</blockquote>
<blockquote>
<p>The test power is
<span class="math display">\[
{\rm power}(\theta) = F_Y(y_{\rm RR} \vert \theta) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="the-binomial-and-related-distributions.html#cb193-1" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb193-2"><a href="the-binomial-and-related-distributions.html#cb193-2" tabindex="-1"></a><span class="fu">pgamma</span>(y.rr,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta)</span></code></pre></div>
<pre><code>## [1] 0.09762911</code></pre>
<blockquote>
<p>The power is 0.097only 9.7% of the time will we reject the null hypothesis
<span class="math inline">\(\theta_o = 2\)</span> when <span class="math inline">\(\theta\)</span> is actually 1.5.</p>
</blockquote>
<hr />
</div>
<div id="ump-test-negative-binomial-distribution" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> UMP Test: Negative Binomial Distribution<a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we sample <span class="math inline">\(n = 5\)</span> data from a negative binomial
distribution
<span class="math display">\[
p_X(x) = \binom{x+s-1}{x} p^s (1-p)^x \,,
\]</span>
with <span class="math inline">\(x \in \{0,1,\ldots,\infty\}\)</span> being the observed number of failures
prior to observed the <span class="math inline">\(s^{\rm th}\)</span> success, <span class="math inline">\(p \in (0,1]\)</span>,
and <span class="math inline">\(s = 3\)</span> successes.
We wish to define the most powerful test of the simple hypotheses
<span class="math inline">\(H_o : p_o = 0.5\)</span> and <span class="math inline">\(H_a : p_a = 0.25\)</span>. We observe the values
<span class="math inline">\(\mathbf{x}_{\rm obs} = \{5,3,10,12,4\}\)</span>, and we assume <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>As in the last example, we begin by determining a sufficient statistic:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^5 \binom{x_i+s-1}{x_i} p^s (1-p)^x_i \propto \prod_{i=1}^5 p^s (1-p)^x_i = p^{ns} (1-p)^{\sum_{i=1}^5 x_i} \,.
\]</span>
We identify <span class="math inline">\(Y = \sum_{i=1}^5 X_i\)</span> as a sufficient statistic. Earlier
in the chapter, we determined that if the <span class="math inline">\(X_i\)</span>s are iid draws from
a negative binomial distribution with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(s\)</span>, then the
sum is also negative binomially distributed, with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(ns\)</span>.
So: we know the sampling
distribution for <span class="math inline">\(Y\)</span>, and we can use it to define the most powerful test.</p>
</blockquote>
<blockquote>
<p>Because <span class="math inline">\(p_a &lt; p_o\)</span>, we will define a lower-tail test. And since
<span class="math inline">\(E[Y] = s(1-p)/p\)</span> <em>decreases</em> with <span class="math inline">\(p\)</span>, we will utilize the formulae from
the hypothesis test reference tables that are on the no line (and we
will reject the null hypothesis if <span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR}\)</span>).</p>
</blockquote>
<blockquote>
<p>The rejection-region boundary is
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(1-\alpha \vert p_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="the-binomial-and-related-distributions.html#cb195-1" tabindex="-1"></a>x.obs  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">10</span>,<span class="dv">12</span>,<span class="dv">4</span>)</span>
<span id="cb195-2"><a href="the-binomial-and-related-distributions.html#cb195-2" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="fu">length</span>(x.obs)</span>
<span id="cb195-3"><a href="the-binomial-and-related-distributions.html#cb195-3" tabindex="-1"></a>(y.obs <span class="ot">&lt;-</span> <span class="fu">sum</span>(x.obs))</span></code></pre></div>
<pre><code>## [1] 34</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="the-binomial-and-related-distributions.html#cb197-1" tabindex="-1"></a>alpha  <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb197-2"><a href="the-binomial-and-related-distributions.html#cb197-2" tabindex="-1"></a>p.o    <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb197-3"><a href="the-binomial-and-related-distributions.html#cb197-3" tabindex="-1"></a>(y.rr  <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o))</span></code></pre></div>
<pre><code>## [1] 25</code></pre>
<blockquote>
<p>Our observed statistic is <span class="math inline">\(y_{\rm obs} = 34\)</span> and the rejection-region
boundary is 25: we reject the null hypothesis and state that there
is sufficient evidence to conclude that <span class="math inline">\(p &lt; 0.5\)</span>.</p>
</blockquote>
<blockquote>
<p>We note that because <span class="math inline">\(y_{\rm RR}\)</span> is not a function of <span class="math inline">\(p_a\)</span>, we have
not only defined the most powerful test, but we have also defined a
uniformly most powerful test for all alternative hypotheses <span class="math inline">\(p_a &lt; p_o\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the <span class="math inline">\(p\)</span>-value, and what is the power of the test if <span class="math inline">\(p = 0.3\)</span>?</p>
</blockquote>
<blockquote>
<p>According to the hypothesis test reference tables (and the description
of discreteness corrections given above), the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[
p = 1 - F_Y(y_{\rm obs}-1 \vert p_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="the-binomial-and-related-distributions.html#cb199-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(y.obs<span class="dv">-1</span>,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o)</span></code></pre></div>
<pre><code>## [1] 0.002757601</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.0028, which is less than <span class="math inline">\(\alpha\)</span>: we would decide to
reject the null hypothesis.</p>
</blockquote>
<blockquote>
<p>The test power (a calculation that in this instance does not require
a discreteness correction) is
<span class="math display">\[
{\rm power}(\theta) = 1 - F_Y(y_{\rm RR} \vert p) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="the-binomial-and-related-distributions.html#cb201-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb201-2"><a href="the-binomial-and-related-distributions.html#cb201-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(y.rr,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p)</span></code></pre></div>
<pre><code>## [1] 0.8074482</code></pre>
<blockquote>
<p>We find that
the power is 0.80780.7% of the time will we reject the null hypothesis
<span class="math inline">\(p = 0.5\)</span> when <span class="math inline">\(p\)</span> is actually 0.3 (and when <span class="math inline">\(n = 5\)</span> and <span class="math inline">\(s = 3\)</span>).</p>
</blockquote>
<hr />
</div>
<div id="defining-a-ump-test-for-the-normal-population-mean" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Defining a UMP Test for the Normal Population Mean<a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In Chapter 2, we use the statistic <span class="math inline">\(\bar{X}\)</span> as the basis for testing
hypotheses about the normal population mean, <span class="math inline">\(\mu\)</span>. We justified this
choice because, at the very least, <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>, so it
is a principled choice. However, beyond that, we did not (and indeed
could not) provide any further justification. Is <span class="math inline">\(\bar{X}\)</span> the basis
of the UMP for <span class="math inline">\(\mu\)</span>?</p>
</blockquote>
<blockquote>
<p>Recall that the NP lemma <em>does not apply</em> if there are
two freely varying parameters. The NP lemma applies to <em>simple hypotheses</em>,
where the hypotheses uniquely specify the population from which data are
drawn. Here, even if we set <span class="math inline">\(H_o: \mu = \mu_o\)</span> and <span class="math inline">\(H_a: \mu = \mu_a\)</span>,
<span class="math inline">\(\sigma^2\)</span> can still freely vary. (So, technically, these hypotheses
are <em>composite hypotheses</em>.) So
to go further here, we must assume <span class="math inline">\(\sigma^2\)</span> is known.</p>
</blockquote>
<blockquote>
<p>When <span class="math inline">\(\sigma^2\)</span> is known, we can factorize the likelihood as
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i,\mu)} \cdot \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)}_{h(\mathbf{x})} \,,
\]</span>
and identify <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> as a sufficient statistic.
We know from using the method of moment-generating
functions that the sum of <span class="math inline">\(n\)</span> iid normal random variables is itself a normal
random variable with mean <span class="math inline">\(n\mu\)</span> and variance <span class="math inline">\(n\sigma^2\)</span>, so
<span class="math inline">\(Y \sim \mathcal{N}(n\mu,n\sigma^2)\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets assume that <span class="math inline">\(\mu_a &gt; \mu_o\)</span>, or, in other words, that we are
performing an upper-tail test. Given that <span class="math inline">\(E[Y] = n\mu\)</span>, we know that
we are on the yes line of the hypothesis test reference tables, so
the rejection region is
<span class="math display">\[
Y &gt; y_{\rm RR} = F_Y^{-1}(1-\alpha,n\mu_o,n\sigma^2) \,,
\]</span>
or, in code,</p>
</blockquote>
<pre><code>qnorm(1-alpha,n*mu.o,n*sigma2)</code></pre>
<blockquote>
<p>The rejection-region boundary does not depend on <span class="math inline">\(\mu_a\)</span>, so
we have defined a uniformly most powerful test of <span class="math inline">\(\mu = \mu_o\)</span>
versus <span class="math inline">\(\mu = \mu_a &gt; \mu_o\)</span>.</p>
</blockquote>
<blockquote>
<p>But wait. What about <span class="math inline">\(\bar{X}\)</span>?</p>
</blockquote>
<blockquote>
<p>Recall that a function of a sufficient statistic
is itself a sufficient statistic, so we could also use
<span class="math inline">\(Y&#39; = \bar{X} = Y/n\)</span> as our statistic, particularly as we
<em>do</em> know its sampling distribution: <span class="math inline">\(\mathcal{N}(\mu,\sigma^2/n)\)</span>. Thus
<span class="math display">\[
Y&#39; &gt; y_{\rm RR}&#39; = F_Y^{-1}(1-\alpha,\mu_o,\sigma^2/n) \,,
\]</span>
or, in code,</p>
</blockquote>
<pre><code>qnorm(1-alpha,mu.o,sigma2/n)</code></pre>
<blockquote>
<p>But<span class="math inline">\(y_{\rm RR}\)</span> and <span class="math inline">\(y_{\rm RR}&#39;\)</span> do not have the same value!</p>
</blockquote>
<blockquote>
<p>This is fine: <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y&#39;\)</span> dont have the same value either. The key
point is that when we compute, e.g., <span class="math inline">\(p\)</span>-values given observed data,
they will be the same in both cases. And the test power will be the
same. We can use any function of a sufficient statistic to define our
test; in practice, we will use any function of a sufficient statistic
for which we know the sampling distribution. Here we know the distributions
for both the sample sum and the sample mean;
in other situations (like when we are working with
binomially distributed data), we will not.</p>
</blockquote>
<blockquote>
<p>Now, what if <span class="math inline">\(\sigma^2\)</span> is unknown? We discuss this possibility
in the next chapter, when we introduce the likelihood ratio test.</p>
</blockquote>
<hr />
</div>
<div id="defining-a-test-that-is-not-uniformly-most-powerful" class="section level3 hasAnchor" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> Defining a Test That is Not Uniformly Most Powerful<a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!--
> people.bath.ac.uk/masmah/MA40092.bho/sols7.pdf
-->
<blockquote>
<p>Lets assume we draw one datum <span class="math inline">\(X\)</span> from a normal distribution with
mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\mu^2\)</span>, and we wish to test <span class="math inline">\(H_o : \mu = \mu_o = 1\)</span>
versus <span class="math inline">\(H_a : \mu = \mu_a\)</span>. Can we define a uniformly most powerful test
of these hypotheses?</p>
</blockquote>
<blockquote>
<p>To try to do this, we utilize the NP lemma. However, there is an immediate
issue that arises: here, there are <em>two</em> sufficient statistics that are
identified via factorization (<span class="math inline">\(\sum_{i=1}^n X_i\)</span> and <span class="math inline">\(\sum_{i=1}^n X_i^2\)</span>),
but just one parameter (<span class="math inline">\(\mu\)</span>). So we cannot circumvent working directly
with the likelihood ratio:
<span class="math display">\[\begin{align*}
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} = \frac{f_X(x \vert \theta_o)}{f_X(x \vert \theta_a)} = \mu_a \exp\left(-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2}\right) \,.
\end{align*}\]</span>
To reject the null hypothesis, the ratio must be less than some constant
<span class="math inline">\(c(\alpha)\)</span>. First, we can divide both sides of the inequality by
<span class="math inline">\(\mu_a\)</span> (which is a set constant), so that now we reject the null if
<span class="math display">\[\begin{align*}
\exp\left(-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2}\right) &lt; c&#39;(\alpha) \,.
\end{align*}\]</span>
We then take the natural logarithm of each side; we would reject the null if
<span class="math display">\[\begin{align*}
-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2} &lt; c&#39;&#39;(\alpha) \,.
\end{align*}\]</span>
It turns out that for
the test to be uniformly most powerful, we would have to perform
further manipulations so as to isolate <span class="math inline">\(x\)</span> on the left-hand side of the
inequality, with only constants appearing on the right-hand side
(i.e., we would need to simplify this expression such that it achieves
the form <span class="math inline">\(x &lt; k\)</span> or <span class="math inline">\(x &gt; k\)</span>). Here,
that is impossible to do, meaning that the value of the
rejection region boundary changes as <span class="math inline">\(\mu_a\)</span> changes. Thus while we can
define the most powerful test of <span class="math inline">\(H_o : \mu = \mu_o = 1\)</span>
versus <span class="math inline">\(H_a : \mu = \mu_a\)</span>, that test is <em>not</em> a uniformly most powerful one.</p>
</blockquote>
<hr />
</div>
<div id="estimating-a-p-value-via-simulation" class="section level3 hasAnchor" number="3.8.5">
<h3><span class="header-section-number">3.8.5</span> Estimating a <span class="math inline">\(p\)</span>-Value via Simulation<a href="the-binomial-and-related-distributions.html#estimating-a-p-value-via-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we sample <span class="math inline">\(n\)</span> iid data from a distribution whose
probability density function is
<span class="math display">\[\begin{align*}
f_X(x \vert \theta) = \theta x^{\theta-1} \,,
\end{align*}\]</span>
where <span class="math inline">\(x \in [0,1]\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Furthermore,
lets assume we wish to test <span class="math inline">\(H_o : \theta = \theta_o = 1\)</span> versus
<span class="math inline">\(H_a : \theta = \theta_a = 2\)</span>.</p>
</blockquote>
<blockquote>
<p>The Neyman-Pearson lemma tells us that a sufficient statistic will provide
the basis for the most powerful test. Likelihood factorization indicates
that one such statistic is
<span class="math display">\[\begin{align*}
Y = \prod_{i=1}^n X_i \,,
\end{align*}\]</span>
but, since functions of sufficient statistics are themselves sufficient, we
could also utilize
<span class="math display">\[\begin{align*}
Y = \log \prod_{i=1}^n X_i = \sum_{i=1}^n \log X_i \,.
\end{align*}\]</span>
We will choose to utilize <span class="math inline">\(Y&#39;\)</span> because in general, summations are
computationally easier to work with than products.
Howeverwe do not know the sampling distribution for <span class="math inline">\(Y\)</span>. So we simulate.
In a simulation, we do the following:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>we sample data from the original distribution given the null;</li>
<li>we form the statistic and store its value;</li>
<li>we repeat steps 1 and 2 a large number of times; and</li>
<li>we see how often the simulated statistic value is (in this case) greater
than the observed valuethat proportion is our estimated <span class="math inline">\(p\)</span>-value.</li>
</ol>
<blockquote>
<p>Below we carry out a simulation of 100,000 datasets where the true
value of <span class="math inline">\(\theta\)</span> is set (arbitrarily) to 1.75.</p>
</blockquote>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="the-binomial-and-related-distributions.html#cb205-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">236</span>)</span>
<span id="cb205-2"><a href="the-binomial-and-related-distributions.html#cb205-2" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb205-3"><a href="the-binomial-and-related-distributions.html#cb205-3" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb205-4"><a href="the-binomial-and-related-distributions.html#cb205-4" tabindex="-1"></a>theta   <span class="ot">&lt;-</span> <span class="fl">1.75</span>                                <span class="co"># arbitrarily chosen true value</span></span>
<span id="cb205-5"><a href="the-binomial-and-related-distributions.html#cb205-5" tabindex="-1"></a>X.obs   <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n,<span class="at">shape1=</span>theta<span class="sc">+</span><span class="dv">1</span>,<span class="at">shape2=</span><span class="dv">1</span>)</span>
<span id="cb205-6"><a href="the-binomial-and-related-distributions.html#cb205-6" tabindex="-1"></a>y.obs   <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(X.obs))</span>
<span id="cb205-7"><a href="the-binomial-and-related-distributions.html#cb205-7" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The observed statistic is &quot;</span>,y.obs,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The observed statistic is  -4.758327</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="the-binomial-and-related-distributions.html#cb207-1" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">*</span>num.sim)                    <span class="co"># the null is equivalent to Uniform(0,1)</span></span>
<span id="cb207-2"><a href="the-binomial-and-related-distributions.html#cb207-2" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">matrix</span>(X,<span class="at">nrow=</span>num.sim)              <span class="co"># each row a dataset of size n</span></span>
<span id="cb207-3"><a href="the-binomial-and-related-distributions.html#cb207-3" tabindex="-1"></a>Y       <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,<span class="cf">function</span>(x){<span class="fu">sum</span>(<span class="fu">log</span>(x))}) <span class="co"># the simulated statistics</span></span>
<span id="cb207-4"><a href="the-binomial-and-related-distributions.html#cb207-4" tabindex="-1"></a>p       <span class="ot">&lt;-</span> <span class="fu">sum</span>(Y<span class="sc">&gt;=</span>y.obs)<span class="sc">/</span>num.sim               <span class="co"># the p-value</span></span>
<span id="cb207-5"><a href="the-binomial-and-related-distributions.html#cb207-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The estimated p-value is &quot;</span>,p,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The estimated p-value is  0.02413</code></pre>
<blockquote>
<p>We can visualize the empirical sampling distribution using a histogram.
See Figure <a href="the-binomial-and-related-distributions.html#fig:cisim">3.11</a>, in which the <span class="math inline">\(p\)</span>-value is the area under the
curve to the right of the observed statistic value (the vertical
red line). To ensure that the <span class="math inline">\(p\)</span>-value estimate is
as precise as possible, we should run as many simulations as our
computer and our time will allow!</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cisim"></span>
<img src="_main_files/figure-html/cisim-1.png" alt="\label{fig:cisim}The empirical sampling distribution for the statistic $Y = \sum_{i=1}^n \log X_i$, determined by simulating 100,000 datasets drawn from the distribution $f_X(x \vert \theta) = \theta x^{\theta-1}$. Here, $n = 10$ and $\theta = 1.75$. The observed statistic value, $y_{\rm obs}$, is indicated by the vertical red line, and the estimated $p$-value is the proportion of simulation statistic values falling to the right of the line." width="50%" />
<p class="caption">
Figure 3.11: The empirical sampling distribution for the statistic <span class="math inline">\(Y = \sum_{i=1}^n \log X_i\)</span>, determined by simulating 100,000 datasets drawn from the distribution <span class="math inline">\(f_X(x \vert \theta) = \theta x^{\theta-1}\)</span>. Here, <span class="math inline">\(n = 10\)</span> and <span class="math inline">\(\theta = 1.75\)</span>. The observed statistic value, <span class="math inline">\(y_{\rm obs}\)</span>, is indicated by the vertical red line, and the estimated <span class="math inline">\(p\)</span>-value is the proportion of simulation statistic values falling to the right of the line.
</p>
</div>
<blockquote>
<p>The estimated <span class="math inline">\(p\)</span>-value is a point estimate. We can add to this estimate
by constructing a confidence interval on the unknown true <span class="math inline">\(p\)</span>-value.
We can do this because our simulation is like a binomial experiment, in which</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>the number of trials <span class="math inline">\(k\)</span> is the number of simulations (<code>num.sim</code>); and</li>
<li>the probability of success <span class="math inline">\(p\)</span> is the probability of sampling a value of <span class="math inline">\(Y \geq y_{\rm obs}\)</span> (given that this is an upper-tail test with <span class="math inline">\(E[Y]\)</span> increasing with <span class="math inline">\(\theta\)</span>).</li>
</ol>
<blockquote>
<p>In the code below, we denote the observed statistic (here, the number of
times we observe <span class="math inline">\(Y \geq y_{\rm obs}\)</span>) as <code>u.obs</code>.</p>
</blockquote>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="the-binomial-and-related-distributions.html#cb209-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(p,k,u.obs,q)</span>
<span id="cb209-2"><a href="the-binomial-and-related-distributions.html#cb209-2" tabindex="-1"></a>{</span>
<span id="cb209-3"><a href="the-binomial-and-related-distributions.html#cb209-3" tabindex="-1"></a>  <span class="fu">pbinom</span>(u.obs,<span class="at">size=</span>k,<span class="at">prob=</span>p) <span class="sc">-</span> q</span>
<span id="cb209-4"><a href="the-binomial-and-related-distributions.html#cb209-4" tabindex="-1"></a>}</span>
<span id="cb209-5"><a href="the-binomial-and-related-distributions.html#cb209-5" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">k=</span>num.sim,<span class="at">u.obs=</span><span class="fu">sum</span>(Y<span class="sc">&gt;=</span>y.obs)<span class="sc">-</span><span class="dv">1</span>,<span class="at">q=</span><span class="fl">0.975</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.02320476</code></pre>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="the-binomial-and-related-distributions.html#cb211-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">k=</span>num.sim,<span class="at">u.obs=</span><span class="fu">sum</span>(Y<span class="sc">&gt;=</span>y.obs),<span class="at">q=</span><span class="fl">0.025</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.02511073</code></pre>
<blockquote>
<p>(Note how we apply a discreteness correction when computing the lower bound.)
Our estimated 95% confidence interval for the true <span class="math inline">\(p\)</span>-value is
[0.0232,0.0251]; we can feel confident when we decide, in this situation,
to reject the null hypothesis that <span class="math inline">\(\theta = \theta_o = 1\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="utilizing-simulations-when-data-reduction-is-not-possible" class="section level3 hasAnchor" number="3.8.6">
<h3><span class="header-section-number">3.8.6</span> Utilizing Simulations When Data Reduction is Not Possible<a href="the-binomial-and-related-distributions.html#utilizing-simulations-when-data-reduction-is-not-possible" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The logistic distribution, whose probability density function is
<span class="math display">\[\begin{align*}
f_X(x \vert \mu) = \frac{\exp(\mu-x)}{[1+\exp(\mu-x)]^2}
\end{align*}\]</span>
for <span class="math inline">\(x \in (-\infty,\infty)\)</span> and <span class="math inline">\(\mu \in (-\infty,\infty)\)</span>, is <em>not</em>
a member of the exponential family of distributions. This means that
if we sample <span class="math inline">\(n\)</span> iid data according to this distribution,
we cannot reduce them so as to define a single-number sufficient statistic
for <span class="math inline">\(\mu\)</span>. What we <em>can</em> do, however, is the following:</p>
</blockquote>
<blockquote>
<ul>
<li>we can simulate a large number of datasets under the null hypothesis <span class="math inline">\(\mu = \mu_o\)</span>;</li>
<li>for each dataset, we can record <span class="math inline">\(\mathcal{L}(\mu_o \vert \mathbf{x})\)</span> and <span class="math inline">\(\mathcal{L}(\mu_a \vert \mathbf{x})\)</span>, and their difference (which is <span class="math inline">\(\log \lambda_{NP}\)</span>); and</li>
<li>we can determine the <span class="math inline">\(\alpha^{\rm th}\)</span> percentile of the distribution of <span class="math inline">\(\log \lambda_{NP}\)</span> values.</li>
</ul>
</blockquote>
<blockquote>
<p>The <span class="math inline">\(\alpha^{\rm th}\)</span> percentile is <span class="math inline">\(c&#39;(\alpha) = \log c(\alpha)\)</span>, i.e., it
is the rejection-region boundary for the most powerful test of
the simple hypotheses <span class="math inline">\(\mu = \mu_o\)</span> and <span class="math inline">\(\mu = \mu_a\)</span>.
(However, note that because <span class="math inline">\(\mu_a\)</span> factors directly into the simulations,
what we define is <em>not</em> a uniformly most powerful test, which is to say
as we change <span class="math inline">\(\mu_a\)</span>, the value of <span class="math inline">\(c&#39;(\alpha)\)</span> will also change, unlike
the rejection-region boundaries that we derive in single-value sufficient
statistic/exponential family distribution contexts.</p>
</blockquote>
<blockquote>
<p>Below, we show how to carry out simulations to determine the
rejection-region boundary, the <span class="math inline">\(p\)</span>-value, and the test power
for a situation in which we
sample <span class="math inline">\(n = 10\)</span> iid data according to a logistic
distribution for which <span class="math inline">\(\mu_o = 1\)</span>, with our alternative hypothesis
being <span class="math inline">\(\mu_a = 3\)</span>. (See Figure <a href="the-binomial-and-related-distributions.html#fig:lognp">3.12</a>.)</p>
</blockquote>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="the-binomial-and-related-distributions.html#cb213-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">36236</span>)</span>
<span id="cb213-2"><a href="the-binomial-and-related-distributions.html#cb213-2" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb213-3"><a href="the-binomial-and-related-distributions.html#cb213-3" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb213-4"><a href="the-binomial-and-related-distributions.html#cb213-4" tabindex="-1"></a>n       <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb213-5"><a href="the-binomial-and-related-distributions.html#cb213-5" tabindex="-1"></a>mu.o    <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb213-6"><a href="the-binomial-and-related-distributions.html#cb213-6" tabindex="-1"></a>mu.a    <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb213-7"><a href="the-binomial-and-related-distributions.html#cb213-7" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rlogis</span>(n<span class="sc">*</span>num.sim,<span class="at">location=</span>mu.o),<span class="at">nrow=</span>num.sim)</span>
<span id="cb213-8"><a href="the-binomial-and-related-distributions.html#cb213-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x,mu.o)</span>
<span id="cb213-9"><a href="the-binomial-and-related-distributions.html#cb213-9" tabindex="-1"></a>{</span>
<span id="cb213-10"><a href="the-binomial-and-related-distributions.html#cb213-10" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">dlogis</span>(x,<span class="at">location=</span>mu.o)))</span>
<span id="cb213-11"><a href="the-binomial-and-related-distributions.html#cb213-11" tabindex="-1"></a>}</span>
<span id="cb213-12"><a href="the-binomial-and-related-distributions.html#cb213-12" tabindex="-1"></a>log.like.o     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,f,<span class="at">mu.o=</span>mu.o)</span>
<span id="cb213-13"><a href="the-binomial-and-related-distributions.html#cb213-13" tabindex="-1"></a>log.like.a     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,f,<span class="at">mu.o=</span>mu.a)</span>
<span id="cb213-14"><a href="the-binomial-and-related-distributions.html#cb213-14" tabindex="-1"></a>delta.log.like <span class="ot">&lt;-</span> log.like.o <span class="sc">-</span> log.like.a</span>
<span id="cb213-15"><a href="the-binomial-and-related-distributions.html#cb213-15" tabindex="-1"></a></span>
<span id="cb213-16"><a href="the-binomial-and-related-distributions.html#cb213-16" tabindex="-1"></a><span class="co"># The rejection-region boundary</span></span>
<span id="cb213-17"><a href="the-binomial-and-related-distributions.html#cb213-17" tabindex="-1"></a>rr <span class="ot">&lt;-</span> <span class="fu">quantile</span>(delta.log.like,<span class="at">probs=</span>alpha)</span>
<span id="cb213-18"><a href="the-binomial-and-related-distributions.html#cb213-18" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The rejection-region boundary is&quot;</span>,<span class="fu">round</span>(rr,<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The rejection-region boundary is -1.362</code></pre>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="the-binomial-and-related-distributions.html#cb215-1" tabindex="-1"></a><span class="co"># The p-value for a dataset sampled under the null</span></span>
<span id="cb215-2"><a href="the-binomial-and-related-distributions.html#cb215-2" tabindex="-1"></a>X                  <span class="ot">&lt;-</span> <span class="fu">rlogis</span>(n,<span class="at">location=</span>mu.o)</span>
<span id="cb215-3"><a href="the-binomial-and-related-distributions.html#cb215-3" tabindex="-1"></a>delta.log.like.obs <span class="ot">&lt;-</span> <span class="fu">f</span>(X,mu.o) <span class="sc">-</span> <span class="fu">f</span>(X,mu.a)</span>
<span id="cb215-4"><a href="the-binomial-and-related-distributions.html#cb215-4" tabindex="-1"></a>p                  <span class="ot">&lt;-</span> <span class="fu">sum</span>(delta.log.like<span class="sc">&lt;=</span>delta.log.like.obs)<span class="sc">/</span>num.sim</span>
<span id="cb215-5"><a href="the-binomial-and-related-distributions.html#cb215-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The p-value for the statistic&quot;</span>,<span class="fu">round</span>(delta.log.like.obs,<span class="dv">3</span>),<span class="st">&quot;is&quot;</span>,<span class="fu">round</span>(p,<span class="dv">4</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The p-value for the statistic 0.834 is 0.3226</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="the-binomial-and-related-distributions.html#cb217-1" tabindex="-1"></a><span class="co"># The test power for mu = mu.a</span></span>
<span id="cb217-2"><a href="the-binomial-and-related-distributions.html#cb217-2" tabindex="-1"></a>X                <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rlogis</span>(n<span class="sc">*</span>num.sim,<span class="at">location=</span>mu.a),<span class="at">nrow=</span>num.sim)</span>
<span id="cb217-3"><a href="the-binomial-and-related-distributions.html#cb217-3" tabindex="-1"></a>log.like.o.p     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,f,<span class="at">mu.o=</span>mu.o)</span>
<span id="cb217-4"><a href="the-binomial-and-related-distributions.html#cb217-4" tabindex="-1"></a>log.like.a.p     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">1</span>,f,<span class="at">mu.o=</span>mu.a)</span>
<span id="cb217-5"><a href="the-binomial-and-related-distributions.html#cb217-5" tabindex="-1"></a>delta.log.like.p <span class="ot">&lt;-</span> log.like.o.p <span class="sc">-</span> log.like.a.p</span>
<span id="cb217-6"><a href="the-binomial-and-related-distributions.html#cb217-6" tabindex="-1"></a>power            <span class="ot">&lt;-</span> <span class="fu">sum</span>(delta.log.like.p<span class="sc">&lt;</span>rr)<span class="sc">/</span>num.sim</span>
<span id="cb217-7"><a href="the-binomial-and-related-distributions.html#cb217-7" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The test power for mu =&quot;</span>,mu.a,<span class="st">&quot;is&quot;</span>,<span class="fu">round</span>(power,<span class="dv">3</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The test power for mu = 2 is 0.568</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lognp"></span>
<img src="_main_files/figure-html/lognp-1.png" alt="\label{fig:lognp}The empirical sampling distribution for the statistic $Y = \log\lambda_{NP}$, determined by simulating 100,000 datasets of size $n = 10$ drawn from a logistic distribution with mean $\mu_o = 1$. The estimated rejection-region boundary $\log c(\alpha)$ is indicated by the vertical red line; if the $y_{\rm obs} &lt; \log c(\alpha)$ is less than the boundary value, we would reject the null hypothesis that $\mu_o = 1$." width="50%" />
<p class="caption">
Figure 3.12: The empirical sampling distribution for the statistic <span class="math inline">\(Y = \log\lambda_{NP}\)</span>, determined by simulating 100,000 datasets of size <span class="math inline">\(n = 10\)</span> drawn from a logistic distribution with mean <span class="math inline">\(\mu_o = 1\)</span>. The estimated rejection-region boundary <span class="math inline">\(\log c(\alpha)\)</span> is indicated by the vertical red line; if the <span class="math inline">\(y_{\rm obs} &lt; \log c(\alpha)\)</span> is less than the boundary value, we would reject the null hypothesis that <span class="math inline">\(\mu_o = 1\)</span>.
</p>
</div>
<hr />
</div>
<div id="large-sample-tests-of-the-binomial-success-probability" class="section level3 hasAnchor" number="3.8.7">
<h3><span class="header-section-number">3.8.7</span> Large-Sample Tests of the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Earlier in the chapter, we saw that if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,,
\]</span>
then, given a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>), we can assume
<span class="math display">\[
X \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(kp,kp(1-p)) \,,
\]</span>
or that
<span class="math display">\[
\hat{p} = \hat{p}_{MLE} = \frac{X}{k} \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}(p,p(1-p)/k) \,.
\]</span>
In hypothesis testing, this approximation forms the basis of two
different tests:
<span class="math display">\[\begin{align*}
\mbox{Score}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,p_o(1-p_o)/k) \\
\mbox{Wald}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,\hat{p}(1-\hat{p})/k) \,.
\end{align*}\]</span>
The only difference between the two is in how the variance is estimated
under the null. To carry out these tests, we can simply adapt the
expressions for the rejection regions, <span class="math inline">\(p\)</span>-values, and power given in
Chapter 2 for tests of the normal population mean with <em>variance known</em>,
with the primary change being the definition of the variance.</p>
</blockquote>
<blockquote>
<p>We are including the details of the Wald and Score tests here for
completeness, in that they are ubiquitous in introductory statistics
settings. As they yield only approximate results, we would encourage
the reader to always use the exact testing mechanisms described above,
while being mindful of the effects of the discreteness of the probability
mass function, especially when <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are both small.</p>
</blockquote>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Logistic Regression<a href="the-binomial-and-related-distributions.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the last chapter, we introduced simple linear regression,
in which we model the data-generating process as
<span class="math display">\[
Y_i = \beta_0 + \beta_1x_i + \epsilon_i \,.
\]</span>
To perform hypothesis testing
(e.g., <span class="math inline">\(H_o : \beta_1 = 0\)</span> vs.<span class="math inline">\(H_a : \beta_1 \neq 0\)</span>), we make the
assumption that <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>,
or equivalently, that <span class="math inline">\(Y_i \vert x_i \sim \mathcal{N}(\beta_0+\beta_1x_i,\sigma^2)\)</span>.
When we make this assumption, we are implicitly stating that the response
variable is continuous and that it can take on
any value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. But what if this
doesnt actually correctly represent the response variable? Maybe
the <span class="math inline">\(Y_i\)</span>s are distances with values <span class="math inline">\(\geq 0\)</span>. Maybe each <span class="math inline">\(Y_i\)</span> belongs
to one of several categories, and thus the <span class="math inline">\(Y_i\)</span>s are
discretely valued. When provided data such as these, it is possible,
though not optimal, to utilize simple linear regression.
A better choice is to generalize the concept of linear regression,
and to utilize this generalization to
implement a more appropriate statistical model.</p>
<p>To implement a <em>generalized linear model</em> (or <em>GLM</em>), we need to do two things:</p>
<ol style="list-style-type: decimal">
<li>examine the <span class="math inline">\(Y_i\)</span> values and select an appropriate distribution
for them (discrete or continuous? what is the functional domain?); and</li>
<li>define a <em>link function</em> <span class="math inline">\(g(\theta \vert x)\)</span> that maps
the line <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, which has infinite range,
into a more limited range (e.g., <span class="math inline">\([0,\infty)\)</span>).</li>
</ol>
<p>Suppose that, in an experiment, the response variable can take on the
values false and true. To generalize linear regression so as
to handle these data, we map false to 0 and true to 1, and assume
that we can model the data-generating process using a
Bernoulli distribution (i.e., a binomial distribution with <span class="math inline">\(k = 1\)</span>):
<span class="math inline">\(Y \sim\)</span> Bernoulli(<span class="math inline">\(p\)</span>). We know that <span class="math inline">\(0 &lt; p &lt; 1\)</span>, so
we adopt a link function that maps <span class="math inline">\(\beta_0 + \beta_1 x\)</span> to the range <span class="math inline">\((0,1)\)</span>.
There is no unique choice, but a conventional one
is the <em>logit</em> function:
<span class="math display">\[
g(p \vert x) = \log\left[\frac{p \vert x}{1-p \vert x}\right] = \beta_0 + \beta_1 x ~\implies~ p \vert x = \frac{\exp\left(\beta_0+\beta_1x\right)}{1+\exp\left(\beta_0+\beta_1x\right)} \,.
\]</span>
Using the logit function to model dichotomous data is dubbed <em>logistic regression</em>. See Figure <a href="the-binomial-and-related-distributions.html#fig:logexp">3.13</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logexp"></span>
<img src="_main_files/figure-html/logexp-1.png" alt="\label{fig:logexp}An example of an estimated logistic regression line. The blue line is a sigmoid function; it represents the probability that we would sample a datum of Class 1 as a function of $x$. The red points are the observed data." width="50%" />
<p class="caption">
Figure 3.13: An example of an estimated logistic regression line. The blue line is a sigmoid function; it represents the probability that we would sample a datum of Class 1 as a function of <span class="math inline">\(x\)</span>. The red points are the observed data.
</p>
</div>
<p>How do we learn a logistic regression model? In linear regression, we
can determine values for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by
formulae, the ones we derive when we minimize the sum of squared
errors (SSE). For logistic regression, such simple formulae do not
exist, and so we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> via numerical
optimization of the likelihood function
<span class="math display">\[
\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y}) = \prod_{i=1}^n p_{Y \vert \beta_0,\beta_1}(y_i \vert \beta_0,\beta_1) = \prod_{i=1}^n p^{y_i}(1-p)^{1-y_i} = p^{\sum_{i=1}^n y_i} (1-p)^{n-\sum_{i=1}^n y_i} \,,
\]</span>
where <span class="math inline">\(p_Y(\cdot)\)</span> is the Bernoulli probability mass function. Specifically,
we would take the first partial derivatives of
<span class="math inline">\(\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y})\)</span> with respect to <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span>, respectively, set them both to zero (thus allowing us to equate
the derivative expressions), and determine the values
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that satisfy the equated derivatives.
These are the values at which the
bivariate likelihood surface achieves its maximum value. Note that
because numerical optimization is an iterative process, logistic regression
models are learned more slowly than simple linear regression models.</p>
<p>Lets step back for a moment. Why would we want to learn a logistic regression
model in the first place? After all, it is a relatively
inflexible model that might lack the ability to mimic the true behavior
of <span class="math inline">\(p \vert x\)</span>. The reason is that because we specify the
mathematical form of the model,
we can after the fact examine the estimated coefficients and
perform <em>inference</em>: we can examine how the response is affected by changes
in the predictor variables values. This can be important in, e.g., scientific
contexts, where explaining how a model works can be as important,
if not more important, than its ability to generate accurate and precise
predictions.
(This is in constrast to commonly used machine
learning models, whose mathematical forms cannot be specified
<em>a priori</em> and are thus less interpretable after they are learned.)</p>
<p>In simple linear regression, if we increase <span class="math inline">\(x\)</span> by one unit,
we can immediately infer how the response value changes:
<span class="math display">\[
\hat{Y}&#39; = \hat{\beta}_0 + \hat{\beta}_1 (x + 1) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_1 = \hat{Y} + \hat{\beta}_1 \,.
\]</span>
For logistic regression, the situation is not as straightforward,
because <span class="math inline">\(p\)</span> changes non-linearly as a function of <span class="math inline">\(x\)</span>. So we fall back
on the concept of <em>odds</em>:
<span class="math display">\[
O(x) = \frac{p(x)}{1-p(x)} = \exp\left(\hat{\beta}_0+\hat{\beta}_1x\right) \,.
\]</span>
If, e.g., <span class="math inline">\(O(x) = 4\)</span>, then that means that, given <span class="math inline">\(x\)</span>, we are four times
more likely to sample a success (1) than a failure (0). How
does the odds change if we add one unit to <span class="math inline">\(x\)</span>?
<span class="math display">\[
O(x+1) = e^{\hat{\beta}_0+\hat{\beta}_1(x+1)} = \exp\left(\hat{\beta}_0 + \hat{\beta}_1x\right)  \times \exp\left(\hat{\beta}_1\right) = O(x) \exp\left(\hat{\beta}_1\right) \,.
\]</span>
The odds change by a factor of <span class="math inline">\(\exp\left(\hat{\beta}_1\right)\)</span>,
which can be greater than or less than one, depending
on the sign of <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>It is important to note that when we perform
logistic regression, we are simply estimating <span class="math inline">\(p \vert x\)</span>, which
is the probability that
we would sample a datum belonging to Class 1, given <span class="math inline">\(x\)</span>.
How we choose to map <span class="math inline">\(p \vert x\)</span> to either 0 or 1,
if we choose to make that mapping, is not actually
part of the logistic regression framework. <em>Classification</em> is
a broad topic within statistical learning whose details are beyond
the scope of this book. We direct the interested reader to,
e.g., <em>Introduction to Statistical Learning</em> by James et al.</p>
<hr />
<div id="logistic-regression-in-r" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Logistic Regression in R<a href="the-binomial-and-related-distributions.html#logistic-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>When observed with a telescope, a star is a point-like object, as opposed to
a galaxy, which has some angular extent. Telling stars and galaxies apart
visually is thus, in relative terms, easy. However, if the galaxy has an
inordinately bright core because the supermassive black hole at its center
is ingesting matter at a high rate, that core can outshine the rest of the
galaxy so much that the galaxy appears to be point-like. Such galaxies
with bright cores are dubbed quasars, or quasi-stellar objects, and
they are much harder to tell apart from stars.</p>
</blockquote>
<blockquote>
<p>Lets say we are given a dataset showing the difference in
magnitude (a logarithmic measure of brightness) at two wavelengths,
for 500 stars and 500 quasars.
The response variable is a factor variable with two levels, which
we dub Class 0 (here, <code>QSO</code>) and Class 1 (<code>STAR</code>). (The mapping
of qualitative factors to quantitative levels is by default
alphabetical; as <code>STAR</code> comes after <code>QSO</code>, <code>STAR</code> gets mapped to Class 1.
One can always override this default behavior.)</p>
</blockquote>
<blockquote>
<p>We learn a simple logistic regression model using the <code>R</code> function
<code>glm()</code>, rather than <code>lm()</code>, and we specify
<code>family=binomial</code>, which means do logistic regression.
(We can add additional arguments to, e.g., change the link function.)</p>
</blockquote>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="the-binomial-and-related-distributions.html#cb219-1" tabindex="-1"></a>glm.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(class<span class="sc">~</span>r,<span class="at">data=</span>df,<span class="at">family=</span>binomial)</span>
<span id="cb219-2"><a href="the-binomial-and-related-distributions.html#cb219-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(glm.out),<span class="at">show.residuals=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## $call
## glm(formula = class ~ r, family = binomial, data = df)
## 
## $terms
## class ~ r
## attr(,&quot;variables&quot;)
## list(class, r)
## attr(,&quot;factors&quot;)
##       r
## class 0
## r     1
## attr(,&quot;term.labels&quot;)
## [1] &quot;r&quot;
## attr(,&quot;order&quot;)
## [1] 1
## attr(,&quot;intercept&quot;)
## [1] 1
## attr(,&quot;response&quot;)
## [1] 1
## attr(,&quot;.Environment&quot;)
## &lt;environment: R_GlobalEnv&gt;
## attr(,&quot;predvars&quot;)
## list(class, r)
## attr(,&quot;dataClasses&quot;)
##     class         r 
##  &quot;factor&quot; &quot;numeric&quot; 
## 
## $family
## 
## Family: binomial 
## Link function: logit 
## 
## 
## $deviance
## [1] 1040.866
## 
## $aic
## [1] 1044.866
## 
## $contrasts
## NULL
## 
## $df.residual
## [1] 998
## 
## $null.deviance
## [1] 1386.294
## 
## $df.null
## [1] 999
## 
## $iter
## [1] 5
## 
## $deviance.resid
##           1           2           3           4           5           6 
## -1.34442934 -0.78010884 -1.03334838 -0.50183305 -0.48084239 -0.53255033 
##           7           8           9          10          11          12 
## -1.36105961 -0.72742397 -1.44022148 -0.80579880 -0.68753122 -0.74292009 
##          13          14          15          16          17          18 
## -0.90876931 -1.05266959 -0.60432058 -0.50608188 -1.02746395 -0.87337666 
##          19          20          21          22          23          24 
## -0.61999843 -1.45900829 -1.15825933 -1.56166854 -0.89622565 -1.28266824 
##          25          26          27          28          29          30 
## -0.86060909 -0.54281000 -1.20050731 -0.52038587 -1.11490087 -2.05406195 
##          31          32          33          34          35          36 
## -1.29208940 -1.33985348 -0.92221142 -0.56511599 -1.04260564 -1.32633073 
##          37          38          39          40          41          42 
## -0.73522857 -0.98755834 -1.15112643 -0.60219055 -1.08692008 -0.90425808 
##          43          44          45          46          47          48 
## -0.51877395 -0.86437954 -1.54123316 -0.73404183 -0.98185422 -0.61741431 
##          49          50          51          52          53          54 
## -0.99911977 -1.36755040 -0.44296378 -1.56407852 -1.27300450 -1.22050334 
##          55          56          57          58          59          60 
## -0.96686583 -1.83665649 -0.75555050 -1.22934101 -0.66532394 -1.20675112 
##          61          62          63          64          65          66 
## -0.82234545 -1.19210795 -0.54636066 -0.69671453 -1.07565805 -0.61816886 
##          67          68          69          70          71          72 
## -0.97053038 -0.64134471 -0.52663037 -0.55560977 -0.74865078 -1.37080379 
##          73          74          75          76          77          78 
## -0.94402785 -0.54747212 -0.44008111 -1.07382568 -0.55831699 -1.08211900 
##          79          80          81          82          83          84 
## -0.51814741 -0.84982289 -0.57132350 -1.12533848 -0.42373351 -1.08329521 
##          85          86          87          88          89          90 
## -0.85702476 -0.82100763 -0.88180610 -1.14292754 -1.08898441 -0.76775691 
##          91          92          93          94          95          96 
## -0.91465737 -0.86606729 -0.89453301 -0.89728573 -0.85268360 -1.13194303 
##          97          98          99         100         101         102 
## -0.98397033 -0.48037571 -1.14961762 -0.95713234 -0.34972811 -0.68767711 
##         103         104         105         106         107         108 
## -1.26127325 -0.96616387 -0.71126400 -0.72263542 -0.70891487 -0.93544530 
##         109         110         111         112         113         114 
## -1.20203094 -1.51563358 -1.16724553 -0.49403825 -0.62179341 -0.68599319 
##         115         116         117         118         119         120 
## -0.91587624 -1.18042430 -0.48077692 -1.22909763 -0.92345052 -1.33546411 
##         121         122         123         124         125         126 
## -0.80205623 -0.69359791 -0.99289733 -1.09840751 -1.05447923 -0.03084916 
##         127         128         129         130         131         132 
## -1.09090108 -0.93181108 -0.54176350 -0.73293709 -1.22403903 -0.83104211 
##         133         134         135         136         137         138 
## -1.23304240 -0.71747389 -0.69403861 -0.80259813 -0.94169427 -0.50525458 
##         139         140         141         142         143         144 
## -0.78492216 -0.96132106 -0.93405419 -0.67811127 -1.60635961 -0.62406885 
##         145         146         147         148         149         150 
## -1.22603243 -1.00597198 -0.33204877 -0.97717652 -1.09464617 -2.64651534 
##         151         152         153         154         155         156 
## -1.61602734 -0.66349994 -0.47519831 -1.62894508 -0.85481870 -0.79001596 
##         157         158         159         160         161         162 
## -0.69572611 -0.66822503 -0.55813227 -0.55579701 -0.48589033 -1.17657855 
##         163         164         165         166         167         168 
## -1.02158696 -0.70955960 -0.94386071 -1.22693496 -1.31289170 -0.72965040 
##         169         170         171         172         173         174 
## -0.91461520 -0.91715241 -0.39819237 -0.69776225 -0.63465318 -0.60229414 
##         175         176         177         178         179         180 
## -0.49234386 -1.17905483 -1.13105762 -1.12092038 -0.40433636 -0.45898979 
##         181         182         183         184         185         186 
## -0.12327042 -1.05772217 -0.97122443 -0.41432777 -1.76562944 -1.76369475 
##         187         188         189         190         191         192 
## -1.11508828 -0.87539938 -0.39011383 -1.59205119 -1.72312989 -1.60512064 
##         193         194         195         196         197         198 
## -0.52522619 -0.98503668 -0.75908384 -0.52557077 -1.06050211 -0.84075930 
##         199         200         201         202         203         204 
## -0.87651680 -1.00665512 -1.28486676 -1.08289965 -1.15961925 -0.70003648 
##         205         206         207         208         209         210 
## -0.43941889 -1.17829309 -0.80308433 -0.83342788 -0.46740450 -0.56612857 
##         211         212         213         214         215         216 
## -0.91962809 -1.35748304 -0.65335759 -0.93128041 -1.03223934 -0.85275980 
##         217         218         219         220         221         222 
## -1.30513061 -1.20711118 -0.68807650 -0.68949117 -0.58886875 -0.59575886 
##         223         224         225         226         227         228 
## -0.92906966 -1.46720181 -0.48867580 -0.43250444 -0.99878448 -0.39411972 
##         229         230         231         232         233         234 
## -0.62686823 -0.55273373 -0.69887728 -0.83511082 -0.91944474 -0.92843116 
##         235         236         237         238         239         240 
## -0.37555874 -0.92480385 -0.33632583 -0.49588110 -0.87523982 -0.41136544 
##         241         242         243         244         245         246 
## -0.59353437 -0.72064386 -0.44560547 -0.78295034 -0.55475497 -1.53122009 
##         247         248         249         250         251         252 
## -0.45459251 -0.80607922 -1.63442428 -0.97093804 -0.47430884 -0.87461089 
##         253         254         255         256         257         258 
## -0.65500060 -0.87829287 -1.29511685 -0.42681800 -0.96619775 -0.56541403 
##         259         260         261         262         263         264 
## -1.33590179 -1.06464559 -1.41095796 -0.56935341 -0.86901804 -1.44946468 
##         265         266         267         268         269         270 
## -1.01075421 -1.20815396 -1.33003206 -0.43665110 -0.88094069 -0.53804686 
##         271         272         273         274         275         276 
## -1.13118858 -0.69652451 -0.53286791 -0.62314508 -0.94469180 -1.09507987 
##         277         278         279         280         281         282 
## -0.94531779 -0.51318292 -1.45756162 -1.20499974 -1.46041010 -0.32729732 
##         283         284         285         286         287         288 
## -0.84492214 -0.52748493 -0.44853951 -1.44585417 -0.98732316 -1.03901850 
##         289         290         291         292         293         294 
## -0.89967260 -0.48505198 -0.51534221 -1.42658556 -1.02859538 -1.34533939 
##         295         296         297         298         299         300 
## -0.96885224 -0.97010827 -0.98304147 -1.99860724 -0.79744750 -1.43773215 
##         301         302         303         304         305         306 
## -0.60102443 -0.64339341 -0.52783389 -1.26298063 -1.50009163 -0.61280814 
##         307         308         309         310         311         312 
## -0.53432092 -0.46830188 -0.83624733 -0.97424168 -1.40809567 -0.80655826 
##         313         314         315         316         317         318 
## -1.64888976 -0.94850883 -0.48548248 -0.49605957 -0.70552947 -0.86598579 
##         319         320         321         322         323         324 
## -0.94313506 -0.46734620 -0.91715711 -1.01485223 -1.47930153 -1.34750967 
##         325         326         327         328         329         330 
## -0.62680757 -0.46463593 -0.66200779 -0.41033543 -0.66970938 -0.47347701 
##         331         332         333         334         335         336 
## -1.26899208 -1.16655575 -0.99519874 -1.62996295 -1.38736771 -0.77574484 
##         337         338         339         340         341         342 
## -1.04072060 -1.32112334 -0.87444687 -0.91908751 -1.00827981 -1.48610824 
##         343         344         345         346         347         348 
## -0.92041815 -0.92024411 -1.26528372 -0.47977020 -0.45310548 -0.36951637 
##         349         350         351         352         353         354 
## -1.57466399 -0.70110751 -1.15652994 -0.99616837 -1.11773947 -0.90487708 
##         355         356         357         358         359         360 
## -1.01066490 -0.98620648 -1.02263514 -0.93608667 -0.32944575 -0.56615480 
##         361         362         363         364         365         366 
## -1.28933840 -0.77240271 -0.50022719 -0.49092998 -1.31814606 -0.94079801 
##         367         368         369         370         371         372 
## -0.99802045 -0.96404041 -0.61113920 -1.16724553 -0.89415868 -1.07218907 
##         373         374         375         376         377         378 
## -0.85703825 -0.97114676 -0.56351341 -1.11034498 -1.45209176 -1.52430164 
##         379         380         381         382         383         384 
## -1.05248213 -1.10866203 -0.61499352 -0.44164297 -0.38023235 -0.47322715 
##         385         386         387         388         389         390 
## -1.02468319 -0.89200703 -0.98239644 -0.92778348 -0.83097162 -0.94802043 
##         391         392         393         394         395         396 
## -0.82954509 -1.20501585 -1.27169212 -1.60930138 -0.40815974 -0.94990778 
##         397         398         399         400         401         402 
## -0.87331293 -0.94271509 -0.66845033 -1.42296081 -0.59601900 -0.47125769 
##         403         404         405         406         407         408 
## -0.82460912 -0.41794648 -0.82081539 -0.72148778 -1.36008600 -1.00804196 
##         409         410         411         412         413         414 
## -0.96304007 -0.70370264 -1.36135450 -1.03952193 -0.37283086 -0.52490024 
##         415         416         417         418         419         420 
## -0.51911486 -0.64491786 -0.93142253 -1.06886330 -1.37992053 -1.29936738 
##         421         422         423         424         425         426 
## -0.58158198 -0.77710225 -1.05913456 -0.99518398 -0.86598579 -0.93967840 
##         427         428         429         430         431         432 
## -0.76339694 -0.53015812 -0.57796191 -0.63538508 -0.47158496 -0.53483309 
##         433         434         435         436         437         438 
## -0.54617614 -1.53795922 -1.04290317 -1.34072400 -0.83469098 -0.96958441 
##         439         440         441         442         443         444 
## -0.91079639 -0.99176737 -0.47637152 -1.44483646 -0.34010750 -0.65111439 
##         445         446         447         448         449         450 
## -0.92116626 -0.93269276 -1.34219365 -0.99732077 -1.17679148 -1.02835502 
##         451         452         453         454         455         456 
## -0.96192927 -0.68752354 -0.97449457 -0.95265648 -1.35568719 -1.24114535 
##         457         458         459         460         461         462 
## -0.97031688 -1.35205816 -1.56642635 -0.58727074 -0.44969922 -0.56225368 
##         463         464         465         466         467         468 
## -0.46571946 -0.51142811 -1.02783935 -0.76233463 -0.55525481 -1.04297378 
##         469         470         471         472         473         474 
## -0.41575882 -0.64929194 -1.51514208 -0.81398026 -0.72153557 -0.73398535 
##         475         476         477         478         479         480 
## -0.44636399 -1.73731528 -0.92350708 -1.41131652 -0.49538109 -0.43405231 
##         481         482         483         484         485         486 
## -1.18996876 -0.85905889 -0.57617894 -0.79407357 -0.64914882 -0.71881925 
##         487         488         489         490         491         492 
## -0.45500424 -0.42617137 -0.55606181 -0.43193271 -0.46756552 -1.31157345 
##         493         494         495         496         497         498 
## -0.67795937 -0.76585763 -1.18594031 -0.61814770 -0.96144172 -1.54113138 
##         499         500         501         502         503         504 
## -1.20191825 -0.89034244  0.87709802  0.38199157  0.99337584  2.07768266 
##         505         506         507         508         509         510 
##  1.30388796  0.34091504  3.91629243  0.63642589  0.66032617  0.29795349 
##         511         512         513         514         515         516 
##  0.42437512  3.60360659  1.93814894  0.56032745  0.46201761  0.21433552 
##         517         518         519         520         521         522 
##  0.28218473  0.47344431  3.11280034  2.26848252  1.37534310  0.59678065 
##         523         524         525         526         527         528 
##  0.70078940  1.44026843  0.34478099  0.57521890  0.56884732  1.80181046 
##         529         530         531         532         533         534 
##  0.25648404  1.41107200  0.97016335  1.94442621  0.55056934  0.41853487 
##         535         536         537         538         539         540 
##  1.33752179  0.65230468  1.05237247  0.46522815  1.21785178  0.94322744 
##         541         542         543         544         545         546 
##  0.44796829  0.71990547  0.11294763  0.32809012  0.62289072  0.73294659 
##         547         548         549         550         551         552 
##  0.24983667  0.69778693  0.60746936  0.44033192  0.72610398  0.90286908 
##         553         554         555         556         557         558 
##  1.23222147  0.58092936  0.11934155  0.93534250  0.42847146  0.53289705 
##         559         560         561         562         563         564 
##  0.66910163  0.35402631  0.63481664  0.86795373  0.30608406  0.59677037 
##         565         566         567         568         569         570 
##  0.41905270  0.80790301  0.14273821  0.92397547  0.74400478  1.13881810 
##         571         572         573         574         575         576 
##  0.46768870  0.72351443  0.34769268  1.15726153  1.29912179  0.13442284 
##         577         578         579         580         581         582 
##  0.25604308  0.43254160  0.29107840  1.32102033  1.62493190  1.58404702 
##         583         584         585         586         587         588 
##  1.27537522  0.43097797  0.92085735  3.25722766  1.12385635  0.36870066 
##         589         590         591         592         593         594 
##  0.38345147  0.75959814  2.52864004  1.66738758  0.32123029  0.25495412 
##         595         596         597         598         599         600 
##  1.68106574  0.65038402  1.32409875  0.71263352  0.25201245  0.18798550 
##         601         602         603         604         605         606 
##  1.68552892  2.27780437  1.86684419  1.59240399  0.66415117  1.50614070 
##         607         608         609         610         611         612 
##  1.48745277  1.31704372  0.95213975  0.55341650  0.19166634  1.43705994 
##         613         614         615         616         617         618 
##  1.21893019  0.63305601  0.42069297  1.20797309  1.41893013  0.24667029 
##         619         620         621         622         623         624 
##  0.21338379  1.84061827  1.52233131  1.61658934  0.67379750  1.00567680 
##         625         626         627         628         629         630 
##  0.23534828  0.14360889  0.62816934  0.67143128  0.66143906  0.64147685 
##         631         632         633         634         635         636 
##  0.19477820  0.67745962  0.60338051  0.26551127  2.25157786  0.41618440 
##         637         638         639         640         641         642 
##  0.19892387  1.07464114  0.63138544  0.17069113  0.71828467  0.23048216 
##         643         644         645         646         647         648 
##  0.59322518  1.17136290  0.88820741  0.98220765  1.19134492  0.20355094 
##         649         650         651         652         653         654 
##  1.22700174  0.62671609  1.37191474  1.65157040  0.91228510  0.22337724 
##         655         656         657         658         659         660 
##  0.58668037  0.59568821  1.86100760  0.42470599  0.52772688  0.78230099 
##         661         662         663         664         665         666 
##  0.96496568  0.39258994  0.64868789  1.43570604  0.62970411  0.22463097 
##         667         668         669         670         671         672 
##  1.29917680  0.17972224  0.93977535  0.15752922  0.53613213  0.39675562 
##         673         674         675         676         677         678 
##  0.36729977  0.56151187  1.05891784  0.34553304  1.30199964  1.28563653 
##         679         680         681         682         683         684 
##  0.63814338  1.60955230  0.59406801  1.67954203  0.18612666  0.45103195 
##         685         686         687         688         689         690 
##  0.23689281  0.31960703  1.02579980  0.56046076  0.42951465  2.07978527 
##         691         692         693         694         695         696 
##  0.73653141  0.20408207  1.44172973  1.22309055  0.13714873  0.51449372 
##         697         698         699         700         701         702 
##  0.86777691  0.09703584  0.46119408  0.97513842  0.09918521  0.82642202 
##         703         704         705         706         707         708 
##  0.77810081  0.56712085  0.38774516  1.01759089  1.29162999  0.29263505 
##         709         710         711         712         713         714 
##  0.45852121  0.28172030  0.68460456  1.25215591  0.58557631  0.30352043 
##         715         716         717         718         719         720 
##  0.36108774  1.57275364  0.89957684  1.28723840  0.16186744  1.49484428 
##         721         722         723         724         725         726 
##  1.16043104  0.69608009  0.18297423  0.83562966  0.26117879  0.65734097 
##         727         728         729         730         731         732 
##  0.50901475  0.85246561  1.34516933  1.48949498  0.79279911  0.45125246 
##         733         734         735         736         737         738 
##  1.51146658  0.32737795  1.92923670  1.76590619  1.05763256  0.84330321 
##         739         740         741         742         743         744 
##  0.50974852  1.79665851  1.52476709  0.86119250  1.39510962  1.48685482 
##         745         746         747         748         749         750 
##  0.73884337  0.40752762  0.26213024  1.33215970  1.30876356  0.44393116 
##         751         752         753         754         755         756 
##  1.42315354  1.10955196  1.50777277  1.35922021  1.25283070  1.00831131 
##         757         758         759         760         761         762 
##  0.25990844  1.29191559  3.33063914  1.53146518  0.20897297  1.27640957 
##         763         764         765         766         767         768 
##  1.13095996  0.67188741  0.90408757  0.96080645  0.25509412  1.34011050 
##         769         770         771         772         773         774 
##  0.20518405  0.51324127  0.83161654  1.39551220  2.95408943  1.06717094 
##         775         776         777         778         779         780 
##  0.66679332  0.78132542  1.02086532  0.63205683  0.52408723  0.50084466 
##         781         782         783         784         785         786 
##  0.24985671  0.59916648  1.46629128  0.85862819  0.35158169  0.77457940 
##         787         788         789         790         791         792 
##  1.32712365  0.55296950  0.35821442  0.46544381  0.41011925  1.42910213 
##         793         794         795         796         797         798 
##  0.63990088  1.25755743  0.69621187  1.39457847  0.15831832  1.53949355 
##         799         800         801         802         803         804 
##  0.23813788  0.32455383  0.92860308  2.06874956  1.01993277  2.64479342 
##         805         806         807         808         809         810 
##  0.57567107  0.39317781  0.60797728  0.71477050  0.92025989  0.50375288 
##         811         812         813         814         815         816 
##  1.54154053  0.77094395  1.63783553  1.26248610  0.45082497  1.30551813 
##         817         818         819         820         821         822 
##  1.33166696  2.81406296  0.68569952  1.53884329  0.88442098  0.28126005 
##         823         824         825         826         827         828 
##  1.70892566  2.25385374  1.20208651  0.56670388  0.96148204  0.12814888 
##         829         830         831         832         833         834 
##  1.00009323  0.51454509  1.66768687  2.27999903  0.58070485  0.26625482 
##         835         836         837         838         839         840 
##  0.88894843  0.35862793  0.54197316  1.33647445  0.33011252  1.28107543 
##         841         842         843         844         845         846 
##  0.93987537  0.76332369  1.01362615  0.93754786  0.75321814  0.80925210 
##         847         848         849         850         851         852 
##  0.98152874  0.75060246  0.85560614  0.80929537  1.02859215  0.95077245 
##         853         854         855         856         857         858 
##  0.72982052  1.00231476  0.92456495  0.96185857  0.93985632  1.06098708 
##         859         860         861         862         863         864 
##  0.97215360  0.83947388  0.74279537  1.07226247  0.71054862  0.84462974 
##         865         866         867         868         869         870 
##  0.88294870  0.81572498  0.72127416  0.71685287  0.79644306  0.94913799 
##         871         872         873         874         875         876 
##  0.81897035  0.77642273  0.77099412  0.90811766  1.07703769  1.08942920 
##         877         878         879         880         881         882 
##  1.10617272  1.04047555  1.03318452  1.03933743  0.73707754  0.86737350 
##         883         884         885         886         887         888 
##  0.81092332  1.08794613  1.01824833  0.89241936  0.98013756  1.05579487 
##         889         890         891         892         893         894 
##  0.83235726  0.74116069  0.85575438  1.01355161  1.03252711  1.07205797 
##         895         896         897         898         899         900 
##  0.96950368  0.77420608  0.83128601  0.70745938  1.10057169  0.84569464 
##         901         902         903         904         905         906 
##  0.84188030  0.76739994  0.79706793  1.00842529  0.88054413  1.10240446 
##         907         908         909         910         911         912 
##  1.02944365  0.96955218  1.01896080  0.97614587  0.95080602  0.83490024 
##         913         914         915         916         917         918 
##  0.73095795  0.89240551  0.95364750  1.03810938  1.01963863  0.81038619 
##         919         920         921         922         923         924 
##  1.02435513  1.03020029  0.73711800  0.75062707  0.94890326  1.09816117 
##         925         926         927         928         929         930 
##  0.94446897  0.90915842  0.78621189  0.77350173  0.87411139  0.91062513 
##         931         932         933         934         935         936 
##  0.93966580  1.01139121  0.70592643  0.70239579  0.99354790  0.90597311 
##         937         938         939         940         941         942 
##  0.75241671  0.87763676  1.07513765  0.84021485  0.87733540  1.10072178 
##         943         944         945         946         947         948 
##  0.87439382  0.73994658  1.10357008  0.94125254  0.91671762  1.08144310 
##         949         950         951         952         953         954 
##  0.81317411  1.03456529  0.89293173  0.91639857  1.04951709  0.83586847 
##         955         956         957         958         959         960 
##  0.93172272  1.02754080  0.90017994  0.94401045  0.69719305  0.94982800 
##         961         962         963         964         965         966 
##  1.10254431  1.08157658  0.94403910  0.79224905  0.80948143  0.77735187 
##         967         968         969         970         971         972 
##  0.40127374  0.71236132  0.15452988  1.35151569  2.00645479  0.41313543 
##         973         974         975         976         977         978 
##  0.87289127  2.28412023  1.40228093  0.30153862  1.85657577  0.97992775 
##         979         980         981         982         983         984 
##  0.67480711  1.81404686  1.02994971  2.59175573  0.63929891  0.41565484 
##         985         986         987         988         989         990 
##  0.47916354  0.92251924  0.46065127  0.53308083  1.08209515  0.59537345 
##         991         992         993         994         995         996 
##  3.32383763  1.13545274  1.33487333  0.16954547  1.82838764  1.87524429 
##         997         998         999        1000 
##  2.20269753  0.77036709  0.37294458  0.56742636 
## 
## $coefficients
##              Estimate Std. Error   z value     Pr(&gt;|z|)
## (Intercept) 23.448009 1.65427782  14.17417 1.324057e-45
## r           -1.253855 0.08817248 -14.22048 6.837764e-46
## 
## $aliased
## (Intercept)           r 
##       FALSE       FALSE 
## 
## $dispersion
## [1] 1
## 
## $df
## [1]   2 998   2
## 
## $cov.unscaled
##             (Intercept)            r
## (Intercept)   2.7366351 -0.145709169
## r            -0.1457092  0.007774386
## 
## $cov.scaled
##             (Intercept)            r
## (Intercept)   2.7366351 -0.145709169
## r            -0.1457092  0.007774386
## 
## attr(,&quot;class&quot;)
## [1] &quot;summary.glm&quot;</code></pre>
<blockquote>
<p>The <code>summary()</code> of a learned logistic regression model is similar to
that for a linear regression model.</p>
</blockquote>
<blockquote>
<p>The deviance residuals, which show the
residuals for each datum, are defined as
<span class="math display">\[
d_i = \mbox{sign}(Y_i - \hat{Y}_i) \sqrt{-2[Y_i \log \hat{Y}_i + (1-Y_i)\log(1-\hat{Y}_i)]} \,,
\]</span>
where
<span class="math display">\[
\hat{Y}_i = \hat{p}_i \vert x_i = \frac{\exp(\hat{\beta}_0+\hat{\beta}_1 x_i)}{1 + \exp(\hat{\beta}_0+\hat{\beta}_1 x_i)} \,.
\]</span>
The sum of the squared deviance residuals is equal to
<span class="math inline">\(-2 \log \mathcal{L}_{\rm max}\)</span>, where <span class="math inline">\(\mathcal{L}_{\rm max}\)</span> is the maximum
value of the joint likelihood
of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, given <span class="math inline">\(\mathbf{x}\)</span>.
Here, we observe that the deviance residuals are seemingly well-balanced
around zero.</p>
</blockquote>
<blockquote>
<p>The coefficients can be translated to <span class="math inline">\(y\)</span>-coordinate values as follows:
the intercept is <span class="math inline">\(e^{23.448}/(1+e^{23.448}) \rightarrow 1\)</span>
(this is the probability
the a sampled datum with <code>r</code> equal to zero is a <code>STAR</code>), while
the odds ratio is <span class="math inline">\(O_{new}/O_{old} = e^{-1.254}\)</span> (so that every time
<code>r</code> increases by one unit, the probability that a sampled datum
is a star is 0.285 times what it had been before: the higher the value
of <code>r</code>, the less and less likely that a sampled datum is actually
a star, and the more and more likely that it is a quasarat least,
according to the logistic regression model.</p>
</blockquote>
<blockquote>
<p>The numbers in the other three columns of the coefficients section
are estimated numerically using the behavior of the likelihood
function (specifically, the rates at which it curves downward
away from the maximum). Some
details about how the numbers are calculated are given in an example
in the Covariance and Correlation section of Chapter 6.
It suffices to say here that if we assume that <span class="math inline">\(\hat{\beta}_0\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span> are both normally distributed random variables, we
reject the null hypothesis that the intercept is 0
(or equivalently <span class="math inline">\(y = 1/2\)</span>), and that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
</blockquote>
<blockquote>
<p>The null deviance is <span class="math inline">\(-2 \log \mathcal{L}_{max,o}\)</span>, where
<span class="math inline">\(\mathcal{L}_{max,o}\)</span> is the maximum likelihood
when <span class="math inline">\(\beta_1\)</span> is set to zero. The residual deviance is
<span class="math inline">\(-2 \log \mathcal{L}_{\rm max}\)</span>. The difference between these values
(here, 1386.3-1040.9 = 345.4), under the null hypothesis that
<span class="math inline">\(\beta_1 = 0\)</span>, is assumed to be sampled from a chi-square distribution for
999-998 = 1 degree of freedom. The <span class="math inline">\(p\)</span>-value is thus
<code>1 - pchisq(345.4,1)</code> or effectively zero: we emphatically reject the
null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.
While this hypothesis test clearly indicates that <span class="math inline">\(\beta_1\)</span> is
not equal to zero, how can we know whether the model itself fits to
the data well in an absolute sense?
This is a models goodness of fit, a
concept we introduce at the end of this chapter. There is no unique
answer to this question in the context of logistic regression; the
interested reader should look up, e.g., the Hosmer-Lemeshow statistic.
(Note that the test carried out here is
analogous to the <span class="math inline">\(F\)</span>-test in linear regression, and is testing
<span class="math inline">\(H_o: \beta_1 = \beta_2 = \cdots = \beta_p = 0\)</span> versus <span class="math inline">\(H_a:\)</span>
at least one of the <span class="math inline">\(\beta_i\)</span>s is non-zero. Because the
sampling distribution here is the chi-square distribution as
opposed to the normal distribution, the <span class="math inline">\(p\)</span>-value here will not
match the <span class="math inline">\(p\)</span>-value seen in the coefficients section in general.)</p>
</blockquote>
<blockquote>
<p>Last, the AIC, or Akaike Information Criterion, is <span class="math inline">\(-2\)</span> times the
model likelihood (or here, the deviance)
plus two times the number of variables (here, 2,
as the intercept is counted as a variable).
Adding the number of variables acts to penalize those models with
more variables: the improvement in the maximum likelihood has to be
sufficient to justify added model complexity.
Discussion of the mathematical details of
AIC is beyond the scope of this book; it suffices to say that
if we compute it when learning a suite of different models, we would
select the model with the smallest value.</p>
</blockquote>
<blockquote>
<p>We wrap up this example by showing how one would start moving from estimating
the Class 1 probabilities for each datum towards predicting
classes for each datum. Logistic regression is not in and of itself a
classifier: it simply outputs probabilities. Naively, we would classify
an object with an estimated probability below 0.5 as being an object of
Class 0 and one with an estimated probability above 0.5 as being an object
of Class 1, but that only works in general if the classes are balanced,
i.e., if there are the same number of data of Class 1 as of Class 0.
(Here, works means acts to minimize misclassification rates across both
classes at the same time.)
If there is class imbalance, then we will almost certainly have to change
0.5 to some other value for optimal classification.
In the current example, the classes <em>are</em> balanced, and so putting the
dividing line at 0.5, as we do in Figure <a href="the-binomial-and-related-distributions.html#fig:glmpred">3.14</a>, is
an optimal choice.</p>
</blockquote>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="the-binomial-and-related-distributions.html#cb221-1" tabindex="-1"></a>glm.predictions <span class="ot">&lt;-</span> <span class="fu">predict.glm</span>(glm.out,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:glmpred"></span>
&lt;img src=_main_files/figure-html/glmpred-1.png alt=Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one woulddraw a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications. width=50% /&gt;
<p class="caption">
Figure 3.14: Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one would draw a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications.
</p>
</div>
</div>
</div>
<div id="naive-bayes-regression" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Naive Bayes Regression<a href="the-binomial-and-related-distributions.html#naive-bayes-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>Naive Bayes</em> model is the basis for perhaps the simplest probabilitic
classifier, one that is used to, e.g., detect spam emails. The meanings
of the words Naive and Bayes will become more clear below. Note that one
would often see this model referred to as the Naive Bayes classifier. However,
as noted in the last section, there are actually two steps in classification,
the first being the estimation of probabilities that a given datum belongs to
each class, and the second being the mapping of those probabilities to class
predictions. In the last section and here, we are only focusing on the
generation of predicted probabilitieshence the title of this section.</p>
<p>(Many would argue that the Naive Bayes model is a machine-learning model.
We would argue that it actually is not: the
final form of a machine-learning model is unknown before we
start the modeling process [e.g., the number of branches that will appear
in a classification tree model is unknown and is learned by the machine],
whereas with Naive Bayes the
mathematical form of the model is fully specified
and all we have to do is estimate unknown
probabilities. This is akin to the situation with linear regression, where the
mathematical form is set and all we have to do is estimate the
coefficientsand no one would argue that linear regression is a
machine-learning model!)</p>
<p>Lets assume that we are in a similar setting as the one for logistic regression,
but instead of having a response that is a two-level factor variable (i.e., one
the represents two classes), the number
of levels is <span class="math inline">\(K \geq 2\)</span>. (So instead of just, e.g., chocolate and vanilla as
our response variable values, we can add strawberry and other ice cream flavors
too!)
The ultimate goal of the model is to assign conditional probabilities for
each response class, given a datum <span class="math inline">\(\mathbf{x}\)</span>:
<span class="math inline">\(p(C_k \vert \mathbf{x})\)</span>, where <span class="math inline">\(C_k\)</span> denotes
class <span class="math inline">\(k\)</span>. How do we derive this quantity?</p>
<p>The first step is to apply Bayes rule (hence the Bayes in Naive Bayes):
<span class="math display">\[
p(C_k \vert \mathbf{x}) = \frac{p(C_k) p(\mathbf{x} \vert C_k)}{p(\mathbf{x})} \,.
\]</span>
The next step is to expand <span class="math inline">\(p(\mathbf{x} \vert C_k)\)</span>:
<span class="math display">\[
p(\mathbf{x} \vert C_k) = p(x_1,\ldots,x_p \vert C_k) = p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) \,.
\]</span>
(This is the multiplicative law of probability in action, as applied to a
conditional probability. See section 1.4.) The right-most expression above is one that
is difficult to evaluate in practice, given all the conditions that must be jointly
appliedso this is where the Naive aspect of the model comes in. We simplify the
expression by assuming (most often incorrectly!) that the predictor variables are
all mutually independent, so that
<span class="math display">\[
p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) ~~ \rightarrow ~~ p(x_1 \vert C_k) \cdots p(x_p \vert C_k)
\]</span>
and thus
<span class="math display">\[
p(C_k \vert \mathbf{x}) = \frac{p(C_k) \prod_{i=1}^p p(x_i \vert C_k)}{p(\mathbf{x})} \,.
\]</span>
OKwhere do we go from here?</p>
<p>We need to make further assumptions!</p>
<ul>
<li>We need to assign prior probabilities <span class="math inline">\(p(C_k)\)</span> to each class. Common choices
are <span class="math inline">\(1/K\)</span> (we view each class as equally probable before we gather data) and
<span class="math inline">\(n_k/n\)</span> (the number of observed data of class <span class="math inline">\(k\)</span> divided by the overall sample size).</li>
<li>We also need to assume probability mass and density functions <span class="math inline">\(p(x_i \vert C_k)\)</span> for
each predictor variable (a pmf if <span class="math inline">\(x_i\)</span> is discrete and a pdf if <span class="math inline">\(x_i\)</span> is continuous).
It is convention to use binomial or multinomial pmfs, depending
on the number of levels, with the observed proportions in each level informing the
category probability estimate, and
normal pdfs, with <span class="math inline">\(\hat{\mu} = \bar{x_i}\)</span> and <span class="math inline">\(\hat{\sigma^2} = s_i^2\)</span> (the
sample variance).</li>
</ul>
<p>Ultimately, this model depends on a number of (perhaps unjustified) assumptions. Why
would we ever use it? Because the assumption of mutual independence makes model
evaluation <em>fast</em>. Naive Bayes is rarely the model underlying the best classifier for
any given problem, but when speed is needed (such as in the identification of spam email),
ones choices are limited. (As as general rule: if a model is simple to implement, one
should implement it, even if the <em>a priori</em> expectation is that another model will
ultimately generate better results. One never knows)</p>
<hr />
<div id="naive-bayes-regression-with-categorical-predictors" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Naive Bayes Regression With Categorical Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have collected the following data:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_1\)</span></th>
<th align="center"><span class="math inline">\(x_2\)</span></th>
<th align="center"><span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Yes</td>
<td align="center">Chocolate</td>
<td align="center">True</td>
</tr>
<tr class="even">
<td align="center">Yes</td>
<td align="center">Chocolate</td>
<td align="center">False</td>
</tr>
<tr class="odd">
<td align="center">No</td>
<td align="center">Vanilla</td>
<td align="center">True</td>
</tr>
<tr class="even">
<td align="center">No</td>
<td align="center">Chocolate</td>
<td align="center">True</td>
</tr>
<tr class="odd">
<td align="center">Yes</td>
<td align="center">Vanilla</td>
<td align="center">False</td>
</tr>
<tr class="even">
<td align="center">No</td>
<td align="center">Chocolate</td>
<td align="center">False</td>
</tr>
<tr class="odd">
<td align="center">No</td>
<td align="center">Vanilla</td>
<td align="center">False</td>
</tr>
</tbody>
</table>
<blockquote>
<p>We learn a Naive Bayes model given these data, in which we
assume False is Class 0 and True is Class 1. If we have a
new datum <span class="math inline">\(\mathbf{x}\)</span> = (Yes,Chocolate), what is the
probability that the response is True?</p>
</blockquote>
<blockquote>
<p>We seek the quantity
<span class="math display">\[
p(C_1 \vert \mathbf{x}) = \frac{p(C_1) p(\mathbf{x} \vert C_1)}{p(\mathbf{x})} = \frac{p(C_1) p(x_1 \vert C_1) p(x_2 \vert C_1)}{p(x_1,x_2)} \,.
\]</span>
When we examine the data, we see that <span class="math inline">\(p(C_1) = 3/7\)</span>, as there are
three data out of seven with the value <span class="math inline">\(Y\)</span> = True. Now, given <span class="math inline">\(C_1\)</span>,
at what rate do we observe Yes? (One time out of threeso
<span class="math inline">\(p(x_1 \vert C_1) = 1/3\)</span>.) What about Chocolate? (Two times out of
threeso <span class="math inline">\(p(x_2 \vert C_1) = 2/3\)</span>.) The numerator is thus
<span class="math inline">\(3/7 \times 1/3 \times 2/3 = 2/21\)</span>. The value of the
denominator, <span class="math inline">\(p(x_1,x_2)\)</span>, is
determined utilizing the Law of Total Probability:
<span class="math display">\[\begin{align*}
p(x_1,x_2) &amp;= p(x_1 \vert C_1) p(x_2 \vert C_1) p(C_1) + p(x_1 \vert C_2) p(x_2 \vert C_2) p(C_2) \\
&amp;= 2/21 + 2/4 \times 2/4 \times 4/7 = 2/21 + 4/28 = 2/21 + 3/21 = 5/21 \,.
\end{align*}\]</span>
And so now we know that
<span class="math display">\[
p(\mbox{True} \vert \mbox{Yes,Chocolate}) = \frac{2/21}{5/21} = \frac{2}{5} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-regression-with-continuous-predictors" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Naive Bayes Regression With Continuous Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume we have the following data regarding credit-card defaults,
where <span class="math inline">\(x\)</span> represents the credit-card balance and <span class="math inline">\(Y\)</span> is a categorical
variable indicating whether a default has occurred:</p>
</blockquote>
<table style="width:100%;">
<colgroup>
<col width="6%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>1487.00</th>
<th>324.74</th>
<th>988.21</th>
<th>836.30</th>
<th>2205.80</th>
<th>927.89</th>
<th>712.28</th>
<th>706.16</th>
<th>1774.69</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Lets now suppose someone came along with a credit balance of $1,200.
According to the Naive Bayes model, given this
balance, what is the probability of a credit default?</p>
</blockquote>
<blockquote>
<p>The Naive Bayes model in this particular case is
<span class="math display">\[
p(Y \vert x) = \frac{p(x \vert Y) p(Y)}{p(x \vert Y) p(Y) + p(x \vert N) p(N)} \,,
\]</span>
where we can observe immediately that <span class="math inline">\(p(Y) = 3/9 = 1/3\)</span> and <span class="math inline">\(p(N) = 6/9 = 2/3\)</span>.
To compute, e.g., <span class="math inline">\(p(x \vert Y)\)</span>, we first compute the sample mean
and sample standard deviation for the observed data for which <span class="math inline">\(Y\)</span> is Yes:</p>
</blockquote>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="the-binomial-and-related-distributions.html#cb222-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1487.00</span>,<span class="fl">2205.80</span>,<span class="fl">1774.69</span>)</span>
<span id="cb222-2"><a href="the-binomial-and-related-distributions.html#cb222-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1822.5</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="the-binomial-and-related-distributions.html#cb224-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 361.78</code></pre>
<blockquote>
<p>The mean is $1,822.50 and the standard deviation is $361.78,
so the probability density associated with observing $1,200 is</p>
</blockquote>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="the-binomial-and-related-distributions.html#cb226-1" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002509367</code></pre>
<blockquote>
<p>The density is 2.51 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. As far as for those data for
which <span class="math inline">\(Y\)</span> is No:</p>
</blockquote>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="the-binomial-and-related-distributions.html#cb228-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">324.74</span>,<span class="fl">988.21</span>,<span class="fl">836.30</span>,<span class="fl">927.89</span>,<span class="fl">712.28</span>,<span class="fl">706.16</span>)</span>
<span id="cb228-2"><a href="the-binomial-and-related-distributions.html#cb228-2" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002748351</code></pre>
<blockquote>
<p>The density is 2.75 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. Thus
<span class="math display">\[
p(Y \vert x) = \frac{2.51 \cdot 0.333}{2.51 \cdot 0.333 + 2.75 \cdot 0.667} = \frac{0.834}{0.834 + 1.834} = 0.313 \,.
\]</span>
We would predict that there
is roughly a 1 in 3 chance that a person with a credit balance of
$1,200 will default on their debt.</p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-applied-to-star-quasar-data" class="section level3 hasAnchor" number="3.10.3">
<h3><span class="header-section-number">3.10.3</span> Naive Bayes Applied to Star-Quasar Data<a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here, we learn a Naive Bayes model that we can use to differentiate between
stars and quasars, utilizing functions from <code>R</code>s <code>e1071</code> package.</p>
</blockquote>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="the-binomial-and-related-distributions.html#cb230-1" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb230-2"><a href="the-binomial-and-related-distributions.html#cb230-2" tabindex="-1"></a><span class="fu">naiveBayes</span>(class<span class="sc">~</span>r,<span class="at">data=</span>df)</span></code></pre></div>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##  QSO STAR 
##  0.5  0.5 
## 
## Conditional probabilities:
##       r
## Y          [,1]      [,2]
##   QSO  19.35770 0.8282726
##   STAR 17.96389 1.3651572</code></pre>
<blockquote>
<p>The summary of the model shows that the classes are balanced
(as the <code>A-priori probabilities</code> are 0.5 for each class).
It then shows the estimated distributions of <code>r</code> values
for quasars (a normal with estimated mean 19.358 and standard
deviation 0.828) and stars (mean 17.964 and standard deviation 1.365).
See Figure <a href="the-binomial-and-related-distributions.html#fig:nbplot">3.15</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbplot"></span>
<img src="_main_files/figure-html/nbplot-1.png" alt="\label{fig:nbplot}An illustration of the Naive Bayes regression model, as applied to the star-quasar dataset. The red curve is the estimated normal probability density function for stars, while the green curve is the estimated pdf for quasars. Because the two classes in this example are balanced, we can state that where the amplitude of the red curve is higher, the predicted class would be STAR; otherwise, it would be QUASAR." width="50%" />
<p class="caption">
Figure 3.15: An illustration of the Naive Bayes regression model, as applied to the star-quasar dataset. The red curve is the estimated normal probability density function for stars, while the green curve is the estimated pdf for quasars. Because the two classes in this example are balanced, we can state that where the amplitude of the red curve is higher, the predicted class would be STAR; otherwise, it would be QUASAR.
</p>
</div>
</div>
</div>
<div id="the-beta-distribution" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> The Beta Distribution<a href="the-binomial-and-related-distributions.html#the-beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>beta distribution</em> is a continuous distribution that is commonly used to
model random variables with finite, bounded domains.
Its probability density function is given by
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\,,
\]</span>
where <span class="math inline">\(x \in [0,1]\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are both <span class="math inline">\(&gt; 0\)</span>, and the normalization constant <span class="math inline">\(B(\alpha,\beta)\)</span> is
<span class="math display">\[
B(\alpha,\beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} \,.
\]</span>
(See Figure <a href="the-binomial-and-related-distributions.html#fig:betapdf">3.16</a>.)
We have seen the gamma function, <span class="math inline">\(\Gamma(\alpha)\)</span>, before; it is defined as
<span class="math display">\[
\Gamma(\alpha) = \int_0^\infty x^{\alpha-1} e^{-x} dx \,.
\]</span>
There are two things to note about the gamma function. The first is its
<em>recursive property</em>: <span class="math inline">\(\Gamma(\alpha+1) = \alpha \Gamma(\alpha)\)</span>.
(This can be shown by applying integration by parts.) The second is that
when <span class="math inline">\(\alpha\)</span> is a positive integer, the gamma function takes on the value
<span class="math inline">\((\alpha-1)! = (\alpha-1)(\alpha-2) \cdots 1\)</span>. (Note that <span class="math inline">\(\Gamma(1) = 0! = 1\)</span>.)</p>
<p>Regarding the statement above about model[ing] random variables with finite,
bounded domains: if a set of <span class="math inline">\(n\)</span> iid random variables
<span class="math inline">\(\mathbf{X}\)</span> has domain <span class="math inline">\([a,b]\)</span>, we can define a new set of random variables
<span class="math inline">\(\mathbf{Y}\)</span> via the transformation
<span class="math display">\[
\mathbf{Y} = \frac{\mathbf{X}-a}{b-a}
\]</span>
such that the domain becomes <span class="math inline">\([0,1]\)</span>. We can model these newly defined data
with the beta distribution. (Note the word <em>can</em>: we can model these data with
the beta distribution, but we dont have to, and it may be the case that
there is another
distribution bounded on the interval <span class="math inline">\([0,1]\)</span> that ultimately better
describes the data-generating process. The beta distribution just happens
to be commonly used.)</p>
<p>Butin the end, what does this all have to do with the binomial
distribution, the subject of this chapter? We know the binomial has a
parameter <span class="math inline">\(p \in [0,1]\)</span> and the domain of the beta distribution is <span class="math inline">\([0,1]\)</span>,
but is there more? Lets write down the binomial pmf:
<span class="math display">\[
\binom{k}{x} p^x (1-p)^{k-x} = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
This pmf dictates the probability of observing a particular value of <span class="math inline">\(x\)</span>
given <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span>. But what if we turn this around a bitand examine
this function if we fix <span class="math inline">\(k\)</span> and <span class="math inline">\(x\)</span> and vary <span class="math inline">\(p\)</span> instead? In other words,
lets examine the likelihood function
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
We can see immediately that the likelihood has the form of a beta
distribution if we set <span class="math inline">\(\alpha = x+1\)</span> and <span class="math inline">\(\beta = k-x+1\)</span>:
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{\Gamma(\alpha+\beta-1)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1} (1-p)^{\beta-1} \,,
\]</span>
<em>except</em> that the normalization term is not quite right: here we have
<span class="math inline">\(\Gamma(\alpha+\beta-1)\)</span> instead of <span class="math inline">\(\Gamma(\alpha+\beta)\)</span>. But thats
fine: there is no requirement that a likelihood function integrate to
one over its domain. (Here, as the interested reader can verify,
the likelihood function integrates to <span class="math inline">\(1/(k+1)\)</span>.)
So, in the end, if we observe a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k,p\)</span>),
then the likelihood function <span class="math inline">\(\mathcal{L}(p \vert k,x)\)</span> has the shape
(if not the normalization) of a Beta(<span class="math inline">\(x+1,k-x+1\)</span>) distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betapdf"></span>
<img src="_main_files/figure-html/betapdf-1.png" alt="\label{fig:betapdf}Three examples of beta probability density functions: Beta(2,2) (solid red line), Beta(4,2) (dashed green line), and Beta(2,3) (dotted blue line)." width="50%" />
<p class="caption">
Figure 3.16: Three examples of beta probability density functions: Beta(2,2) (solid red line), Beta(4,2) (dashed green line), and Beta(2,3) (dotted blue line).
</p>
</div>
<hr />
<div id="the-expected-value-of-a-beta-random-variable" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> The Expected Value of a Beta Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a random variable sampled from a Beta(<span class="math inline">\(\alpha,\beta\)</span>)
distribution is
<span class="math display">\[\begin{align*}
E[X] = \int_0^1 x f_X(x) dx &amp;= \int_0^1 x \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \,.
\end{align*}\]</span>
The last result follows from the fact that the integrand is the pdf for
a Beta(<span class="math inline">\(\alpha+1,\beta\)</span>) distribution, and the integral is over the entire domain,
hence the integral evaluates to 1.</p>
</blockquote>
<blockquote>
<p>Continuing,
<span class="math display">\[\begin{align*}
E[X] &amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \\
&amp;= \frac{\Gamma(\alpha+1) \Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
&amp;= \frac{\alpha \Gamma(\alpha)}{(\alpha+\beta)\Gamma(\alpha+\beta)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \\
&amp;= \frac{\alpha}{\alpha+\beta} \,.
\end{align*}\]</span>
Here, we take advantage of the recursive property of the gamma function.</p>
</blockquote>
<blockquote>
<p>We can utilize a similar strategy to determine the variance of a beta random variable,
starting by computing <span class="math inline">\(E[X^2]\)</span> and utilizing the shortcut formula
<span class="math inline">\(V[X] = E[X]^2 - (E[X])^2\)</span>. The final result is
<span class="math display">\[
V[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-sample-median-of-a-uniform01-distribution" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> The Sample Median of a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1 ~~~ x \in [0,1] \,.
\]</span>
Lets assume that we draw <span class="math inline">\(n\)</span> iid data from this distribution, with <span class="math inline">\(n\)</span> odd.
Then we can utilize a result from order statistics to write down the pdf of
the sample median, which is the <span class="math inline">\(j^{\rm th}\)</span> order statstic (where <span class="math inline">\(j = (n+1)/2\)</span>):
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} f_X(x) \left[ F_X(x) \right]^{(n-1)/2} \left[ 1 - F_X(x) \right]^{(n-1)/2} \,.
\]</span>
Given that
<span class="math display">\[
F_X(x) = \int_0^x dy = x ~~~ x \in [0,1] \,,
\]</span>
we can write
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} x^{(n-1)/2} (1-x)^{(n-1)/2} \,,
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. This function has both the form of a beta distribution
(with <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>) and the domain of a beta distribution,
so <span class="math inline">\(\tilde{X} \sim\)</span> Beta<span class="math inline">\(\left(\frac{n+1}{2},\frac{n+1}{2}\right)\)</span>.</p>
</blockquote>
<blockquote>
<p>(In fact, we can go further and state a more general result:
<span class="math inline">\(X_{(j)} \sim\)</span> Beta(<span class="math inline">\(j,n-j+1\)</span>): <em>all</em> the order statistics for data
drawn from a Uniform(0,1) distribution are beta-distributed random variables!
See Figure <a href="the-binomial-and-related-distributions.html#fig:betaunif">3.17</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betaunif"></span>
<img src="_main_files/figure-html/betaunif-1.png" alt="\label{fig:betaunif}The order statistic probability density functions $f_{(j)}(x)$ for, from left to right, $j = 1$ through $j = 5$, for the situation in which $n = 5$ iid data are drawn from a Uniform(0,1) distribution (overlaid in red). Each pdf is itself a beta distribution, with parameter values $j$ and $n-j+1$." width="50%" />
<p class="caption">
Figure 3.17: The order statistic probability density functions <span class="math inline">\(f_{(j)}(x)\)</span> for, from left to right, <span class="math inline">\(j = 1\)</span> through <span class="math inline">\(j = 5\)</span>, for the situation in which <span class="math inline">\(n = 5\)</span> iid data are drawn from a Uniform(0,1) distribution (overlaid in red). Each pdf is itself a beta distribution, with parameter values <span class="math inline">\(j\)</span> and <span class="math inline">\(n-j+1\)</span>.
</p>
</div>
<!--
---

### Testing Hypotheses Using the Sample Median

> Let's assume that we draw $n$ iid data, where $n$ is an odd number,
> from a Uniform($0,\theta$) 
> distribution, and that we wish to test the hypothesis $H_o: \mu = \mu_o
> = 1/2$ versus $H_a: \mu = \mu_a > \mu_o$ at level $\alpha$.
> (Let's also assume that all
> the data we observe lie in the range $[0,1]$...otherwise the null cannot
> be correct. We will return to this point in Chapter 5.) It sounds like
> we might use the Neymann-Pearson lemma here, but as we will see in Chapter
> 5, that would dictate a different test statistic than what we want to
> use here, which is the sample median (which is not a sufficient statistic).
> So here we will simply fall back on the methodology shown in Chapters 1 
> and 2, i.e., we will solve
$$
F_Y(y_{RR} \vert \mu_o) - q = 0 \,,
$$
> where $Y = X_{((n+1)/2)}$ is the sample median. We start by deriving
> the cdf $F_{((n+1)/2)}$:
\begin{align*}
F_{((n+1)/2)}(x) &= \int_0^x f_{((n+1)/2)}(v) dv = \ldots \,.
\end{align*}
> But, there's an issue. While the expressions for the cdfs for the
> minimum and maximum values are comprised of single terms, the expression
> for the cdf of the median is comprised of a summation of terms. (Go back
> to the section on order statistics earlier in this chapter to verify this.)
> Hence we cannot work with this cdf by hand. So...what can we do?

> Utilize numerical integration, that's what.

> Here is the code we need:

``` r
# This computes the cdf of the sample median at coordinate x
f <- function(x,n)
{
  j <- (n-1)/2
  # lfactorial(a) == log(a!)
  # exp(lfactorial(a)) == a!
  exp(lfactorial(n)-2*lfactorial(j))*x^j*(1-x)^j
}

# Find root of F_Y(y) - (1-alpha)
g <- function(y,n,alpha)
{
  F.med <- integrate(f,0,y,n=n)
  F.med$value - (1-alpha)
}

uniroot(g,interval=c(0,1),n=5,alpha=0.05)$root
```

```
## [1] 0.8107542
```
> In words, the function `f` above computes $F_{((n+1)/2)}(x)$, while
> the function `g` evaluates the function that we are trying to
> solve, specifically $F_{((n+1)/2)}(x) - (1-\alpha) = 0$. The call
> to `uniroot()` specifies that the root is between 0 and 1 (since
> $\theta_o = 2\mu_o = 1$). We see that if $n = 5$, we would reject the null
> hypothesis that $\mu_o = 1/2$ if and only if $X_{((n+1)/2)} \geq 0.811$,
> i.e., if three of the five values are greater than 0.811.

> Something to take away from this example is to realize that just because
> we cannot write down an analytical expression (in this case, for the 
> cdf of the median of $n$ Uniform(0,1) random variables), we should not
> just give up, as it may be easy to implement numerical methods! (As it 
> was in this case.)
-->
</div>
</div>
<div id="the-multinomial-distribution" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> The Multinomial Distribution<a href="the-binomial-and-related-distributions.html#the-multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets suppose we are in a situation in which we are gathering categorical data. For instance, we might be</p>
<ul>
<li>throwing a ball and recording which of bins numbered 1 through <span class="math inline">\(m\)</span> it lands in;</li>
<li>categorizing the condition of old coins as mint, very good, fine, etc.; or</li>
<li>classifying a galaxy as being a spiral galaxy, an elliptical galaxy, or an irregular galaxy.</li>
</ul>
<p>These are examples of <em>multinomial trials</em>, which is a generalization of the concept of binomial trials to
situations where the number of possible outcomes is <span class="math inline">\(m &gt; 2\)</span> rather than <span class="math inline">\(m = 2\)</span>. Earlier in this chapter,
we wrote down the five properties of binomial trials. The analogous properties of a multinomial experiment
are the following.</p>
<ol style="list-style-type: decimal">
<li>It consists of <span class="math inline">\(k\)</span> trials, with <span class="math inline">\(k\)</span> chosen in advance.</li>
<li>There are <span class="math inline">\(m\)</span> possible outcomes for each trial.</li>
<li>A trial may have no more than one realized outcome.</li>
<li>The outcomes of each trial are independent.</li>
<li>The probability of achieving the <span class="math inline">\(i^{th}\)</span> outcome is <span class="math inline">\(p_i\)</span>, a constant quantity.</li>
<li>The probabilities of each outcome sum to one: <span class="math inline">\(\sum_{i=1}^m p_i = 1\)</span>.</li>
<li>The number of trials that achieve a particular outcome is <span class="math inline">\(X_i\)</span>, with <span class="math inline">\(\sum_{i=1}^m X_i = k\)</span>.</li>
</ol>
<p>The probability of any given outcome <span class="math inline">\(\{X_1,\ldots,X_m\}\)</span> is given by the multinomial probability mass
function:
<span class="math display">\[
p(x_1,\ldots,x_m \vert p_1,\ldots,p_m) = \frac{k!}{x_1! \cdots x_m!}p_1^{x_1}\cdots p_m^{x_m} \,.
\]</span>
The distribution for any given <span class="math inline">\(X_i\)</span> itself is binomial, with
<span class="math inline">\(E[X_i] = kp_i\)</span> and <span class="math inline">\(V[X_i] = kp_i(1-p_i)\)</span>. This makes intuitive sense,
as one either observes <span class="math inline">\(i\)</span> as a trial outcome (success),
or something else (failure). However, the <span class="math inline">\(X_i\)</span>s are <em>not</em> independent random
variables; the covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>, a metric of linear
dependence, is Cov(<span class="math inline">\(X_i\)</span>,<span class="math inline">\(X_j\)</span>) = <span class="math inline">\(-kp_ip_j\)</span> if <span class="math inline">\(i \neq j\)</span>. (We will discuss
the concept of covariance in Chapter 6.)
This also makes intuitive sense: given that the number of trials <span class="math inline">\(k\)</span> is
fixed, observing more data that achieve one outcome will usually mean
we will observe fewer data achieving any other given outcome.</p>
<hr />
<p>For our immediate purposes, the context in which we work with the
multinomial distribution is hypothesis testing.</p>
<p>A single multinomial experiment yields a single data vector
<span class="math inline">\(\mathbf{X} = \{X_1,\ldots,X_m\}\)</span>. This vector is trivially
a sufficient statistic
for the vector of probabilities <span class="math inline">\(\mathbf{p} = \{p_1,\ldots,p_m\}\)</span>, and
the probability mass function of the sampling distribution for this
statistic is simply the multinomial pmf given above. Because we are dealing
with a multivariate distribution, we cannot easily determine hypothesis test
<span class="math inline">\(p\)</span>-values by hand; rather, we fall back on the definition that a <span class="math inline">\(p\)</span>-value
is the probability of observing the statistic value we see or a value
that is more extreme, and state that the <span class="math inline">\(p\)</span>-value for a multinomial
test is the probability that
a data vector simulated under the null hypothesis
has a pmf value that is smaller than the observed pmf value.
Estimating the <span class="math inline">\(p\)</span>-value for a multinomial test is easily done, as we
show in the example below.</p>
<hr />
<div id="performing-a-multinomial-test" class="section level3 hasAnchor" number="3.12.1">
<h3><span class="header-section-number">3.12.1</span> Performing a Multinomial Test<a href="the-binomial-and-related-distributions.html#performing-a-multinomial-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Imagine that we are scientists, sitting on a platform in the middle of a plain.
We are recording the number of animals of a particular species that we see, and
the azimuthal angle for each one. (An azimuthal angle is, e.g.,
<span class="math inline">\(0^\circ\)</span> when looking directly north, and 90<span class="math inline">\(^\circ\)</span>, 180<span class="math inline">\(^\circ\)</span>, and
270<span class="math inline">\(^\circ\)</span> as we look east, south, and west, etc.) The data we observe are</p>
</blockquote>
<table>
<colgroup>
<col width="35%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">angle range</th>
<th align="center">0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th align="center">90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th align="center">180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th align="center">270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">number of animals</td>
<td align="center">28</td>
<td align="center">32</td>
<td align="center">17</td>
<td align="center">23</td>
</tr>
</tbody>
</table>
<blockquote>
<p>and the question we wish to
answer is whether the animals uniformly distributed as a function of angle.</p>
</blockquote>
<blockquote>
<p>To answer this question, we simulate <span class="math inline">\(100{,}000\)</span> data vectors given the
null hypothesis <span class="math inline">\(H_o : \{p_1 = 0.25, p_2 = 0.25, p_3 = 0.25, p_4 = 0.25\}\)</span>
and determine the proportion of vectors with multinomial pmf values that
are smaller than the value we observe:</p>
</blockquote>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="the-binomial-and-related-distributions.html#cb232-1" tabindex="-1"></a>O       <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb232-2"><a href="the-binomial-and-related-distributions.html#cb232-2" tabindex="-1"></a>p       <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>,<span class="dv">4</span>)</span>
<span id="cb232-3"><a href="the-binomial-and-related-distributions.html#cb232-3" tabindex="-1"></a>pmf.obs <span class="ot">&lt;-</span> <span class="fu">dmultinom</span>(O,<span class="at">prob=</span>p)    <span class="co"># the observed multinomial pmf value</span></span>
<span id="cb232-4"><a href="the-binomial-and-related-distributions.html#cb232-4" tabindex="-1"></a></span>
<span id="cb232-5"><a href="the-binomial-and-related-distributions.html#cb232-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb232-6"><a href="the-binomial-and-related-distributions.html#cb232-6" tabindex="-1"></a></span>
<span id="cb232-7"><a href="the-binomial-and-related-distributions.html#cb232-7" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb232-8"><a href="the-binomial-and-related-distributions.html#cb232-8" tabindex="-1"></a>k       <span class="ot">&lt;-</span> <span class="fu">sum</span>(O)</span>
<span id="cb232-9"><a href="the-binomial-and-related-distributions.html#cb232-9" tabindex="-1"></a>m       <span class="ot">&lt;-</span> <span class="fu">length</span>(O)</span>
<span id="cb232-10"><a href="the-binomial-and-related-distributions.html#cb232-10" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">rmultinom</span>(num.sim,k,p) <span class="co"># generates an m x num.sim matrix</span></span>
<span id="cb232-11"><a href="the-binomial-and-related-distributions.html#cb232-11" tabindex="-1"></a>                                  <span class="co"># (m determined as the length of p)</span></span>
<span id="cb232-12"><a href="the-binomial-and-related-distributions.html#cb232-12" tabindex="-1"></a>pmf     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">2</span>,<span class="cf">function</span>(x){<span class="fu">dmultinom</span>(x,<span class="at">prob=</span>p)}) <span class="co"># pmf vector</span></span>
<span id="cb232-13"><a href="the-binomial-and-related-distributions.html#cb232-13" tabindex="-1"></a><span class="fu">sum</span>(pmf<span class="sc">&lt;</span>pmf.obs)<span class="sc">/</span>num.sim</span></code></pre></div>
<pre><code>## [1] 0.15974</code></pre>
<blockquote>
<p>What does <code>apply()</code> do? It calls the function given as
the third argument (which evaluates the multinomial probability
mass function given a set of four data <span class="math inline">\(\mathbf{x}\)</span> and a
a set of four probabilities <span class="math inline">\(\mathbf{p}\)</span>) to each <em>column</em> (hence
the 2 as the second argument1 would denote rows) of the
matrix <span class="math inline">\(\mathbf{X}\)</span>. It is a convenient function that allows us to
not have to embed the evaluation of the multinomial pmf into a <code>for</code>
loop that would iterate over all the rows of the matrix.
We note that the above computation takes $$1 CPU second on
a standard desktop/laptop computer.</p>
</blockquote>
<blockquote>
<p>We observe <span class="math inline">\(p = 0.160\)</span>, i.e., approximately <span class="math inline">\(16{,}000\)</span> of the <span class="math inline">\(100{,}000\)</span>
simulated data vectors have pmf values smaller than what we observe.
We thus fail to reject the null hypothesis and conclude that it is
plausible that the animals are distributed uniformly as a function of angle.</p>
</blockquote>
<blockquote>
<p>The multinomial hypothesis test is <em>exact</em>, in the sense that the
data are distributed according to the multinomial distribution, but because
we only sample a finite number of data vectors, the <span class="math inline">\(p\)</span>-value is an
estimate and not an exact value. We can utilize the framework given
earlier in the chapter for constructing confidence intervals for the
binomial probability <span class="math inline">\(p\)</span> to ascertain the level of uncertainty in the
estimated <span class="math inline">\(p\)</span>-value:</p>
</blockquote>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="the-binomial-and-related-distributions.html#cb234-1" tabindex="-1"></a>y.obs <span class="ot">&lt;-</span> <span class="fu">sum</span>(pmf<span class="sc">&lt;</span>pmf.obs)</span>
<span id="cb234-2"><a href="the-binomial-and-related-distributions.html#cb234-2" tabindex="-1"></a>f     <span class="ot">&lt;-</span> <span class="cf">function</span>(p,k,y.obs,q)</span>
<span id="cb234-3"><a href="the-binomial-and-related-distributions.html#cb234-3" tabindex="-1"></a>{</span>
<span id="cb234-4"><a href="the-binomial-and-related-distributions.html#cb234-4" tabindex="-1"></a>  <span class="fu">pbinom</span>(y.obs,<span class="at">size=</span>k,<span class="at">prob=</span>p) <span class="sc">-</span> q</span>
<span id="cb234-5"><a href="the-binomial-and-related-distributions.html#cb234-5" tabindex="-1"></a>}</span>
<span id="cb234-6"><a href="the-binomial-and-related-distributions.html#cb234-6" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">k=</span>num.sim,<span class="at">y.obs=</span>y.obs<span class="dv">-1</span>,<span class="at">q=</span><span class="fl">0.975</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.1574441</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="the-binomial-and-related-distributions.html#cb236-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">k=</span>num.sim,<span class="at">y.obs=</span>y.obs  ,<span class="at">q=</span><span class="fl">0.025</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.1620439</code></pre>
<blockquote>
<p>The 95% two-sided confidence interval on the true <span class="math inline">\(p\)</span>-value
is <span class="math inline">\([0.1574,0.1620]\)</span>.
To decrease the interval length, we simply increase <code>num.sim</code>,
with the tradeoff that the computation time becomes that much longer.</p>
</blockquote>
</div>
</div>
<div id="chi-square-based-hypothesis-testing" class="section level2 hasAnchor" number="3.13">
<h2><span class="header-section-number">3.13</span> Chi-Square-Based Hypothesis Testing<a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Performing a hypothesis test that utilizes the multinomial
distribution is essentially
impossible to do by hand; multinomial distributions are
intrinsically high-dimensional and the data (the numbers of counts in each
bin) are not iid. Thus in the era before computers, statisticians could not
carry out exact hypothesis
tests. In a paper published in 1900, the statistician
Karl Pearson noted that as <span class="math inline">\(k \rightarrow \infty\)</span>, multinomial random variables
converge in distribution to multivariate normal random variables (with the
latter being something we will discuss in Chapter 6), and that the
statistic
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i-E[X_i])^2}{E[X_i]} = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i}
\]</span>
converges in distribution to a chi-square random variable for
<span class="math inline">\(m-1\)</span> degrees of freedom.
(We subtract 1 because only <span class="math inline">\(m-1\)</span> of the multinomial
probabilities can freely vary; the <span class="math inline">\(m^{th}\)</span> one is constrained by
the fact that the probabilities must sum to 1. This constraint is what
makes multinomial data <em>not</em> iid.)</p>
<p>The computation of the test statistic <span class="math inline">\(W\)</span> is the
basis of the <em>chi-square goodness-of-fit test</em>, or chi-square GoF test.
For this test,</p>
<ul>
<li>we reject the null hypothesis if <span class="math inline">\(W &gt; w_{RR}\)</span>;</li>
<li>the rejection region boundary is <span class="math inline">\(w_{RR} = F_{W(m-1)}^{-1}(1-\alpha)\)</span> (in <code>R</code>, <code>qchisq(1-alpha,m-1)</code>);</li>
<li>the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(1 - F_{W(m-1)}(w_{\rm obs})\)</span> (e.g., <code>1-pchisq(w.obs,m-1)</code>); and</li>
<li>by convention, <span class="math inline">\(kp_i\)</span> must be <span class="math inline">\(\geq\)</span> 5 in each bin for the test to yield a valid result.</li>
</ul>
<p>At this point, we should state that
<strong>one should always implement exact multinomial tests rather
than chi-square GoF tests,</strong> given the choice.
We are discussing the chi-square GoF test here
because its use is ubiquitous in statistics and thus every student
should know what it is and how it is carried outbut not because one should
actually use it now, in the age of computers. The price that we pay when
we use the chi-square GoF test is that, for relatively small values of <span class="math inline">\(k\)</span>, it
yields <em>biased</em> <span class="math inline">\(p\)</span>-value estimates.
In an example below, we demonstrate that
the chi-square GoF test yields <span class="math inline">\(p\)</span>-values that are, on average,
smaller than expected, making it less conservative than the exact
multinomial test (i.e., it rejects correct null hypotheses at a higher
rate than we would expect).</p>
<hr />
<p>There are two variations on the chi-square GoF test for which no
exact analogues exist, ones in which the inputs are tables of
observed data. The tests differ depending upon how we collect data:</p>
<ul>
<li><em>chi-square test of independence</em>: the question is whether two variables are
associated with each other in a population; subjects are selected and the
values of two variables are recorded for each. For instance, we might select
<span class="math inline">\(k\)</span> people at random and record whether or not they have had Covid-19, and
also record whether or not they initially had zero, one, or two vaccine shots,
and put these data into a table with, e.g., yes and no defining the rows
and 0, 1, and 2 defining the columns. If we reject the null, we are stating
that the distributions of data along, e.g., each row are statistically
significantly different from each other.</li>
<li><em>chi-square test of homogeneity</em>: the question is whether the distribution
of a single variable is the same for two subgroups of a population, with subjects
being selected randomly from each subgroup separately. For instance, we might
select <span class="math inline">\(k\)</span> people under 20 years of age and ask if they prefer vanilla, chocolate,
or strawberry ice cream, and then repeat the process for people of age 20 or over, and put the data into a table similar to that described for the test
of independence above.</li>
</ul>
<p>Whether we perform a test of independence versus one of homogeneity affects the
interpretation of results, but algorithmically the tests are identical.
Under the null hypothesis,
<span class="math display">\[
\widehat{E[X_{ij}]} = \frac{r_i c_j}{k} \,,
\]</span>
i.e., the expected value in the cell in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> is the product of the total number
of data in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span>, divided by the total number of data overall. Then,
<span class="math display">\[
W = \sum_{i=1}^r \sum_{j=1}^c \frac{(X_{ij} - \widehat{E[X_{ij}]})^2}{\widehat{E[X_{ij}]}} \mathrel{\dot\sim} \chi_{(r-1)(c-1)}^2 \,,
\]</span>
i.e., the test statistic <span class="math inline">\(W\)</span> is, under the null, assumed to be
chi-square distributed for <span class="math inline">\((r-1) \times (c-1)\)</span> degrees of freedom.
(Because we do not specify exact probabilities for observing data in
each table cell under the null, the observed data
are <em>not</em> sampled according to a multinomial distributionand, in fact,
we cannot specify an exact distribution. Hence for testing independence
and homogeneity given categorical data, the chi-square framework is
going to be the most appropriate framework for us to use.)</p>
<hr />
<div id="chi-square-goodness-of-fit-test" class="section level3 hasAnchor" number="3.13.1">
<h3><span class="header-section-number">3.13.1</span> Chi-Square Goodness of Fit Test<a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets go back to working with our animal/angle data, and determine,
via the use of the chi-square GoF test, whether it is
plausible that the number of animals is distributed uniformly as
a function of angle.</p>
</blockquote>
<blockquote>
<p>We first compute the test statistic:
<span class="math display">\[\begin{align*}
W &amp;= \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} = \frac{(28-25)^2}{25} + \frac{(32-25)^2}{25} + \frac{(17-25)^2}{25} + \frac{(23-25)^2}{25} \\
&amp;= \frac{9}{25} + \frac{49}{25} + \frac{64}{25} + \frac{4}{25} = \frac{126}{25} = 5.04 \,.
\end{align*}\]</span>
The number of degrees of freedom is <span class="math inline">\(m-1 = 3\)</span>, so the rejection region boundary
is <span class="math inline">\(F_{W}^{-1}(1-\alpha)\)</span> = <code>qchisq(0.95,3)</code> = 7.815. Since 5.04 <span class="math inline">\(&lt;\)</span> 7.815, we
fail to reject the null hypothesis that the animals are distributed uniformly as
a function of azimuthal angle.
See Figure <a href="the-binomial-and-related-distributions.html#fig:chi2gof">3.18</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2gof"></span>
<img src="_main_files/figure-html/chi2gof-1.png" alt="\label{fig:chi2gof}An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, hence we fail to reject the null hypothesis." width="50%" />
<p class="caption">
Figure 3.18: An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, hence we fail to reject the null hypothesis.
</p>
</div>
<blockquote>
<p>The observed <span class="math inline">\(p\)</span>-value is <code>1 - pchisq(5.04,3)</code> = 0.169, which is
5.6% larger than the value observed for the exact
multinomial test (0.160, with 95% CI <span class="math inline">\([0.1574,0.1620]\)</span> for <span class="math inline">\(100{,}000\)</span>
simulations). As we will see below, sometimes the <span class="math inline">\(p\)</span>-value is larger and
sometimes smaller, but on average the <span class="math inline">\(p\)</span>-value observed for the
chi-square GoF test will be <em>smaller</em> than that observed for the
multinomial test, at least for small values of <span class="math inline">\(k\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-average-p-value-difference-between-the-multinomial-and-chi-square-gof-tests" class="section level3 hasAnchor" number="3.13.2">
<h3><span class="header-section-number">3.13.2</span> The Average p-Value Difference Between the Multinomial and Chi-Square GoF Tests<a href="the-binomial-and-related-distributions.html#the-average-p-value-difference-between-the-multinomial-and-chi-square-gof-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that in an experiment, we toss a six-sided die <span class="math inline">\(k\)</span> times and
record the result. For instance, we might toss it <span class="math inline">\(k = 10\)</span> times,
and record the
data vector <span class="math inline">\(\mathbf{X} = \{2,1,3,1,0,3\}\)</span> (i.e., we observe 1 two times,
2 one time, etc.). We wish to test the hypothesis that the die is
fair. On average, what is the difference in the <span class="math inline">\(p\)</span>-value observed when
utilize the exact multinomial test versus when we utilize the approximate
chi-square GoF test?</p>
</blockquote>
<blockquote>
<p>Lets first illustrate how we would determine the average difference when
<span class="math inline">\(k = 30\)</span>. (See Figure <a href="the-binomial-and-related-distributions.html#fig:deltap">3.19</a>.) For this number of trials, the
expected number of counts in each bin is exactly 5, i.e., this is the
minimum number of trials required by convention when carrying out
the chi-square GoF test.</p>
</blockquote>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="the-binomial-and-related-distributions.html#cb238-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">30</span></span>
<span id="cb238-2"><a href="the-binomial-and-related-distributions.html#cb238-2" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb238-3"><a href="the-binomial-and-related-distributions.html#cb238-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span>m,m)</span>
<span id="cb238-4"><a href="the-binomial-and-related-distributions.html#cb238-4" tabindex="-1"></a></span>
<span id="cb238-5"><a href="the-binomial-and-related-distributions.html#cb238-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb238-6"><a href="the-binomial-and-related-distributions.html#cb238-6" tabindex="-1"></a></span>
<span id="cb238-7"><a href="the-binomial-and-related-distributions.html#cb238-7" tabindex="-1"></a>num.rep <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb238-8"><a href="the-binomial-and-related-distributions.html#cb238-8" tabindex="-1"></a>p.chi   <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.rep)</span>
<span id="cb238-9"><a href="the-binomial-and-related-distributions.html#cb238-9" tabindex="-1"></a>p.mult  <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.rep)</span>
<span id="cb238-10"><a href="the-binomial-and-related-distributions.html#cb238-10" tabindex="-1"></a>delta.p <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>,num.rep)</span>
<span id="cb238-11"><a href="the-binomial-and-related-distributions.html#cb238-11" tabindex="-1"></a></span>
<span id="cb238-12"><a href="the-binomial-and-related-distributions.html#cb238-12" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb238-13"><a href="the-binomial-and-related-distributions.html#cb238-13" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>num.rep ) {</span>
<span id="cb238-14"><a href="the-binomial-and-related-distributions.html#cb238-14" tabindex="-1"></a>  X.obs       <span class="ot">&lt;-</span> <span class="fu">rmultinom</span>(<span class="dv">1</span>,k,p)</span>
<span id="cb238-15"><a href="the-binomial-and-related-distributions.html#cb238-15" tabindex="-1"></a>  W           <span class="ot">&lt;-</span> <span class="fu">sum</span>((X.obs<span class="sc">-</span>k<span class="sc">*</span>p)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(k<span class="sc">*</span>p))</span>
<span id="cb238-16"><a href="the-binomial-and-related-distributions.html#cb238-16" tabindex="-1"></a>  p.chi[ii]   <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(W,m<span class="dv">-1</span>)</span>
<span id="cb238-17"><a href="the-binomial-and-related-distributions.html#cb238-17" tabindex="-1"></a>  obs         <span class="ot">&lt;-</span> <span class="fu">dmultinom</span>(X.obs,<span class="at">prob=</span>p)</span>
<span id="cb238-18"><a href="the-binomial-and-related-distributions.html#cb238-18" tabindex="-1"></a>  X           <span class="ot">&lt;-</span> <span class="fu">rmultinom</span>(num.sim,k,p)</span>
<span id="cb238-19"><a href="the-binomial-and-related-distributions.html#cb238-19" tabindex="-1"></a>  a           <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">2</span>,<span class="cf">function</span>(x,p){<span class="fu">dmultinom</span>(x,<span class="at">prob=</span>p)},<span class="at">p=</span>p)</span>
<span id="cb238-20"><a href="the-binomial-and-related-distributions.html#cb238-20" tabindex="-1"></a>  p.mult[ii]  <span class="ot">&lt;-</span> <span class="fu">sum</span>(a<span class="sc">&lt;=</span>obs)<span class="sc">/</span>num.sim</span>
<span id="cb238-21"><a href="the-binomial-and-related-distributions.html#cb238-21" tabindex="-1"></a>  delta.p[ii] <span class="ot">&lt;-</span> p.mult[ii] <span class="sc">-</span> p.chi[ii]</span>
<span id="cb238-22"><a href="the-binomial-and-related-distributions.html#cb238-22" tabindex="-1"></a>}</span>
<span id="cb238-23"><a href="the-binomial-and-related-distributions.html#cb238-23" tabindex="-1"></a></span>
<span id="cb238-24"><a href="the-binomial-and-related-distributions.html#cb238-24" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Mean Difference:&quot;</span>,<span class="fu">round</span>(<span class="fu">mean</span>(delta.p),<span class="dv">5</span>),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Mean Difference: 0.0098</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="the-binomial-and-related-distributions.html#cb240-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;SE Difference:  &quot;</span>,<span class="fu">round</span>(<span class="fu">sd</span>(delta.p)<span class="sc">/</span><span class="fu">sqrt</span>(num.rep),<span class="dv">5</span>),<span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## SE Difference:   0.00135</code></pre>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="the-binomial-and-related-distributions.html#cb242-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span><span class="fu">data.frame</span>(delta.p),<span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">x=</span>delta.p,<span class="at">y=</span><span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb242-2"><a href="the-binomial-and-related-distributions.html#cb242-2" tabindex="-1"></a>      <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;dodgerblue&quot;</span>,<span class="at">bins=</span><span class="dv">25</span>) <span class="sc">+</span></span>
<span id="cb242-3"><a href="the-binomial-and-related-distributions.html#cb242-3" tabindex="-1"></a>      <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="dv">0</span>,<span class="at">col=</span><span class="st">&quot;seagreen&quot;</span>,<span class="at">lty=</span><span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb242-4"><a href="the-binomial-and-related-distributions.html#cb242-4" tabindex="-1"></a>      <span class="fu">xlab</span>(<span class="fu">expression</span>(Delta<span class="sc">*</span><span class="st">&quot;p&quot;</span>)) <span class="sc">+</span></span>
<span id="cb242-5"><a href="the-binomial-and-related-distributions.html#cb242-5" tabindex="-1"></a>      base_theme</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deltap"></span>
<img src="_main_files/figure-html/deltap-1.png" alt="\label{fig:deltap}Histogram showing the empirical distribution of values of $\Delta p = p_{\rm mult} - p_{\rm chi}$ that arise when we repeatedly toss a die 30 times and test whether the die is fair. If $\Delta p &gt; 0$, then the chi-square test is less conservative, i.e., we are more likely to reject a correct null than if we are to use the exact multinomial test. Here, the average value of $\Delta p$ is $\approx 0.01$." width="50%" />
<p class="caption">
Figure 3.19: Histogram showing the empirical distribution of values of <span class="math inline">\(\Delta p = p_{\rm mult} - p_{\rm chi}\)</span> that arise when we repeatedly toss a die 30 times and test whether the die is fair. If <span class="math inline">\(\Delta p &gt; 0\)</span>, then the chi-square test is less conservative, i.e., we are more likely to reject a correct null than if we are to use the exact multinomial test. Here, the average value of <span class="math inline">\(\Delta p\)</span> is <span class="math inline">\(\approx 0.01\)</span>.
</p>
</div>
<blockquote>
<p>We find that for <span class="math inline">\(k = 30\)</span>, the average
value of <span class="math inline">\(\Delta p = p_{\rm mult} - p_{\rm chi}\)</span>
is <span class="math inline">\(\approx 0.01\)</span>, i.e., on average the chi-square GoF test is
less conservative (or more likely to result in the rejection
of a correct null hypothesis).</p>
</blockquote>
<blockquote>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:deltap2">3.20</a>, we show how the average value of
<span class="math inline">\(\Delta p\)</span> changes as a function of <span class="math inline">\(k\)</span>. We see here that while by
convention the expected number of counts per bin is supposed to be
at least five, it is only when the expected counts reach
<span class="math inline">\(\approx\)</span> 20 or more that we no longer observed a statistically significant
difference between the average <span class="math inline">\(p\)</span>-values.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:deltap2"></span>
<img src="_main_files/figure-html/deltap2-1.png" alt="\label{fig:deltap2}The average value of $\Delta p = p_{\rm mult} - p_{\rm chi}$ as a function of the number of tosses of a six-sided die. The vertical dashed green line at $k = 30$ represents where the expected number of counts in each data bin is exactly 5, the minimum value conventionally required for use of the chi-square GoF test." width="50%" />
<p class="caption">
Figure 3.20: The average value of <span class="math inline">\(\Delta p = p_{\rm mult} - p_{\rm chi}\)</span> as a function of the number of tosses of a six-sided die. The vertical dashed green line at <span class="math inline">\(k = 30\)</span> represents where the expected number of counts in each data bin is exactly 5, the minimum value conventionally required for use of the chi-square GoF test.
</p>
</div>
<hr />
</div>
<div id="chi-square-test-of-independence" class="section level3 hasAnchor" number="3.13.3">
<h3><span class="header-section-number">3.13.3</span> Chi-Square Test of Independence<a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets go back to our animal data. When we observe the animals in each quadrant, lets suppose we also
record their colors: black or red. So now are data look like this:</p>
</blockquote>
<table>
<colgroup>
<col width="23%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>black</td>
<td>20</td>
<td>18</td>
<td>5</td>
<td>14</td>
<td>57</td>
</tr>
<tr class="even">
<td>red</td>
<td>8</td>
<td>14</td>
<td>12</td>
<td>9</td>
<td>43</td>
</tr>
<tr class="odd">
<td></td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>When we record two attributes for each subject (here, azimuthal angle and color), we
can perform a chi-square test of independence to answer the question of whether
the attributes are independent random variables. In other words, here, does the
coloration <em>depend</em> on angle? The null hypothesis is no. We will test this hypothesis
assuming <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>We first determine the expected number of counts in each bin,
<span class="math inline">\(\widehat{E[X_{ij}]} = r_i c_j / k\)</span>:</p>
</blockquote>
<table>
<colgroup>
<col width="8%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"></th>
<th align="center">0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th align="center">90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th align="center">180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th align="center">270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">black</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 28/100 = 15.96</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 32/100 = 18.24</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 17/100 = 9.69</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 23/100 = 13.11</td>
<td align="center">57</td>
</tr>
<tr class="even">
<td align="right">red</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 28/100 = 12.04</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 32/100 = 13.76</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 17/100 = 7.31</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 23/100 = 9.89</td>
<td align="center">43</td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="center">28</td>
<td align="center">32</td>
<td align="center">17</td>
<td align="center">23</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<blockquote>
<p>We can already see that working with the numbers directly is tedious. Can we make
a matrix of such numbers using <code>R</code>?</p>
</blockquote>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="the-binomial-and-related-distributions.html#cb243-1" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">57</span>,<span class="dv">43</span>)</span>
<span id="cb243-2"><a href="the-binomial-and-related-distributions.html#cb243-2" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb243-3"><a href="the-binomial-and-related-distributions.html#cb243-3" tabindex="-1"></a>E <span class="ot">&lt;-</span> (r <span class="sc">%*%</span> <span class="fu">t</span>(c))<span class="sc">/</span><span class="fu">sum</span>(r)  <span class="co"># multiply r and the transpose of c </span></span>
<span id="cb243-4"><a href="the-binomial-and-related-distributions.html#cb243-4" tabindex="-1"></a><span class="fu">print</span>(E)                  <span class="co"># much better</span></span></code></pre></div>
<pre><code>##       [,1]  [,2] [,3]  [,4]
## [1,] 15.96 18.24 9.69 13.11
## [2,] 12.04 13.76 7.31  9.89</code></pre>
<blockquote>
<p>If we wish to continue using <code>R</code>, we need to define a matrix of observed data values:</p>
</blockquote>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="the-binomial-and-related-distributions.html#cb245-1" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">8</span>,<span class="dv">18</span>,<span class="dv">14</span>,<span class="dv">5</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">9</span>),<span class="at">nrow=</span><span class="dv">2</span>) <span class="co"># fills in column-by-column</span></span>
<span id="cb245-2"><a href="the-binomial-and-related-distributions.html#cb245-2" tabindex="-1"></a><span class="fu">print</span>(O)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   20   18    5   14
## [2,]    8   14   12    9</code></pre>
<blockquote>
<p>Now we have what we need. The test statistic is</p>
</blockquote>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="the-binomial-and-related-distributions.html#cb247-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>( (O<span class="sc">-</span>E)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>E ),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.805</code></pre>
<blockquote>
<p>and the number of degrees of freedom is <span class="math inline">\((r-1)(c-1) = 1 \cdot 3 = 3\)</span>, so the
rejection region boundary is</p>
</blockquote>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="the-binomial-and-related-distributions.html#cb249-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qchisq</span>(<span class="fl">0.95</span>,<span class="dv">3</span>),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.815</code></pre>
<blockquote>
<p>We find that if <span class="math inline">\(\alpha = 0.05\)</span>, we <em>cannot</em> reject the null hypothesis. We might be
tempted to do so, as our test statistic <em>very nearly</em> falls into the rejection region,
but we cannot. We could, if we were so inclined, remind ourselves that chi-square-based
hypothesis tests are <em>approximate</em>, and run a simulation to try to estimate the
true distribution of <span class="math inline">\(W\)</span>, and see what the rejection region and <span class="math inline">\(p\)</span>-value actually
arethat way, we <em>might</em> be able to actually reject the null.
But really, at the end of the day, such a result should simply motivate
us to gather more data!</p>
</blockquote>
</div>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="3.14">
<h2><span class="header-section-number">3.14</span> Exercises<a href="the-binomial-and-related-distributions.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_1,...,X_n\)</span> be <span class="math inline">\(n\)</span> data drawn from a <span class="math inline">\(\mathcal{N}(2,4)\)</span> distribution. Write down the probability that exactly <span class="math inline">\(m\)</span> of the <span class="math inline">\(n\)</span> data have values <span class="math inline">\(&gt; 2\)</span>.</p></li>
<li><p>You are a habitual buyer of raffle tickets. Each ticket costs $1, and each time you buy a ticket, you have a 40% chance of having a winning ticket, for which you will receive $3. You decide to keep buying tickets until you win for the first time. Let the random variable <span class="math inline">\(F\)</span> denote the total number of <em>losing</em> tickets that you buy, and let <span class="math inline">\(W\)</span> denote the total amount of money you win (or lose!). (a) <span class="math inline">\(F\)</span> is sampled from what distribution that has what parameter value(s)? (b) What is the probability that you will end up making money, i.e., what is <span class="math inline">\(P(W &gt; 0)\)</span>? (c) Lets say instead of buying until you win, you decide you will buy exactly two tickets, then stop. What is the probability of buying a winning ticket exactly once, given that you buy at least one winning ticket?</p></li>
<li><p>Suppose 50% of licensed drivers in a state are insured. If three licensed drivers are selected at random, what is the expected number of insured drivers given that the number of insured drivers is odd? (Assume the states population is effectively infinite, so that the probability of selecting an insured driver stays constant from trial to trial.)</p></li>
<li><p>In an experiment, you take shots at a target until you hit that target twice. On any given shot, you have a 50% change of hitting the target. (a) What is the probability that you hit the target for the second time on your fourth shot overall? (b) You run your experiment twice. What is the probability of needing to take two shots one of the experiments to hit the target twice, and three shots during the other one to hit the target twice? Assume the results are independent (and of course identically distributed). (c) Lets suppose you do not actually know the success probability <span class="math inline">\(p\)</span>, and you wish to determine a one-sided 90% upper bound on <span class="math inline">\(p\)</span>. As you are dealing with a discrete distribution, you need to build a <code>uniroot()</code>-style interval estimator. In your code, what value would you adopt for <span class="math inline">\(q\)</span>?</p></li>
<li><p>You collect <span class="math inline">\(n=4\)</span> iid samples from the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \left\{ \begin{array}{cc} 3x^2 &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{eqnarray*}\]</span>
(a) Write down the sampling distribution for the minimum observed value, <span class="math inline">\(X_{(1)}\)</span>. (b) The sampling distribution for the maximum observed value is <span class="math inline">\(g_{(4)}(x) = 12x^{11}\)</span>. What is <span class="math inline">\(E[X_{(4)}]\)</span>?</p></li>
<li><p>You are to sample <span class="math inline">\(n = 3\)</span> data from the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \left\{ \begin{array}{cc} x &amp; 0 \leq x \leq \sqrt{2} \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{eqnarray*}\]</span>
(a) Specify the distribution from which <span class="math inline">\(X_{(3)}\)</span> is to be sampled. (b) Specify the variance of the distribution you derived in part (a).</p></li>
<li><p>If we sample 3 iid data from a Uniform(0,1) distribution, what is the probability that the median value lies between 1/3 and 2/3?</p></li>
<li><p>You sample three data from a Exp(1) distribution. What is
the expected value for the median datum, i.e., the second ordered datum?</p></li>
<li><p>Let the cdf for a given distribution be <span class="math inline">\(F_X(x) = x^3\)</span> for <span class="math inline">\(x \in [0,1]\)</span>, and lets assume that you have sampled <span class="math inline">\(n\)</span> iid data from this distribution. (a) What is the pdf within the domain <span class="math inline">\(x \in [0,1]\)</span>? (b) What is the pdf for <span class="math inline">\(X_{(n)}\)</span> within the domain <span class="math inline">\(x \in [0,1]\)</span>? (c) What is the cdf for <span class="math inline">\(X_{(n)}\)</span> within the domain <span class="math inline">\(x \in [0,1]\)</span>? (d) What is <span class="math inline">\(E[X_{(n)}]\)</span>?</p></li>
<li><p>You sample <span class="math inline">\(n = 2\)</span> iid random variables from a distribution with pdf
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac12 x \,,
\end{eqnarray*}\]</span>
for <span class="math inline">\(x \in [0,2]\)</span>. (a) What is <span class="math inline">\(F_X(x)\)</span> within the domain <span class="math inline">\([0,2]\)</span>? (b) What is <span class="math inline">\(f_{(2)}(x)\)</span>? (c) What is <span class="math inline">\(E[X_{(2)}]\)</span>? (d) Are <span class="math inline">\(X_{(1)}\)</span> and <span class="math inline">\(X_{(2)}\)</span> independent random variables?</p></li>
<li><p>Find the asymptotic distribution of the MLE of <span class="math inline">\(n\)</span> i.i.d. samples from the Bernoulli distribution with parameter <span class="math inline">\(p\)</span>. (<span class="math inline">\(p_X(x) = p^x(1-p)^{1-x}\)</span> for <span class="math inline">\(x = \{0,1\}\)</span> and for <span class="math inline">\(p \in (0,1)\)</span>.)</p></li>
<li><p>You are given <span class="math inline">\(n\)</span> iid data that are sampled from the following pmf:
<span class="math display">\[\begin{equation*}
p_X(x) = (1-p)^{x-1}p \,,
\end{equation*}\]</span>
with <span class="math inline">\(0 &lt; p &lt; 1\)</span> and <span class="math inline">\(x = \{1,2,3,\ldots\}\)</span>. For this distribution, <span class="math inline">\(E[X] = 1/p\)</span> and <span class="math inline">\(V[X] = (1-p)/p^2\)</span>. (a) What is the maximum likelihood estimator for <span class="math inline">\(1/p\)</span>? Note: you are not required to confirm that the derivative of the score function is negative at <span class="math inline">\(1/p = \widehat{1/p}_{MLE}\)</span>. Also note: a property of MLEs may help you here. (b) Write down <span class="math inline">\(V[\widehat{1/p}_{MLE}]\)</span>, i.e., the variance of the MLE for <span class="math inline">\(1/p\)</span>.</p></li>
<li><p>The probability mass function for the logarithmic distribution is
<span class="math display">\[\begin{eqnarray*}
p_X(x) = -\frac{1}{\log(1-p)} \frac{p^x}{x} \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \in \{1,2,3,...\}\)</span>. You sample <span class="math inline">\(n\)</span> iid data from this distribution. Write down a sufficient statistic for <span class="math inline">\(p\)</span>.</p></li>
<li><p>You are given the following probability density function:
<span class="math display">\[\begin{equation*}
f_X(x) = a b x^{a-1} (1-x^a)^{b-1} \,,
\end{equation*}\]</span>
defined over the domain <span class="math inline">\(0 &lt; x &lt; 1\)</span>. <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the parameters of the distribution, and both are positive and real-valued. Let <span class="math inline">\(\mathbf{X} = \{X_1,\cdots,X_n\}\)</span> be <span class="math inline">\(n\)</span> i.i.d.~samples from this distribution. (a) If you are able to define joint sufficient statistics for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, write them down; otherwise, explain why you cannot. (b) Now, let <span class="math inline">\(a=1\)</span>. What is a sufficient statistic for <span class="math inline">\(b\)</span>?</p></li>
<li><p>You sample <span class="math inline">\(n\)</span> iid data from the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{x}{\beta^2} e^{-x/\beta} \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span>, and where <span class="math inline">\(E[X] = 2\beta\)</span> and <span class="math inline">\(V[X] = 2\beta^2\)</span>. (a) Write down a sufficient statistic for <span class="math inline">\(\beta\)</span> that arises directly from likelihood factorization. (b) Using your answer from part (a), determine the MVUE for <span class="math inline">\(\beta\)</span>. (c) Compute the variance of the MVUE for <span class="math inline">\(\beta\)</span>. (d) Compute the Cramer-Rao Lower Bound for the MVUE for <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1,\ldots, X_n\)</span> be <span class="math inline">\(n\)</span> iid samples from the following distribution, where <span class="math inline">\(\theta &gt; 0\)</span>,
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{1}{\theta} \exp\left(x-\frac{1}{\theta} e^x\right) \,.
\end{eqnarray*}\]</span>
Note that <span class="math inline">\(e^X \sim \text{Exp}(\theta)\)</span> (= Gamma(1,<span class="math inline">\(\theta\)</span>)) and <span class="math inline">\(Y = \sum_{i=1}^n e^{X_i} \sim \text{Gamma}(n,\theta)\)</span> (with <span class="math inline">\(E[Y] = n\theta\)</span> and <span class="math inline">\(V[Y] = n\theta^2\)</span>). (a) Find a sufficient statistic for <span class="math inline">\(\theta\)</span> using the factorization criterion. (b) Find the MVUE for <span class="math inline">\(\theta\)</span>. (c) Find the MVUE for <span class="math inline">\(\theta^2\)</span>.</p></li>
<li><p>Lets assume that we have sampled <span class="math inline">\(n\)</span> iid data, <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>, from a Maxwell-Boltzmann distribution with pdf
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \sqrt{\frac{2}{\pi}} \frac{x^2}{a^3} e^{-x^2/(2a^2)} \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(a &gt; 0\)</span>. The expected value and variance for an MB-distributed random variable are
<span class="math display">\[\begin{eqnarray*}
E[X] = 2 a \sqrt{\frac{2}{\pi}} ~~~ \mbox{and} ~~~ V[X] = a^2 \frac{(3 \pi - 8)}{\pi}
\end{eqnarray*}\]</span>
respectively. (a) Identify a sufficient statistic for <span class="math inline">\(a^2\)</span>. (b) Derive <span class="math inline">\(E[X^2]\)</span>. (Hint: there is no need for integration here.) (c) Determine the MVUE for <span class="math inline">\(a^2\)</span>. (d) Can we use an invariance principle to state that the MVUE for <span class="math inline">\(a\)</span> is <span class="math inline">\((\widehat{a^2}_{MVUE})^{1/2}\)</span>?</p></li>
<li><p>You sample a single datum <span class="math inline">\(X\)</span> from the following pdf:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{\theta}{2^\theta} x^{\theta-1} \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \in [0,2]\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Construct the most powerful level-<span class="math inline">\(\alpha\)</span> test of <span class="math inline">\(H_o: \theta = \theta_o\)</span> versus <span class="math inline">\(H_a: \theta = \theta_a\)</span>, where <span class="math inline">\(\theta_a &gt; \theta_o\)</span>. Is your hypothesis test a uniformly most powerful hypothesis test?</p></li>
<li><p>A common distribution used for the lengths of life of physical systems is the Weibull. Let <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> be <span class="math inline">\(n\)</span> iid data sampled from this distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \frac{\theta}{\beta} \hspace{1mm} x^{\theta-1} \hspace{1mm} \exp\left(-\frac{x^\theta}{\beta}\right) \hspace{5mm} x,\beta, \theta &gt;0
\end{eqnarray*}\]</span>
Assume that <span class="math inline">\(\theta\)</span> is known and that <span class="math inline">\(\beta\)</span> is freely varying, and that we are interested in finding the most powerful test of <span class="math inline">\(H_o: \beta = \beta_o\)</span> versus <span class="math inline">\(H_a: \beta = \beta_a\)</span>, where <span class="math inline">\(\beta_a &gt; \beta_o\)</span>, at the level <span class="math inline">\(\alpha\)</span>.</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What is a sufficient statistic for <span class="math inline">\(\beta\)</span>? (Do not include a minus sign.)</li>
<li>Working with the Weibull directly is difficult; however, it turns out that
<span class="math inline">\(X^\theta \sim \text{Exp}(\beta)\)</span>. What is the distribution of the sufficient
statistic found in part (a), and what are the parameter values?
(This will require examining the gamma distribution and its properties.)</li>
<li>Write, in <code>R</code> code, the rejection-region boundary for the test.</li>
</ol>
<ol start="20" style="list-style-type: decimal">
<li>Let <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> be <span class="math inline">\(n\)</span> iid data sampled from the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_{X}(x) = \left\{ \begin {array}{ll} \frac{1}{\theta}\exp\left(-\frac{1}{\theta}(x-b)\right) &amp; x &gt; b, \theta &gt; 0 \\ 0&amp;\mbox{otherwise} \end{array} \right. \,.
\end{eqnarray*}\]</span></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Derive the moment-generating function for <span class="math inline">\(X\)</span>.</li>
<li>Now derive the mgf for <span class="math inline">\(\sum_{i=1}^n X_i\)</span>.</li>
<li>The answer for (b) contains the term <span class="math inline">\(e^{nbt}\)</span>. Going
back to our original discussion of the method of moment-generating functions,
this means that we can write that <span class="math inline">\(\sum_{i=1}^n X_i = nb + \sum_{i=1}^n U_i\)</span>.
What distribution is <span class="math inline">\(\sum_{i=1}^n U_i\)</span> sampled from? (Hint: look at the
the rest of the mgf for <span class="math inline">\(\sum_{i=1}^n X_i\)</span> and, if necessary, refer back
to the previous problem.)</li>
<li>Write, in <code>R</code> code, the rejection-region boundary for the <span class="math inline">\(\alpha\)</span>-level
test of
<span class="math inline">\(H_o : \theta = \theta_o\)</span> versus <span class="math inline">\(H_a : \theta &lt; \theta_o\)</span>. Is this a
uniformly most powerful test of these hypotheses?</li>
</ol>
<ol start="21" style="list-style-type: decimal">
<li><p>In an experiment, you sample <span class="math inline">\(n\)</span> data from the distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \theta e^{-\theta x} \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \geq 0\)</span>, <span class="math inline">\(\theta &gt; 0\)</span>, and <span class="math inline">\(E[X] = 1/\theta\)</span>. You wish to test the null hypothesis <span class="math inline">\(H_o : \theta_o = 4\)</span> versus <span class="math inline">\(H_a : \theta_a = 2\)</span>. (a) Define the most powerful hypothesis test. By define, we mean write down a rejection region of the form <span class="math inline">\(Y &lt; y_{\rm RR}\)</span> or <span class="math inline">\(Y &gt; y_{\rm RR}\)</span>, where you plug in a specific statistic for <span class="math inline">\(Y\)</span>. (You cannot evaluate <span class="math inline">\(y_{\rm RR}\)</span> given the information you have herethat can stay as is.) (b) The cdf of the sampling distribution for the appropriate statistic in part (a) is <span class="math inline">\(\gamma(n,\theta y)/(n-1)!\)</span>, where <span class="math inline">\(\gamma(\cdot,\cdot)\)</span> is the lower incomplete gamma function. Given this information, and given what you would have to do to define the rejection region boundary, can you conclude that we are defining a uniformly most powerful test?</p></li>
<li><p>Lets assume that we have sampled <span class="math inline">\(n\)</span> iid data from a Binom(<span class="math inline">\(k,p\)</span>) distribution, and that we wish to test <span class="math inline">\(H_o: p = p_o\)</span> versus <span class="math inline">\(H_a: p &gt; p_o\)</span>. For our statistic, we will use <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>. (a) What is the sampling distribution for <span class="math inline">\(Y\)</span>? Provide the name and the value(s) of the parameters. (b) You code elements of the test in <code>R</code>. Assume you have the initialized variables <code>alpha</code>, <code>n</code>, <code>k</code>, <code>y.obs</code>, <code>p.o</code>, and <code>p.a</code> at your disposal. Write out the (one-line!) function call youd use to compute the rejection region boundary. (c) Now provide code of the <span class="math inline">\(p\)</span>-value. Include a discreteness correction factor if it is needed.</p></li>
<li><p>The negative inverse link function is <span class="math inline">\(-(Y \vert x)^{-1}\)</span>. Assume that we are in a simple setting (i.e., that there is one predictor variable), and write the regression line formula <span class="math inline">\(Y \vert x\)</span> as a function of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(x\)</span>.</p></li>
<li><p>You use <code>R</code> to learn a logistic regression model, and you observe the output shown below: (a) What is the sample size <span class="math inline">\(n\)</span>? (Hint: take into account how many parameter values are being estimated to convert a number shown in the output to the sample size.) (b) What is the value of <span class="math inline">\(-2\log\mathcal{L}_{\rm max}\)</span> for the model in which <span class="math inline">\(\beta_1\)</span> is set to zero? (c) What is the value of <span class="math inline">\(O(x)\)</span> for <span class="math inline">\(x = 0\)</span>? You may leave your answer in the form <span class="math inline">\(e^a\)</span> or <span class="math inline">\(\log(a)\)</span> or <span class="math inline">\(\sqrt{a}\)</span>, etc., while being sure to fill in the value of the constant <span class="math inline">\(a\)</span>. (d) Do the odds <em>increase</em> as <span class="math inline">\(x\)</span> increases, or do the odds <em>decrease</em> as <span class="math inline">\(x\)</span> increases?
<img src="figures/logreg.png" width="60%" style="display: block; margin: auto;" /></p></li>
<li><p>Below is the observed output from <code>R</code>s <code>glm()</code> function when learning a logistic regression model. For these data, Class 0 is <code>No</code> (not a student) and Class 1 is <code>Yes</code> (is a student). (a) If the balance is $589, then the predicted probability that the datum belongs to the <code>Yes</code> class is 0.1. What is the odds ratio for $589 dollars? (b) What is the odds ratio if the balance is $689? (c) For the first datum, the observed response is <code>No</code> and the predicted probability that the datum is of the <code>Yes</code> class is 0.07. Compute the deviance for this datum. (d) Whoopsthe null deviance value got expunged from the output. Would this value be higher or lower than the observed residual deviance value?</p></li>
</ol>
<pre><code>Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.1243  -0.5539  -0.4409  -0.3460   2.4901

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.0616324  0.3936314  -7.778 7.37e-15 ***
Balance      0.0014684  0.0004124   3.561  0.00037 ***
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1

Residual deviance: 221.47  on 308  degrees of freedom 
AIC: 225.47 </code></pre>
<ol start="26" style="list-style-type: decimal">
<li><p>Refer to the displayed data below. Here, <code>x1</code> and <code>x2</code> are the two predictor variables, and <span class="math inline">\(Y\)</span> is the response variable. Use these data to learn a Naive Bayes model and use that model to compute <span class="math inline">\(p(0 \vert \mbox{Yes},\mbox{False}) = p(0 \vert \mbox{Y},\mbox{F})\)</span>. Leave your answer as a fraction. Assume that <span class="math inline">\(p(C_k) = n_k/n\)</span>, and that you would estimate <span class="math inline">\(p(x \vert C_k)\)</span> as the proportion of times you observe a particular datum value <span class="math inline">\(x\)</span> (e.g., True) given <span class="math inline">\(C_k\)</span>.
<img src="figures/nb.png" width="35%" style="display: block; margin: auto;" /></p></li>
<li><p>You are given the following pdf:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = \left\{ \begin{array}{cc} 2(1-x) &amp; 0 \leq x \leq 1 \\ 0 &amp; \mbox{otherwise} \end{array} \right. \,.
\end{eqnarray*}\]</span>
An associated cost is <span class="math inline">\(C = 10X\)</span>. (a) Identify the named distribution for <span class="math inline">\(X\)</span> and state its parameters. (b) Given your answer for (a), determine <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span>. (c) Now determine <span class="math inline">\(E[C]\)</span> and <span class="math inline">\(V[C]\)</span>.</p></li>
<li><p>You are given the following cdf:
<span class="math display">\[\begin{eqnarray*}
F_X(x) = \left\{ \begin{array}{cc} 0 &amp; x &lt; 0 \\ 6x^2 - 8x^3 + 3x^4 &amp; 0 \leq x \leq 1 \\ 1 &amp; x &gt; 1 \end{array} \right. \,.
\end{eqnarray*}\]</span>
(a) <span class="math inline">\(F_X(x)\)</span> is the cdf for what named distribution? (b) What is the expected value of <span class="math inline">\(X\)</span>?</p></li>
<li><p>You are given <span class="math inline">\(X \sim\)</span> Beta(2,3). What is <span class="math inline">\(E[X^2]\)</span>?</p></li>
<li><p>You sample 3 data from a Beta(3,1) distribution. (a) Write down the pdf for the median of the sampled data. (b) Compute the expected value for the median of the sampled data.</p></li>
<li><p>You are given the following distribution:
<span class="math display">\[\begin{eqnarray*}
f_X(x) = c x (1-x) \,,
\end{eqnarray*}\]</span>
where <span class="math inline">\(x \in [0,1]\)</span> and <span class="math inline">\(c\)</span> is a constant. (a) Identify this distribution by name and provide the value(s) of its parameter(s). (b) Determine the numerical value of the constant <span class="math inline">\(c\)</span>. (c) Is the distribution symmetric about its mean value or it is a skewed distribution? (d) Compute <span class="math inline">\(P(X \leq 1/4 \vert X \leq 1/2)\)</span>. Leave your answer as a fraction.</p></li>
<li><p>Twenty-one (21) students enter a hallway off of which are three rooms. A researcher expects each student to randomly choose a room to enter (and to stay in!). That researcher observes that 10 students choose Room A, 5 students choose Room B, and 6 students choose Room C, and she decides to use these data to perform a test of the null hypothesis <span class="math inline">\(p_1 = p_2 = p_3\)</span> at the level <span class="math inline">\(\alpha = 0.1\)</span>. (a) Choose an appropriate hypothesis test and evaluate the test statistic. To be clear, your final answer should be a number. (b) What is the number of degrees of freedom associated with the test statistic? (c) The rejection region boundary value is 4.61. Do we reject the null hypothesis?</p></li>
<li><p>A political scientist has developed four pamphlets with different types of information about climate change  one focused on environmental impact, one on economic impact, one on socio-cultural impact, and one about the origins of climate change. She prints 100 of each and randomly gives them out, asks people to read them, and records their answer to the question: how much money should the government spend on counteracting climate change? The answer options are: [a] more than, [b] as much as, and [c] less than they currently do. She plans to do a chi-square test to check whether willingness to spend government funding is related to which pamphlet the respondent read. (Note that she does not segment the respondents into groups.) (a) What kind of chi-square test is conducted here (goodness-of-fit, independence, or homogeneity)? (b) The observed test statistic turns out to be <span class="math inline">\(w_{\rm obs} = 24.3\)</span>. How many terms did the researcher have to sum over to compute this statistic? (c) The test statistic follows a chi-square distribution with <span class="math inline">\(m\)</span> degrees of freedom. What is the value of <span class="math inline">\(m\)</span>? (d) Which of the following expressions corresponds to the <span class="math inline">\(p\)</span>-value associated with the test carried out in part (a): (i) <span class="math inline">\(P(W \geq w_{\rm obs})\)</span>, (ii) <span class="math inline">\(P(W \leq w_{\rm obs})\)</span>, (iii) <span class="math inline">\(P(\frac{(n-1) W }{ \sigma^2} \geq w_{\rm obs})\)</span>, or (iv) <span class="math inline">\(P(\frac{(n-1) W}{\sigma^2} \leq w_{\rm obs})\)</span>? (<span class="math inline">\(W\)</span> is the random variable, chi-square-distributed for <span class="math inline">\(m\)</span> degrees of freedom; <span class="math inline">\(n\)</span> is the sample size; and <span class="math inline">\(\sigma^2\)</span> is the population variance.)</p></li>
<li><p>You are recording the amount of time that elapses between when one person walks through a particular door until the next person walks through that same door. You have a weird stopwatch that changes from 0 to 1 after one minute, and from 1 to 2 after two minutes. Thats it. You take 100 measurements, and your data are given below. You wish to test the null hypothesis that each elapsed time is sampled from an exponential distribution with mean <span class="math inline">\(\beta\)</span> = 1 minute. (a) Under this null hypothesis, the probability that someone ends up in the category 1-2 minutes is <span class="math inline">\(\int_1^2 e^{-x} dx\)</span>. Explain why this is and calculate the probability. (b) Carry out an appropriate test to test the null hypothesis, provide either the rejection region or the <span class="math inline">\(p\)</span>-value, and state your conclusion. Assume <span class="math inline">\(\alpha = 0.05\)</span>. (c) A friend of yours decides they will repeat the experiment youve done, including the statistical analysis, but he is in a hurry and decides that he will only take 10 measurements overall. Explain to your friend why his experimental design is flawed.</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="center">0-1 min</th>
<th align="center">1-2 min</th>
<th align="center">&gt;2 min</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">52</td>
<td align="center">23</td>
<td align="center">25</td>
</tr>
</tbody>
</table>
<ol start="35" style="list-style-type: decimal">
<li><p>You play a (long) game in which you shoot 180 shots at a 3-by-3 square target, inside of which is a 1-by-1 square bullseye. Of your shots, 30 hit the bullseye while the remainder all hit the target, but hit outside the bullseye. You wish to test the null hypothesis that your shots hit the target at random locations. (a) Specify the number of shots that are expected to hit outside of, and inside of, the bullseye if the null hypothesis is correct. (b) Compute the value of an appropriate statistic for this test. (c) Specify the sampling distribution for this statistic (if the null hypothesis is correct); give the name and the values of any distribution parameters. (d) The rejection region boundary for this test is 3.841. State a conclusion regarding the null hypothesis: reject or
fail to reject.</p></li>
<li><p>We collect <span class="math inline">\(k\)</span> independent datasets, and perform hypothesis tests for the population mean given each. When we are done, we have <span class="math inline">\(k\)</span> separate <span class="math inline">\(p\)</span>-values, which are all independent of each other. In every case, it turns out that the the null hypothesis is correct. Let <span class="math inline">\(X\)</span> represent the number of <span class="math inline">\(p\)</span>-values that you observe are less than <span class="math inline">\(\alpha\)</span>, the Type I error. Write down a general expression for the probability <span class="math inline">\(P(X = x)\)</span>.</p></li>
<li><p>We are given three iid data sampled according to a geometric distribution with parameter <span class="math inline">\(p\)</span>. What is the distribution of <span class="math inline">\(Y = X_1 + X_2 + X_3\)</span>? State both the name of the distribution, and specific values for its parameters.</p></li>
<li><p>We sample a random variable <span class="math inline">\(X_1\)</span> from a degenerate distribution:
<span class="math display">\[\begin{align*}
p_X(x) = 1 \,,
\end{align*}\]</span>
for <span class="math inline">\(x = a\)</span>. We then sample another random variable, <span class="math inline">\(X_2\)</span>, from a Bernoulli distribution. Assume <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent. Derive the moment-generating functions for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and use them to compute the expected value and the variance of the sum <span class="math inline">\(X_1 + X_2\)</span>.</p></li>
<li><p>We play a game in which we sample data from a standard normal distribution at a rate of one datum per second. (a) Write down the probability of observing exactly one datum with a value <span class="math inline">\(&gt;\)</span> 3 during a 100-second game. Leave the answer in terms of a CDF symbol or an inverse CDF symbol, whichever is appropriate. (b) In another version of the game, we play until we observe one datum with value <span class="math inline">\(&gt;\)</span> 3. How long, in (integer) seconds, would the average game last? Again, leave your answer in terms of either a CDF or an inverse CDF symbol.</p></li>
<li><p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be iid random variables drawn from the distribution <span class="math inline">\(f_X(x) = e^{-x}\)</span> for <span class="math inline">\(x \geq 0\)</span>. (a) Write down the pdf for the smaller of the two values <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. (b) Compute the moment-generating function for the smaller of the two values <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. (c) Using the results from (a) and (b), identify the distribution of the smaller of the two values <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (by name and parameter value).</p></li>
<li><p>The probability density function for the beta prime distribution is
<span class="math display">\[\begin{align*}
f_X(x) = \frac{x^{\alpha-1}(1+x)^{-\alpha-\beta}}{B(\alpha,\beta)} ~~~~~~ x \in [0,1] ~~~~~~ \alpha,\beta &gt; 0 \,.
\end{align*}\]</span>
What is the expected value of <span class="math inline">\(X\)</span>? (Note that this expression will only be valid for values <span class="math inline">\(\beta &gt; 1\)</span>.) Hint: consider working with the expected value of <span class="math inline">\(1+X\)</span> instead of <span class="math inline">\(X\)</span> itself.</p></li>
<li><p>We conduct an experiment involving 24 trials, after which we record the number of counts seen in each of three separate bins; the data we observe are 8, 8, and 8. Before the experiment began, we had an expectation that one-quarter of the overall number of counts would be recorded in the first bin, that one-quarter of the counts would be recorded in the second bin, and that one-half would be recorded in the third bin. We now test this hypothesis at the level <span class="math inline">\(\alpha = 0.05\)</span>. (a) What is the test statistic? (b) Write down an <code>R</code> function call that outputs the rejection-region boundary for this test. (c) Write down an <code>R</code> function call that outputs the <span class="math inline">\(p\)</span>-value for this test. (d) The observed data are sampled according to what discrete family of distributions?</p></li>
</ol>

<div style="page-break-after: always;"></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-normal-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-poisson-and-related-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

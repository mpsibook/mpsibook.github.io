<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-normal-and-related-distributions.html"/>
<link rel="next" href="the-poisson-and-related-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>2.14.1</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> The Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Testing: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#test-power-sample-size-computation"><i class="fa fa-check"></i><b>2.15.6</b> Test Power: Sample-Size Computation</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.1</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> The Expected Value of a Negative Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.3</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.2.4</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data From an Arbitrary Probability Mass Function</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The MGF for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The PMF for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.6.1</b> The MLE for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.3</b> The MVUE for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution"><i class="fa fa-check"></i><b>3.6.4</b> The MVUE for the Geometric Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Success Probability</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.7.3</b> Wald Interval for the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution"><i class="fa fa-check"></i><b>3.8.1</b> UMP Test: Exponential Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution"><i class="fa fa-check"></i><b>3.8.2</b> UMP Test: Negative Binomial Distribution</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining a UMP Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful"><i class="fa fa-check"></i><b>3.8.4</b> Defining a Test That is Not Uniformly Most Powerful</a></li>
<li class="chapter" data-level="3.8.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability"><i class="fa fa-check"></i><b>3.8.5</b> Large-Sample Tests of the Binomial Success Probability</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-9"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-random-variables-1"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-crlb-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The CRLB on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks’ Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Covariance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-binomial-and-related-distributions" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> The Binomial (and Related) Distributions<a href="the-binomial-and-related-distributions.html#the-binomial-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Motivation<a href="the-binomial-and-related-distributions.html#motivation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s assume we are holding a coin. It may be a fair coin
(meaning that the probabilities of observing heads or tails in a
given flip of the coin are each 0.5)…or perhaps it is not. We decide that we are
going to flip the coin some fixed number of times <span class="math inline">\(k\)</span>, and we will record
the outcome of each flip:
<span class="math display">\[
\mathbf{Y} = \{Y_1,Y_2,\ldots,Y_k\} \,,
\]</span>
where, e.g., <span class="math inline">\(Y_1 = 1\)</span> if we observe heads or <span class="math inline">\(0\)</span> if we observe tails.
This is an example of a <em>Bernoulli process</em>, where “process” denotes a
sequence of observations, and “Bernoulli” indicates that there are two
possible discrete outcomes for each observation.</p>
<p>A <em>binomial experiment</em> is one that generates Bernoulli process data through
the running of <span class="math inline">\(k\)</span> trials (e.g., <span class="math inline">\(k\)</span> separate coin flips).
The properties of such an experiment are that:</p>
<ol style="list-style-type: decimal">
<li>The number of trials <span class="math inline">\(k\)</span> is fixed in advance.</li>
<li>Each trial has two possible outcomes, generically denoted as
<span class="math inline">\(S\)</span> (success) or <span class="math inline">\(F\)</span> (failure).</li>
<li>The probability of success remains <span class="math inline">\(p\)</span> throughout the experiment.</li>
<li>The outcome of any one trial is independent of the outcomes of the others.</li>
</ol>
<p>The random variable of interest for a binomial experiment is the number
of observed successes. A closely related alternative to a binomial experiment
is a <em>negative binomial experiment</em>, where the number of successes <span class="math inline">\(s\)</span>
is fixed in advance, instead of the number of trials <span class="math inline">\(k\)</span>,
and the random variable
of interest is the number of failures that we observe
before achieving <span class="math inline">\(s\)</span> successes.
A simple example would
be flipping a coin until <span class="math inline">\(s\)</span> heads are observed and recording the overall
number of tails that we observe.</p>
<p>As a side note to the third point above, about the probability of success
remaining <span class="math inline">\(p\)</span> throughout the experiment: binomial and negative binomial
experiments rely on <em>sampling with replacement</em>…if we observe a head
for a given coin flip, we can observe heads again in the future.
In the real world, however, the reader will observe instances where, e.g.,
a binomial distribution is used to model experiments featuring <em>sampling
without replacement</em>: we have <span class="math inline">\(K = 100\)</span> widgets, of which ten are defective;
we check one to see if it is defective (with probability <span class="math inline">\(p = 0.1\)</span>) and
set it aside, then check another (with probability either 10/99 or 9/99,
depending on the outcome of the first trial), etc. The convention for
using the binomial distribution to model data in such a situation is that
it is fine to do so if the number of trials <span class="math inline">\(k \lesssim K/10\)</span>.
However, in the age of computers, there is no reason to apply the binomial
distribution when we can apply the hypergeometric distribution instead.</p>
<p>And, as a side note to the fourth point above, about the outcome of each trial
being independent of the outcomes of the others: in a general process,
each datum can be dependent on the data observed previously. How each datum
is dependent on previous data defines the type of process that is observed:
a Markov process, a Gaussian process, etc.
A Bernoulli process is termed a <em>memoryless</em> process because
it is comprised of independent (and identically distributed) data.</p>
</div>
<div id="probability-mass-function" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Probability Mass Function<a href="the-binomial-and-related-distributions.html#probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s focus first on the outcome of a binomial experiment,
with the random variable <span class="math inline">\(X\)</span> being the number of observed successes
in <span class="math inline">\(k\)</span> trials. What is the probability
of observing <span class="math inline">\(X=x\)</span> successes, if the probability of observing a success
in any one trial is <span class="math inline">\(p\)</span>?
<span class="math display">\[\begin{align*}
\mbox{$x$ successes}&amp;: p^x \\
\mbox{$k-x$ failures}&amp;: (1-p)^{k-x} \,.
\end{align*}\]</span>
So <span class="math inline">\(P(X=x) = p^x (1-p)^{k-x}\)</span>…but, no, this isn’t right. Let’s
start again and think this
through. Assume <span class="math inline">\(k = 2\)</span>. The sample space of possible experimental outcomes is
<span class="math display">\[
\Omega = \{ SS, SF, FS, FF \} \,.
\]</span>
If <span class="math inline">\(p\)</span> = 0.5, then we can see that the probability of observing one
success in two trials is 0.5…but our proposed probability mass
function tells us that
<span class="math inline">\(P(X=1) = (0.5)^1 (1-0.5)^1 = 0.25\)</span>. What are we missing? We are missing
that there are two ways of observing a single success…and we need to count
both. Because we ultimately do not
care about the order in which successes and failures are observed,
we utilize counting via combination:
<span class="math display">\[
\binom{k}{x} = \frac{k!}{x! (k-x)!} \,,
\]</span>
where the exclamation point represents the factorial function
<span class="math inline">\(x! = x(x-1)(x-2)\cdots 1\)</span>. So now we can correctly write down the
binomial probability mass function:
<span class="math display">\[
P(X=x) = p_X(x) = \binom{k}{x} p^x (1-p)^{k-x} ~~~ x \in \{0,\ldots,k\} \,.
\]</span>
We denote the distribution
of the random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Note that
when <span class="math inline">\(k = 1\)</span>, we have a <em>Bernoulli distribution</em>.</p>
<p><strong>Recall</strong>: <em>a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) <span class="math inline">\(0 \leq p_X(x) \leq 1\)</span> and (b) <span class="math inline">\(\sum_x p_X(x) = 1\)</span>, where the sum is over all values of <span class="math inline">\(x\)</span> in the distribution’s domain.</em></p>
<p>(The reader should note
that the number of trials is commonly denoted as <span class="math inline">\(n\)</span>, not as
<span class="math inline">\(k\)</span>. However, since <span class="math inline">\(n\)</span> is conventionally used to denote the sample size in
an experiment, to avoid confusion we use <span class="math inline">\(k\)</span> to denote the number
of trials in this text.)</p>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:bpmf">3.1</a>, we display three binomial pmfs,
one each for probabilities of success 0.1 (red, to the left), 0.5 (green,
to the center), and 0.8 (blue, to the right). This figure indicates an
important aspect of the binomial pmf, namely that it <em>can</em> attain a shape
akin to that of a normal distribution, if <span class="math inline">\(p\)</span> is such that any truncation
observed at the values <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=k\)</span> is minimal. In fact, a binomial
random variable converges in distribution to a normal random variable
in certain limiting situations, as we show in an example below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpmf"></span>
<img src="_main_files/figure-html/bpmf-1.png" alt="\label{fig:bpmf}Binomial probability mass functions for number of trials $k = 10$ and success probabilities $p = 0.1$ (red squares), 0.5 (green triangles), and 0.8 (blue circles)." width="50%" />
<p class="caption">
Figure 3.1: Binomial probability mass functions for number of trials <span class="math inline">\(k = 10\)</span> and success probabilities <span class="math inline">\(p = 0.1\)</span> (red squares), 0.5 (green triangles), and 0.8 (blue circles).
</p>
</div>
<p><strong>Recall:</strong> <em>the expected value of a discretely distributed random variable is</em>
<span class="math display">\[
E[X] = \sum_x x p_X(x) \,,
\]</span>
<em>where the sum is over all values of <span class="math inline">\(x\)</span> within the domain of the pmf p_X(x). The expected value is equivalent to a weighted average, with the weight for each possible value of <span class="math inline">\(x\)</span> given by <span class="math inline">\(p_X(x)\)</span>.</em></p>
<p>For the binomial distribution, the expected value is
<span class="math display">\[
E[X] = \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \,.
\]</span>
At first, this does not appear to be easy to evaluate.
One trick in our arsenal
is to pull constants out of the summation such that whatever is left
as the summand is a pmf (and thus sums to 1). Let’s try this here:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k x \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k \frac{k!}{(x-1)!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \,.
\end{align*}\]</span>
The summation appears almost like that of a binomial random variable.
Let’s set <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!(k-(y+1))!} p^y (1-p)^{k-(y+1)} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!((k-1)-y)!} p^y (1-p)^{(k-1)-y} \,.
\end{align*}\]</span>
The summand is now the pmf for the random variable
<span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-1\)</span>,<span class="math inline">\(p\)</span>), summed over all values
of <span class="math inline">\(y\)</span> in the domain of the distribution. Thus the
summation evaluates to 1: <span class="math inline">\(E[X] = kp\)</span>. In an example below,
we use a similar strategy to determine the variance <span class="math inline">\(V[X] = kp(1-p)\)</span>.</p>
<p>A negative binomial experiment is governed by the <em>negative binomial
distribution</em>, whose pmf is
<span class="math display">\[
p_X(x) = \binom{x+s-1}{x} p^s (1-p)^x ~~~ x \in \{0,1,\ldots,\infty\}
\]</span>
The form of this pmf follows from the fact that the underlying Bernoulli
process would consist of <span class="math inline">\(x+s\)</span> data, <em>with the last datum being the observed
success that ends the experiment</em>. The first <span class="math inline">\(x+s-1\)</span> data
would feature <span class="math inline">\(s-1\)</span> successes and <span class="math inline">\(x\)</span> failures, with the order of
success and failure not mattering…so we can view these data
as being binomially distributed (albeit with <span class="math inline">\(x\)</span> representing
failures…hence the “negative” in negative binomial!):
<span class="math display">\[
p_X(x) = \underbrace{\binom{x+s-1}{x} p^{s-1} (1-p)^x}_{\mbox{first $x+s-1$ trials}} \cdot \underbrace{p}_{\mbox{last trial}} \,.
\]</span>
Note that when <span class="math inline">\(s = 1\)</span>, the resulting distribution is called
the <em>geometric distribution</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbpmf"></span>
<img src="_main_files/figure-html/nbpmf-1.png" alt="\label{fig:nbpmf}Negative binomial probability mass functions for the number of successes $s = 2$ and success probabilities $p = 0.7$ (red squares), 0.5 (green triangles), and 0.2 (blue circles)." width="50%" />
<p class="caption">
Figure 3.2: Negative binomial probability mass functions for the number of successes <span class="math inline">\(s = 2\)</span> and success probabilities <span class="math inline">\(p = 0.7\)</span> (red squares), 0.5 (green triangles), and 0.2 (blue circles).
</p>
</div>
<p>In an example below, we show how one would derive the expected value
of a negative binomial random variable, <span class="math inline">\(E[X] = s(1-p)/p\)</span>. (We can enact
a similar calculation to show that the variance is <span class="math inline">\(V[X] = s(1-p)/p^2\)</span>.)</p>
<hr />
<div id="variance-of-a-binomial-random-variable" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Variance of a Binomial Random Variable<a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall:</strong> <em>the variance of a discretely distributed random variable is</em>
<span class="math display">\[
V[X] = \sum_x (x-\mu)^2 p_X(x) = E[X^2] - (E[X])^2\,,
\]</span>
<em>where the sum is over all values of <span class="math inline">\(x\)</span> in the domain of the pmf <span class="math inline">\(p_X(x)\)</span>. The variance represents the square of the “width” of a probability mass function, where by “width” we mean the range of values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(p_X(x)\)</span> is effectively non-zero.</em></p>
<blockquote>
<p>The variance of a random variable is given by the shortcut formula that
we have been using since Chapter 1: <span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>.
So we would expect that we would need to compute <span class="math inline">\(E[X^2]\)</span> here, since
we already know that <span class="math inline">\(E[X] = kp\)</span>. But for reasons that will become
apparent below, it is actually far easier for us to compute <span class="math inline">\(E[X(X-1)]\)</span>,
and to work with that to eventually derive the variance:
<span class="math display">\[\begin{align*}
E[X(X-1)] &amp;= \sum_{x=0}^k x(x-1) \binom{k}{x} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=0}^k x(x-1) \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=2}^k \frac{k!}{(x-2)!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= k(k-1) p^2 \sum_{x=2}^k \frac{(k-2)!}{(x-2)!(k-x)!} p^{x-2} (1-p)^{k-x} \,.
\end{align*}\]</span>
The advantage to using <span class="math inline">\(x(x-1)\)</span> was that it matches the first two terms
of <span class="math inline">\(x! = x(x-1)\cdots(1)\)</span>, allowing easy cancellation.
If we set <span class="math inline">\(y = x-2\)</span>, we find that the summand above will, in a similar
manner as in the calculation of <span class="math inline">\(E[X]\)</span>, become the pmf for the
random variable <span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-2,p\)</span>)…and thus the summation will
evaluate to 1.</p>
</blockquote>
<blockquote>
<p>So <span class="math inline">\(E[X(X-1)] = E[X^2] - E[X] = k(k-1)p^2\)</span>, and
<span class="math inline">\(E[X^2] = k^2p^2-kp^2 + kp = V[X] + (E[X])^2\)</span>, and
<span class="math inline">\(V[X] = k^2p^2-kp^2+kp-k^2p^2 = kp-kp^2 = kp(1-p)\)</span>. Done.</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-of-a-negative-binomial-random-variable" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> The Expected Value of a Negative Binomial Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The calculation for the expected value <span class="math inline">\(E[X]\)</span> for a negative binomial
random variable is similar to that for a binomial random variable:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^{\infty} x \binom{x+s-1}{x} p^s (1-p)^x \\
&amp;= \sum_{x=0}^{\infty} x \frac{(x+s-1)!}{(s-1)!x!} p^s (1-p)^x \\
&amp;= \sum_{x=1}^{\infty} \frac{(x+s-1)!}{(s-1)!(x-1)!} p^s (1-p)^x \,.
\end{align*}\]</span>
Let <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{y=0}^{\infty} \frac{(y+s)!}{(s-1)!y!} p^s (1-p)^{y+1} \\
&amp;= \sum_{y=0}^{\infty} s(1-p) \frac{(y+s)!}{s!y!} p^s (1-p)^y \\
&amp;= \sum_{y=0}^{\infty} \frac{s(1-p)}{p} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \sum_{y=0}^{\infty} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \,.
\end{align*}\]</span>
The summand is that of a negative binomial distribution for <span class="math inline">\(s+1\)</span> successes,
hence the summation is 1, and thus <span class="math inline">\(E[X] = s(1-p)/p\)</span>.</p>
</blockquote>
<blockquote>
<p>We can use a similar calculation in which we evaluate <span class="math inline">\(E[X(X-1)]\)</span> in order
to derive the variance of a negative binomial random variable: <span class="math inline">\(V[X] = s(1-p)/p^2\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="binomial-distribution-normal-approximation" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Binomial Distribution: Normal Approximation<a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In certain limiting situations, a binomial random variable converges in
distribution to a normal random variable. In other words, if
<span class="math display">\[
P\left(\frac{X-\mu}{\sigma} &lt; a \right) = P\left(\frac{X-kp}{\sqrt{kp(1-p)}} &lt; a \right) \approx P(Z &lt; a) = \Phi(a) \,,
\]</span>
then we can state that
<span class="math inline">\(X \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(kp,kp(1-p))\)</span>, or that
<span class="math inline">\(X\)</span> converges in distribution to a normal random variable <span class="math inline">\(Y\)</span>.
Now, what do we mean by “certain
limiting situations”? For instance, if <span class="math inline">\(p\)</span> is close to zero or one,
then the binomial distribution is truncated at 0 or at <span class="math inline">\(k\)</span>,
and the shape of the pmf does <em>not</em> appear to be like that of a normal pdf.
One convention is that the normal approximation is adequate if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,.
\]</span>
The reader might question why we would mention this approximation at all:
if we have binomially distributed data and a computer, then we need not
ever utilize such an approximation to, e.g., compute probabilities. This
point is correct (and is the reason why, for instance, we do not mention
the so-called <em>continuity correction</em> here; our goal is not to compute
probabilities). The reason we mention this is that this
approximation underlies a commonly used hypothesis test framework,
the <em>Wald interval</em>, that we will mention later in the chapter.</p>
</blockquote>
<hr />
</div>
<div id="computing-probabilities-7" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Computing Probabilities<a href="the-binomial-and-related-distributions.html#computing-probabilities-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be a random variable sampled from a binomial distribution with
number of trials <span class="math inline">\(k = 6\)</span> and success probability <span class="math inline">\(p = 0.4\)</span>, and let
<span class="math inline">\(Y\)</span> be a random variable sampled from a negative binomial distribution
with number of successes <span class="math inline">\(s = 3\)</span> and success probability <span class="math inline">\(p = 0.6\)</span>.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(2 \leq X &lt; 4)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>To find this probability, we sum over the binomial probability mass
function for values <span class="math inline">\(x = 2\)</span> and <span class="math inline">\(x = 3\)</span>. (The form of the
inequalities matter for discrete distributions!) We can perform this
computation analytically:
<span class="math display">\[\begin{align*}
P(2 \leq X &lt; 4) &amp;= \binom{6}{2} (0.4)^2 (1-0.4)^4 + \binom{6}{3} (0.4)^3 (1-0.4)^3 \\
&amp;= \frac{6!}{2!4!} \cdot 0.16 \cdot 0.1296 + \frac{6!}{3!3!} \cdot 0.064 \cdot 0.216 = 15 \cdot 0.0207 + 20 \cdot 0.0138 = 0.5875 \,.
\end{align*}\]</span>
There is a 58.75% chance that the next time we sample data according to
this distribution, we will observe a value of 2 or 3.</p>
</blockquote>
<blockquote>
<p>It is ultimately simpler to use <code>R</code> to perform this calculation. While we
can compute the probability as the difference of two cumulative distribution
function values, for discrete distributions it is often more straightforward
to sum over the relevant probability masses directly, since then we need
not worry about whether the input to the cdf is correct given the form of
the inequality:</p>
</blockquote>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="the-binomial-and-related-distributions.html#cb166-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>,<span class="at">size=</span><span class="dv">6</span>,<span class="at">prob=</span><span class="fl">0.4</span>))</span></code></pre></div>
<pre><code>## [1] 0.58752</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>What is <span class="math inline">\(P(Y &gt; 1)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>If we were to determine this probability by hand, the first thing we would
do is specify that we will compute <span class="math inline">\(1 - P(Y \leq 3)\)</span>, as this has a finite
(and small!) number of terms:
<span class="math display">\[\begin{align*}
P(Y &gt; 1) &amp;= 1 - \binom{0+3-1}{0} (0.6)^3 (1-0.6)^0 - \binom{1+3-1}{1} (0.6)^3 (1-0.6)^1 \\
&amp;= 1 - 1 \cdot 0.216 \cdot 1 - 3 \cdot 0.216 \cdot 0.4 = 0.5248 \,.
\end{align*}\]</span>
There is a 52.48% chance that we would observe more than one failure the
next time we sample data according to this distribution.</p>
</blockquote>
<blockquote>
<p>Like before, it is ultimately simpler to use <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="the-binomial-and-related-distributions.html#cb168-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">dnbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>,<span class="at">size=</span><span class="dv">3</span>,<span class="at">prob=</span><span class="fl">0.6</span>))</span></code></pre></div>
<pre><code>## [1] 0.5248</code></pre>
</div>
</div>
<div id="cumulative-distribution-function-1" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Cumulative Distribution Function<a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as <span class="math inline">\(F_X(x) = \sum_{y\leq x} p_Y(y)\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>For the binomial distribution, the cdf is
<span class="math display">\[
F_X(x) = \sum_{y=0}^{\lfloor x \rfloor} p_Y(y) = \sum_{y=0}^{\lfloor x \rfloor} \binom{k}{y} p^y (1-p)^{k-y} \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the <em>floor function</em>, which
returns the largest integer that is less than or equal
to <span class="math inline">\(x\)</span> (e.g., if <span class="math inline">\(x\)</span> = 6.75, <span class="math inline">\(\lfloor x \rfloor\)</span> = 6).
(In closed form, we can represent this cdf with a regularized incomplete
beta function, which is not analytically easy to work with.)
Also, because a pmf is defined at discrete values of <span class="math inline">\(x\)</span>, its associated
cdf is a step function, as illustrated in the left panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>.
As we can see in this figure, the cdf steps up at each value of <span class="math inline">\(x\)</span>
in the domain of <span class="math inline">\(p_X(x)\)</span>, and unlike the case for continuous distributions,
the form of the inequalities in a probabilistic statement matter:
<span class="math inline">\(P(X &lt; x)\)</span> and <span class="math inline">\(P(X \leq x)\)</span> will not be the same, if <span class="math inline">\(x\)</span>
is an integer with value <span class="math inline">\(\{0,1,2,\ldots,k\}\)</span>.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(x = F_X^{-1}(q)\)</span>
takes as input a distribution quantile
<span class="math inline">\(q \in [0,1]\)</span> and returns the value of <span class="math inline">\(x\)</span>.
A discrete distribution has no unique inverse cdf; it is convention to
utilize the generalized inverse cdf,</em>
<span class="math inline">\(x = \mbox{inf}\{x : F_X(x) \geq q\}\)</span>,
<em>where “inf” indicates that the function is to return
the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q\)</span>.</em></p>
<p>In the right panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>, we display the inverse cdf
for the same distribution used to generate the figure in the left panel
(<span class="math inline">\(k=4\)</span> and <span class="math inline">\(p=0.5\)</span>). Like the cdf, the inverse cdf for a discrete distribution
is a step function. Below, in an example, we show how we adapt the inverse
transform sampler algorithm of Chapter 1 to accommodate the step-function
nature of an inverse cdf.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bincdf"></span>
<img src="_main_files/figure-html/bincdf-1.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" /><img src="_main_files/figure-html/bincdf-2.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" />
<p class="caption">
Figure 3.3: Illustration of the cumulative distribution function <span class="math inline">\(F_X(x)\)</span> (left) and inverse cumulative distribution function <span class="math inline">\(F_X^{-1}(q)\)</span> (right) for a binomial distribution with number of trials <span class="math inline">\(k = 4\)</span> and probability of success <span class="math inline">\(p=0.5\)</span>.
</p>
</div>
<hr />
<div id="computing-probabilities-8" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Computing Probabilities<a href="the-binomial-and-related-distributions.html#computing-probabilities-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Because computing the binomial pmf for a range of values of <span class="math inline">\(x\)</span> can
be laborious, we typically utilize <code>R</code> functions
when computing probabilities.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Binomial(10,0.6), which is <span class="math inline">\(P(4 \leq X &lt; 6)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We first note that due to the form of the inequality, we do <em>not</em> include <span class="math inline">\(X=6\)</span>
in the computation. Thus <span class="math inline">\(P(4 \leq X &lt; 6) = p_X(4) + p_X(5)\)</span>, which equals
<span class="math display">\[
\binom{10}{4} (0.6)^4 (1-0.6)^6 + \binom{10}{5} (0.6)^5 (1-0.6)^5 \,.
\]</span>
Even computing this is unnecessarily laborious; instead, we call on <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="the-binomial-and-related-distributions.html#cb170-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>))</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<p>(This utilizes <code>R</code>’s vectorization feature: we need not explicitly define
a <code>for</code>-loop to evaluate <code>dbinom()</code> for <span class="math inline">\(x=4\)</span> and then at <span class="math inline">\(x=5\)</span>.)
We can also utilize cdf functions here: <span class="math inline">\(P(4 \leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4)
= P(X \leq 5) - P(X \leq 3) = F_X(5) - F_X(3)\)</span>, which in <code>R</code> is computed via</p>
</blockquote>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="the-binomial-and-related-distributions.html#cb172-1" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>) <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="dv">3</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<p>As we can see, the direct summation approach is the more straightforward one.</p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(X \sim\)</span> Binomial(10,0.6), what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(X \leq a) = 0.9\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we set up the inverse cdf formula:
<span class="math display">\[
P(X \leq a) = F_X(a) = 0.9 ~~ \Rightarrow ~~ a = F_X^{-1}(0.9)
\]</span>
Note that we didn’t do anything differently here than we would have done
in a continuous distribution setting…and we can proceed directly to
<code>R</code> because it utilizes the generalized inverse cdf algorithm.</p>
</blockquote>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="the-binomial-and-related-distributions.html#cb174-1" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fl">0.9</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<blockquote>
<p>We can see immediately how the cdf for a discrete distribution is not
a one-to-one function, as if we plug <span class="math inline">\(x = 8\)</span> into the cdf, we will not
recover the initial value <span class="math inline">\(q = 0.9\)</span>:</p>
</blockquote>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="the-binomial-and-related-distributions.html#cb176-1" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">8</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.9536426</code></pre>
<hr />
</div>
<div id="sampling-data-from-an-arbitrary-probability-mass-function" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Sampling Data From an Arbitrary Probability Mass Function<a href="the-binomial-and-related-distributions.html#sampling-data-from-an-arbitrary-probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>While we would always utilize <code>R</code> shortcut functions like <code>rbinom()</code> when they exist,
there may be instances when we need to code our own functions for sampling data from
discrete distributions. The code below shows such a function for an arbitrary probability
mass function.</p>
</blockquote>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="the-binomial-and-related-distributions.html#cb178-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb178-2"><a href="the-binomial-and-related-distributions.html#cb178-2" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>)              <span class="co"># domain of x</span></span>
<span id="cb178-3"><a href="the-binomial-and-related-distributions.html#cb178-3" tabindex="-1"></a>p.x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>,<span class="fl">0.35</span>,<span class="fl">0.15</span>,<span class="fl">0.3</span>)    <span class="co"># p_X(x)</span></span>
<span id="cb178-4"><a href="the-binomial-and-related-distributions.html#cb178-4" tabindex="-1"></a>F.x <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(p.x)             <span class="co"># cumulative sum -&gt; produces F_X(x)</span></span>
<span id="cb178-5"><a href="the-binomial-and-related-distributions.html#cb178-5" tabindex="-1"></a>n   <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb178-6"><a href="the-binomial-and-related-distributions.html#cb178-6" tabindex="-1"></a>q   <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)                <span class="co"># we still ultimately need runif!</span></span>
<span id="cb178-7"><a href="the-binomial-and-related-distributions.html#cb178-7" tabindex="-1"></a>i   <span class="ot">&lt;-</span> <span class="fu">findInterval</span>(q,F.x)<span class="sc">+</span><span class="dv">1</span>   <span class="co"># the output are bin numbers [0,3], and not [1,4]</span></span>
<span id="cb178-8"><a href="the-binomial-and-related-distributions.html#cb178-8" tabindex="-1"></a>                               <span class="co"># hence we add 1</span></span>
<span id="cb178-9"><a href="the-binomial-and-related-distributions.html#cb178-9" tabindex="-1"></a>                               <span class="co"># 1 means q is between 0 and F.x[1], etc.</span></span>
<span id="cb178-10"><a href="the-binomial-and-related-distributions.html#cb178-10" tabindex="-1"></a>x.sample <span class="ot">&lt;-</span> x[i]</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pmfsamp"></span>
<img src="_main_files/figure-html/pmfsamp-1.png" alt="\label{fig:pmfsamp}Histogram of $n = 100$ iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of $x$." width="50%" />
<p class="caption">
Figure 3.4: Histogram of <span class="math inline">\(n = 100\)</span> iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of <span class="math inline">\(x\)</span>.
</p>
</div>
</div>
</div>
<div id="linear-functions-of-random-variables" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Linear Functions of Random Variables<a href="the-binomial-and-related-distributions.html#linear-functions-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Can we determine the distribution of
the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>? Yes, we can…via the method of moment-generating functions.</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(m_X(t) = E[e^{tX}]\)</span>. Also, if <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> as the mgf for a known family of distributions, then we can immediately identify the distribution for <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for the binomial distribution is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^k e^{tx} \binom{k}{x} p^x (1-p)^{k-x} \\
                   &amp;= \sum_{x=0}^k \binom{k}{x} (pe^t)^x (1-p)^{k-x} \,.
\end{align*}\]</span>
We utilize the binomial theorem
<span class="math display">\[
(x+y)^k = \sum_{i=0}^k \binom{k}{x} x^i y^{k-i}
\]</span>
to re-express <span class="math inline">\(m_X(t)\)</span>:
<span class="math display">\[
m_X(t) = [pe^t + (1-p)]^k \,.
\]</span>
Note that one may see this written as <span class="math inline">\((pe^t+q)^k\)</span>, where <span class="math inline">\(q = 1-p\)</span>.</p>
<p>The mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is thus
<span class="math display">\[
m_Y(t) = \prod_{i=1}^n m_{X_i}(t) = [m_X(t)]^n = [pe^t + (1-p)]^{nk} \,.
\]</span>
We can see that this has the form of a binomial mgf:
<span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>),
with expected value <span class="math inline">\(E[Y] = nkp\)</span> and variance <span class="math inline">\(V[Y] = nkp(1-p)\)</span>.
This makes sense, as the act of summing binomial data is equivalent to concatenating
<span class="math inline">\(n\)</span> separate Bernoulli processes into one longer Bernoulli process…whose
data can subsequently be modeled using a binomial distribution.</p>
<p>While we can identify the distribution of the sum by name, we cannot
say the same about the sample mean.
We know that the expected value is <span class="math inline">\(E[\bar{X}] = \mu = kp\)</span>
and that the variance is <span class="math inline">\(V[\bar{X}] = \sigma^2/n = kp(1-p)/n\)</span>,
but when we attempt to use the mgf method with
<span class="math inline">\(a_i = 1/n\)</span> instead of <span class="math inline">\(a_i = 1\)</span>, we find that
<span class="math display">\[
m_{\bar{X}}(t) = [pe^{t/n} + (1-p)]^{nk} \,.
\]</span>
Changing <span class="math inline">\(t\)</span> to <span class="math inline">\(t/n\)</span> has the effect of creating an mgf that does not
have the form of any known mgf.
However, we <em>do</em> know the distribution: it has a pmf that is identical
in form to that of the binomial distribution, but has the domain
<span class="math inline">\(\{0,1/n,2/n,...,k\}\)</span>.
(We can derive this result mathematically by making
the transformation <span class="math inline">\(\sum_{i=1}^n X_i \rightarrow (\sum_{i=1}^n X_i)/n\)</span>,
as we see below in an example.)
We could define the pmf ourselves
using our own <code>R</code> function, but there is no real need to: as we will see,
if we wish to construct a confidence interval for <span class="math inline">\(p\)</span>, we can just use the sum
<span class="math inline">\(\sum_{i=1}^n X_i\)</span> as our statistic.
(We could also, in theory, utilize the Central Limit Theorem if
<span class="math inline">\(n \gtrsim 30\)</span>, but there is absolutely no reason to do that to make
inferences about <span class="math inline">\(p\)</span>: we know the distribution of the sum of the data
exactly, and thus there is no need to fall back upon approximations.)</p>
<hr />
<div id="the-mgf-for-a-geometric-random-variable" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The MGF for a Geometric Random Variable<a href="the-binomial-and-related-distributions.html#the-mgf-for-a-geometric-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that a geometric distribution is equivalent to a
negative binomial distribution
with number of successes <span class="math inline">\(s = 1\)</span>; its probability mass function is
<span class="math display">\[
p_X(x) = p (1-p)^x \,,
\]</span>
with <span class="math inline">\(x = \{0,1,\ldots\}\)</span> and <span class="math inline">\(p \in [0,1]\)</span>.
The moment-generating function for a geometric random variable is thus
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^{\infty} e^{tx} p (1-p)^x = p \sum_{x=0}^\infty e^{tx} (1-p)^x \\
&amp;= p \sum_{x=0}^\infty [e^t(1-p)]^x = \frac{p}{1-e^t(1-p)} \,.
\end{align*}\]</span>
The last equality utilizes the
formula for the sum of an infinite geometric series:
<span class="math inline">\(\sum_{i=0}^\infty x^i = (1-x)^{-1}\)</span>, when <span class="math inline">\(\vert x \vert &lt; 1\)</span>.
(If <span class="math inline">\(t &lt; 0\)</span> and <span class="math inline">\(p &gt; 0\)</span>, then the condition that
<span class="math inline">\(\vert e^t(1-p) \vert &lt; 1\)</span> holds.)</p>
</blockquote>
<blockquote>
<p>The sum of <span class="math inline">\(s\)</span> geometric random variables has the moment-generating
function
<span class="math display">\[
m_Y(t) = \prod_{i=1}^s m_{X_i}(t) = \left[\frac{p}{1-e^t(1-p)}\right]^s \,.
\]</span>
This is the mgf for a negative binomial distribution for <span class="math inline">\(s\)</span> successes.
In the same way that the sum of <span class="math inline">\(k\)</span> Bernoulli random variables is a
binomially distributed random variable, the sum of <span class="math inline">\(s\)</span> geometric random
variables is a negative binomially distributed random variable.</p>
</blockquote>
<hr />
</div>
<div id="the-pmf-for-the-sample-mean" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> The PMF for the Sample Mean<a href="the-binomial-and-related-distributions.html#the-pmf-for-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). As we observe above, the
distribution of the sum <span class="math inline">\(Y = \sum_{i=1}^k X_i\)</span> is binomial with mean
<span class="math inline">\(nkp\)</span> and variance <span class="math inline">\(nkp(1-p)\)</span>.</p>
</blockquote>
<blockquote>
<p>The sample mean is <span class="math inline">\(\bar{X} = Y/n\)</span>, and so
<span class="math display">\[
F_{\bar{X}}(\bar{x}) = P(\bar{X} \leq \bar{x}) = P(Y \leq n\bar{x}) = \sum_{y=0}^{n\bar{x}} p_Y(y) \,.
\]</span>
(We note that <span class="math inline">\(n\bar{x}\)</span> is integer-valued by definition; we do not need
to round down here.) Because we are dealing with a pmf, we cannot
simply take the derivative of <span class="math inline">\(F_{\bar{X}}(\bar{x})\)</span> to find
<span class="math inline">\(f_{\bar{X}}(\bar{x})\)</span>…but what we can do is assess the jump
in the cumulative distribution function at each step, because that
<em>is</em> the pmf. In other words, we can compute
<span class="math display">\[
f_{\bar{X}}(\bar{x}) = P(Y \leq n\bar{x}) - P(Y \leq n\bar{x}-1)
\]</span>
and store this as a numerically expressed pmf for <span class="math inline">\(\bar{X}\)</span>.
See Figure <a href="the-binomial-and-related-distributions.html#fig:xbarpmf">3.5</a>.</p>
</blockquote>
<blockquote>
<p>But it turns out we can say more about this pmf, by looking at the problem
in a different way.
We know that <span class="math inline">\(Y \sim\)</span> Binomial<span class="math inline">\((nkp,nkp(1-p))\)</span> and
thus that <span class="math inline">\(Y \in [0,1,\ldots,nk]\)</span>.
When we compute the quotient <span class="math inline">\(\bar{X} = Y/n\)</span>, <em>all we are doing is redefining the domain of the pmf</em>
from being <span class="math inline">\([0,1,\ldots,nk]\)</span> to being
<span class="math inline">\([0,1/n,2/n,\ldots,k]\)</span>. We do not actually change the probability masses!
So we can write
<span class="math display">\[
p_{\bar{X}}(\bar{x}) = \binom{nk}{n\bar{x}} p^{n\bar{x}} (1-p)^{nk-n\bar{x}} ~~ \bar{x} \in [0,1/n,2/n,\ldots,k] \,.
\]</span>
This pmf has the functional form of a binomial pmf…but <em>not</em> the domain
of a binomial pmf. For that reason, we cannot say that <span class="math inline">\(\bar{X}\)</span> is
binomially distributed. The pmf has a functional form, it has a domain,
but it has no known “name” and thus no associated <code>R</code> functions that
we can utilize when performing statistical inference.
(This is why we will utilize the sum of the data instead:
<code>R</code> functions for its distribution exist!)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xbarpmf"></span>
<img src="_main_files/figure-html/xbarpmf-1.png" alt="\label{fig:xbarpmf}Probability mass function for the sample mean of $n = 10$ iid binomial random variables, for $k = 10$ and $p = 0.6$." width="50%" />
<p class="caption">
Figure 3.5: Probability mass function for the sample mean of <span class="math inline">\(n = 10\)</span> iid binomial random variables, for <span class="math inline">\(k = 10\)</span> and <span class="math inline">\(p = 0.6\)</span>.
</p>
</div>
</div>
</div>
<div id="order-statistics" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Order Statistics<a href="the-binomial-and-related-distributions.html#order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s suppose that we have sampled <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> from
some arbitrary distribution. Previously, we
have summarized such data with the sample mean and the sample variance.
However, there are other summary statistics, some of which
are only calculable if we sort the data into ascending order:
<span class="math inline">\(\{X_{(1)},\ldots,X_{(n)}\}\)</span>. These are dubbed <em>order statistics</em> and
the <span class="math inline">\(j^{th}\)</span> order statistic is the sample’s <span class="math inline">\(j^{th}\)</span> smallest value
(i.e., the smallest-valued datum in the sample
is <span class="math inline">\(X_{(1)}\)</span> and the largest-valued datum is <span class="math inline">\(X_{(n)}\)</span>). Examples of
statistics based on ordering include
<span class="math display">\[\begin{align*}
\mbox{Range:}&amp; ~~X_{(n)} - X_{(1)} \\
\mbox{Median:}&amp; ~~X_{(n+1)/2} ~ \mbox{if $n$ is odd} \\
&amp; ~~(X_{n/2}+X_{(n+1)/2})/2 ~ \mbox{if $n$ is even} \,.
\end{align*}\]</span>
The most important point to keep in mind is that the probability mass
and density functions for order statistics differ from
the pmfs and pdfs for their constituent iid data. For instance, if we
sample <span class="math inline">\(n\)</span> data from a <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution, we
would not expect the minimum value to be distributed the same way;
if anything, the mean should take on larger and larger negative values,
and the variance on those values should decrease, as <span class="math inline">\(n\)</span> increases.</p>
<p>So: why are we discussing order statistics here, in the middle of
a discussion of the binomial distribution? It is because
we can derive, e.g., the pdf for an order statistic of a
continuous distribution using the binomial pmf. (Note that order statistics
exist for discretely valued data, but the probability mass functions for
them are not easily derived and thus we will only consider
order statistics for continuously valued data here.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:order"></span>
<img src="figures/order.png" alt="\label{fig:order}If we have, e.g., a probability density function $f_X(x)$ whose domain is $[a,b]$, and we view success as sampling a datum less than a given value $x$, then when we sample $n$ data, the number that have values $\leq x$ is a binomial random variable with $k=n$ and $p = F_X(x)$." width="60%" />
<p class="caption">
Figure 3.6: If we have, e.g., a probability density function <span class="math inline">\(f_X(x)\)</span> whose domain is <span class="math inline">\([a,b]\)</span>, and we view success as sampling a datum less than a given value <span class="math inline">\(x\)</span>, then when we sample <span class="math inline">\(n\)</span> data, the number that have values <span class="math inline">\(\leq x\)</span> is a binomial random variable with <span class="math inline">\(k=n\)</span> and <span class="math inline">\(p = F_X(x)\)</span>.
</p>
</div>
<p>See Figure <a href="the-binomial-and-related-distributions.html#fig:order">3.6</a>. Without loss of generality, we can assume that
<span class="math inline">\(f_X(x) &gt; 0\)</span> for <span class="math inline">\(x \in [a,b]\)</span> and that we sample <span class="math inline">\(n\)</span> data from this distribution.
The number of data <span class="math inline">\(X\)</span> that have value less than some arbitrarily chosen <span class="math inline">\(x\)</span> is a binomial random variable:
<span class="math display">\[
Y \sim \mbox{Binomial}(n,p=F_X(x))
\]</span>
What is the probability that the <span class="math inline">\(j^{th}\)</span> ordered datum has a value <span class="math inline">\(\leq x\)</span>?
That’s equivalent to asking for the probability that <span class="math inline">\(Y \geq j\)</span>, i.e., did we
see at least <span class="math inline">\(j\)</span> successes in <span class="math inline">\(n\)</span> trials?
<span class="math display">\[
F_{(j)}(x) = P(X_{(j)} \leq x) = P(Y \geq j) = \sum_{i=j}^n \binom{n}{i} [F_X(x)]^i [1 - F_X(x)]^{n-i} \,.
\]</span>
<span class="math inline">\(F_{(j)}(x)\)</span> is the cdf for the <span class="math inline">\(j^{th}\)</span> ordered datum.</p>
<p><strong>Recall:</strong> <em>a continuous distribution’s pdf is the derivative of its cdf.</em></p>
<p>Leaving aside algebraic details, we can write down the pdf for <span class="math inline">\(X_{(j)}\)</span>:
<span class="math display">\[
f_{(j)}(x) = \frac{d}{dx}F_{(j)}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j} \,,
\]</span>
and write down simplified expressions for the pdfs for the minimum and maximum data values:
<span class="math display">\[
f_{(1)}(x) = n f_X(x) [1 - F_X(x)]^{n-1} ~~\mbox{and}~~ f_{(n)}(x) = n f_X(x) [F_X(x)]^{n-1} \,.
\]</span></p>
<hr />
<div id="distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Distribution of the Minimum Value Sampled from an Exponential Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for an exponential random variable is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and the expected value of <span class="math inline">\(X\)</span> is
<span class="math inline">\(E[X] = \theta\)</span>. What is the pdf for the smallest value among
<span class="math inline">\(n\)</span> iid data sampled from an exponential distribution? What is the expected value
for the smallest value?</p>
</blockquote>
<blockquote>
<p>First, if we do not immediately recall the cumulative distribution function
<span class="math inline">\(F_X(x)\)</span>, we can easily derive it:
<span class="math display">\[
F_X(x) = \int_0^x \frac{1}{\theta} e^{-y/\theta} dy = 1 - e^{-x/\theta} \,.
\]</span>
We plug <span class="math inline">\(F_X(x)\)</span> into the expression of the pdf of the minimum datum given
above:
<span class="math display">\[\begin{align*}
f_{(1)}(x) &amp;= n \frac{1}{\theta} e^{-x/\theta} \left[ 1 - (1-e^{-x/\theta}) \right]^{n-1} \\
&amp;= n \frac{1}{\theta} e^{-x/\theta} e^{-(n-1)x/\theta} \\
&amp;= \frac{n}{\theta} e^{-nx/\theta} \,.
\end{align*}\]</span>
<span class="math inline">\(X_{(1)}\)</span> is thus an exponentially distributed random variable with
parameter <span class="math inline">\(\theta/n\)</span> and expected value <span class="math inline">\(\theta/n\)</span>. We can derive this
result as follows.
<span class="math display">\[
E[X_{(1)}] = \int_0^\infty x \frac{n}{\theta} e^{-nx/\theta} dx \,.
\]</span>
We recognize this as <em>almost</em> having the form of a gamma-function integral:
<span class="math display">\[
\Gamma(u) = \int_0^\infty x^{u-1} e^{-x} dx \,.
\]</span>
We affect a variable transformation <span class="math inline">\(y = nx/\theta\)</span>; for this transformation,
<span class="math inline">\(dy = (n/\theta)dx\)</span>,
and if <span class="math inline">\(x = 0\)</span> or <span class="math inline">\(\infty\)</span>, <span class="math inline">\(y = 0\)</span> or <span class="math inline">\(\infty\)</span> (meaning the integral bounds
are unchanged). Our new integral is
<span class="math display">\[
E[X] = \int_0^\infty \frac{\theta y}{n} \frac{n}{\theta} e^{-y} \frac{\theta}{n} dy = \frac{\theta}{n} \int_0^\infty y e^{-y} dy = \frac{\theta}{n} \Gamma(2) = \frac{\theta}{n} 1! = \frac{\theta}{n} = \frac{E[X]}{n} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for a Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. The cdf for this distribution is thus
<span class="math display">\[
F_X(x) = \int_0^x 1 dy = x \,.
\]</span>
Let’s assume that we sample <span class="math inline">\(n\)</span> iid data from this distribution, where <span class="math inline">\(n\)</span> is an
odd number. The index of the median value is thus <span class="math inline">\((n+1)/2\)</span>, and if we plug into
the general expression for the pdf of the <span class="math inline">\(m^{th}\)</span> ordered datum, we find that
<span class="math display">\[\begin{align*}
f_{(n+1)/2} &amp;= \frac{n!}{\left(\frac{n+1}{2}-1\right)!\left(n - \frac{n+1}{2}\right)!} \cdot 1 \cdot x^{\left(\frac{n+1}{2}\right)-1} \cdot (1-x)^{n - \left(\frac{n+1}{2}\right)} \\
&amp;= \frac{n!}{2\left(\frac{n-1}{2}\right)!} x^{\left(\frac{n-1}{2}\right)} (1-x)^{\left(\frac{n-1}{2}\right)} \,.
\end{align*}\]</span>
As we will see later,
this is a beta distribution with parameters <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>. The median value has
expected value 1/2 and a variance that shrinks with <span class="math inline">\(n\)</span>.</p>
</blockquote>
</div>
</div>
<div id="point-estimation-2" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Point Estimation<a href="the-binomial-and-related-distributions.html#point-estimation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the first two chapters, we introduced a number of concepts related to point estimation, the
act of using statistics to make inferences about a population parameter <span class="math inline">\(\theta\)</span>. We…</p>
<ul>
<li>assess point estimators using the metrics of bias, variance, mean-squared error, and consistency;</li>
<li>utilize the Fisher information metric to determine the lower bound on the variance
for unbiased estimators (the Cramer-Rao Lower Bound, or CRLB); and</li>
<li>define estimators via the maximum likelihood algorithm, which generates estimators
that are at least asymptotically unbiased and at least asymptotically reach the CRLB, and
which converge in distribution to normal random variables.</li>
</ul>
<p>We will review these concepts in the context of estimating population quantities for
binomial distributions below, in the body of the text and in examples. For now…</p>
<p><strong>Recall</strong>: <em>the bias of an estimator is the difference between the average value of the estimates it generates and the true parameter value. If <span class="math inline">\(E[\hat{\theta}-\theta] = 0\)</span>, then the estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be unbiased.</em></p>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>. Also recall the invariance property of the MLE: if <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(g(\hat{\theta}_{MLE})\)</span> is the MLE for <span class="math inline">\(g(\theta)\)</span>.</em></p>
<p>Here we will introduce another means by which to define an estimator. The
<em>minimum variance unbiased estimator</em> (or <em>MVUE</em>) is the one
that has the smallest variance among all unbiased estimators of <span class="math inline">\(\theta\)</span>.
The reader’s first thought might be “well, why didn’t we use this estimator
in the first place…after all, the MLE is not guaranteed to yield an
unbiased estimator, so why have we put off discussing the MVUE?”
The primary reasons are that MVUEs are sometimes not definable (i.e., we
can reach insurmountable roadblocks when trying to derive them),
and unlike MLEs, they do not exhibit the invariance property. (For
instance, if
<span class="math inline">\(\hat{\theta}_{MLE} = \bar{X}\)</span>, then <span class="math inline">\(\hat{\theta^2}_{MLE} = \bar{X}^2\)</span>, but
if <span class="math inline">\(\hat{\theta}_{MVUE} = \bar{X}\)</span>, it is not necessarily the case that
<span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.) <em>However, we should always at least
try to define the MVUE, because if we can, it will be at least equal the
performance of, if not do better than, the MLE, in terms of bias and/or
variance.</em></p>
<p>There are two steps to carry out when deriving the MVUE:</p>
<ol style="list-style-type: decimal">
<li>determining a <em>sufficient statistic</em> for <span class="math inline">\(\theta\)</span>; and</li>
<li>correcting any bias that is observed when we utilize that sufficient
statistic as our initial estimator.</li>
</ol>
<p>A sufficient statistic for a parameter <span class="math inline">\(\theta\)</span>
captures all information about <span class="math inline">\(\theta\)</span> contained in the sample.
In other words, any additional statistic, beyond the sufficient statistic,
will not provide any additional information about <span class="math inline">\(\theta\)</span>.
This does not mean that a sufficient statistic is necessarily unique, as
<em>any function of a sufficient statistic is also a
sufficient statistic</em>. For instance, if <span class="math inline">\(\sum_{i=1}^n X_i\)</span> is a sufficient
statistic, so is <span class="math inline">\(\bar{X}\)</span>.
It just means that any additional statistic that is
<em>not</em> a function of the sufficient statistic will not help us when we try
to estimate <span class="math inline">\(\theta\)</span>. For instance, if <span class="math inline">\(\bar{X}\)</span> is a sufficient statistic
for <span class="math inline">\(\theta\)</span>,
then using <span class="math inline">\(\bar{X}\)</span> and the sample median to infer <span class="math inline">\(\theta\)</span>
will lead to no better result than when we just use <span class="math inline">\(\bar{X}\)</span> by itself.)</p>
<p>The simplest way by which to identify a sufficient statistic is
by writing down the likelihood function and factorizing it into two
functions, one of which depends <em>only</em> on the observed data and the
other of which depends on both the observed data and the parameter of
interest:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = g(\mathbf{x},\theta) \cdot h(\mathbf{x}) \,.
\]</span>
This is the so-called <em>factorization criterion</em>.
In the expression <span class="math inline">\(g(\mathbf{x},\theta)\)</span>, the data will appear within, e.g., a
summation (e.g., <span class="math inline">\(\sum_{i=1}^n x_i\)</span>) or a product
(e.g., <span class="math inline">\(\prod_{i=1}^n x_i\)</span>), and
we would identify that summation or product as a sufficient statistic.</p>
<p>Let’s assume we are given a sample of <span class="math inline">\(n\)</span> iid binomial random variables.
The following is the factorized likelihood:
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n \binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \underbrace{\left[ \prod_{i=1}^n \binom{k}{x_i} \right]}_{h(\mathbf{x})} \underbrace{p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i}}_{g(\sum_{i=1}^n x_i,p)} \,.
\]</span>
By inspecting the function <span class="math inline">\(g(\mathbf{x},\theta)\)</span>, we determine that
a sufficient statistic is <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>.</p>
<p>(The next step, in general, is to demonstrate whether the identified statistic is both
minimally sufficient and complete; it needs to be both so that we can use it to determine the MVUE.
It suffices to say here that the factorization criterion typically identifies statistics
that are minimally sufficient and complete, particularly for those distributions
that we examine in this book.
See Chapter 7 for more details on how to determine if a sufficient
statistic is a minimally sufficient statistic and if it is complete. Note that complete statistics
are always minimally sufficient, but not necessarily vice-versa, so we
would always check for completeness first!)</p>
<p>If <span class="math inline">\(U\)</span> is a minimally sufficient and complete statistic, and there is a function <span class="math inline">\(h(U)\)</span> that is an
unbiased estimator for <span class="math inline">\(\theta\)</span> and that depends on the data only through <span class="math inline">\(U\)</span>, then <span class="math inline">\(h(U)\)</span> is the MVUE for
<span class="math inline">\(\theta\)</span>. Here, given <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>, we need to find a function
<span class="math inline">\(h(\cdot)\)</span> such that <span class="math inline">\(E[h(U)] = p\)</span>. Earlier in this chapter, we determined that the
distribution for the sum of iid binomial random variables is Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>), and thus we
know that this distribution has expected value <span class="math inline">\(nkp\)</span>. Thus
<span class="math display">\[
E\left[U\right] = nkp ~\implies~ E\left[\frac{U}{nk}\right] = p ~\implies~ h(U) = \frac{U}{nk} = \frac{\bar{X}}{k} ~\mbox{is the MVUE for}~p \,.
\]</span></p>
<p>The variance of <span class="math inline">\(\hat{p}\)</span> is
<span class="math display">\[
V[\hat{p}] = V\left[\frac{\bar{X}}{k}\right] = \frac{1}{k^2}V[\bar{X}] = \frac{1}{k^2} \frac{V[X]}{n} = \frac{1}{k^2}\frac{kp(1-p)}{n} = \frac{p(1-p)}{nk} \,.
\]</span>
We know that this variance abides by the restriction
<span class="math display">\[
V[\hat{p}] \geq \frac{1}{nI(p)} = -\frac{1}{nE\left[\frac{d^2}{dp^2} \log p_X(X \vert p) \right]} \,.
\]</span>
But is it equivalent to the lower bound for unbiased estimators, the CRLB? (Note that in particular
situations, the MVUE <em>may</em> have a variance larger than the CRLB; when this is the case,
unbiased estimators that achieve the CRLB simply do not exist.) For the binomial distribution,
<span class="math display">\[\begin{align*}
p_{X}(x) &amp;= \binom{k}{x} p^{x} (1-p)^{k-x} \\
\log p_{X}(x) &amp;= \log \binom{k}{x} + x \log p + (k-x) \log (1-p) \\
\frac{d}{dp} \log p_{X}(x) &amp;= 0 + \frac{x}{p} - \frac{k-x}{(1-p)} \\
\frac{d^2}{dp^2} \log p_{X}(x) &amp;= -\frac{x}{p^2} - \frac{k-x}{(1-p)^2} \\
E\left[\frac{d^2}{dp^2} \log p_{X}(X)\right] &amp;= -\frac{1}{p^2}E[X] - \frac{1}{(1-p)^2}E[k-X] \\
&amp;= -\frac{kp}{p^2}-\frac{k-kp}{(1-p)^2} \\
&amp;= -\frac{k}{p}-\frac{k}{1-p} = -\frac{k}{p(1-p)} \,.
\end{align*}\]</span>
The lower bound on the variance is thus <span class="math inline">\(p(1-p)/(nk)\)</span>, and so
the MVUE <em>does</em> achieve the CRLB.
We cannot define a better unbiased estimator for <span class="math inline">\(p\)</span> than <span class="math inline">\(\bar{X}/k\)</span>.</p>
<hr />
<div id="the-mle-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> The MLE for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#the-mle-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.</em></p>
<blockquote>
<p>Above, we determined that
the likelihood function for <span class="math inline">\(n\)</span> iid binomial random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> is
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \left[\prod_{i=1}^n \binom{k}{x_i} \right] p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i} \,.
\]</span>
Recall that the value <span class="math inline">\(\hat{p}_{MLE}\)</span> that maximizes <span class="math inline">\(\mathcal{L}(p \vert x)\)</span> also maximizes
<span class="math inline">\(\ell(p \vert x) = \log \mathcal{L}(p \vert x)\)</span>, which is considerably easier to work with:
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) &amp;= \left(\sum_{i=1}^n x_i\right) \log p + \left(nk - \sum_{i=1}^n x_i\right) \log (1-p) \\
\frac{d}{dp} \ell(p \vert \mathbf{x}) &amp;= \frac{1}{p} \sum_{i=1}^n x_i - \frac{1}{1-p} \left(nk - \sum_{i=1}^n x_i\right) = 0 \,.
\end{align*}\]</span>
(Here, we drop the binomial coefficient, which does not depend on
<span class="math inline">\(p\)</span> and thus differentiates to zero.)
After rearranging terms, we find that
<span class="math display">\[
p = \frac{1}{nk}\sum_{i=1}^n x_i ~\implies~ \hat{p}_{MLE} = \frac{\bar{X}}{k} \,.
\]</span>
The MLE matches the MVUE, thus we know that the MLE is unbiased and we know that it achieves the CRLB.</p>
</blockquote>
<blockquote>
<p>A useful property of MLEs is the invariance property, whereby the MLE for a function of <span class="math inline">\(\theta\)</span>
is given by applying the same function to the MLE itself. Thus</p>
</blockquote>
<blockquote>
<ul>
<li>the MLE for the population mean <span class="math inline">\(E[X] = \mu = kp\)</span> is <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>; and</li>
<li>the MLE for the population variance <span class="math inline">\(V[X] = \sigma^2 = kp(1-p)\)</span> is <span class="math inline">\(\widehat{\sigma^2}_{MLE} = \bar{X}(1-\bar{X}/k)\)</span>.</li>
</ul>
</blockquote>
<blockquote>
<p>Last, note that asymptotically, <span class="math inline">\(\hat{p}_{MLE}\)</span> converges in distribution to a normal random variable:
<span class="math display">\[
\hat{p}_{MLE} \stackrel{d}{\rightarrow} Y \sim \mathcal{N}\left(p,\frac{1}{nI(p)} = \frac{p(1-p)}{nk}\right) \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="sufficient-statistics-for-the-normal-distribution" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Sufficient Statistics for the Normal Distribution<a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>If we have <span class="math inline">\(n\)</span> iid data drawn from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and
unknown variance <span class="math inline">\(\sigma^2\)</span>, then the factorized likelihood is
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i^2, \sum x_i,\mu,\sigma)} \cdot \underbrace{1}_{h(\mathbf{x})} \,.
\]</span>
Here, we identify <span class="math inline">\(\sum x_i^2\)</span> and <span class="math inline">\(\sum x_i\)</span> as <em>joint</em> sufficient statistics: we
need two pieces of information to jointly estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. (To be
clear: it is not necessarily the case that one of the parameters matches up to
one of the sufficient statistics…rather, the two statistics are jointly
sufficient for estimation.) We thus cannot proceed further to define an MVUE for
<span class="math inline">\(\mu\)</span> or for <span class="math inline">\(\sigma^2\)</span>, without knowing the joint bivariate probability
density function for <span class="math inline">\(U_1 = \sum_{i=1}^n X_i^2\)</span> and <span class="math inline">\(U_2 = \sum_{i=1}^n X_i\)</span>.</p>
</blockquote>
<blockquote>
<p>(Note that we <em>can</em> proceed if we happen to know either <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma^2\)</span>;
if one
of these values is fixed, then there will only be one sufficient statistic
and we can determine the MVUE for the other, freely varying parameter.)</p>
</blockquote>
<hr />
</div>
<div id="the-mvue-for-the-exponential-mean" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> The MVUE for the Exponential Mean<a href="the-binomial-and-related-distributions.html#the-mvue-for-the-exponential-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The exponential distribution is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and where <span class="math inline">\(E[X] = \theta\)</span> and <span class="math inline">\(V[X] = \theta^2\)</span>.
Let’s assume that we have <span class="math inline">\(n\)</span> iid data drawn from this distribution.
Can we define the MVUE for <span class="math inline">\(\theta\)</span>? For <span class="math inline">\(\theta^2\)</span>?</p>
</blockquote>
<blockquote>
<p>The likelihood function is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \frac{1}{\theta^n}\exp\left(-\frac{1}{\theta}\sum_{i=1}^n x_i \right) = h(\mathbf{x}) \cdot g(\theta,\mathbf{x}) \,.
\]</span>
Here, there are no terms that are functions of only the data, so <span class="math inline">\(h(\mathbf{x}) = 1\)</span> and
a sufficient statistic is <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>. We compute the expected value
of <span class="math inline">\(U\)</span>:
<span class="math display">\[
E[U] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \theta = n\theta \,.
\]</span>
The expected value of <span class="math inline">\(U\)</span> is not <span class="math inline">\(\theta\)</span>, so <span class="math inline">\(U\)</span> is not unbiased…but we can see
immediately that <span class="math inline">\(E[U/n] = \theta\)</span>, so that <span class="math inline">\(U/n\)</span> is unbiased. Thus the MVUE for <span class="math inline">\(\theta\)</span>
is thus <span class="math inline">\(\hat{\theta}_{MVUE} = U/n = \bar{X}\)</span>.</p>
</blockquote>
<blockquote>
<p>Note that the MVUE does not possess the invariance property…it is not necessarily the
case that <span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.</p>
</blockquote>
<blockquote>
<p>Let’s propose a function of <span class="math inline">\(U\)</span> and see if we can use that to define <span class="math inline">\(\hat{\theta^2}_{MVUE}\)</span>:
<span class="math inline">\(h(U) = U^2/n^2 = \bar{X}^2\)</span>. (To be clear, we are simply proposing a function and seeing if
it helps us define what we are looking for. It might not. If not, we can try again with
another function of <span class="math inline">\(U\)</span>.) Utilizing what we know about the sample mean, we can write down that
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{V[X]}{n} + (E[X])^2 = \frac{\theta^2}{n}+\theta^2 = \theta^2\left(\frac{1}{n} + 1\right) \,.
\]</span>
So <span class="math inline">\(\bar{X}^2\)</span> itself is <em>not</em> an unbiased estimator of <span class="math inline">\(\theta^2\)</span>…but we can see that
<span class="math inline">\(\bar{X}^2/(1/n+1)\)</span> is. Hence
<span class="math display">\[
\hat{\theta^2}_{MVUE} = \frac{\bar{X}^2}{\left(\frac{1}{n}+1\right)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-mvue-for-the-geometric-distribution" class="section level3 hasAnchor" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> The MVUE for the Geometric Distribution<a href="the-binomial-and-related-distributions.html#the-mvue-for-the-geometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that the geometric distribution is a negative binomial distribution
with <span class="math inline">\(s = 1\)</span>:
<span class="math display">\[\begin{align*}
p_X(x) = \binom{x + s - 1}{x} p^s (1-p)^x ~ \rightarrow ~ p(1-p)^x \,,
\end{align*}\]</span>
where <span class="math inline">\(p \in (0,1]\)</span> and where <span class="math inline">\(E[X] = (1-p)/p\)</span> and <span class="math inline">\(V[X] = (1-p)/p^2\)</span>.
Let’s assume that we sample <span class="math inline">\(n\)</span> iid data from this distribution.
Can we define the MVUE for <span class="math inline">\(p\)</span>? For <span class="math inline">\(1/p\)</span>?</p>
</blockquote>
<blockquote>
<p>The likelihood function is
<span class="math display">\[\begin{align*}
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n p_X(x_i \vert p) = p^n (1-p)^{\sum_{i=1}^n x_i} = 1 \cdot g(p,\mathbf{x}) \,.
\end{align*}\]</span>
We identify a sufficient statistic as <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>; the expected
value of <span class="math inline">\(U\)</span> is
<span class="math display">\[\begin{align*}
E[U] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \frac{1-p}{p} = n\left(\frac{1}{p}-1\right) \,.
\end{align*}\]</span>
We cannot “debias” this expression to find the MVUE for <span class="math inline">\(p\)</span>:
<span class="math display">\[\begin{align*}
E\left[\frac{U}{n}+1\right] = \frac{1}{p} \,.
\end{align*}\]</span>
Specifically,
<span class="math display">\[\begin{align*}
E\left[\frac{1}{U/n+1}\right] \neq 1/E\left[\frac{U}{n}+1\right] = p \,.
\end{align*}\]</span>
However, we did determine the MVUE for <span class="math inline">\(1/p\)</span>: <span class="math inline">\(U/n+1 = \bar{X}+1\)</span>.
But as there is no invariance property for the MVUE, we cannot use this
expression to write down an MVUE for <span class="math inline">\(p\)</span>.</p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-2" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Confidence Intervals<a href="the-binomial-and-related-distributions.html#confidence-intervals-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}\)</span> by solving the following equation:</em>
<span class="math display">\[
F_Y(y_{\rm obs} \vert \theta) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span>, <span class="math inline">\(y_{\rm obs}\)</span> is the observed value of the statistic,
and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined using
the confidence interval reference table introduced in
section 16 of Chapter 1.</em></p>
<p>The only new element to consider here regarding the construction of
confidence intervals is that now the sampling distribution for our
adopted statistic (<span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>, where the <span class="math inline">\(X_i\)</span>’s are
iid samples from a binomial distribution) is a <em>discrete</em> distribution
as opposed to begin a continuous distribution.
As it turns out, this does not impact our use of
<code>uniroot()</code>: even though <span class="math inline">\(F_Y(y_{\rm obs})\)</span> is discrete, the success
probability <span class="math inline">\(p\)</span> is still a continuously valued quantity,
and we can tune its value so that
<span class="math inline">\(F_Y(y_{\rm obs} \vert p)\)</span> will match, e.g., <span class="math inline">\(\alpha/2\)</span> or <span class="math inline">\(1-\alpha/2\)</span> with
arbitrary precision.
<em>What the discreteness of the sampling distribution does impact is the coverage, or the fraction of intervals that overlap the true value.</em></p>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:normcdfci">3.7</a>, we display a continuous sampling distribution
for a statistic <span class="math inline">\(Y\)</span>, given a specific (unknown to us!) true value <span class="math inline">\(\theta\)</span>.
(Here, without
loss of generality, we use a normal distribution, with <span class="math inline">\(\theta = \mu = 0\)</span>.
We also assume that <span class="math inline">\(E[Y]\)</span> increases with <span class="math inline">\(\theta\)</span>.)
When running an experiment, if we sample a value of <span class="math inline">\(y_{\rm obs}\)</span> in the red
polygon, then turn around to derive a one-sided upper bound, that upper bound
will <em>not</em> overlap the true value of <span class="math inline">\(\theta\)</span>; otherwise, if we sample a
value not in the red polygon, the upper bound will overlap the true value.
Since the probability of sampling a value in the red polygon is exactly
<span class="math inline">\(\alpha\)</span>, then the probability that the interval will not overlap the true
value is also exactly <span class="math inline">\(\alpha\)</span>…meaning that the interval coverage is
exactly <span class="math inline">\(100(1-\alpha)\)</span> percent.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normcdfci"></span>
<img src="_main_files/figure-html/normcdfci-1.png" alt="\label{fig:normcdfci}An example of a continuous sampling distribution for a statistic $Y$. If we sample $y_{\rm obs}$ in the region of the red polygon (which has area $\alpha$), then the one-sided upper bound that we would compute would not overlap with the true value $\theta$. In this situation, the interval coverage is exactly $100(1-\alpha)$ percent." width="50%" />
<p class="caption">
Figure 3.7: An example of a continuous sampling distribution for a statistic <span class="math inline">\(Y\)</span>. If we sample <span class="math inline">\(y_{\rm obs}\)</span> in the region of the red polygon (which has area <span class="math inline">\(\alpha\)</span>), then the one-sided upper bound that we would compute would not overlap with the true value <span class="math inline">\(\theta\)</span>. In this situation, the interval coverage is exactly <span class="math inline">\(100(1-\alpha)\)</span> percent.
</p>
</div>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:bincdfci">3.8</a>, we display a discrete sampling distribution
for a statistic <span class="math inline">\(Y\)</span>, given a specific (and still unknown to us!) true value
<span class="math inline">\(\theta\)</span>. (This distribution is specifically a binomial distribution with
<span class="math inline">\(k = 12\)</span> and <span class="math inline">\(p = 0.5\)</span>.) The three points that are colored red are the ones with
cdf values less than <span class="math inline">\(\alpha\)</span>. (Given an arbitrary value of <span class="math inline">\(\theta\)</span>, we will
never observe a cdf value for any value of <span class="math inline">\(y\)</span> that is <em>exactly</em> <span class="math inline">\(\alpha\)</span>
due to the discreteness of the sampling distribution,
unless we have an
infinite number of data. For instance, in this figure the cdf for <span class="math inline">\(y = 2\)</span> is
0.019, whereas the cdf for <span class="math inline">\(y = 3\)</span> is 0.073. Neither value matches
<span class="math inline">\(\alpha = 0.05\)</span>.) Above, we indicated that for a
continuous distribution, sampling a datum <span class="math inline">\(y_{\rm obs}\)</span> with a cdf value less
than <span class="math inline">\(\alpha\)</span> leads to the computation of a one-sided upper bound that does
not cover the true value. The same situation holds here: sampling
either <span class="math inline">\(y_{\rm obs} = 0\)</span>, 1, or 2, data for whom the cdf values are
less than <span class="math inline">\(\alpha\)</span>, will lead to the computation of upper bounds
that do not overlap <span class="math inline">\(\theta\)</span>. But what is different here is that the
probability of sampling one of these values is no longer <span class="math inline">\(\alpha\)</span>, but
some quantity less than <span class="math inline">\(\alpha\)</span>…let’s call this quantity <span class="math inline">\(\alpha&#39;\)</span>.
The probability that our one-sided upper bounds do not cover <span class="math inline">\(\theta\)</span> is
thus <span class="math inline">\(\alpha&#39;\)</span>…which means that our coverage is actually going to
be <span class="math inline">\(100(1-\alpha&#39;)\)</span> percent, which is <em>greater than</em>
<span class="math inline">\(100(1-\alpha)\)</span> percent. For one-sided lower bounds, we
can apply a similar set of arguments:
we would identify all values of <span class="math inline">\(y\)</span> with cdf values less than <span class="math inline">\(1 - \alpha\)</span>,
with <span class="math inline">\(\alpha&#39; &gt; \alpha\)</span> being the sum of the
masses for the remaining values of <span class="math inline">\(y\)</span>. Since <span class="math inline">\(\alpha&#39; &gt; \alpha\)</span>,
the coverage, which is <span class="math inline">\(100(1-\alpha&#39;)\)</span> percent, would be <em>less than</em>
<span class="math inline">\(100(1-\alpha)\)</span> percent.
(For two-sided intervals, we would compute <span class="math inline">\(\alpha_{\rm lo}&#39;/2\)</span>, the sum
of all masses associated with values of <span class="math inline">\(y\)</span> with cdf values less than
<span class="math inline">\(\alpha/2\)</span>, and <span class="math inline">\(\alpha_{\rm hi}&#39;/2\)</span>, the sum of all masses associated with
values of <span class="math inline">\(y\)</span> that do <em>not</em> have cdf values less than <span class="math inline">\(1-\alpha/2\)</span>. The
coverage would then be <span class="math inline">\(100(1-\alpha_{\rm lo}&#39;/2-\alpha_{\rm hi}&#39;/2)\)</span> percent,
which can be less than or greater than <span class="math inline">\(100(1-\alpha)\)</span> percent.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bincdfci"></span>
<img src="_main_files/figure-html/bincdfci-1.png" alt="\label{fig:bincdfci}An example of a discrete sampling distribution for a statistic $Y$. If we sample $y_{\rm obs} = 0$, 1, or 2 (which have cdf values $&lt; \alpha$), then the one-sided upper bound that we would compute would not overlap with the true value $\theta$. In this situation, the interval coverage is greater than $100(1-\alpha)$ percent." width="50%" />
<p class="caption">
Figure 3.8: An example of a discrete sampling distribution for a statistic <span class="math inline">\(Y\)</span>. If we sample <span class="math inline">\(y_{\rm obs} = 0\)</span>, 1, or 2 (which have cdf values <span class="math inline">\(&lt; \alpha\)</span>), then the one-sided upper bound that we would compute would not overlap with the true value <span class="math inline">\(\theta\)</span>. In this situation, the interval coverage is greater than <span class="math inline">\(100(1-\alpha)\)</span> percent.
</p>
</div>
<hr />
<p>Discrete distributions like the binomial have historically been
difficult to work with analytically. Because of this,
a number of algorithms have been developed through the years for
constructing confidence intervals for binomial probabilities.
(See, e.g., <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">this Wikipedia page</a>.)
It is our opinion that there is no particular reason to utilize <em>any</em> of
these algorithms when one can compute exact intervals. (Albeit ones
with coverages that are not exactly <span class="math inline">\(100(1-\alpha)\)</span> percent!)
However, for completeness, we illustrate how one would construct
the most commonly seen approximating interval, the <em>Wald interval</em>,
in an example below.</p>
<hr />
<div id="confidence-interval-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Confidence Interval for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Assume that we sample <span class="math inline">\(n\)</span> iid data from
a binomial distribution with number of trials <span class="math inline">\(k\)</span> and probability <span class="math inline">\(p\)</span>.
Then, as shown above, <span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Binom(<span class="math inline">\(nk,p\)</span>);
our observed test statistic is <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i\)</span>.
For this statistic, <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, so
<span class="math inline">\(q = 1-\alpha/2\)</span> maps to the lower bound, while <span class="math inline">\(q = \alpha/2\)</span> maps to
the upper bound.</p>
</blockquote>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="the-binomial-and-related-distributions.html#cb179-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb179-2"><a href="the-binomial-and-related-distributions.html#cb179-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb179-3"><a href="the-binomial-and-related-distributions.html#cb179-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb179-4"><a href="the-binomial-and-related-distributions.html#cb179-4" tabindex="-1"></a>k     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb179-5"><a href="the-binomial-and-related-distributions.html#cb179-5" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb179-6"><a href="the-binomial-and-related-distributions.html#cb179-6" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="at">size=</span>k,<span class="at">prob=</span>p)</span>
<span id="cb179-7"><a href="the-binomial-and-related-distributions.html#cb179-7" tabindex="-1"></a></span>
<span id="cb179-8"><a href="the-binomial-and-related-distributions.html#cb179-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(p,y.obs,n,k,q)</span>
<span id="cb179-9"><a href="the-binomial-and-related-distributions.html#cb179-9" tabindex="-1"></a>{</span>
<span id="cb179-10"><a href="the-binomial-and-related-distributions.html#cb179-10" tabindex="-1"></a>  <span class="fu">pbinom</span>(y.obs,<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p)<span class="sc">-</span>q</span>
<span id="cb179-11"><a href="the-binomial-and-related-distributions.html#cb179-11" tabindex="-1"></a>}</span>
<span id="cb179-12"><a href="the-binomial-and-related-distributions.html#cb179-12" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,k,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.2607037</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="the-binomial-and-related-distributions.html#cb181-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,k,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.5010387</code></pre>
<blockquote>
<p>We find that the interval is <span class="math inline">\([\hat{p}_L,\hat{p}_U] =
[0.261,0.501]\)</span>, which overlaps the true value of 0.4.
(See Figure <a href="the-binomial-and-related-distributions.html#fig:binci">3.9</a>.)
Note that, unlike in Chapter 2, the interval over which we
search for the root is [0,1], which is the range of possible
values for <span class="math inline">\(p\)</span>.</p>
</blockquote>
<blockquote>
<p>We can compute the coverage of this interval following the
prescription given above:</p>
</blockquote>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="the-binomial-and-related-distributions.html#cb183-1" tabindex="-1"></a>w.lo <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pbinom</span>(<span class="dv">0</span><span class="sc">:</span>(n<span class="sc">*</span>k),<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p) <span class="sc">&lt;</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb183-2"><a href="the-binomial-and-related-distributions.html#cb183-2" tabindex="-1"></a>alpha_over_2.lo.prime <span class="ot">&lt;-</span> <span class="fu">pbinom</span>(<span class="dv">0</span><span class="sc">:</span>(n<span class="sc">*</span>k),<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p)[w.lo]</span>
<span id="cb183-3"><a href="the-binomial-and-related-distributions.html#cb183-3" tabindex="-1"></a>w.hi <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pbinom</span>(<span class="dv">0</span><span class="sc">:</span>(n<span class="sc">*</span>k),<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p) <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb183-4"><a href="the-binomial-and-related-distributions.html#cb183-4" tabindex="-1"></a>alpha_over_2.hi.prime <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pbinom</span>(<span class="dv">0</span><span class="sc">:</span>(n<span class="sc">*</span>k),<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p)[w.hi]</span>
<span id="cb183-5"><a href="the-binomial-and-related-distributions.html#cb183-5" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>alpha_over_2.lo.prime<span class="sc">-</span>alpha_over_2.hi.prime),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 95.29</code></pre>
<blockquote>
<p>Due to the discreteness of the binomial distribution, the true coverage
of our two-sided intervals is 95.29%, not 95%.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binci"></span>
<img src="_main_files/figure-html/binci-1.png" alt="\label{fig:binci}Probability mass functions for binomial distributions for which $n \cdot k= 12 \cdot 5 = 60$ and (left) $p=0.261$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.261$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" /><img src="_main_files/figure-html/binci-2.png" alt="\label{fig:binci}Probability mass functions for binomial distributions for which $n \cdot k= 12 \cdot 5 = 60$ and (left) $p=0.261$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.261$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" />
<p class="caption">
Figure 3.9: Probability mass functions for binomial distributions for which <span class="math inline">\(n \cdot k= 12 \cdot 5 = 60\)</span> and (left) <span class="math inline">\(p=0.261\)</span> and (right) <span class="math inline">\(p=0.501\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 22\)</span> successes and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.261\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 22\)</span>, while <span class="math inline">\(p=0.501\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 22\)</span>.
</p>
</div>
<hr />
</div>
<div id="confidence-interval-for-the-negative-binomial-success-probability" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Confidence Interval for the Negative Binomial Success Probability<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have performed <span class="math inline">\(n = 10\)</span> separate negative binomial
trials, each with a target number of successes <span class="math inline">\(s\)</span>,
and recorded the number of failures <span class="math inline">\(X_1,\ldots,X_{n}\)</span> for
each. Further, assume that the probability of success is <span class="math inline">\(p\)</span>. Below we will
show how to compute the confidence interval for <span class="math inline">\(p\)</span>, but before we start,
we recall that <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is a negative binomially distributed
random variable for <span class="math inline">\(ns\)</span> successes and probability of success <span class="math inline">\(p\)</span>.
Here, <span class="math inline">\(E[Y] = s(1-p)/p\)</span>…as <span class="math inline">\(p\)</span> increases, <span class="math inline">\(E[Y]\)</span> <em>decreases</em>. Thus
when we adapt the confidence interval code we use for binomially
distributed data, we need to switch the mapping of <span class="math inline">\(q = 1-\alpha/2\)</span> and
<span class="math inline">\(q = \alpha/2\)</span> to the <em>upper</em> and <em>lower</em> bounds, respectively.</p>
</blockquote>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="the-binomial-and-related-distributions.html#cb185-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb185-2"><a href="the-binomial-and-related-distributions.html#cb185-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb185-3"><a href="the-binomial-and-related-distributions.html#cb185-3" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb185-4"><a href="the-binomial-and-related-distributions.html#cb185-4" tabindex="-1"></a>s     <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb185-5"><a href="the-binomial-and-related-distributions.html#cb185-5" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb185-6"><a href="the-binomial-and-related-distributions.html#cb185-6" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rnbinom</span>(n,<span class="at">size=</span>s,<span class="at">prob=</span>p)</span>
<span id="cb185-7"><a href="the-binomial-and-related-distributions.html#cb185-7" tabindex="-1"></a></span>
<span id="cb185-8"><a href="the-binomial-and-related-distributions.html#cb185-8" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(p,y.obs,n,s,q)</span>
<span id="cb185-9"><a href="the-binomial-and-related-distributions.html#cb185-9" tabindex="-1"></a>{     </span>
<span id="cb185-10"><a href="the-binomial-and-related-distributions.html#cb185-10" tabindex="-1"></a>  <span class="fu">pnbinom</span>(y.obs,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p)<span class="sc">-</span>q</span>
<span id="cb185-11"><a href="the-binomial-and-related-distributions.html#cb185-11" tabindex="-1"></a>}     </span>
<span id="cb185-12"><a href="the-binomial-and-related-distributions.html#cb185-12" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,s,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.3255305</code></pre>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="the-binomial-and-related-distributions.html#cb187-1" tabindex="-1"></a><span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),<span class="at">y.obs=</span><span class="fu">sum</span>(X),n,s,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.4822869</code></pre>
<blockquote>
<p>The confidence interval is <span class="math inline">\([\hat{p}_L,\hat{p}_U] = [0.326,0.482]\)</span>,
which overlaps the true value 0.4. We note that in the code, we change
the lower bound on the interval from 0 (in the binomial case) to
0.0001 (something suitably small but non-zero): a probability of success
of 0 maps to an infinite number of failures, which <code>R</code> cannot tolerate!</p>
</blockquote>
<blockquote>
<p>We can compute the coverage of this interval following the
prescription given above:</p>
</blockquote>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="the-binomial-and-related-distributions.html#cb189-1" tabindex="-1"></a>large <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb189-2"><a href="the-binomial-and-related-distributions.html#cb189-2" tabindex="-1"></a>w.lo <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pnbinom</span>(<span class="dv">0</span><span class="sc">:</span>large,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p) <span class="sc">&lt;</span> alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb189-3"><a href="the-binomial-and-related-distributions.html#cb189-3" tabindex="-1"></a>alpha_over_2.lo.prime <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>(<span class="dv">0</span><span class="sc">:</span>large,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p)[w.lo]</span>
<span id="cb189-4"><a href="the-binomial-and-related-distributions.html#cb189-4" tabindex="-1"></a>w.hi <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pnbinom</span>(<span class="dv">0</span><span class="sc">:</span>large,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p) <span class="sc">&lt;</span> <span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb189-5"><a href="the-binomial-and-related-distributions.html#cb189-5" tabindex="-1"></a>alpha_over_2.hi.prime <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span><span class="fu">pnbinom</span>(<span class="dv">0</span><span class="sc">:</span>large,<span class="at">size=</span>n<span class="sc">*</span>s,<span class="at">prob=</span>p)[w.hi]</span>
<span id="cb189-6"><a href="the-binomial-and-related-distributions.html#cb189-6" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">100</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>alpha_over_2.lo.prime<span class="sc">-</span>alpha_over_2.hi.prime),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 94.77</code></pre>
<blockquote>
<p>Due to the discreteness of the binomial distribution, the true coverage
of our two-sided intervals is 94.77%, not 95%.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbinci"></span>
<img src="_main_files/figure-html/nbinci-1.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions for which $n \cdot s = 12 \cdot 5 = 60$ and (left) $p=0.326$ and (right) $p=0.482$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.482$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" /><img src="_main_files/figure-html/nbinci-2.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions for which $n \cdot s = 12 \cdot 5 = 60$ and (left) $p=0.326$ and (right) $p=0.482$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.482$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" />
<p class="caption">
Figure 3.10: Probability mass functions for negative binomial distributions for which <span class="math inline">\(n \cdot s = 12 \cdot 5 = 60\)</span> and (left) <span class="math inline">\(p=0.326\)</span> and (right) <span class="math inline">\(p=0.482\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 88\)</span> failures and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.326\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 88\)</span>, while <span class="math inline">\(p=0.482\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 88\)</span>.
</p>
</div>
<hr />
</div>
<div id="wald-interval-for-the-binomial-success-probability" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Wald Interval for the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#wald-interval-for-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have sampled <span class="math inline">\(n\)</span> iid binomial variables with number
of trials <span class="math inline">\(k\)</span> and probability of success <span class="math inline">\(p\)</span>.
When <span class="math inline">\(k\)</span> is sufficiently large and <span class="math inline">\(p\)</span> is sufficiently
far from 0 or 1, we can assume that
<span class="math inline">\(\bar{X}\)</span> has a distribution whose shape is approximately
that of a normal distribution, with mean <span class="math inline">\(E[\bar{X}] = kp\)</span> and with
variance and standard error
<span class="math display">\[
V[\bar{X}] = \frac{kp(1-p)}{n} ~~~ \mbox{and} ~~~ se(\bar{X}) = \sqrt{V[\bar{X}]} = \sqrt{\frac{kp(1-p)}{n}} \,.
\]</span>
Furthermore, we can assume
that <span class="math inline">\(\hat{p} = \bar{X}/k\)</span> is approximately normally distributed, with
mean <span class="math inline">\(p\)</span>, variance <span class="math inline">\(V[\hat{p}] = V[\bar{X}]/k^2 = p(1-p)/nk\)</span>, and
standard error <span class="math inline">\(\sqrt{p(1-p)/nk}\)</span>. Given this information, it is simple to
express, e.g., an approximate two-sided <span class="math inline">\(100(1-\alpha)\)</span>% confidence
interval for <span class="math inline">\(p\)</span>:
<span class="math display">\[
\hat{p} \pm z_{1-\alpha/2} se(\hat{p}) ~~ \Rightarrow ~~ \left[ \hat{p} - z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \, , \, \hat{p} + z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \right] \,,
\]</span>
where <span class="math inline">\(z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)\)</span>. However, we don’t
actually know the true value of <span class="math inline">\(p\)</span>…so we plug in <span class="math inline">\(p = \hat{p}\)</span>.</p>
</blockquote>
<blockquote>
<p>This is the so-called <em>Wald interval</em>
that is typically provided to students in introductory statistics courses,
although it is typically provided assuming <span class="math inline">\(n = 1\)</span> and assuming that
<span class="math inline">\(\alpha = 0.05\)</span>:
<span class="math display">\[
\left[ \hat{p} - 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \, , \, \hat{p} + 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \right] \,,
\]</span>
where in this case <span class="math inline">\(\hat{p} = X/k\)</span>.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-1" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Hypothesis Testing<a href="the-binomial-and-related-distributions.html#hypothesis-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-tail test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a one-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bound given by <span class="math inline">\(y_{RR}\)</span>, which is a solution to the equation</em>
<span class="math display">\[
F_Y(y_{RR} \vert \theta_o) - q = 0 \,,
\]</span>
<em>where <span class="math inline">\(F_Y(\cdot)\)</span> is the cumulative distribution function for
the statistic <span class="math inline">\(Y\)</span> and <span class="math inline">\(q\)</span> is an appropriate quantile value that is determined
using the hypothesis test reference table introduced in
section 17 of Chapter 1. Note that the hypothesis test framework only
allows us to make a decision about a null hypothesis; nothing is proven.</em></p>
<p>In the previous chapter, we utilized <span class="math inline">\(\bar{X}\)</span> when testing hypotheses about the
normal mean <span class="math inline">\(\mu\)</span>. This is a principled choice for a test statistic<span class="math inline">\(-\)</span>after all,
<span class="math inline">\(\bar{X}\)</span> is the MLE for <span class="math inline">\(\mu-\)</span>but we do not yet know whether or not we can
choose a better one. Can we differentiate hypotheses more easily if we use a
test statistic other than <span class="math inline">\(\bar{X}\)</span>?</p>
<p>To help answer this question, we now introduce a method for
defining the <em>most powerful test</em> of a <em>simple</em> null hypothesis versus
a <em>simple</em> alternative hypothesis:
<span class="math display">\[
H_o : \theta = \theta_o ~~\mbox{and}~~ H_a : \theta = \theta_a \,.
\]</span>
Note that the word “simple” has a precise meaning here: it means that
when we set <span class="math inline">\(\theta\)</span> to a particular value,
we are completely fixing the shape and location of the
pmf or pdf from which data are sampled. If, for instance, we are dealing
with a normal distribution with unknown variance <span class="math inline">\(\sigma^2\)</span>, the hypothesis
<span class="math inline">\(\mu = \mu_o\)</span> would not be simple, since the width of the pdf
can still vary: the shape is not completely fixed.
(The hypothesis <span class="math inline">\(\mu = \mu_o\)</span> with variance unknown is dubbed a <em>composite</em>
hypothesis. We will examine how to work with composite hypotheses in the next chapter.)
For a given test level <span class="math inline">\(\alpha\)</span>, the <em>Neyman-Pearson lemma</em> states that the
test that maximizes the power has a rejection region of the form
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} &lt; c(\alpha) \,,
\]</span>
where <span class="math inline">\(c\)</span> is a constant whose value depends on <span class="math inline">\(\alpha\)</span> that we have to
determine. While this formulation initially
appears straightforward, it is in fact not necessarily clear how to
derive <span class="math inline">\(c(\alpha)\)</span>. However: in typical analysis situations,
<em>we don’t need to</em>! The NP lemma utilizes the likelihood function,
so implicitly, what it is telling us that <em>the best statistic for
differentiating between two simple hypotheses is a sufficient statistic</em>.
We simply determine a sufficient statistic for which we
know the sampling distribution, and use it to define a hypothesis
test via the procedure we laid out in previous chapters. Full stop.</p>
<p>For instance, when we draw <span class="math inline">\(n\)</span> iid data from a binomial distribution,
the sufficient statistic <span class="math inline">\(\sum_{i=1}^n X_i\)</span> has a known and easily
utilized sampling distribution<span class="math inline">\(-\)</span>namely, Binom(<span class="math inline">\(nk,p\)</span>)<span class="math inline">\(-\)</span>while
<span class="math inline">\(\bar{X} = (\sum_{i=1}^n X_i)/n\)</span> does not. So we would utilize
<span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>. (Note that it ultimately doesn’t matter which
function of a sufficient statistic we use: if we can carry out the math,
we will end up with tests that have the same power and result in
the same <span class="math inline">\(p\)</span>-values. It would be <em>very</em> problematic if this wasn’t
the case: we’d have to “fish around” to determine which function of
a sufficient statistic would give us the best test, which clearly would
not be a good situation to find ourselves in.)</p>
<p>Let’s assume that we use <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span> to define, e.g.,
a lower-tail test <span class="math inline">\(H_o : p = p_o\)</span> versus <span class="math inline">\(H_a : p = p_a &lt; p_o\)</span>.
Since <span class="math inline">\(E[U] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, we can go to the hypothesis test
reference tables and write (in code) that</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="the-binomial-and-related-distributions.html#cb191-1" tabindex="-1"></a>u.rr <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span>k,<span class="at">prob=</span>p.o)</span></code></pre></div>
<p>We see that our rejection region boundary depends on the value of <span class="math inline">\(p_o\)</span>,
but <em>not</em> on the value of <span class="math inline">\(p_a\)</span>. This means that the test we define above
is the most-powerful test regardless of the value <span class="math inline">\(p_a &lt; p_o\)</span>. We have
thus constructed a <em>uniformly most powerful</em> (or <em>UMP</em>) test for
disambiguating the simple hypotheses <span class="math inline">\(H_o : p = p_o\)</span> and <span class="math inline">\(H_a : p = p_a &lt; p_o\)</span>.
It is typically the case that when we use the NP lemma to define a most
powerful test for <span class="math inline">\(\theta_o\)</span> versus <span class="math inline">\(\theta_a\)</span>, we end up defining a
UMP test as well.</p>
<p>We note that we cannot use the NP lemma to construct most powerful
<em>two-tail</em> hypothesis tests. When we construct a two-tail test, it is
convention to define one rejection region boundary assuming <span class="math inline">\(q = \alpha/2\)</span>
and the other assuming <span class="math inline">\(q = 1 - \alpha/2\)</span>. But that is just convention;
we could put <span class="math inline">\(\alpha/10\)</span> on “one side” and <span class="math inline">\(1-9\alpha/10\)</span> on the other,
etc., and in general we cannot guarantee that any
one way of splitting <span class="math inline">\(\alpha\)</span> will yield a more powerful test in a given
situation than any other possible split.</p>
<hr />
<div id="ump-test-exponential-distribution" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> UMP Test: Exponential Distribution<a href="the-binomial-and-related-distributions.html#ump-test-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s suppose we sample <span class="math inline">\(n = 3\)</span> iid data from the exponential distribution
<span class="math display">\[
f_X(x) = \frac{1}{\theta} e^{-x/\theta} \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and we wish to define the most powerful
test of the simple hypotheses <span class="math inline">\(H_o : \theta_o = 2\)</span> and
<span class="math inline">\(H_a : \theta_a = 1\)</span>. We observe the values <span class="math inline">\(\mathbf{x}_{\rm obs} =
\{0.215,1.131,2.064\}\)</span>, and we assume <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>We begin by determining a sufficient statistic:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^3 \frac{1}{\theta} e^{-x_i/\theta} = \frac{1}{\theta^3} e^{(\sum_{i=1}^3 x_i)/\theta} \,.
\]</span>
We identify <span class="math inline">\(Y = \sum_{i=1}^3 X_i\)</span> as a sufficient statistic. The next
question is whether we can determine the sampling distribution for <span class="math inline">\(Y\)</span>.
The moment-generating function for each of the <span class="math inline">\(X_i\)</span>’s is
<span class="math display">\[
m_{X_i}(t) = (1 - \theta t)^{-1} \,,
\]</span>
and so the mgf for <span class="math inline">\(Y\)</span> is
<span class="math display">\[
m_Y(t) = \prod_{i=1}^3 m_{X_i}(t) = (1 - \theta t)^{-3} \,,
\]</span>
We (might!) recognize
this as the mgf for a Gamma(3,<span class="math inline">\(\theta\)</span>) distribution. (The gamma distribution
will be officially introduced in Chapter 4.) So: we know the sampling
distribution for <span class="math inline">\(Y\)</span>, and we can use it to define the most powerful test.
To reiterate: the only thing that the NP lemma is doing for us here is
guiding our selection of a test statistic. Beyond that, we construct the
test using the framework we already learned in Chapters 1 and 2.</p>
</blockquote>
<blockquote>
<p>Because <span class="math inline">\(\theta_a &lt; \theta_o\)</span>, we define a lower-tail test. And since
<span class="math inline">\(E[Y] = 3\theta\)</span> increases with <span class="math inline">\(\theta\)</span>, we utilize the formulae from the
hypothesis test reference tables that are on the “yes” line. The
rejection-region boundary is thus
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(\alpha \vert \theta_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="the-binomial-and-related-distributions.html#cb192-1" tabindex="-1"></a>x.obs   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.215</span>,<span class="fl">1.131</span>,<span class="fl">2.064</span>)</span>
<span id="cb192-2"><a href="the-binomial-and-related-distributions.html#cb192-2" tabindex="-1"></a>(y.obs  <span class="ot">&lt;-</span> <span class="fu">sum</span>(x.obs))</span></code></pre></div>
<pre><code>## [1] 3.41</code></pre>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="the-binomial-and-related-distributions.html#cb194-1" tabindex="-1"></a>alpha   <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb194-2"><a href="the-binomial-and-related-distributions.html#cb194-2" tabindex="-1"></a>theta.o <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb194-3"><a href="the-binomial-and-related-distributions.html#cb194-3" tabindex="-1"></a>(y.rr <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(alpha,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta.o))</span></code></pre></div>
<pre><code>## [1] 1.635383</code></pre>
<blockquote>
<p>Our observed statistic is <span class="math inline">\(y_{\rm obs} = 3.410\)</span> and the rejection-region
boundary is 1.635: we fail to reject the null and conclude that
<span class="math inline">\(\theta_o = 2\)</span> is a plausible value of <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<blockquote>
<p>We note that because <span class="math inline">\(y_{\rm RR}\)</span> is not a function of <span class="math inline">\(\theta_a\)</span>, we have
not only defined the most powerful test, but we have also defined a
uniformly most powerful test for all alternative hypotheses
<span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the <span class="math inline">\(p\)</span>-value, and what is the power of the test if <span class="math inline">\(\theta = 1.5\)</span>?</p>
</blockquote>
<blockquote>
<p>According to the hypothesis test reference tables, the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[
p = F_Y(y_{\rm obs} \vert \theta_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="the-binomial-and-related-distributions.html#cb196-1" tabindex="-1"></a><span class="fu">pgamma</span>(y.obs,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta.o)</span></code></pre></div>
<pre><code>## [1] 0.2440973</code></pre>
<blockquote>
<p>The <span class="math inline">\(p\)</span>-value is 0.244, which is greater than <span class="math inline">\(\alpha\)</span>, as we expect.</p>
</blockquote>
<blockquote>
<p>The test power is
<span class="math display">\[
{\rm power}(\theta) = F_Y(y_{\rm RR} \vert \theta) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="the-binomial-and-related-distributions.html#cb198-1" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fl">1.5</span></span>
<span id="cb198-2"><a href="the-binomial-and-related-distributions.html#cb198-2" tabindex="-1"></a><span class="fu">pgamma</span>(y.rr,<span class="at">shape=</span><span class="dv">3</span>,<span class="at">scale=</span>theta)</span></code></pre></div>
<pre><code>## [1] 0.09762911</code></pre>
<blockquote>
<p>The power is 0.097…only 9.7% of the time will we reject the null hypothesis
<span class="math inline">\(\theta_o = 2\)</span> when <span class="math inline">\(\theta\)</span> is actually 1.5.</p>
</blockquote>
<hr />
</div>
<div id="ump-test-negative-binomial-distribution" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> UMP Test: Negative Binomial Distribution<a href="the-binomial-and-related-distributions.html#ump-test-negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we sample <span class="math inline">\(n = 5\)</span> data from a negative binomial
distribution
<span class="math display">\[
p_X(x) = \binom{x+s-1}{x} p^s (1-p)^x \,,
\]</span>
with <span class="math inline">\(x \in \{0,1,\ldots,\infty\}\)</span> being the observed number of failures
prior to observed the <span class="math inline">\(s^{\rm th}\)</span> success, <span class="math inline">\(p \in (0,1]\)</span>,
and <span class="math inline">\(s = 3\)</span> successes.
We wish to define the most powerful test of the simple hypotheses
<span class="math inline">\(H_o : p_o = 0.5\)</span> and <span class="math inline">\(H_a : p_a = 0.25\)</span>. We observe the values
<span class="math inline">\(\mathbf{x}_{\rm obs} = \{5,3,10,12,4\}\)</span>, and we assume <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>As in the last example, we begin by determining a sufficient statistic:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^5 \binom{x_i+s-1}{x_i} p^s (1-p)^x_i \propto \prod_{i=1}^5 p^s (1-p)^x_i = p^{ns} (1-p)^{\sum_{i=1}^5 x_i} \,.
\]</span>
We identify <span class="math inline">\(Y = \sum_{i=1}^5 X_i\)</span> as a sufficient statistic. Earlier
in the chapter, we determined that if the <span class="math inline">\(X_i\)</span>’s are iid draws from
a negative binomial distribution with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(s\)</span>, then the
sum is also negative binomially distributed, with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(ns\)</span>.
So: we know the sampling
distribution for <span class="math inline">\(Y\)</span>, and we can use it to define the most powerful test.</p>
</blockquote>
<blockquote>
<p>Because <span class="math inline">\(p_a &lt; p_o\)</span>, we will define a lower-tail test. And since
<span class="math inline">\(E[Y] = s(1-p)/p\)</span> <em>decreases</em> with <span class="math inline">\(p\)</span>, we will utilize the formulae from
the hypothesis test reference tables that are on the “no” line (and we
will reject the null hypothesis if <span class="math inline">\(y_{\rm obs} &gt; y_{\rm RR}\)</span>).</p>
</blockquote>
<blockquote>
<p>The rejection-region boundary is
<span class="math display">\[
y_{\rm RR} = F_Y^{-1}(1-\alpha \vert p_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="the-binomial-and-related-distributions.html#cb200-1" tabindex="-1"></a>x.obs  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">10</span>,<span class="dv">12</span>,<span class="dv">4</span>)</span>
<span id="cb200-2"><a href="the-binomial-and-related-distributions.html#cb200-2" tabindex="-1"></a>n      <span class="ot">&lt;-</span> <span class="fu">length</span>(x.obs)</span>
<span id="cb200-3"><a href="the-binomial-and-related-distributions.html#cb200-3" tabindex="-1"></a>(y.obs <span class="ot">&lt;-</span> <span class="fu">sum</span>(x.obs))</span></code></pre></div>
<pre><code>## [1] 34</code></pre>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="the-binomial-and-related-distributions.html#cb202-1" tabindex="-1"></a>alpha  <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb202-2"><a href="the-binomial-and-related-distributions.html#cb202-2" tabindex="-1"></a>p.o    <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb202-3"><a href="the-binomial-and-related-distributions.html#cb202-3" tabindex="-1"></a>(y.rr  <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o))</span></code></pre></div>
<pre><code>## [1] 25</code></pre>
<blockquote>
<p>Our observed statistic is <span class="math inline">\(y_{\rm obs} = 34\)</span> and the rejection-region
boundary is 25: we reject the null hypothesis and state that there
is sufficient evidence to conclude that <span class="math inline">\(p &lt; 0.5\)</span>.</p>
</blockquote>
<blockquote>
<p>We note that because <span class="math inline">\(y_{\rm RR}\)</span> is not a function of <span class="math inline">\(p_a\)</span>, we have
not only defined the most powerful test, but we have also defined a
uniformly most powerful test for all alternative hypotheses <span class="math inline">\(p_a &lt; p_o\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the <span class="math inline">\(p\)</span>-value, and what is the power of the test if <span class="math inline">\(p = 0.3\)</span>?</p>
</blockquote>
<blockquote>
<p>According to the hypothesis test reference tables, the <span class="math inline">\(p\)</span>-value is
<span class="math display">\[
p = 1 - F_Y(y_{\rm obs} \vert p_o) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="the-binomial-and-related-distributions.html#cb204-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(y.obs,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o)</span></code></pre></div>
<pre><code>## [1] 0.001900827</code></pre>
<blockquote>
<p>Our <span class="math inline">\(p\)</span>-value is supposedly 0.0019, which is less than <span class="math inline">\(\alpha\)</span>, as we expect.
<em>However,</em> there is a problem here! The function call
<code>1 - pnbinom(y.obs,size=3*n,prob=p.o)</code> is equivalent to the function
call</p>
</blockquote>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="the-binomial-and-related-distributions.html#cb206-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dnbinom</span>((y.obs<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span><span class="dv">100</span>,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o))</span></code></pre></div>
<pre><code>## [1] 0.001900827</code></pre>
<blockquote>
<p>where we use the number 100 as a stand-in for infinity. However, a
<span class="math inline">\(p\)</span>-value calculation should include the observed statistic value, and
the above call does not. (It begins the calculation at <code>y.obs+1</code>, and not
at <code>y.obs</code>.) The <span class="math inline">\(p\)</span>-value is thus actually</p>
</blockquote>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="the-binomial-and-related-distributions.html#cb208-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">dnbinom</span>(y.obs<span class="sc">:</span><span class="dv">100</span>,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o))</span></code></pre></div>
<pre><code>## [1] 0.002757601</code></pre>
<blockquote>
<p>or</p>
</blockquote>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="the-binomial-and-related-distributions.html#cb210-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(y.obs<span class="dv">-1</span>,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p.o)</span></code></pre></div>
<pre><code>## [1] 0.002757601</code></pre>
<blockquote>
<p>The value 0.0028 is still less than <span class="math inline">\(\alpha\)</span>, so our qualitative conclusion
is unchanged.</p>
</blockquote>
<blockquote>
<p><strong>So: if our data are drawn from a discrete distribution, and we are computing a <span class="math inline">\(p\)</span>-value for an upper-tail/yes test, or a lower-tail/no test (as is the case here), we need to subtract 1 from the observed statistic value when passing it into a cdf function. (And similarly, if we are computing the test power for an upper-tail/no test, or a lower-tail/yes test (as is not the case here), we need to subtract 1 from the rejection-region-boundary value.)</strong></p>
</blockquote>
<blockquote>
<p>The test power is
<span class="math display">\[
{\rm power}(\theta) = 1 - F_Y(y_{\rm RR} \vert p) \,,
\]</span>
which in code is</p>
</blockquote>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="the-binomial-and-related-distributions.html#cb212-1" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb212-2"><a href="the-binomial-and-related-distributions.html#cb212-2" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnbinom</span>(y.rr,<span class="at">size=</span><span class="dv">3</span><span class="sc">*</span>n,<span class="at">prob=</span>p)</span></code></pre></div>
<pre><code>## [1] 0.8074482</code></pre>
<blockquote>
<p>We find that
the power is 0.807…80.7% of the time will we reject the null hypothesis
<span class="math inline">\(p = 0.5\)</span> when <span class="math inline">\(p\)</span> is actually 0.3 (and when <span class="math inline">\(n = 5\)</span> and <span class="math inline">\(s = 3\)</span>).</p>
</blockquote>
<hr />
</div>
<div id="defining-a-ump-test-for-the-normal-population-mean" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Defining a UMP Test for the Normal Population Mean<a href="the-binomial-and-related-distributions.html#defining-a-ump-test-for-the-normal-population-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In Chapter 2, we use the statistic <span class="math inline">\(\bar{X}\)</span> as the basis for testing
hypotheses about the normal population mean, <span class="math inline">\(\mu\)</span>. We justified this
choice because, at the very least, <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>, so it
is a principled choice. However, beyond that, we did not (and indeed
could not) provide any further justification. Is <span class="math inline">\(\bar{X}\)</span> the basis
of the UMP for <span class="math inline">\(\mu\)</span>?</p>
</blockquote>
<blockquote>
<p>Recall that the NP lemma <em>does not apply</em> if there are
two freely varying parameters. The NP lemma applies to <em>simple hypotheses</em>,
where the hypotheses uniquely specify the population from which data are
drawn. Here, even if we set <span class="math inline">\(H_o: \mu = \mu_o\)</span> and <span class="math inline">\(H_a: \mu = \mu_a\)</span>,
<span class="math inline">\(\sigma^2\)</span> can still freely vary. (So, technically, these hypotheses
are <em>composite hypotheses</em>.) So
to go further here, we must assume <span class="math inline">\(\sigma^2\)</span> is known.</p>
</blockquote>
<blockquote>
<p>When <span class="math inline">\(\sigma^2\)</span> is known, we can factorize the likelihood as
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i,\mu)} \cdot \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)}_{h(\mathbf{x})} \,,
\]</span>
and identify <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> as a sufficient statistic.
We know from using the method of moment-generating
functions that the sum of <span class="math inline">\(n\)</span> iid normal random variables is itself a normal
random variable with mean <span class="math inline">\(n\mu\)</span> and variance <span class="math inline">\(n\sigma^2\)</span>, so
<span class="math inline">\(Y \sim \mathcal{N}(n\mu,n\sigma^2)\)</span>.</p>
</blockquote>
<blockquote>
<p>Let’s assume that <span class="math inline">\(\mu_a &gt; \mu_o\)</span>, or, in other words, that we are
performing an upper-tail test. Given that <span class="math inline">\(E[Y] = n\mu\)</span>, we know that
we are on the “yes” line of the hypothesis test reference tables, so
the rejection region is
<span class="math display">\[
Y &gt; y_{\rm RR} = F_Y^{-1}(1-\alpha,n\mu_o,n\sigma^2) \,,
\]</span>
or, in code,</p>
</blockquote>
<pre><code>qnorm(1-alpha,n*mu.o,n*sigma2)</code></pre>
<blockquote>
<p>The rejection-region boundary does not depend on <span class="math inline">\(\mu_a\)</span>, so
we have defined a uniformly most powerful test of <span class="math inline">\(\mu = \mu_o\)</span>
versus <span class="math inline">\(\mu = \mu_a &gt; \mu_o\)</span>.</p>
</blockquote>
<blockquote>
<p>“But wait. What about <span class="math inline">\(\bar{X}\)</span>?”</p>
</blockquote>
<blockquote>
<p>Recall that a function of a sufficient statistic
is itself a sufficient statistic, so we could also use
<span class="math inline">\(Y&#39; = \bar{X} = Y/n\)</span> as our statistic, particularly as we
<em>do</em> know its sampling distribution: <span class="math inline">\(\mathcal{N}(\mu,\sigma^2/n)\)</span>. Thus
<span class="math display">\[
Y&#39; &gt; y_{\rm RR}&#39; = F_Y^{-1}(1-\alpha,\mu_o,\sigma^2/n) \,,
\]</span>
or, in code,</p>
</blockquote>
<pre><code>qnorm(1-alpha,mu.o,sigma2/n)</code></pre>
<blockquote>
<p>“But…<span class="math inline">\(y_{\rm RR}\)</span> and <span class="math inline">\(y_{\rm RR}&#39;\)</span> do not have the same value!”</p>
</blockquote>
<blockquote>
<p>This is fine: <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y&#39;\)</span> don’t have the same value either. The key
point is that when we compute, e.g., <span class="math inline">\(p\)</span>-values given observed data,
they will be the same in both cases. And the test power will be the
same. We can use any function of a sufficient statistic to define our
test; in practice, we will use any function of a sufficient statistic
for which we know the sampling distribution. Here we know the distributions
for both the sample sum and the sample mean;
in other situations (like when we are working with
binomially distributed data), we will not.</p>
</blockquote>
<blockquote>
<p>Now, what if <span class="math inline">\(\sigma^2\)</span> is unknown? We discuss this possibility
in the next chapter, when we introduce the likelihood ratio test.</p>
</blockquote>
<hr />
</div>
<div id="defining-a-test-that-is-not-uniformly-most-powerful" class="section level3 hasAnchor" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> Defining a Test That is Not Uniformly Most Powerful<a href="the-binomial-and-related-distributions.html#defining-a-test-that-is-not-uniformly-most-powerful" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!--
> people.bath.ac.uk/masmah/MA40092.bho/sols7.pdf
-->
<blockquote>
<p>Let’s assume we draw one datum <span class="math inline">\(X\)</span> from a normal distribution with
mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\mu^2\)</span>, and we wish to test <span class="math inline">\(H_o : \mu = \mu_o = 1\)</span>
versus <span class="math inline">\(H_a : \mu = \mu_a\)</span>. Can we define a uniformly most powerful test
of these hypotheses?</p>
</blockquote>
<blockquote>
<p>To try to do this, we utilize the NP lemma. However, there is an immediate
issue that arises: here, there are <em>two</em> sufficient statistics that are
identified via factorization (<span class="math inline">\(\sum_{i=1}^n X_i\)</span> and <span class="math inline">\(\sum_{i=1}^n X_i^2\)</span>),
but just one parameter (<span class="math inline">\(\mu\)</span>). So we cannot circumvent working directly
with the likelihood ratio:
<span class="math display">\[\begin{align*}
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} = \frac{f_X(x \vert \theta_o)}{f_X(x \vert \theta_a)} = \mu_a \exp\left(-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2}\right) \,.
\end{align*}\]</span>
To reject the null hypothesis, the ratio must be less than some constant
<span class="math inline">\(c(\alpha)\)</span>. First, we can divide both sides of the inequality by
<span class="math inline">\(\mu_a\)</span> (which is a set constant), so that now we reject the null if
<span class="math display">\[\begin{align*}
\exp\left(-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2}\right) &lt; c&#39;(\alpha) \,.
\end{align*}\]</span>
We then take the natural logarithm of each side; we would reject the null if
<span class="math display">\[\begin{align*}
-\frac{(x-1)^2}{2}+\frac{(x-\mu_a)^2}{2\mu_a^2} &lt; c&#39;&#39;(\alpha) \,.
\end{align*}\]</span>
It turns out that for
the test to be uniformly most powerful, we would have to perform
further manipulations so as to isolate <span class="math inline">\(x\)</span> on the left-hand side of the
inequality, with only constants appearing on the right-hand side
(i.e., we would need to simplify this expression such that it achieves
the form <span class="math inline">\(x &lt; k\)</span> or <span class="math inline">\(x &gt; k\)</span>). Here,
that is impossible to do, meaning that the value of the
rejection region boundary changes as <span class="math inline">\(\mu_a\)</span> changes. Thus while we can
define the most powerful test of <span class="math inline">\(H_o : \mu = \mu_o = 1\)</span>
versus <span class="math inline">\(H_a : \mu = \mu_a\)</span>, that test is <em>not</em> a uniformly most powerful one.</p>
</blockquote>
<hr />
</div>
<div id="large-sample-tests-of-the-binomial-success-probability" class="section level3 hasAnchor" number="3.8.5">
<h3><span class="header-section-number">3.8.5</span> Large-Sample Tests of the Binomial Success Probability<a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-success-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Earlier in the chapter, we saw that if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,,
\]</span>
then, given a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>), we can assume
<span class="math display">\[
X \stackrel{d}{\rightarrow} Y \sim \mathcal{N}(kp,kp(1-p)) \,,
\]</span>
or that
<span class="math display">\[
\hat{p} = \hat{p}_{MLE} = \frac{X}{k} \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}(p,p(1-p)/k) \,.
\]</span>
In hypothesis testing, this approximation forms the basis of two
different tests:
<span class="math display">\[\begin{align*}
\mbox{Score}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,p_o(1-p_o)/k) \\
\mbox{Wald}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,\hat{p}(1-\hat{p})/k) \,.
\end{align*}\]</span>
The only difference between the two is in how the variance is estimated
under the null. To carry out these tests, we can simply adapt the
expressions for the rejection regions, <span class="math inline">\(p\)</span>-values, and power given in
Chapter 2 for tests of the normal population mean with <em>variance known</em>,
with the primary change being the definition of the variance.</p>
</blockquote>
<blockquote>
<p>We are including the details of the Wald and Score tests here for
completeness, in that they are ubiquitous in introductory statistics
settings. As they yield only approximate results, we would encourage
the reader to always use the exact testing mechanisms described above,
while being mindful of the effects of the discreteness of the probability
mass function, especially when <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are both small.</p>
</blockquote>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Logistic Regression<a href="the-binomial-and-related-distributions.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the last chapter, we introduced simple linear regression,
in which we model the data-generating process as
<span class="math display">\[
Y_i = \beta_0 + \beta_1x_i + \epsilon_i \,.
\]</span>
To perform hypothesis testing
(e.g., <span class="math inline">\(H_o : \beta_1 = 0\)</span> vs. <span class="math inline">\(H_a : \beta_1 \neq 0\)</span>), we make the
assumption that <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>,
or equivalently, that <span class="math inline">\(Y_i \vert x_i \sim \mathcal{N}(\beta_0+\beta_1x_i,\sigma^2)\)</span>.
When we make this assumption, we are implicitly stating that the response
variable is continuous and that it can take on
any value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. But what if this
doesn’t actually correctly represent the response variable? Maybe
the <span class="math inline">\(Y_i\)</span>’s are distances with values <span class="math inline">\(\geq 0\)</span>. Maybe each <span class="math inline">\(Y_i\)</span> belongs
to one of several categories, and thus the <span class="math inline">\(Y_i\)</span>’s are
discretely valued. When provided data such as these, it is possible,
though not optimal, to utilize simple linear regression.
A better choice is to generalize the concept of linear regression,
and to utilize this generalization to
implement a more appropriate statistical model.</p>
<p>To implement a <em>generalized linear model</em> (or <em>GLM</em>), we need to do two things:</p>
<ol style="list-style-type: decimal">
<li>examine the <span class="math inline">\(Y_i\)</span> values and select an appropriate distribution
for them (discrete or continuous? what is the functional domain?); and</li>
<li>define a <em>link function</em> <span class="math inline">\(g(\theta \vert x)\)</span> that maps
the line <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, which has infinite range,
into a more limited range (e.g., <span class="math inline">\([0,\infty)\)</span>).</li>
</ol>
<p>Suppose that, in an experiment, the response variable can take on the
values “false” and “true.” To generalize linear regression so as
to handle these data, we map “false” to 0 and “true” to 1, and assume
that we can model the data-generating process using a
Bernoulli distribution (i.e., a binomial distribution with <span class="math inline">\(k = 1\)</span>):
<span class="math inline">\(Y \sim\)</span> Bernoulli(<span class="math inline">\(p\)</span>). We know that <span class="math inline">\(0 &lt; p &lt; 1\)</span>, so
we adopt a link function that maps <span class="math inline">\(\beta_0 + \beta_1 x\)</span> to the range <span class="math inline">\((0,1)\)</span>.
There is no unique choice, but a conventional one
is the <em>logit</em> function:
<span class="math display">\[
g(p \vert x) = \log\left[\frac{p \vert x}{1-p \vert x}\right] = \beta_0 + \beta_1 x ~\implies~ p \vert x = \frac{\exp\left(\beta_0+\beta_1x\right)}{1+\exp\left(\beta_0+\beta_1x\right)} \,.
\]</span>
Using the logit function to model dichotomous data is dubbed <em>logistic regression</em>. See Figure <a href="the-binomial-and-related-distributions.html#fig:logexp">3.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logexp"></span>
<img src="_main_files/figure-html/logexp-1.png" alt="\label{fig:logexp}An example of an estimated logistic regression line. The blue line is a sigmoid function; it represents the probability that we would sample a datum of Class 1 as a function of $x$. The red points are the observed data." width="50%" />
<p class="caption">
Figure 3.11: An example of an estimated logistic regression line. The blue line is a sigmoid function; it represents the probability that we would sample a datum of Class 1 as a function of <span class="math inline">\(x\)</span>. The red points are the observed data.
</p>
</div>
<p>How do we learn a logistic regression model? In linear regression, we
can determine values for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by
formulae, the ones we derive when we minimize the sum of squared
errors (SSE). For logistic regression, such simple formulae do not
exist, and so we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> via numerical
optimization of the likelihood function
<span class="math display">\[
\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y}) = \prod_{i=1}^n p_{Y \vert \beta_0,\beta_1}(y_i \vert \beta_0,\beta_1) = \prod_{i=1}^n p^{y_i}(1-p)^{1-y_i} = p^{\sum_{i=1}^n y_i} (1-p)^{n-\sum_{i=1}^n y_i} \,,
\]</span>
where <span class="math inline">\(p_Y(\cdot)\)</span> is the Bernoulli probability mass function. Specifically,
we would take the first partial derivatives of
<span class="math inline">\(\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y})\)</span> with respect to <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span>, respectively, set them both to zero (thus allowing us to equate
the derivative expressions), and determine the values
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that satisfy the equated derivatives.
These are the values at which the
bivariate likelihood surface achieves its maximum value. Note that
because numerical optimization is an iterative process, logistic regression
models are learned more slowly than simple linear regression models.</p>
<p>Let’s step back for a moment. Why would we want to learn a logistic regression
model in the first place? After all, it is a relatively
inflexible model that might lack the ability to mimic the true behavior
of <span class="math inline">\(p \vert x\)</span>. The reason is that because we specify the
mathematical form of the model,
we can after the fact examine the estimated coefficients and
perform <em>inference</em>: we can examine how the response is affected by changes
in the predictor variables’ values. This can be important in, e.g., scientific
contexts, where explaining how a model works can be as important,
if not more important, than its ability to generate accurate and precise
predictions.
(This is in constrast to commonly used machine
learning models, whose mathematical forms cannot be specified
<em>a priori</em> and are thus less interpretable after they are learned.)</p>
<p>In simple linear regression, if we increase <span class="math inline">\(x\)</span> by one unit,
we can immediately infer how the response value changes:
<span class="math display">\[
\hat{Y}&#39; = \hat{\beta}_0 + \hat{\beta}_1 (x + 1) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_1 = \hat{Y} + \hat{\beta}_1 \,.
\]</span>
For logistic regression, the situation is not as straightforward,
because <span class="math inline">\(p\)</span> changes non-linearly as a function of <span class="math inline">\(x\)</span>. So we fall back
on the concept of <em>odds</em>:
<span class="math display">\[
O(x) = \frac{p(x)}{1-p(x)} = \exp\left(\hat{\beta}_0+\hat{\beta}_1x\right) \,.
\]</span>
If, e.g., <span class="math inline">\(O(x) = 4\)</span>, then that means that, given <span class="math inline">\(x\)</span>, we are four times
more likely to sample a success (1) than a failure (0). How
does the odds change if we add one unit to <span class="math inline">\(x\)</span>?
<span class="math display">\[
O(x+1) = e^{\hat{\beta}_0+\hat{\beta}_1(x+1)} = \exp\left(\hat{\beta}_0 + \hat{\beta}_1x\right)  \times \exp\left(\hat{\beta}_1\right) = O(x) \exp\left(\hat{\beta}_1\right) \,.
\]</span>
The odds change by a factor of <span class="math inline">\(\exp\left(\hat{\beta}_1\right)\)</span>,
which can be greater than or less than one, depending
on the sign of <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>It is important to note that when we perform
logistic regression, we are simply estimating <span class="math inline">\(p \vert x\)</span>, which
is the probability that
we would sample a datum belonging to Class 1, given <span class="math inline">\(x\)</span>.
How we choose to map <span class="math inline">\(p \vert x\)</span> to either 0 or 1,
if we choose to make that mapping, is not actually
part of the logistic regression framework. <em>Classification</em> is
a broad topic within statistical learning whose details are beyond
the scope of this book. We direct the interested reader to,
e.g., <em>Introduction to Statistical Learning</em> by James et al.</p>
<hr />
<div id="logistic-regression-in-r" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Logistic Regression in R<a href="the-binomial-and-related-distributions.html#logistic-regression-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>When observed with a telescope, a star is a point-like object, as opposed to
a galaxy, which has some angular extent. Telling stars and galaxies apart
visually is thus, in relative terms, easy. However, if the galaxy has an
inordinately bright core because the supermassive black hole at its center
is ingesting matter at a high rate, that core can outshine the rest of the
galaxy so much that the galaxy appears to be point-like. Such galaxies
with bright cores are dubbed “quasars,” or “quasi-stellar objects,” and
they are much harder to tell apart from stars.</p>
</blockquote>
<blockquote>
<p>Let’s say we are given a dataset showing the difference in
magnitude (a logarithmic measure of brightness) at two wavelengths,
for 500 stars and 500 quasars.
The response variable is a factor variable with two levels, which
we dub “Class 0” (here, <code>QSO</code>) and “Class 1” (<code>STAR</code>). (The mapping
of qualitative factors to quantitative levels is by default
alphabetical; as <code>STAR</code> comes after <code>QSO</code>, <code>STAR</code> gets mapped to Class 1.
One can always override this default behavior.)</p>
</blockquote>
<blockquote>
<p>We learn a simple logistic regression model using the <code>R</code> function
<code>glm()</code>, rather than <code>lm()</code>, and we specify
<code>family=binomial</code>, which means “do logistic regression.”
(We can add additional arguments to, e.g., change the link function.)</p>
</blockquote>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="the-binomial-and-related-distributions.html#cb216-1" tabindex="-1"></a>glm.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(class<span class="sc">~</span>r,<span class="at">data=</span>df,<span class="at">family=</span>binomial)</span>
<span id="cb216-2"><a href="the-binomial-and-related-distributions.html#cb216-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(glm.out),<span class="at">show.residuals=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = class ~ r, family = binomial, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6465  -0.8950   0.0331   0.8436   3.9163  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) 23.44801    1.65428   14.17   &lt;2e-16 ***
## r           -1.25386    0.08817  -14.22   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1386.3  on 999  degrees of freedom
## Residual deviance: 1040.9  on 998  degrees of freedom
## AIC: 1044.9
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<blockquote>
<p>The <code>summary()</code> of a learned logistic regression model is similar to
that for a linear regression model.</p>
</blockquote>
<blockquote>
<p>The “deviance residuals,” which show the
residuals for each datum, are defined as
<span class="math display">\[
d_i = \mbox{sign}(Y_i - \hat{Y}_i) \sqrt{-2[Y_i \log \hat{Y}_i + (1-Y_i)\log(1-\hat{Y}_i)]} \,,
\]</span>
where
<span class="math display">\[
\hat{Y}_i = \hat{p}_i \vert x_i = \frac{\exp(\hat{\beta}_0+\hat{\beta}_1 x_i)}{1 + \exp(\hat{\beta}_0+\hat{\beta}_1 x_i)} \,.
\]</span>
The sum of the squared deviance residuals is equal to
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>, where <span class="math inline">\(\mathcal{L}_{max}\)</span> is the maximum
value of the joint likelihood
of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, given <span class="math inline">\(\mathbf{x}\)</span>.
Here, we observe that the deviance residuals are seemingly well-balanced
around zero.</p>
</blockquote>
<blockquote>
<p>The coefficients can be translated to <span class="math inline">\(y\)</span>-coordinate values as follows:
the intercept is <span class="math inline">\(e^{23.448}/(1+e^{23.448}) \rightarrow 1\)</span>
(this is the probability
the a sampled datum with <code>r</code> equal to zero is a <code>STAR</code>), while
the odds ratio is <span class="math inline">\(O_{new}/O_{old} = e^{-1.254}\)</span> (so that every time
<code>r</code> increases by one unit, the probability that a sampled datum
is a star is 0.285 times what it had been before: the higher the value
of <code>r</code>, the less and less likely that a sampled datum is actually
a star, and the more and more likely that it is a quasar…at least,
according to the logistic regression model.</p>
</blockquote>
<blockquote>
<p>The numbers in the other three columns of the coefficients section
are estimated numerically using the behavior of the likelihood
function (specifically, the rates at which it curves downward
away from the maximum). Some
details about how the numbers are calculated are given in an example
in the “Covariance and Correlation” section of Chapter 6.
It suffices to say here that if we assume that <span class="math inline">\(\hat{\beta}_0\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span> are both normally distributed random variables, we
reject the null hypothesis that the intercept is 0
(or equivalently <span class="math inline">\(y = 1/2\)</span>), and that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
</blockquote>
<blockquote>
<p>The null deviance is <span class="math inline">\(-2 \log \mathcal{L}_{max,o}\)</span>, where
<span class="math inline">\(\mathcal{L}_{max,o}\)</span> is the maximum likelihood
when <span class="math inline">\(\beta_1\)</span> is set to zero. The residual deviance is
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>. The difference between these values
(here, 1386.3-1040.9 = 345.4), under the null hypothesis that
<span class="math inline">\(\beta_1 = 0\)</span>, is assumed to be sampled from a chi-square distribution for
999-998 = 1 degree of freedom. The <span class="math inline">\(p\)</span>-value is thus
<code>1 - pchisq(345.4,1)</code> or effectively zero: we emphatically reject the
null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.
While this hypothesis test clearly indicates that <span class="math inline">\(\beta_1\)</span> is
not equal to zero, how can we know whether the model itself fits to
the data well in an absolute sense?
This is a model’s “goodness of fit,” a
concept we introduce at the end of this chapter. There is no unique
answer to this question in the context of logistic regression; the
interested reader should look up, e.g., the Hosmer-Lemeshow statistic.
(Note that the test carried out here is
analogous to the <span class="math inline">\(F\)</span>-test in linear regression, and is testing
<span class="math inline">\(H_o: \beta_1 = \beta_2 = \cdots = \beta_p = 0\)</span> versus <span class="math inline">\(H_a:\)</span>
at least one of the <span class="math inline">\(\beta_i\)</span>’s is non-zero. Because the
sampling distribution here is the chi-square distribution as
opposed to the normal distribution, the <span class="math inline">\(p\)</span>-value here will not
match the <span class="math inline">\(p\)</span>-value seen in the coefficients section in general.)</p>
</blockquote>
<blockquote>
<p>Last, the AIC, or Akaike Information Criterion, is <span class="math inline">\(-2\)</span> times the
model likelihood (or here, the deviance)
plus two times the number of variables (here, 2,
as the intercept is counted as a variable).
Adding the number of variables acts to penalize those models with
more variables: the improvement in the maximum likelihood has to be
sufficient to justify added model complexity.
Discussion of the mathematical details of
AIC is beyond the scope of this book; it suffices to say that
if we compute it when learning a suite of different models, we would
select the model with the smallest value.</p>
</blockquote>
<blockquote>
<p>We wrap up this example by showing how one would start moving from estimating
the Class 1 probabilities for each datum towards predicting
classes for each datum. Logistic regression is not in and of itself a
“classifier”: it simply outputs probabilities. Naively, we would classify
an object with an estimated probability below 0.5 as being an object of
Class 0 and one with an estimated probability above 0.5 as being an object
of Class 1, but that only works in general if the classes are balanced,
i.e., if there are the same number of data of Class 1 as of Class 0.
(Here, “works” means “acts to minimize misclassification rates across both
classes at the same time.”)
If there is class imbalance, then we will almost certainly have to change
0.5 to some other value for optimal classification.
In the current example, the classes <em>are</em> balanced, and so putting the
“dividing line” at 0.5, as we do in Figure <a href="the-binomial-and-related-distributions.html#fig:glmpred">3.12</a>, is
an optimal choice.</p>
</blockquote>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="the-binomial-and-related-distributions.html#cb218-1" tabindex="-1"></a>glm.predictions <span class="ot">&lt;-</span> <span class="fu">predict.glm</span>(glm.out,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:glmpred"></span>
<img src="_main_files/figure-html/glmpred-1.png" alt="\label{fig:glmpred}Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one would &quot;draw&quot; a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications." width="50%" />
<p class="caption">
Figure 3.12: Boxplots showing the estimated probability that a datum is a star, as a function of the object type (quasar or star). To convert estimated probabilities to predicted classes, one would “draw” a dividing line between the two boxes: all objects with probabilities below the line would be predicted to be quasars and all others would be predicted to be stars. The line would be placed to, e.g., minimize the number of misclassifications.
</p>
</div>
</div>
</div>
<div id="naive-bayes-regression" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Naive Bayes Regression<a href="the-binomial-and-related-distributions.html#naive-bayes-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>Naive Bayes</em> model is the basis for perhaps the simplest probabilitic
classifier, one that is used to, e.g., detect spam emails. The meanings
of the words “Naive” and “Bayes” will become more clear below. Note that one
would often see this model referred to as the “Naive Bayes classifier.” However,
as noted in the last section, there are actually two steps in classification,
the first being the estimation of probabilities that a given datum belongs to
each class, and the second being the mapping of those probabilities to class
predictions. In the last section and here, we are only focusing on the
generation of predicted probabilities…hence the title of this section.</p>
<p>(Many would argue that the Naive Bayes model is a machine-learning model.
We would argue that it actually is not: the
final form of a machine-learning model is unknown before we
start the modeling process [e.g., the number of branches that will appear
in a classification tree model is unknown and is learned by the machine],
whereas with Naive Bayes the
mathematical form of the model is fully specified
and all we have to do is estimate unknown
probabilities. This is akin to the situation with linear regression, where the
mathematical form is set and all we have to do is estimate the
coefficients…and no one would argue that linear regression is a
machine-learning model!)</p>
<p>Let’s assume that we are in a similar setting as the one for logistic regression,
but instead of having a response that is a two-level factor variable (i.e., one
the represents two classes), the number
of levels is <span class="math inline">\(K \geq 2\)</span>. (So instead of just, e.g., “chocolate” and “vanilla” as
our response variable values, we can add “strawberry” and other ice cream flavors
too!)
The ultimate goal of the model is to assign conditional probabilities for
each response class, given a datum <span class="math inline">\(\mathbf{x}\)</span>:
<span class="math inline">\(p(C_k \vert \mathbf{x})\)</span>, where <span class="math inline">\(C_k\)</span> denotes
“class <span class="math inline">\(k\)</span>.” How do we derive this quantity?</p>
<p>The first step is to apply Bayes’ rule (hence the “Bayes” in “Naive Bayes”):
<span class="math display">\[
p(C_k \vert \mathbf{x}) = \frac{p(C_k) p(\mathbf{x} \vert C_k)}{p(\mathbf{x})} \,.
\]</span>
The next step is to expand <span class="math inline">\(p(\mathbf{x} \vert C_k)\)</span>:
<span class="math display">\[
p(\mathbf{x} \vert C_k) = p(x_1,\ldots,x_p \vert C_k) = p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) \,.
\]</span>
(This is the multiplicative law of probability in action, as applied to a
conditional probability. See section 1.4.) The right-most expression above is one that
is difficult to evaluate in practice, given all the conditions that must be jointly
applied…so this is where the “Naive” aspect of the model comes in. We simplify the
expression by assuming (most often incorrectly!) that the predictor variables are
all mutually independent, so that
<span class="math display">\[
p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) ~~ \rightarrow ~~ p(x_1 \vert C_k) \cdots p(x_p \vert C_k)
\]</span>
and thus
<span class="math display">\[
p(C_k \vert \mathbf{x}) = \frac{p(C_k) \prod_{i=1}^p p(x_i \vert C_k)}{p(\mathbf{x})} \,.
\]</span>
OK…where do we go from here?</p>
<p>We need to make further assumptions!</p>
<ul>
<li>We need to assign “prior probabilities” <span class="math inline">\(p(C_k)\)</span> to each class. Common choices
are <span class="math inline">\(1/K\)</span> (we view each class as equally probable before we gather data) and
<span class="math inline">\(n_k/n\)</span> (the number of observed data of class <span class="math inline">\(k\)</span> divided by the overall sample size).</li>
<li>We also need to assume probability mass and density functions <span class="math inline">\(p(x_i \vert C_k)\)</span> for
each predictor variable (a pmf if <span class="math inline">\(x_i\)</span> is discrete and a pdf if <span class="math inline">\(x_i\)</span> is continuous).
It is convention to use binomial or multinomial pmfs, depending
on the number of levels, with the observed proportions in each level informing the
category probability estimate, and
normal pdfs, with <span class="math inline">\(\hat{\mu} = \bar{x_i}\)</span> and <span class="math inline">\(\hat{\sigma^2} = s_i^2\)</span> (the
sample variance).</li>
</ul>
<p>Ultimately, this model depends on a number of (perhaps unjustified) assumptions. Why
would we ever use it? Because the assumption of mutual independence makes model
evaluation <em>fast</em>. Naive Bayes is rarely the model underlying the best classifier for
any given problem, but when speed is needed (such as in the identification of spam email),
one’s choices are limited. (As as general rule: if a model is simple to implement, one
should implement it, even if the <em>a priori</em> expectation is that another model will
ultimately generate better results. One never knows…)</p>
<hr />
<div id="naive-bayes-regression-with-categorical-predictors" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Naive Bayes Regression With Categorical Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume that we have collected the following data:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_1\)</span></th>
<th align="center"><span class="math inline">\(x_2\)</span></th>
<th align="center"><span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Yes</td>
<td align="center">Chocolate</td>
<td align="center">True</td>
</tr>
<tr class="even">
<td align="center">Yes</td>
<td align="center">Chocolate</td>
<td align="center">False</td>
</tr>
<tr class="odd">
<td align="center">No</td>
<td align="center">Vanilla</td>
<td align="center">True</td>
</tr>
<tr class="even">
<td align="center">No</td>
<td align="center">Chocolate</td>
<td align="center">True</td>
</tr>
<tr class="odd">
<td align="center">Yes</td>
<td align="center">Vanilla</td>
<td align="center">False</td>
</tr>
<tr class="even">
<td align="center">No</td>
<td align="center">Chocolate</td>
<td align="center">False</td>
</tr>
<tr class="odd">
<td align="center">No</td>
<td align="center">Vanilla</td>
<td align="center">False</td>
</tr>
</tbody>
</table>
<blockquote>
<p>We learn a Naive Bayes model given these data, in which we
assume “False” is Class 0 and “True” is Class 1. If we have a
new datum <span class="math inline">\(\mathbf{x}\)</span> = (“Yes”,“Chocolate”), what is the
probability that the response is “True”?</p>
</blockquote>
<blockquote>
<p>We seek the quantity
<span class="math display">\[
p(C_1 \vert \mathbf{x}) = \frac{p(C_1) p(\mathbf{x} \vert C_1)}{p(\mathbf{x})} = \frac{p(C_1) p(x_1 \vert C_1) p(x_2 \vert C_1)}{p(x_1,x_2)} \,.
\]</span>
When we examine the data, we see that <span class="math inline">\(p(C_1) = 3/7\)</span>, as there are
three data out of seven with the value <span class="math inline">\(Y\)</span> = “True.” Now, given <span class="math inline">\(C_1\)</span>,
at what rate do we observe “Yes”? (One time out of three…so
<span class="math inline">\(p(x_1 \vert C_1) = 1/3\)</span>.) What about “Chocolate”? (Two times out of
three…so <span class="math inline">\(p(x_2 \vert C_1) = 2/3\)</span>.) The numerator is thus
<span class="math inline">\(3/7 \times 1/3 \times 2/3 = 2/21\)</span>. The value of the
denominator, <span class="math inline">\(p(x_1,x_2)\)</span>, is
determined utilizing the Law of Total Probability:
<span class="math display">\[\begin{align*}
p(x_1,x_2) &amp;= p(x_1 \vert C_1) p(x_2 \vert C_1) p(C_1) + p(x_1 \vert C_2) p(x_2 \vert C_2) p(C_2) \\
&amp;= 2/21 + 2/4 \times 2/4 \times 4/7 = 2/21 + 4/28 = 2/21 + 3/21 = 5/21 \,.
\end{align*}\]</span>
And so now we know that
<span class="math display">\[
p(\mbox{True} \vert \mbox{Yes,Chocolate}) = \frac{2/21}{5/21} = \frac{2}{5} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-regression-with-continuous-predictors" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Naive Bayes Regression With Continuous Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume we have the following data regarding credit-card defaults,
where <span class="math inline">\(x\)</span> represents the credit-card balance and <span class="math inline">\(Y\)</span> is a categorical
variable indicating whether a default has occurred:</p>
</blockquote>
<table style="width:100%;">
<colgroup>
<col width="6%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>1487.00</th>
<th>324.74</th>
<th>988.21</th>
<th>836.30</th>
<th>2205.80</th>
<th>927.89</th>
<th>712.28</th>
<th>706.16</th>
<th>1774.69</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Let’s now suppose someone came along with a credit balance of $1,200.
According to the Naive Bayes model, given this
balance, what is the probability of a credit default?</p>
</blockquote>
<blockquote>
<p>The Naive Bayes model in this particular case is
<span class="math display">\[
p(Y \vert x) = \frac{p(x \vert Y) p(Y)}{p(x \vert Y) p(Y) + p(x \vert N) p(N)} \,,
\]</span>
where we can observe immediately that <span class="math inline">\(p(Y) = 3/9 = 1/3\)</span> and <span class="math inline">\(p(N) = 6/9 = 2/3\)</span>.
To compute, e.g., <span class="math inline">\(p(x \vert Y)\)</span>, we first compute the sample mean
and sample standard deviation for the observed data for which <span class="math inline">\(Y\)</span> is “Yes”:</p>
</blockquote>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="the-binomial-and-related-distributions.html#cb219-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1487.00</span>,<span class="fl">2205.80</span>,<span class="fl">1774.69</span>)</span>
<span id="cb219-2"><a href="the-binomial-and-related-distributions.html#cb219-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1822.5</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="the-binomial-and-related-distributions.html#cb221-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 361.78</code></pre>
<blockquote>
<p>The mean is $1,822.50 and the standard deviation is $361.78,
so the probability density associated with observing $1,200 is</p>
</blockquote>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="the-binomial-and-related-distributions.html#cb223-1" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002509367</code></pre>
<blockquote>
<p>The density is 2.51 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. As far as for those data for
which <span class="math inline">\(Y\)</span> is “No”:</p>
</blockquote>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="the-binomial-and-related-distributions.html#cb225-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">324.74</span>,<span class="fl">988.21</span>,<span class="fl">836.30</span>,<span class="fl">927.89</span>,<span class="fl">712.28</span>,<span class="fl">706.16</span>)</span>
<span id="cb225-2"><a href="the-binomial-and-related-distributions.html#cb225-2" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002748351</code></pre>
<blockquote>
<p>The density is 2.75 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. Thus
<span class="math display">\[
p(Y \vert x) = \frac{2.51 \cdot 0.333}{2.51 \cdot 0.333 + 2.75 \cdot 0.667} = \frac{0.834}{0.834 + 1.834} = 0.313 \,.
\]</span>
We would predict that there
is roughly a 1 in 3 chance that a person with a credit balance of
$1,200 will default on their debt.</p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-applied-to-star-quasar-data" class="section level3 hasAnchor" number="3.10.3">
<h3><span class="header-section-number">3.10.3</span> Naive Bayes Applied to Star-Quasar Data<a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Here, we learn a Naive Bayes model that we can use to differentiate between
stars and quasars, utilizing functions from <code>R</code>’s <code>e1071</code> package.</p>
</blockquote>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="the-binomial-and-related-distributions.html#cb227-1" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb227-2"><a href="the-binomial-and-related-distributions.html#cb227-2" tabindex="-1"></a><span class="fu">naiveBayes</span>(class<span class="sc">~</span>r,<span class="at">data=</span>df)</span></code></pre></div>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##  QSO STAR 
##  0.5  0.5 
## 
## Conditional probabilities:
##       r
## Y          [,1]      [,2]
##   QSO  19.35770 0.8282726
##   STAR 17.96389 1.3651572</code></pre>
<blockquote>
<p>The summary of the model shows that the classes are balanced
(as the <code>A-priori probabilities</code> are 0.5 for each class).
It then shows the estimated distributions of <code>r</code> values
for quasars (a normal with estimated mean 19.358 and standard
deviation 0.828) and stars (mean 17.964 and standard deviation 1.365).
See Figure <a href="the-binomial-and-related-distributions.html#fig:nbplot">3.13</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbplot"></span>
<img src="_main_files/figure-html/nbplot-1.png" alt="\label{fig:nbplot}An illustration of the Naive Bayes regression model, as applied to the star-quasar dataset. The red curve is the estimated normal probability density function for stars, while the green curve is the estimated pdf for quasars. Because the two classes in this example are balanced, we can state that where the amplitude of the red curve is higher, the predicted class would be STAR; otherwise, it would be QUASAR." width="50%" />
<p class="caption">
Figure 3.13: An illustration of the Naive Bayes regression model, as applied to the star-quasar dataset. The red curve is the estimated normal probability density function for stars, while the green curve is the estimated pdf for quasars. Because the two classes in this example are balanced, we can state that where the amplitude of the red curve is higher, the predicted class would be STAR; otherwise, it would be QUASAR.
</p>
</div>
</div>
</div>
<div id="the-beta-distribution" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> The Beta Distribution<a href="the-binomial-and-related-distributions.html#the-beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>beta distribution</em> is a continuous distribution that is commonly used to
model random variables with finite, bounded domains.
Its probability density function is given by
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\,,
\]</span>
where <span class="math inline">\(x \in [0,1]\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are both <span class="math inline">\(&gt; 0\)</span>, and the normalization constant <span class="math inline">\(B(\alpha,\beta)\)</span> is
<span class="math display">\[
B(\alpha,\beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} \,.
\]</span>
(See Figure <a href="the-binomial-and-related-distributions.html#fig:betapdf">3.14</a>.)
We have seen the gamma function, <span class="math inline">\(\Gamma(\alpha)\)</span>, before; it is defined as
<span class="math display">\[
\Gamma(\alpha) = \int_0^\infty x^{\alpha-1} e^{-x} dx \,.
\]</span>
There are two things to note about the gamma function. The first is its
<em>recursive property</em>: <span class="math inline">\(\Gamma(\alpha+1) = \alpha \Gamma(\alpha)\)</span>.
(This can be shown by applying integration by parts.) The second is that
when <span class="math inline">\(\alpha\)</span> is a positive integer, the gamma function takes on the value
<span class="math inline">\((\alpha-1)! = (\alpha-1)(\alpha-2) \cdots 1\)</span>. (Note that <span class="math inline">\(\Gamma(1) = 0! = 1\)</span>.)</p>
<p>Regarding the statement above about “model[ing] random variables with finite,
bounded domains”: if a set of <span class="math inline">\(n\)</span> iid random variables
<span class="math inline">\(\mathbf{X}\)</span> has domain <span class="math inline">\([a,b]\)</span>, we can define a new set of random variables
<span class="math inline">\(\mathbf{Y}\)</span> via the transformation
<span class="math display">\[
\mathbf{Y} = \frac{\mathbf{X}-a}{b-a}
\]</span>
such that the domain becomes <span class="math inline">\([0,1]\)</span>. We can model these newly defined data
with the beta distribution. (Note the word <em>can</em>: we can model these data with
the beta distribution, but we don’t have to, and it may be the case that
there is another
distribution bounded on the interval <span class="math inline">\([0,1]\)</span> that ultimately better
describes the data-generating process. The beta distribution just happens
to be commonly used.)</p>
<p>But…in the end, what does this all have to do with the binomial
distribution, the subject of this chapter? We know the binomial has a
parameter <span class="math inline">\(p \in [0,1]\)</span> and the domain of the beta distribution is <span class="math inline">\([0,1]\)</span>,
but is there more? Let’s write down the binomial pmf:
<span class="math display">\[
\binom{k}{x} p^x (1-p)^{k-x} = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
This pmf dictates the probability of observing a particular value of <span class="math inline">\(x\)</span>
given <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span>. But what if we turn this around a bit…and examine
this function if we fix <span class="math inline">\(k\)</span> and <span class="math inline">\(x\)</span> and vary <span class="math inline">\(p\)</span> instead? In other words,
let’s examine the likelihood function
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
We can see immediately that the likelihood has the form of a beta
distribution if we set <span class="math inline">\(\alpha = x+1\)</span> and <span class="math inline">\(\beta = k-x+1\)</span>:
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{\Gamma(\alpha+\beta-1)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1} (1-p)^{\beta-1} \,,
\]</span>
<em>except</em> that the normalization term is not quite right: here we have
<span class="math inline">\(\Gamma(\alpha+\beta-1)\)</span> instead of <span class="math inline">\(\Gamma(\alpha+\beta)\)</span>. But that’s
fine: there is no requirement that a likelihood function integrate to
one over its domain. (Here, as the interested reader can verify,
the likelihood function integrates to <span class="math inline">\(1/(k+1)\)</span>.)
So, in the end, if we observe a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k,p\)</span>),
then the likelihood function <span class="math inline">\(\mathcal{L}(p \vert k,x)\)</span> has the shape
(if not the normalization) of a Beta(<span class="math inline">\(x+1,k-x+1\)</span>) distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betapdf"></span>
<img src="_main_files/figure-html/betapdf-1.png" alt="\label{fig:betapdf}Three examples of beta probability density functions: Beta(2,2) (solid red line), Beta(4,2) (dashed green line), and Beta(2,3) (dotted blue line)." width="50%" />
<p class="caption">
Figure 3.14: Three examples of beta probability density functions: Beta(2,2) (solid red line), Beta(4,2) (dashed green line), and Beta(2,3) (dotted blue line).
</p>
</div>
<hr />
<div id="the-expected-value-of-a-beta-random-variable" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> The Expected Value of a Beta Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a random variable sampled from a Beta(<span class="math inline">\(\alpha,\beta\)</span>)
distribution is
<span class="math display">\[\begin{align*}
E[X] = \int_0^1 x f_X(x) dx &amp;= \int_0^1 x \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \,.
\end{align*}\]</span>
The last result follows from the fact that the integrand is the pdf for
a Beta(<span class="math inline">\(\alpha+1,\beta\)</span>) distribution, and the integral is over the entire domain,
hence the integral evaluates to 1.</p>
</blockquote>
<blockquote>
<p>Continuing,
<span class="math display">\[\begin{align*}
E[X] &amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \\
&amp;= \frac{\Gamma(\alpha+1) \Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
&amp;= \frac{\alpha \Gamma(\alpha)}{(\alpha+\beta)\Gamma(\alpha+\beta)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \\
&amp;= \frac{\alpha}{\alpha+\beta} \,.
\end{align*}\]</span>
Here, we take advantage of the recursive property of the gamma function.</p>
</blockquote>
<blockquote>
<p>We can utilize a similar strategy to determine the variance of a beta random variable,
starting by computing <span class="math inline">\(E[X^2]\)</span> and utilizing the shortcut formula
<span class="math inline">\(V[X] = E[X]^2 - (E[X])^2\)</span>. The final result is
<span class="math display">\[
V[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-sample-median-of-a-uniform01-distribution" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> The Sample Median of a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1 ~~~ x \in [0,1] \,.
\]</span>
Let’s assume that we draw <span class="math inline">\(n\)</span> iid data from this distribution, with <span class="math inline">\(n\)</span> odd.
Then we can utilize a result from order statistics to write down the pdf of
the sample median, which is the <span class="math inline">\(j^{\rm th}\)</span> order statstic (where <span class="math inline">\(j = (n+1)/2\)</span>):
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} f_X(x) \left[ F_X(x) \right]^{(n-1)/2} \left[ 1 - F_X(x) \right]^{(n-1)/2} \,.
\]</span>
Given that
<span class="math display">\[
F_X(x) = \int_0^x dy = x ~~~ x \in [0,1] \,,
\]</span>
we can write
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} x^{(n-1)/2} (1-x)^{(n-1)/2} \,,
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. This function has both the form of a beta distribution
(with <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>) and the domain of a beta distribution,
so <span class="math inline">\(\tilde{X} \sim\)</span> Beta<span class="math inline">\(\left(\frac{n+1}{2},\frac{n+1}{2}\right)\)</span>.</p>
</blockquote>
<blockquote>
<p>(In fact, we can go further and state a more general result:
<span class="math inline">\(X_{(j)} \sim\)</span> Beta(<span class="math inline">\(j,n-j+1\)</span>): <em>all</em> the order statistics for data
drawn from a Uniform(0,1) distribution are beta-distributed random variables!
See Figure <a href="the-binomial-and-related-distributions.html#fig:betaunif">3.15</a>.)</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betaunif"></span>
<img src="_main_files/figure-html/betaunif-1.png" alt="\label{fig:betaunif}The order statistic probability density functions $f_{(j)}(x)$ for, from left to right, $j = 1$ through $j = 5$, for the situation in which $n = 5$ iid data are drawn from a Uniform(0,1) distribution (overlaid in red). Each pdf is itself a beta distribution, with parameter values $j$ and $n-j+1$." width="50%" />
<p class="caption">
Figure 3.15: The order statistic probability density functions <span class="math inline">\(f_{(j)}(x)\)</span> for, from left to right, <span class="math inline">\(j = 1\)</span> through <span class="math inline">\(j = 5\)</span>, for the situation in which <span class="math inline">\(n = 5\)</span> iid data are drawn from a Uniform(0,1) distribution (overlaid in red). Each pdf is itself a beta distribution, with parameter values <span class="math inline">\(j\)</span> and <span class="math inline">\(n-j+1\)</span>.
</p>
</div>
<!--
---

### Testing Hypotheses Using the Sample Median

> Let's assume that we draw $n$ iid data, where $n$ is an odd number,
> from a Uniform($0,\theta$) 
> distribution, and that we wish to test the hypothesis $H_o: \mu = \mu_o
> = 1/2$ versus $H_a: \mu = \mu_a > \mu_o$ at level $\alpha$.
> (Let's also assume that all
> the data we observe lie in the range $[0,1]$...otherwise the null cannot
> be correct. We will return to this point in Chapter 5.) It sounds like
> we might use the Neymann-Pearson lemma here, but as we will see in Chapter
> 5, that would dictate a different test statistic than what we want to
> use here, which is the sample median (which is not a sufficient statistic).
> So here we will simply fall back on the methodology shown in Chapters 1 
> and 2, i.e., we will solve
$$
F_Y(y_{RR} \vert \mu_o) - q = 0 \,,
$$
> where $Y = X_{((n+1)/2)}$ is the sample median. We start by deriving
> the cdf $F_{((n+1)/2)}$:
\begin{align*}
F_{((n+1)/2)}(x) &= \int_0^x f_{((n+1)/2)}(v) dv = \ldots \,.
\end{align*}
> But, there's an issue. While the expressions for the cdfs for the
> minimum and maximum values are comprised of single terms, the expression
> for the cdf of the median is comprised of a summation of terms. (Go back
> to the section on order statistics earlier in this chapter to verify this.)
> Hence we cannot work with this cdf by hand. So...what can we do?

> Utilize numerical integration, that's what.

> Here is the code we need:

``` r
# This computes the cdf of the sample median at coordinate x
f <- function(x,n)
{
  j <- (n-1)/2
  # lfactorial(a) == log(a!)
  # exp(lfactorial(a)) == a!
  exp(lfactorial(n)-2*lfactorial(j))*x^j*(1-x)^j
}

# Find root of F_Y(y) - (1-alpha)
g <- function(y,n,alpha)
{
  F.med <- integrate(f,0,y,n=n)
  F.med$value - (1-alpha)
}

uniroot(g,interval=c(0,1),n=5,alpha=0.05)$root
```

```
## [1] 0.8107542
```
> In words, the function `f` above computes $F_{((n+1)/2)}(x)$, while
> the function `g` evaluates the function that we are trying to
> solve, specifically $F_{((n+1)/2)}(x) - (1-\alpha) = 0$. The call
> to `uniroot()` specifies that the root is between 0 and 1 (since
> $\theta_o = 2\mu_o = 1$). We see that if $n = 5$, we would reject the null
> hypothesis that $\mu_o = 1/2$ if and only if $X_{((n+1)/2)} \geq 0.811$,
> i.e., if three of the five values are greater than 0.811.

> Something to take away from this example is to realize that just because
> we cannot write down an analytical expression (in this case, for the 
> cdf of the median of $n$ Uniform(0,1) random variables), we should not
> just give up, as it may be easy to implement numerical methods! (As it 
> was in this case.)
-->
</div>
</div>
<div id="the-multinomial-distribution" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> The Multinomial Distribution<a href="the-binomial-and-related-distributions.html#the-multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s suppose we are in a situation in which we are gathering categorical data. For instance, we might be</p>
<ul>
<li>throwing a ball and recording which of bins numbered 1 through <span class="math inline">\(m\)</span> it lands in;</li>
<li>categorizing the condition of old coins as “mint,” “very good,” “fine,” etc.; or</li>
<li>classifying a galaxy as being a spiral galaxy, an elliptical galaxy, or an irregular galaxy.</li>
</ul>
<p>These are examples of <em>multinomial trials</em>, which is a generalization of the concept of binomial trials to
situations where the number of possible outcomes is <span class="math inline">\(m &gt; 2\)</span> rather than <span class="math inline">\(m = 2\)</span>. Earlier in this chapter,
we wrote down the five properties of binomial trials. The analogous properties of a multinomial experiment
are the following.</p>
<ol style="list-style-type: decimal">
<li>It consists of <span class="math inline">\(k\)</span> trials, with <span class="math inline">\(k\)</span> chosen in advance.</li>
<li>There are <span class="math inline">\(m\)</span> possible outcomes for each trial.</li>
<li>A trial may have no more than one realized outcome.</li>
<li>The outcomes of each trial are independent.</li>
<li>The probability of achieving the <span class="math inline">\(i^{th}\)</span> outcome is <span class="math inline">\(p_i\)</span>, a constant quantity.</li>
<li>The probabilities of each outcome sum to one: <span class="math inline">\(\sum_{i=1}^m p_i = 1\)</span>.</li>
<li>The number of trials that achieve a particular outcome is <span class="math inline">\(X_i\)</span>, with <span class="math inline">\(\sum_{i=1}^m X_i = k\)</span>.</li>
</ol>
<p>The probability of any given outcome <span class="math inline">\(\{X_1,\ldots,X_m\}\)</span> is given by the multinomial probability mass
function:
<span class="math display">\[
p(x_1,\ldots,x_m \vert p_1,\ldots,p_m) = \frac{k!}{x_1! \cdots x_m!}p_1^{x_1}\cdots p_m^{x_m} \,.
\]</span>
The distribution for any given <span class="math inline">\(X_i\)</span> itself is binomial, with
<span class="math inline">\(E[X_i] = kp_i\)</span> and <span class="math inline">\(V[X_i] = kp_i(1-p_i)\)</span>. This makes intuitive sense,
as one either observes <span class="math inline">\(i\)</span> as a trial outcome (success),
or something else (failure). However, the <span class="math inline">\(X_i\)</span>’s are <em>not</em> independent random
variables; the covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>, a metric of linear
dependence, is Cov(<span class="math inline">\(X_i\)</span>,<span class="math inline">\(X_j\)</span>) = <span class="math inline">\(-kp_ip_j\)</span> if <span class="math inline">\(i \neq j\)</span>. (We will discuss
the concept of covariance in Chapter 6.)
This also makes intuitive sense: given that the number of trials <span class="math inline">\(k\)</span> is
fixed, observing more data that achieve one outcome will usually mean
we will observe fewer data achieving any other given outcome.</p>
</div>
<div id="chi-square-based-hypothesis-testing" class="section level2 hasAnchor" number="3.13">
<h2><span class="header-section-number">3.13</span> Chi-Square-Based Hypothesis Testing<a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine that we are scientists, sitting on a platform in the middle of a plain.
We are recording the number of animals of a particular species that we see, and
the azimuthal angle for each one. (An azimuthal angle is, e.g.,
<span class="math inline">\(0^\circ\)</span> when looking directly north, and 90<span class="math inline">\(^\circ\)</span>, 180<span class="math inline">\(^\circ\)</span>, and
270<span class="math inline">\(^\circ\)</span> as we look east, south, and west, etc.)
The question we want to answer is, are the animals uniformly distributed as a function
of angle?</p>
<p>There is no unique way to answer this question. However, a very common approach is
to bin the data and use a <em>chi-square goodness-of-fit test</em>. Let’s
assume we’ve observed 100 animals and that we record the numbers seen in each of
four quadrants:</p>
<table>
<colgroup>
<col width="35%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">angle range</th>
<th align="center">0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th align="center">90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th align="center">180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th align="center">270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">number of animals</td>
<td align="center">28</td>
<td align="center">32</td>
<td align="center">17</td>
<td align="center">23</td>
</tr>
</tbody>
</table>
<p>There is nothing “special” about the choice of four quadrants<span class="math inline">\(-\)</span>we could have
chosen eight, etc.<span class="math inline">\(-\)</span>but as we’ll see below, the chi-square GoF test is an
“approximate” test and its results become more precise as the numbers of data in each
bin get larger. So there is a tradeoff if we increase the number of quadrants: we might
be able to detect smaller-scale non-uniformities, but are test results will become less
precise.</p>
<p>To perform a chi-square GoF test with these data, we first specify are null and
alternative hypotheses:
<span class="math display">\[
H_o : p_1 = p_{1,o},\cdots,p_m = p_{m,o} ~~ vs. ~~ H_a : \mbox{at least two of the probabilities differ} \,,
\]</span>
where <span class="math inline">\(p_{i,o}\)</span> is the null hypothesis proportion in bin <span class="math inline">\(i\)</span>.
Here, <span class="math inline">\(k = 100\)</span>, <span class="math inline">\(m = 4\)</span>, and
<span class="math inline">\(p_{1,o} = p_{2,o} = p_{3,o} = p_{4,o} = 0.25\)</span>…we expect, under
the null, to see 25 animals in each quadrant.</p>
<p>The null hypothesis is that the data are
<em>multinomially distributed</em> with specified proportions being expected in each of
the <span class="math inline">\(m\)</span> defined bins.
As we might imagine, performing a hypothesis test of the form given above
that utilizes the multinomial distribution
would be difficult to do by hand; multinomial distributions are intrinsically
high-dimensional and the data (the numbers of counts in each bin) are not iid.
In 1900, the statistician
Karl Pearson proposed a “workaround” for testing
multinomial hypotheses.
He noted that as <span class="math inline">\(k \rightarrow \infty\)</span>, multinomial random variables
converge in distribution to multivariate normal random variables (with the
latter being something we will discuss in Chapter 6), and that the
statistic
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i-E[X_i])^2}{E[X_i]} = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i}
\]</span>
converges in distribution to the chi-square random variable for
<span class="math inline">\(m-1\)</span> degrees of freedom.
(We subtract 1 because only <span class="math inline">\(m-1\)</span> of the multinomial
probabilities can freely vary; the <span class="math inline">\(m^{th}\)</span> one is constrained by
the fact that the probabilities must sum to 1. This constraint is what
makes multinomial data <em>not</em> iid.) The computation of <span class="math inline">\(W\)</span> is thus the
basis of the <em>chi-square goodness-of-fit test</em>, or chi-square GoF test.
For this test,</p>
<ul>
<li>we reject the null hypothesis if <span class="math inline">\(W &gt; w_{RR}\)</span>;</li>
<li>the rejection region boundary is <span class="math inline">\(w_{RR} = F_{W(m-1)}^{-1}(1-\alpha)\)</span> (in <code>R</code>, <code>qchisq(1-alpha,m-1)</code>);</li>
<li>the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(1 - F_{W(m-1)}(w_{\rm obs})\)</span> (e.g., <code>1-pchisq(w.obs,m-1)</code>); and</li>
<li>by convention, <span class="math inline">\(kp_i\)</span> must be <span class="math inline">\(\geq\)</span> 5 in each bin for the test to yield a valid result.</li>
</ul>
<p>This last point underscores the tradeoff between splitting the data over more bins
and test precision!</p>
<p>In a chi-square GoF test, the inputs are the observed data and the hypothesized
proportions. There are variations on this test in which the inputs are tables of
observed data, ones that differ depending upon how the data are collected:</p>
<ul>
<li><em>chi-square test of independence</em>: the question is whether two variables are
associated with each other in a population; subjects are selected and the
values of two variables are recorded for each. For instance, we might select
<span class="math inline">\(k\)</span> people at random and record whether or not they have had Covid-19, and
also record whether or not they initially had zero, one, or two vaccine shots,
and put these data into a table with, e.g., “yes” and “no” defining the rows
and 0, 1, and 2 defining the columns. If we reject the null, we are stating
that the distributions of data along, e.g., each row are statistically
significantly different from each other.</li>
<li><em>chi-square test of homogeneity</em>: the question is whether the distribution
of a single variable is the same for two subgroups of a population, with subjects
being selected randomly from each subgroup separately. For instance, we might
select <span class="math inline">\(k\)</span> people under 20 years of age and ask if they prefer vanilla, chocolate,
or strawberry ice cream, and then repeat the process for people of age 20 or over, and put the data into a table similar to that described for the test
of independence above.</li>
</ul>
<p>Whether we perform a test of independence versus one of homogeneity affects the
interpretation of results, but algorithmically the tests are identical.
Under the null hypothesis,
<span class="math display">\[
\widehat{E[X_{ij}]} = \frac{r_i c_j}{k} \,,
\]</span>
i.e., the expected value in the cell in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> is the product of the total number
of data in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span>, divided by the total number of data overall. Then,
<span class="math display">\[
W = \sum_{i=1}^r \sum_{j=1}^c \frac{(X_{ij} - \widehat{E[X_{ij}]})^2}{\widehat{E[X_{ij}]}} \stackrel{\dot}{\sim} \chi_{(r-1)(c-1)}^2 \,,
\]</span>
i.e., the test statistic <span class="math inline">\(W\)</span> is, under the null, assumed to be
chi-square distributed for <span class="math inline">\((r-1) \times (c-1)\)</span> degrees of freedom.</p>
<hr />
<div id="chi-square-goodness-of-fit-test" class="section level3 hasAnchor" number="3.13.1">
<h3><span class="header-section-number">3.13.1</span> Chi-Square Goodness of Fit Test<a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s work with the data presented above at the beginning of this section:</p>
</blockquote>
<table>
<colgroup>
<col width="35%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">angle range</th>
<th align="center">0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th align="center">90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th align="center">180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th align="center">270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">number of animals</td>
<td align="center">28</td>
<td align="center">32</td>
<td align="center">17</td>
<td align="center">23</td>
</tr>
</tbody>
</table>
<blockquote>
<p>As stated above, we expect 25 counts in each bin. Given this expectation, are these
data plausible, at level <span class="math inline">\(\alpha = 0.05\)</span>?</p>
</blockquote>
<blockquote>
<p>We first compute the test statistic:
<span class="math display">\[\begin{align*}
W &amp;= \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} = \frac{(28-25)^2}{25} + \frac{(32-25)^2}{25} + \frac{(17-25)^2}{25} + \frac{(23-25)^2}{25} \\
&amp;= \frac{9}{25} + \frac{49}{25} + \frac{64}{25} + \frac{4}{25} = \frac{126}{25} = 5.04 \,.
\end{align*}\]</span>
The number of degrees of freedom is <span class="math inline">\(m-1 = 3\)</span>, so the rejection region boundary
is $F_{W(3)}^{-1}(1-) = $ <code>qchisq(0.95,3)</code> = 7.815. Since 5.04 <span class="math inline">\(&lt;\)</span> 7.815, we
fail to reject the null hypothesis that the animals are distributed uniformly as
a function of azimuthal angle. (The <span class="math inline">\(p\)</span>-value is <code>1-pchisq(5.04,3)</code> = 0.169.)
See Figure <a href="the-binomial-and-related-distributions.html#fig:chi2gof">3.16</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2gof"></span>
<img src="_main_files/figure-html/chi2gof-1.png" alt="\label{fig:chi2gof}An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, hence we fail to reject the null hypothesis." width="50%" />
<p class="caption">
Figure 3.16: An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, hence we fail to reject the null hypothesis.
</p>
</div>
<hr />
</div>
<div id="simulating-an-exact-multinomial-test" class="section level3 hasAnchor" number="3.13.2">
<h3><span class="header-section-number">3.13.2</span> Simulating an Exact Multinomial Test<a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s assume the same data as in the last example. One reason<span class="math inline">\(-\)</span>in fact,
<em>the</em> reason<span class="math inline">\(-\)</span>why we utilize the chi-square GoF test when analyzing these
data is that “it has always been done this way.” Stated differently, we
have historically not worked with the multinomial distribution directly
because we couldn’t…at least, not until computers came along. But now
we <em>can</em> estimate the <span class="math inline">\(p\)</span>-value for the hypothesis in the last example
via simulation. Will we achieve a result very different from that above,
<span class="math inline">\(p = 0.169\)</span>?</p>
</blockquote>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="the-binomial-and-related-distributions.html#cb229-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb229-2"><a href="the-binomial-and-related-distributions.html#cb229-2" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb229-3"><a href="the-binomial-and-related-distributions.html#cb229-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>,<span class="dv">4</span>)</span>
<span id="cb229-4"><a href="the-binomial-and-related-distributions.html#cb229-4" tabindex="-1"></a></span>
<span id="cb229-5"><a href="the-binomial-and-related-distributions.html#cb229-5" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb229-6"><a href="the-binomial-and-related-distributions.html#cb229-6" tabindex="-1"></a>k       <span class="ot">&lt;-</span> <span class="fu">sum</span>(O)</span>
<span id="cb229-7"><a href="the-binomial-and-related-distributions.html#cb229-7" tabindex="-1"></a>m       <span class="ot">&lt;-</span> <span class="fu">length</span>(O)</span>
<span id="cb229-8"><a href="the-binomial-and-related-distributions.html#cb229-8" tabindex="-1"></a>pmf.obs <span class="ot">&lt;-</span> <span class="fu">dmultinom</span>(O,<span class="at">prob=</span>p)    <span class="co"># the observed multinomial pmf</span></span>
<span id="cb229-9"><a href="the-binomial-and-related-distributions.html#cb229-9" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">rmultinom</span>(num.sim,k,p) <span class="co"># generates an m x num.sim matrix</span></span>
<span id="cb229-10"><a href="the-binomial-and-related-distributions.html#cb229-10" tabindex="-1"></a>                                  <span class="co"># (m determined as the length of p)</span></span>
<span id="cb229-11"><a href="the-binomial-and-related-distributions.html#cb229-11" tabindex="-1"></a>pmf     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">2</span>,<span class="cf">function</span>(x){<span class="fu">dmultinom</span>(x,<span class="at">prob=</span>p)}) <span class="co"># simulated pmf&#39;s</span></span>
<span id="cb229-12"><a href="the-binomial-and-related-distributions.html#cb229-12" tabindex="-1"></a><span class="fu">sum</span>(pmf<span class="sc">&lt;</span>pmf.obs)<span class="sc">/</span>num.sim </span></code></pre></div>
<pre><code>## [1] 0.15974</code></pre>
<blockquote>
<p>What does <code>apply()</code> do? It applies the function given as
the third argument (which evaluates the multinomial probability
mass function given a set of four data <span class="math inline">\(\mathbf{x}\)</span> and a
a set of four probabilities <span class="math inline">\(\mathbf{p}\)</span>) to each <em>column</em> (hence
the “2” as the second argument…“1” would denote rows) of the
matrix <span class="math inline">\(\mathbf{X}\)</span>. It is a convenient function that allows us to
not have to embed the evaluation of the multinomial pmf into a <code>for</code>
loop that would iterate over all the rows of the matrix.</p>
</blockquote>
<blockquote>
<p>We observe <span class="math inline">\(p = 0.160\)</span>. This is close to, but at the same time still
substantially
less than, 0.169. In case the reader is to say “but, the computation
time must be much longer than for the chi-square GoF test,”
the above computation takes <span class="math inline">\(\sim\)</span> 1 CPU second.
The chi-square test is definitely important to know, in part because
testing hypotheses about multinomial probabilities
“has always been done this way”…but we would argue
that when a simulation of the exact test is possible, one should
code that simulation! (And run as many simulations as possible, to
reduce uncertainty in the final result. Here, we can take our estimated
<span class="math inline">\(p\)</span>-value of 0.160 and state that our one standard error uncertainty
is approximately <span class="math inline">\(\sqrt{kp(1-p)}/k = 0.001\)</span>, i.e., we expect the true
<span class="math inline">\(p\)</span>-value to be in the range <span class="math inline">\(0.160 \pm 3 \cdot 0.001\)</span> or
<span class="math inline">\([0.157,0.163]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="chi-square-test-of-independence" class="section level3 hasAnchor" number="3.13.3">
<h3><span class="header-section-number">3.13.3</span> Chi-Square Test of Independence<a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let’s go back to our animal data. When we observe the animals in each quadrant, we
record their color: black or red. So now are data look like this:</p>
</blockquote>
<table>
<colgroup>
<col width="23%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>black</td>
<td>20</td>
<td>18</td>
<td>5</td>
<td>14</td>
<td>57</td>
</tr>
<tr class="even">
<td>red</td>
<td>8</td>
<td>14</td>
<td>12</td>
<td>9</td>
<td>43</td>
</tr>
<tr class="odd">
<td></td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>When we record two attributes for each subject (here, azimuthal angle and color), we
can perform a chi-square test of independence to answer the question of whether
the attributes are independent random variables. In other words, here, does the
coloration <em>depend</em> on angle? The null hypothesis is no. We will test this hypothesis
assuming <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>We first determine the expected number of counts in each bin,
<span class="math inline">\(\widehat{E[X_{ij}]} = r_i c_j / k\)</span>:</p>
</blockquote>
<table>
<colgroup>
<col width="8%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="right"></th>
<th align="center">0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th align="center">90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th align="center">180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th align="center">270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">black</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 28/100 = 15.96</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 32/100 = 18.24</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 17/100 = 9.69</td>
<td align="center">57 <span class="math inline">\(\cdot\)</span> 23/100 = 13.11</td>
<td align="center">57</td>
</tr>
<tr class="even">
<td align="right">red</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 28/100 = 12.04</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 32/100 = 13.76</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 17/100 = 7.31</td>
<td align="center">43 <span class="math inline">\(\cdot\)</span> 23/100 = 9.89</td>
<td align="center">43</td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="center">28</td>
<td align="center">32</td>
<td align="center">17</td>
<td align="center">23</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<blockquote>
<p>We can already see that working with the numbers directly is tedious. Can we make
a matrix of such numbers using <code>R</code>?</p>
</blockquote>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="the-binomial-and-related-distributions.html#cb231-1" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">57</span>,<span class="dv">43</span>)</span>
<span id="cb231-2"><a href="the-binomial-and-related-distributions.html#cb231-2" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb231-3"><a href="the-binomial-and-related-distributions.html#cb231-3" tabindex="-1"></a>E <span class="ot">&lt;-</span> (r <span class="sc">%*%</span> <span class="fu">t</span>(c))<span class="sc">/</span><span class="fu">sum</span>(r)  <span class="co"># multiply r and the transpose of c </span></span>
<span id="cb231-4"><a href="the-binomial-and-related-distributions.html#cb231-4" tabindex="-1"></a><span class="fu">print</span>(E)                  <span class="co"># much better</span></span></code></pre></div>
<pre><code>##       [,1]  [,2] [,3]  [,4]
## [1,] 15.96 18.24 9.69 13.11
## [2,] 12.04 13.76 7.31  9.89</code></pre>
<blockquote>
<p>If we wish to continue using <code>R</code>, we need to define a matrix of observed data values:</p>
</blockquote>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="the-binomial-and-related-distributions.html#cb233-1" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">8</span>,<span class="dv">18</span>,<span class="dv">14</span>,<span class="dv">5</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">9</span>),<span class="at">nrow=</span><span class="dv">2</span>) <span class="co"># fills in column-by-column</span></span>
<span id="cb233-2"><a href="the-binomial-and-related-distributions.html#cb233-2" tabindex="-1"></a><span class="fu">print</span>(O)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   20   18    5   14
## [2,]    8   14   12    9</code></pre>
<blockquote>
<p>Now we have what we need. The test statistic is</p>
</blockquote>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="the-binomial-and-related-distributions.html#cb235-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>( (O<span class="sc">-</span>E)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>E ),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.805</code></pre>
<blockquote>
<p>and the number of degrees of freedom is <span class="math inline">\((r-1)(c-1) = 1 \cdot 3 = 3\)</span>, so the
rejection region boundary is</p>
</blockquote>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="the-binomial-and-related-distributions.html#cb237-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qchisq</span>(<span class="fl">0.95</span>,<span class="dv">3</span>),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.815</code></pre>
<blockquote>
<p>We find that if <span class="math inline">\(\alpha = 0.05\)</span>, we <em>cannot</em> reject the null hypothesis. We might be
tempted to do so, as our test statistic <em>very nearly</em> falls into the rejection region,
but we cannot. We could, if we were so inclined, remind ourselves that chi-square-based
hypothesis tests are <em>approximate</em>, and run a simulation to try to estimate the
true distribution of <span class="math inline">\(W\)</span>, and see what the rejection region and <span class="math inline">\(p\)</span>-value actually
are…that way, we <em>might</em> be able to actually reject the null.
But really, at the end of the day, such a result should simply motivate
us to gather more data!</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-normal-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-poisson-and-related-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

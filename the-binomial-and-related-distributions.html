<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 The Binomial (and Related) Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-normal-and-related-distributions.html"/>
<link rel="next" href="the-poisson-and-related-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance*</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-sample-mean-and-standard-deviation-are-independent-random-variables"><i class="fa fa-check"></i><b>2.10.1</b> The Sample Mean and Standard Deviation Are Independent Random Variables</a></li>
<li class="chapter" data-level="2.10.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval for the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#example-of-the-ks-test-in-r"><i class="fa fa-check"></i><b>2.14.1</b> Example of the KS Test in R</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-for-normality-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> Testing for Normality: Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-test-about-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Test About the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Extension to Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#how-many-data-do-we-need-to-achieve-a-given-test-power"><i class="fa fa-check"></i><b>2.15.6</b> How Many Data Do We Need to Achieve a Given Test Power?</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Extension to Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.1</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.3</b> The Expected Value of a Negative Binomial Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-binomial-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Binomial Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-moment-generating-function-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The Moment-Generating Function for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-minimum-variance-unbiased-estimator-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.1</b> The Minimum Variance Unbiased Estimator for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-maximum-likelihood-estimator-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.6.3</b> The Maximum Likelihood Estimator for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-confidence-intervals-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.3</b> Large-Sample Confidence Intervals for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-beta1theta-distribution"><i class="fa fa-check"></i><b>3.8.1</b> Defining the Uniformly Most-Powerful Test for the Beta(1,<span class="math inline">\(\theta\)</span>) Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-of-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.8.2</b> Defining the Uniformly Most-Powerful Test of the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining the Uniformly Most-Powerful Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-proportion"><i class="fa fa-check"></i><b>3.8.4</b> Large-Sample Tests of the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R: Star-Quasar Classification</a></li>
<li class="chapter" data-level="3.9.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification-1"><i class="fa fa-check"></i><b>3.9.2</b> Logistic Regression in R: Star-Quasar Classification</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#testing-hypotheses-using-the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Testing Hypotheses Using the Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-poisson-random-variables"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Poisson Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
<li class="chapter" data-level="4.4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean-1"><i class="fa fa-check"></i><b>4.4.3</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-cramer-rao-lower-bound-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The Cramer-Rao Lower Bound on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-coviarance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Coviarance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.4</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-binomial-and-related-distributions" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> The Binomial (and Related) Distributions<a href="the-binomial-and-related-distributions.html#the-binomial-and-related-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="motivation-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Motivation<a href="the-binomial-and-related-distributions.html#motivation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are holding a coin. It may be fair
(meaning that the probabilities of observing heads or tails in a
given flip of the coin are each 0.5)or perhaps not. We decide that we are
going to flip the coin some fixed number of times <span class="math inline">\(k\)</span>, and we will record
the outcome of each flip:
<span class="math display">\[
\mbox{Results of each flip:}~~\mathbf{Y} = \{Y_1,Y_2,\ldots,Y_k\} \,,
\]</span>
where, e.g., <span class="math inline">\(Y_1 = 1\)</span> if we observe heads or <span class="math inline">\(0\)</span> if we observe tails.
This is an example of a <em>Bernoulli process</em>, where process denotes a
sequence of observations, and Bernoulli indicates that there are two
possible discrete outcomes.</p>
<p>A <em>binomial experiment</em> is one that generates Bernoulli process data through
the running of <span class="math inline">\(k\)</span> trials (e.g., <span class="math inline">\(k\)</span> separate coin flips).
The properties of such an experiment are as follows:</p>
<ol style="list-style-type: decimal">
<li>The number of trials <span class="math inline">\(k\)</span> is fixed in advance.</li>
<li>Each trial has two possible outcomes, generically denoted as
<span class="math inline">\(S\)</span> (success) or <span class="math inline">\(F\)</span> (failure).</li>
<li>The probability of success remains <span class="math inline">\(p\)</span> throughout the experiment.</li>
<li>The outcome of each trial is independent of the outcomes of the others.</li>
</ol>
<p>The random variable of interest for a binomial experiment is the number
of observed successes. A closely related alternative to a binomial experiment
is a <em>negative binomial experiment</em>, wherein the number of successes <span class="math inline">\(s\)</span>
is fixed in advance, instead of the number of trials <span class="math inline">\(k\)</span>,
and the random variable
of interest is the number of failures that we observe
before achieving <span class="math inline">\(s\)</span> successes.
A simple example would
be flipping a coin until <span class="math inline">\(s\)</span> heads are observed and recording the overall
number of tails that are seen.</p>
<p>As a side note to the third point above, about the probability of success
remaining <span class="math inline">\(p\)</span> throughout the experiment: binomial and negative binomial
experiments rely on <em>sampling with replacement</em>if we observe a head
for a given coin flip, we can indeed observe heads again in the future.
In the real world, however, the reader will observe instances where, e.g.,
a binomial distribution is used to model experiments featuring <em>sampling
without replacement</em>: we have <span class="math inline">\(K = 100\)</span> widgets, of which ten are defective;
we check one to see if it is defective (with probability <span class="math inline">\(p = 0.1\)</span>) and
set it aside, then check another (with probability either 10/99 or 9/99,
depending on the outcome of the first trial), etc. The rule-of-thumb for
using the binomial distribution to model data in such a situation is that
it is fine if the number of trials <span class="math inline">\(k \lesssim K/10\)</span>.
However, in the age of computers, there is no reason to apply the binomial
distribution when we can apply the hypergeometric distribution instead.
We will return to this point later in this chapter.</p>
<p>As a side note to the fourth point above, about the outcome of each trial
being independent of the outcomes of the others: in a general process,
each datum can be dependent on the data observed previously. How each datum
is dependent on previous data defines the type of process that is observed:
Markov processes, Gaussian processes, etc.
A Bernoulli process is said to be a <em>memoryless</em> process
and it is comprised of iid data.</p>
</div>
<div id="probability-mass-function" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Probability Mass Function<a href="the-binomial-and-related-distributions.html#probability-mass-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets focus first on the outcome of a binomial experiment,
with the random variable <span class="math inline">\(X\)</span> being the number of observed successes
in <span class="math inline">\(k\)</span> trials. What is the probability
of observing <span class="math inline">\(X=x\)</span> successes, if the probability of observing a success
in any one trial is <span class="math inline">\(p\)</span>?
<span class="math display">\[\begin{align*}
\mbox{$x$ successes}&amp;: p^x \\
\mbox{$k-x$ failures}&amp;: (1-p)^{k-x} \,.
\end{align*}\]</span>
So <span class="math inline">\(P(X=x) = p^x (1-p)^{k-x}\)</span>but, no, this isnt right. Lets think this
through. Assume <span class="math inline">\(k = 2\)</span>. The sample space of possible experimental outcomes is
<span class="math display">\[
\Omega = \{ SS, SF, FS, FF \} \,.
\]</span>
If <span class="math inline">\(p\)</span> = 0.5, then we can see that the probability of observing one
success in two trials is 0.5but our equation tells us that
<span class="math inline">\(P(X=1) = (0.5)^1 (1-0.5)^1 = 0.25\)</span>. What are we missing? We are missing
that there are two ways of observing a single successand we need to count
both. Because we ultimately do not
care about the order in which successes and failures are observed,
we utilize counting via combination:
<span class="math display">\[
\binom{k}{x} = \frac{k!}{x! (k-x)!} \,,
\]</span>
where the exclamation point represents the factorial function
<span class="math inline">\(x! = x(x-1)(x-2)\cdots 1\)</span>. So now we can correctly write down the
probability <span class="math inline">\(P(X=x)\)</span>:
<span class="math display">\[
P(X=x) = \binom{k}{x} p^x (1-p)^{k-x} ~~~ x \in \{0,\ldots,k\} \,.
\]</span>
This is the <em>binomial</em> probability mass function. We denote the distribution
of the random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Note a special
case: if <span class="math inline">\(k = 1\)</span>, the distribution is dubbed a <em>Bernoulli distribution</em>.</p>
<p><strong>Recall</strong>: <em>a probability mass function is one way to represent a discrete probability distribution, and it has the properties (a) <span class="math inline">\(0 \leq P(X=x) \leq 1\)</span> and (b) <span class="math inline">\(\sum_x P(X=x) = 1\)</span>, where the sum is over all possible values of <span class="math inline">\(x\)</span>.</em></p>
<p>(Note that it is conventional to denote the number of trials as <span class="math inline">\(n\)</span>, not
<span class="math inline">\(k\)</span>. However, <span class="math inline">\(n\)</span> is also conventionally used to denote the sample size in
any experiment, so to avoid confusion, we use <span class="math inline">\(k\)</span> to denote the number
of trials here.)</p>
<p>In Figure <a href="the-binomial-and-related-distributions.html#fig:bpmf">3.1</a>, we display three binomial probability mass functions,
one each for success probabilities 0.1 (red, to the left), 0.5 (green,
to the center), and 0.8 (blue, to the right). This figure indicates an
important aspect of the binomial pmf, namely that it <em>can</em> attain a shape
akin to that of a normal distribution, if <span class="math inline">\(p\)</span> is such that any truncation
observed at values <span class="math inline">\(x=0\)</span> or at <span class="math inline">\(x=k\)</span> is not apparent. In fact, a binomial
random variable converges in distribution to a normal random variable
in certain limiting situations, as we indicate in an example below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpmf"></span>
<img src="_main_files/figure-html/bpmf-1.png" alt="\label{fig:bpmf}Binomial probability mass functions for number of trials $k = 10$ and success probabilities $p = 0.1$ (red), 0.5 (green), and 0.8 (blue)." width="50%" />
<p class="caption">
Figure 3.1: Binomial probability mass functions for number of trials <span class="math inline">\(k = 10\)</span> and success probabilities <span class="math inline">\(p = 0.1\)</span> (red), 0.5 (green), and 0.8 (blue).
</p>
</div>
<p><strong>Recall:</strong> <em>the expected value of a discretely distributed random variable is</em>
<span class="math display">\[
E[X] = \sum_x x P(X=x) = \sum_x x p_X(x) \,,
\]</span>
<em>where the sum is over all <span class="math inline">\(x\)</span> for which <span class="math inline">\(P(X = x) &gt; 0\)</span>.</em></p>
<p>For the binomial distribution, the expected value is
<span class="math display">\[
E[X] = \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \,.
\]</span>
At first, this does not appear to be easy to evaluate.
One possibility is to utilize the <em>binomial theorem</em>,
<span class="math display">\[
(x+y)^k = \sum_{i=0}^k \binom{k}{x} x^i y^{k-i} \,,
\]</span>
but this will not help here, as the <span class="math inline">\(x\)</span> in the expression above
for <span class="math inline">\(E[X]\)</span> is not raised to a power. Another trick in our arsenal
is to pull constants out of the summation such that whatever is left
as the summand is a pmf (and thus sums to 1). Lets try this here:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^k x \binom{k}{x} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k x \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= \sum_{x=1}^k \frac{k!}{(x-1)!(k-x)!} p^x (1-p)^{k-x} \\
     &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \,.
\end{align*}\]</span>
The summation appears almost like that of a binomial random variable.
Lets set <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= kp \sum_{x=1}^k \frac{(k-1)!}{(x-1)!(k-x)!} p^{x-1} (1-p)^{k-x} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!(k-(y+1))!} p^y (1-p)^{k-(y+1)} \\
     &amp;= kp \sum_{y=0}^{k-1} \frac{(k-1)!}{y!((k-1)-y)!} p^y (1-p)^{(k-1)-y} \,.
\end{align*}\]</span>
The summand is now the pmf for the random variable
<span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-1\)</span>,<span class="math inline">\(p\)</span>), summed over all values
of <span class="math inline">\(y\)</span> in the domain of the distribution. Thus the
summation evaluates to 1, and thus <span class="math inline">\(E[X] = kp\)</span>.</p>
<table>
<caption>Binomial Distribution - <code>R</code> Functions</caption>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PMF</td>
<td><code>dbinom(x,k,p)</code></td>
</tr>
<tr class="even">
<td>CDF</td>
<td><code>pbinom(x,k,p)</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td><code>qbinom(q,k,p)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rbinom(n,k,p)</code></td>
</tr>
</tbody>
</table>
<p>A negative binomial experiment is governed by the <em>negative binomial
distribution</em>, whose pmf is
<span class="math display">\[
p_X(x) = \binom{x+s-1}{x} p^s (1-p)^x ~~~ x \in \{0,1,\ldots,\infty\}
\]</span>
The form of this pmf follows from the fact that the underlying Bernoulli
process would consist of <span class="math inline">\(x+s\)</span> data, <em>with the last datum being the observed
success that ends the experiment</em>. The first <span class="math inline">\(x+s-1\)</span> data
would feature <span class="math inline">\(s-1\)</span> successes and <span class="math inline">\(x\)</span> failures, with the order of
success and failure not matteringso we can view these data
as being binomially distributed (albeit with <span class="math inline">\(x\)</span> represent
failureshence the negative in negative binomial!):
<span class="math display">\[
p_X(x) = \underbrace{\binom{x+s-1}{x} p^{s-1} (1-p)^x}_{\mbox{first $x+s-1$ trials}} \cdot \underbrace{p}_{\mbox{last trial}} \,.
\]</span>
Note the special case of <span class="math inline">\(s = 1\)</span>: this is dubbed the <em>geometric distribution</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbpmf"></span>
<img src="_main_files/figure-html/nbpmf-1.png" alt="\label{fig:nbpmf}Negative binomial probability mass functions for the number of successes $s = 2$ and success probabilities $p = 0.7$ (red), 0.5 (green), and 0.2 (blue)." width="50%" />
<p class="caption">
Figure 3.2: Negative binomial probability mass functions for the number of successes <span class="math inline">\(s = 2\)</span> and success probabilities <span class="math inline">\(p = 0.7\)</span> (red), 0.5 (green), and 0.2 (blue).
</p>
</div>
<table>
<caption>Negative Binomial Distribution - <code>R</code> Functions</caption>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PMF</td>
<td><code>dnbinom(x,s,p)</code></td>
</tr>
<tr class="even">
<td>CDF</td>
<td><code>pnbinom(x,s,p)</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td><code>qnbinom(q,s,p)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rnbinom(n,s,p)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="binomial-distribution-normal-approximation" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Binomial Distribution: Normal Approximation<a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In certain limiting situations, a binomial random variable converges in
distribution to a normal random variable, i.e., if
<span class="math display">\[
P\left(\frac{X-\mu}{\sigma} &lt; a \right) = P\left(\frac{X-kp}{\sqrt{kp(1-p)}} &lt; a \right) \approx P(Z &lt; a) = \Phi(a) \,,
\]</span>
then we can state that at least approximately
<span class="math inline">\(X \sim \mathcal{N}(kp,kp(1-p))\)</span>. Now, what do we mean by certain
limiting situations? For instance, if <span class="math inline">\(p\)</span> is close to zero or one,
then the binomial distribution is truncated at 0 or at <span class="math inline">\(k\)</span>,
and the shape of the pmf does <em>not</em> appear to be like that of a normal pdf.
One conventional rule-of-thumb is that the normal approximation is adequate if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,.
\]</span>
The reader might question why we would mention this approximation at all:
if we have binomially distributed data and a computer, then we need not
ever utilize such an approximation to, e.g., compute probabilities. This
point is correct (and is the reason why, for instance, we do not mention
the so-called <em>continuity correction</em> here; our goal is not to compute
probabilities). The reason we mention this is that, as we will see, this
approximation underlies a commonly used hypothesis test framework,
and thus needs to be mentioned for completeness.</p>
</blockquote>
<hr />
</div>
<div id="variance-of-a-binomial-random-variable" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Variance of a Binomial Random Variable<a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The variance of a random variable is given by the shortcut formula that
we have been using since Chapter 1: <span class="math inline">\(V[X] = E[X^2] - (E[X])^2\)</span>.
So we would expect that we would need to compute <span class="math inline">\(E[X^2]\)</span> here, since
we already know that <span class="math inline">\(E[X] = kp\)</span>. But for reasons that will become
apparent below, it is actually far easier for us to compute <span class="math inline">\(E[X(X-1)]\)</span>
and work with that to eventually derive the variance:
<span class="math display">\[\begin{align*}
E[X(X-1)] &amp;= \sum_{x=0}^k x(x-1) \binom{k}{x} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=0}^k x(x-1) \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= \sum_{x=2}^k \frac{k!}{(x-2)!(k-x)!} p^x (1-p)^{k-x} \\
&amp;= k(k-1) p^2 \sum_{x=2}^k \frac{(k-2)!}{(x-2)!(k-x)!} p^{x-2} (1-p)^{k-x} \,.
\end{align*}\]</span>
The advantage to using <span class="math inline">\(x(x-1)\)</span> was that it matches the first two terms
of <span class="math inline">\(x! = x(x-1)\cdots(1)\)</span>, allowing easy cancellation.
If we set <span class="math inline">\(y = x-2\)</span>, we find that the summand above will, in a similar
manner as in the calculation of <span class="math inline">\(E[X]\)</span>, become the pmf for the
random variable <span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(k-2,p\)</span>)and thus the summation will
evaluate to 1.</p>
</blockquote>
<blockquote>
<p>So <span class="math inline">\(E[X(X-1)] = E[X^2] - E[X] = k(k-1)p^2\)</span>, and
<span class="math inline">\(E[X^2] = k^2p^2-kp^2 + kp = V[X] + (E[X])^2\)</span>, and
<span class="math inline">\(V[X] = k^2p^2-kp^2+kp-k^2p^2 = kp-kp^2 = kp(1-p)\)</span>. Done.</p>
</blockquote>
<hr />
</div>
<div id="the-expected-value-of-a-negative-binomial-random-variable" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> The Expected Value of a Negative Binomial Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The calculation for the expected value <span class="math inline">\(E[X]\)</span> for a negative binomial
random variable is similar to that for a binomial random variable:
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^{\infty} x \binom{x+s-1}{x} p^s (1-p)^x \\
&amp;= \sum_{x=0}^{\infty} x \frac{(x+s-1)!}{(s-1)!x!} p^s (1-p)^x \\
&amp;= \sum_{x=1}^{\infty} \frac{(x+s-1)!}{(s-1)!(x-1)!} p^s (1-p)^x \,.
\end{align*}\]</span>
Let <span class="math inline">\(y = x-1\)</span>. Then
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{y=0}^{\infty} \frac{(y+s)!}{(s-1)!y!} p^s (1-p)^{y+1} \\
&amp;= \sum_{y=0}^{\infty} s(1-p) \frac{(y+s)!}{s!y!} p^s (1-p)^y \\
&amp;= \sum_{y=0}^{\infty} \frac{s(1-p)}{p} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \sum_{y=0}^{\infty} \frac{(y+s)!}{s!y!} p^{s+1} (1-p)^y \\
&amp;= \frac{s(1-p)}{p} \,.
\end{align*}\]</span>
The summand is that of a negative binomial distribution for <span class="math inline">\(s+1\)</span> successes,
hence the summation is 1, and thus <span class="math inline">\(E[X] = s(1-p)/p\)</span>.</p>
</blockquote>
</div>
</div>
<div id="cumulative-distribution-function-1" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Cumulative Distribution Function<a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>the cumulative distribution function, or cdf, is another means by which to encapsulate information about a probability distribution. For a discrete distribution, it is defined as <span class="math inline">\(F_X(x) = \sum_{y\leq x} p_Y(y)\)</span>, and it is defined for all values <span class="math inline">\(x \in (-\infty,\infty)\)</span>, with <span class="math inline">\(F_X(-\infty) = 0\)</span> and <span class="math inline">\(F_X(\infty) = 1\)</span>.</em></p>
<p>For the binomial distribution, the cdf is
<span class="math display">\[
F_X(x) = \sum_{y=0}^{\lfloor x \rfloor} p_Y(y) = \sum_{y=0}^{\lfloor x \rfloor} \binom{k}{y} p^y (1-p)^{k-y} \,,
\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the largest integer that is less than or equal
to <span class="math inline">\(x\)</span> (e.g., if <span class="math inline">\(x\)</span> = 6.75, <span class="math inline">\(\lfloor x \rfloor\)</span> = 6).
In general, there is no closed-form representation for the binomial cdf
(i.e., one cannot replace the summation with a formula).
Also, because a pmf is defined at discrete values of <span class="math inline">\(x\)</span>, its associated
cdf is a step function, as illustrated in the left panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>.
As we can see in this figure, the cdf steps up at each value of <span class="math inline">\(x\)</span>
in the domain of <span class="math inline">\(p_X(x)\)</span>, and unlike the case for continuous distributions,
the form of the inequalities in a probabilistic statement matter:
<span class="math inline">\(P(X &lt; x)\)</span> and <span class="math inline">\(P(X \leq x)\)</span> may not be the same, if <span class="math inline">\(x\)</span> is in the domain of <span class="math inline">\(p_X(x)\)</span>.</p>
<p><strong>Recall</strong>: <em>an inverse cdf function <span class="math inline">\(F_X^{-1}(\cdot)\)</span>
takes as input the total probability
<span class="math inline">\(q \in [0,1]\)</span> in the range <span class="math inline">\((-\infty,x]\)</span> and returns the value of <span class="math inline">\(x\)</span>.
A discrete distribution has no unique inverse cdf; it is convention to
utilize the generalized inverse cdf,</em>
<span class="math inline">\(x = F_X^{-1}(q) = \mbox{inf}\{x : F_X(x) \geq q\}\)</span>,
<em>where inf indicates the return
the smallest value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x) \geq q\)</span>.</em></p>
<p>In the right panel of Figure <a href="the-binomial-and-related-distributions.html#fig:bincdf">3.3</a>, we display the inverse cdf
for the same distribution used to generate the figure in the left panel
(<span class="math inline">\(k=4\)</span> and <span class="math inline">\(p=0.5\)</span>). Like the cdf, the inverse cdf for a discrete distribution
is a step function. Below, in an example, we show how we adapt the inverse
transform sampler algorithm of Chapter 1 to accommodate the step-function
nature of an inverse cdf.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bincdf"></span>
<img src="_main_files/figure-html/bincdf-1.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" /><img src="_main_files/figure-html/bincdf-2.png" alt="\label{fig:bincdf}Illustration of the cumulative distribution function $F_X(x)$ (left) and inverse cumulative distribution function $F_X^{-1}(q)$ (right) for a binomial distribution with number of trials $k = 4$ and probability of success $p=0.5$." width="45%" />
<p class="caption">
Figure 3.3: Illustration of the cumulative distribution function <span class="math inline">\(F_X(x)\)</span> (left) and inverse cumulative distribution function <span class="math inline">\(F_X^{-1}(q)\)</span> (right) for a binomial distribution with number of trials <span class="math inline">\(k = 4\)</span> and probability of success <span class="math inline">\(p=0.5\)</span>.
</p>
</div>
<hr />
<div id="computing-probabilities-7" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Computing Probabilities<a href="the-binomial-and-related-distributions.html#computing-probabilities-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Because there is no closed-form expression for the binomial cdf, and
because computing the binomial pmf for a range of values of <span class="math inline">\(x\)</span> can
be laborious, we almost always utilize <code>R</code> shortcut functions from the
beginning when computing probabilities.</p>
</blockquote>
<blockquote>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X \sim\)</span> Binomial(10,0.6), which is <span class="math inline">\(P(4 \leq X &lt; 6)\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>We first note that due to the form of the inequality, we do <em>not</em> include <span class="math inline">\(X=6\)</span>
in the computation. Thus <span class="math inline">\(P(4 \leq X &lt; 6) = p_X(4) + p_X(5)\)</span>, which equals
<span class="math display">\[
\binom{10}{4} (0.6)^4 (1-0.6)^6 + \binom{10}{5} (0.6)^5 (1-0.6)^5 \,.
\]</span>
Even computing this is unnecessarily laborious; instead, we call on <code>R</code>:</p>
</blockquote>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="the-binomial-and-related-distributions.html#cb166-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">4</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>) <span class="sc">+</span> <span class="fu">dbinom</span>(<span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<p>We can also utilize cdf functions here: <span class="math inline">\(P(4 \leq X &lt; 6) = P(X &lt; 6) - P(X &lt; 4) = P(X \leq 5) - P(X \leq 3) = F_X(5) - F_X(3)\)</span>, which in <code>R</code> is computed via</p>
</blockquote>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="the-binomial-and-related-distributions.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">5</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>) <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="dv">3</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 0.3121349</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(X \sim\)</span> Binomial(10,0.6), what is the value of <span class="math inline">\(a\)</span> such that
<span class="math inline">\(P(X \leq a) = 0.9\)</span>?</li>
</ol>
</blockquote>
<blockquote>
<p>First, we set up the inverse cdf formula:
<span class="math display">\[
P(X \leq a) = F_X(a) = 0.9 ~~ \Rightarrow ~~ a = F_X^{-1}(0.9)
\]</span>
Note that we didnt do anything differently here than we would have done
in a continuous distribution settingand we can proceed directly to
<code>R</code> because it utilizes the generalized inverse cdf algorithm.</p>
</blockquote>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="the-binomial-and-related-distributions.html#cb170-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fl">0.9</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.6</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<hr />
</div>
<div id="sampling-data" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Sampling Data<a href="the-binomial-and-related-distributions.html#sampling-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>While we would always utilize <code>R</code> shortcut functions like <code>rbinom()</code> when they exist,
there may be instances when we need to code our own functions for sampling data from
discrete distributions. The code below shows such a function for an arbitrary probability
mass function.</p>
</blockquote>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="the-binomial-and-related-distributions.html#cb172-1" aria-hidden="true" tabindex="-1"></a>x   <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">8</span>)              <span class="co"># domain of x</span></span>
<span id="cb172-2"><a href="the-binomial-and-related-distributions.html#cb172-2" aria-hidden="true" tabindex="-1"></a>p.x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.2</span>,<span class="fl">0.35</span>,<span class="fl">0.15</span>,<span class="fl">0.3</span>)    <span class="co"># p(x)</span></span>
<span id="cb172-3"><a href="the-binomial-and-related-distributions.html#cb172-3" aria-hidden="true" tabindex="-1"></a>F.x <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(p.x)             <span class="co"># cumulative sum -&gt; produces F(x)</span></span>
<span id="cb172-4"><a href="the-binomial-and-related-distributions.html#cb172-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-5"><a href="the-binomial-and-related-distributions.html#cb172-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">235</span>)</span>
<span id="cb172-6"><a href="the-binomial-and-related-distributions.html#cb172-6" aria-hidden="true" tabindex="-1"></a>n  <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb172-7"><a href="the-binomial-and-related-distributions.html#cb172-7" aria-hidden="true" tabindex="-1"></a>q  <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)                <span class="co"># we still ultimately need runif!</span></span>
<span id="cb172-8"><a href="the-binomial-and-related-distributions.html#cb172-8" aria-hidden="true" tabindex="-1"></a>i  <span class="ot">&lt;-</span> <span class="fu">findInterval</span>(q,F.x)<span class="sc">+</span><span class="dv">1</span>   <span class="co"># the output is [0,3] and not [1,4]</span></span>
<span id="cb172-9"><a href="the-binomial-and-related-distributions.html#cb172-9" aria-hidden="true" tabindex="-1"></a>                              <span class="co"># 0 means q is between Fx[1] and Fx[2], etc.</span></span>
<span id="cb172-10"><a href="the-binomial-and-related-distributions.html#cb172-10" aria-hidden="true" tabindex="-1"></a>x.sample <span class="ot">&lt;-</span> x[i]</span>
<span id="cb172-11"><a href="the-binomial-and-related-distributions.html#cb172-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the output is a bit tricky: we override the default definition of breaks</span></span>
<span id="cb172-12"><a href="the-binomial-and-related-distributions.html#cb172-12" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(x.sample,<span class="at">main=</span><span class="cn">NULL</span>,<span class="at">prob=</span><span class="cn">TRUE</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="fl">0.5</span>,<span class="fl">8.5</span>,<span class="at">by=</span><span class="dv">1</span>),<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">9</span>),<span class="at">xlab=</span><span class="st">&quot;x&quot;</span>)</span>
<span id="cb172-13"><a href="the-binomial-and-related-distributions.html#cb172-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ( ii <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x) ) {</span>
<span id="cb172-14"><a href="the-binomial-and-related-distributions.html#cb172-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">c</span>(x[ii]<span class="sc">-</span><span class="fl">0.5</span>,x[ii]<span class="sc">+</span><span class="fl">0.5</span>),<span class="fu">c</span>(p.x[ii],p.x[ii]),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span>
<span id="cb172-15"><a href="the-binomial-and-related-distributions.html#cb172-15" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pmfsamp"></span>
<img src="_main_files/figure-html/pmfsamp-1.png" alt="\label{fig:pmfsamp}Histogram of $n = 100$ iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of $x$." width="50%" />
<p class="caption">
Figure 3.4: Histogram of <span class="math inline">\(n = 100\)</span> iid data drawn using an inverse tranform sampler adapted to the discrete distribution setting. The red lines indicate the true density for each value of <span class="math inline">\(x\)</span>.
</p>
</div>
</div>
</div>
<div id="linear-functions-of-binomial-random-variables" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Linear Functions of Binomial Random Variables<a href="the-binomial-and-related-distributions.html#linear-functions-of-binomial-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). Can we determine the distribution of
the sum <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>? Yes, we canvia the method of moment-generating functions.</p>
<p><strong>Recall</strong>: <em>the moment-generating function, or mgf, is a means by which to encapsulate information about a probability distribution. When it exists, the mgf is given by <span class="math inline">\(E[e^{tX}]\)</span>. If <span class="math inline">\(Y = \sum_{i=1}^n a_iX_i\)</span>, then <span class="math inline">\(m_Y(t) = m_{X_1}(a_1t) m_{X_2}(a_2t) \cdots m_{X_n}(a_nt)\)</span>; if we can identify <span class="math inline">\(m_Y(t)\)</span> os the mgf for a known family of distributions, then we can immediately identify the distribution of <span class="math inline">\(Y\)</span> and the parameters of that distribution.</em></p>
<p>The mgf for the binomial distribution is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^k e^{tx} \binom{k}{x} p^x (1-p)^{k-x} \\
                   &amp;= \sum_{x=0}^k \binom{k}{x} (pe^t)^x (1-p)^{k-x} \,.
\end{align*}\]</span>
Here we do utilize the binomial theorem (unlike when we were computing <span class="math inline">\(E[X]\)</span>),
<span class="math display">\[
(x+y)^k = \sum_{i=0}^k \binom{k}{x} x^i y^{k-i} \,,
\]</span>
to write down that
<span class="math display">\[
m_X(t) = [pe^t + (1-p)]^k \,,
\]</span>
or <span class="math inline">\((pe^t+q)^k\)</span>, where <span class="math inline">\(q = 1-p\)</span>.</p>
<p>The mgf for <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is thus
<span class="math display">\[
m_Y(t) = (pe^t + (1-p))^{k} (pe^t + (1-p))^{k} \cdots (pe^t + (1-p))^{k} = (pe^t + (1-p))^{nk}\,.
\]</span>
We can see that this has the form of a binomial mgf, and furthermore that <span class="math inline">\(Y \sim\)</span> Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>),
with expected value <span class="math inline">\(E[Y] = nkp\)</span> and variance <span class="math inline">\(V[Y] = nkp(1-p)\)</span>.
This makes sense, as the act of summing binomial data is equivalent to concatenating
<span class="math inline">\(n\)</span> separate Bernoulli processes into one longer Bernoulli processwhose
data can subsequently be modeled using a binomial distribution.</p>
<p>While we can identify the distribution of the sum, we cannot identify the distribution of the sample mean.
To see this, we will attempt to use
the mgf method again, this time plugging in <span class="math inline">\(a_i = 1/n\)</span> instead of <span class="math inline">\(a_i = 1\)</span>. We find that
<span class="math display">\[
m_{\bar{X}}(t) = (pe^{t/n} + (1-p))^{nk} \,.
\]</span>
Changing <span class="math inline">\(t\)</span> to <span class="math inline">\(t/n\)</span> has the effect of creating an mgf that does not
have the form of any known mgfthus
<em>we cannot identify the family of distribution for <span class="math inline">\(\bar{X}\)</span></em>. To learn more
about the distribution of <span class="math inline">\(\bar{X}\)</span>, we have two alternatives:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(n \gtrsim 30\)</span>, we can utilize the Central Limit Theorem to state that
<span class="math display">\[
\bar{X} \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}\left(kp,\frac{kp(1-p)}{n}\right) \,,
\]</span>
i.e., that the random variable <span class="math inline">\(\bar{X}\)</span> converges in distribution to a normal random variable
with mean <span class="math inline">\(E[\bar{X}] = \mu = kp\)</span> and variance <span class="math inline">\(V[\bar{X}] = \sigma^2/n = kp(1-p)/n\)</span>.</p></li>
<li><p>We can perform a general transformation to express the probability mass
function for <span class="math inline">\(\bar{X}\)</span> (i.e., so as to provide all the values of
<span class="math inline">\(p_{\bar{X}}(\bar{x})\)</span>, which is not a named distribution).
We show how to derive this pmf for <span class="math inline">\(\bar{X}\)</span> in an example below;
with it, we can tackle computing confidence intervals and
performing hypothesis tests for the population mean. (We note in passing
that we cannot utilize this same technique to easily determine the pmf for
the sample variance <span class="math inline">\(S^2\)</span>; if we wish to construct confidence intervals,
etc., for the population variance, we have to fall back on simulations.)</p></li>
</ol>
<hr />
<div id="the-moment-generating-function-for-a-geometric-random-variable" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> The Moment-Generating Function for a Geometric Random Variable<a href="the-binomial-and-related-distributions.html#the-moment-generating-function-for-a-geometric-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Recall that a geometric distribution is equivalent to a
negative binomial distribution
with number of successes <span class="math inline">\(s = 1\)</span>; the probability mass function is thus
<span class="math display">\[
p_X(x) = p (1-p)^x \,,
\]</span>
with <span class="math inline">\(x = \{0,1,\ldots\}\)</span> and <span class="math inline">\(p \in [0,1]\)</span>.
The moment-generating function for a geometric random variable is
<span class="math display">\[\begin{align*}
m_X(t) = E[e^{tX}] &amp;= \sum_{x=0}^{\infty} e^{tx} p (1-p)^x \\
&amp;= p \sum_{x=0}^\infty e^{tx} (1-p)^x \\
&amp;= p \sum_{x=0}^\infty [e^t(1-p)]^x \\
&amp;= p \frac{1}{1-e^t(1-p)} \,.
\end{align*}\]</span>
The jump from the penultimate to the last line follows from knowing the
formula for the sum of an infinite geometric series.</p>
</blockquote>
<blockquote>
<p>We note that in the same way that the sum of Bernoulli random variables is
binomially
distributed (for <span class="math inline">\(n\)</span> trials), the sum of geometric random variables is
negative binomially distributed (for <span class="math inline">\(s\)</span> successes).
That means that the mgf for a negative binomial random variable is
<span class="math display">\[
m_Y(t) = \prod_{i=1}^s m_{X_i}(t) = \prod_{i=1}^s \frac{p}{1-e^t(1-p)} = \left(\frac{p}{1-e^t(1-p)}\right)^s \,.
\]</span>
We also note that it is far easier to derive the mgf for the negative
binomial in this fashion than via direct computation of <span class="math inline">\(E[e^{tX}]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-probability-mass-function-for-the-sample-mean" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> The Probability Mass Function for the Sample Mean<a href="the-binomial-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume we are given <span class="math inline">\(n\)</span> iid binomial random variables:
<span class="math inline">\(X_1,X_2,\ldots,X_n \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>). As we observe above, the
distribution of the sum <span class="math inline">\(Y = \sum_{i=1}^k X_i\)</span> is binomial with mean
<span class="math inline">\(nkp\)</span> and variance <span class="math inline">\(nkp(1-p)\)</span>.</p>
</blockquote>
<blockquote>
<p>The sample mean is <span class="math inline">\(\bar{X} = Y/n\)</span>, and so
<span class="math display">\[
F_{\bar{X}}(\bar{x}) = P(\bar{X} \leq \bar{x}) = P(Y \leq n\bar{x}) = \sum_{y=0}^{n\bar{x}} p_Y(y) \,.
\]</span>
(We note that <span class="math inline">\(n\bar{x}\)</span> is integer-valued by definition; we do not need
to round down here.) Because we are dealing with a pmf, we cannot
simply take the derivative of <span class="math inline">\(F_{\bar{X}}(\bar{x})\)</span> to find
<span class="math inline">\(f_{\bar{X}}(\bar{x})\)</span>but what we can do is assess the jump
in the cumulative distribution function at each step, because that
<em>is</em> the pmf. In other words, we can compute
<span class="math display">\[
f_{\bar{X}}(\bar{x}) = P(Y \leq n\bar{x}) - P(Y \leq n\bar{x}-1)
\]</span>
and store this as a numerically expressed pmf for <span class="math inline">\(\bar{X}\)</span>.
See Figure <a href="the-binomial-and-related-distributions.html#fig:xbarpmf">3.5</a>.</p>
</blockquote>
<blockquote>
<p>But it turns out we can say more about this pmf, by looking at the problem
in a different way.
We know that <span class="math inline">\(Y \sim\)</span> Binomial<span class="math inline">\((nkp,nkp(1-p))\)</span> and
thus that <span class="math inline">\(Y \in [0,1,\ldots,nk]\)</span>.
When we compute the quotient <span class="math inline">\(\bar{X} = Y/n\)</span>, <em>all we are doing is redefining the domain of the pmf</em>
from being <span class="math inline">\([0,1,\ldots,nk]\)</span> to being
<span class="math inline">\([0,1/n,2/n,\ldots,k]\)</span>. We do not actually change the probability masses!
So we can write
<span class="math display">\[
p_{\bar{X}}(\bar{x}) = \binom{nk}{n\bar{x}} p^{n\bar{x}} (1-p)^{nk-n\bar{x}} ~~ \bar{x} \in [0,1/n,2/n,\ldots,k] \,.
\]</span>
This pmf has the functional form of a binomial pmfbut <em>not</em> the domain
of a binomial pmf. For that reason, we cannot say that <span class="math inline">\(\bar{X}\)</span> is
binomially distributed. The pmf has a functional form, it has a domain,
but it has no known name and thus has no tabulated properties that
one can just look up when doing calculations.
(However, we do know that the expected value is <span class="math inline">\(E[\bar{X}] = \mu = kp\)</span>
and the variance is <span class="math inline">\(V[\bar{X}] = \sigma^2/n = kp(1-p)/n\)</span>.)</p>
</blockquote>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="the-binomial-and-related-distributions.html#cb173-1" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb173-2"><a href="the-binomial-and-related-distributions.html#cb173-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span>  <span class="co"># we perform n = 10 sets of k = 10 coin flips</span></span>
<span id="cb173-3"><a href="the-binomial-and-related-distributions.html#cb173-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.6</span> <span class="co"># assume the coin is slightly unfair</span></span>
<span id="cb173-4"><a href="the-binomial-and-related-distributions.html#cb173-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-5"><a href="the-binomial-and-related-distributions.html#cb173-5" aria-hidden="true" tabindex="-1"></a>x.bar   <span class="ot">&lt;-</span> <span class="dv">0</span><span class="sc">:</span>(n<span class="sc">*</span>k<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>k</span>
<span id="cb173-6"><a href="the-binomial-and-related-distributions.html#cb173-6" aria-hidden="true" tabindex="-1"></a>p.x.bar <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(n<span class="sc">*</span>x.bar,<span class="at">size=</span>n<span class="sc">*</span>k,<span class="at">prob=</span>p)</span>
<span id="cb173-7"><a href="the-binomial-and-related-distributions.html#cb173-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-8"><a href="the-binomial-and-related-distributions.html#cb173-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x.bar,p.x.bar,<span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;x.bar&quot;</span>,<span class="at">ylab=</span><span class="fu">expression</span>(p[X]<span class="sc">*</span><span class="st">&quot;(x.bar)&quot;</span>),<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">8</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xbarpmf"></span>
<img src="_main_files/figure-html/xbarpmf-1.png" alt="\label{fig:xbarpmf}Probability mass function for the sample mean of $n = 10$ iid binomial random variables, for $k = 10$ and $p = 0.6$." width="50%" />
<p class="caption">
Figure 3.5: Probability mass function for the sample mean of <span class="math inline">\(n = 10\)</span> iid binomial random variables, for <span class="math inline">\(k = 10\)</span> and <span class="math inline">\(p = 0.6\)</span>.
</p>
</div>
</div>
</div>
<div id="order-statistics" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Order Statistics<a href="the-binomial-and-related-distributions.html#order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets suppose that we have sampled <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> from
some arbitrary distribution. Previously, we
have summarized such data with the sample mean and the sample variance.
However, there are other summary statistics, some of which
are only calculable if we sort the data into ascending order:
<span class="math inline">\(\{X_{(1)},\ldots,X_{(n)}\}\)</span>. These are dubbed <em>order statistics</em> and
the <span class="math inline">\(j^{th}\)</span> order statistic is the samples <span class="math inline">\(j^{th}\)</span> smallest value
(i.e., the smallest-valued datum in the sample
is <span class="math inline">\(X_{(1)}\)</span> and the largest-valued datum is <span class="math inline">\(X_{(n)}\)</span>). Examples of
statistics based on ordering include
<span class="math display">\[\begin{align*}
\mbox{Range:}&amp; ~~X_{(n)} - X_{(1)} \\
\mbox{Median:}&amp; ~~X_{(n+1)/2} ~ \mbox{if $n$ is odd} \\
&amp; ~~(X_{n/2}+X_{(n+1)/2})/2 ~ \mbox{if $n$ is even} \,.
\end{align*}\]</span>
The most important point to keep in mind is that the probability mass
and density functions for order statistics differ from
the pmfs and pdfs for their constituent data. For instance, if we
sample <span class="math inline">\(n\)</span> data from a <span class="math inline">\(\mathcal{N}(0,1)\)</span> distribution, we
would not expect the minimum value to be distributed the same way;
if anything, the mean should become more negative, and the variance
should decrease, as <span class="math inline">\(n\)</span> increases.</p>
<p>So: why are we discussing order statistics here, in the middle of
a discussion of the binomial distribution? It is because
we can derive, e.g., the pdf for an order statistic of a
continuous distribution using the binomial pmf.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:order"></span>
<img src="figures/order.png" alt="\label{fig:order}If we have, e.g., a probability density function $f_X(x)$ whose domain is $[a,b]$, and we view success as sampling a datum less than a given value $x$, then the number of data sampled to the left of $x$ is a binomial random variable with number of trials $k$ and success probability $p = F_X(x)$." width="60%" />
<p class="caption">
Figure 3.6: If we have, e.g., a probability density function <span class="math inline">\(f_X(x)\)</span> whose domain is <span class="math inline">\([a,b]\)</span>, and we view success as sampling a datum less than a given value <span class="math inline">\(x\)</span>, then the number of data sampled to the left of <span class="math inline">\(x\)</span> is a binomial random variable with number of trials <span class="math inline">\(k\)</span> and success probability <span class="math inline">\(p = F_X(x)\)</span>.
</p>
</div>
<p>See Figure <a href="the-binomial-and-related-distributions.html#fig:order">3.6</a>. Without loss of generality, we can assume that
<span class="math inline">\(f_X(x) &gt; 0\)</span> for <span class="math inline">\(x \in [a,b]\)</span> and that we sample <span class="math inline">\(n\)</span> data from this distribution.
The number of data <span class="math inline">\(X\)</span> that have value less than some arbitrarily chosen <span class="math inline">\(x\)</span> is a random variable:
<span class="math display">\[
Y \sim \mbox{Binomial}(n,p=F_X(x))
\]</span>
What is the probability that the <span class="math inline">\(j^{th}\)</span> ordered datum has a value <span class="math inline">\(\leq x\)</span>?
Thats equivalent to asking for the probability that <span class="math inline">\(Y \geq j\)</span>, i.e., did we
see at least <span class="math inline">\(j\)</span> successes in <span class="math inline">\(n\)</span> trials?
<span class="math display">\[
P(X_{(j)} \leq x) = P(Y \geq j) = \sum_{i=j}^n \binom{n}{i} [F_X(x)]^j [1 - F_X(x)]^{n-j} \,.
\]</span>
This expression defines the cdf for the <span class="math inline">\(j^{th}\)</span> ordered datum: <span class="math inline">\(F_{(j)}(x)\)</span>.</p>
<p><strong>Recall:</strong> <em>a continuous distributions pdf is the derivative of its cdf.</em></p>
<p>Leaving aside algebraic details, we can write down the pdf for <span class="math inline">\(X_{(j)}\)</span>:
<span class="math display">\[
f_{(j)}(x) = \frac{d}{dx}F_{(j)}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x) [F_X(x)]^{j-1} [1 - F_X(x)]^{n-j} \,,
\]</span>
and write down simplified expressions for the pdfs for the minimum and maximum data values:
<span class="math display">\[
f_{(1)}(x) = n f_X(x) [1 - F_X(x)]^{n-1} ~~\mbox{and}~~ f_{(n)}(x) = n f_X(x) [F_X(x)]^{n-1} \,.
\]</span></p>
<hr />
<div id="distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Distribution of the Minimum Value Sampled from an Exponential Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for an exponential random variable is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and the expected value of <span class="math inline">\(X\)</span> is
<span class="math inline">\(E[X] = \theta\)</span>. What is the pdf for the smallest value among
<span class="math inline">\(n\)</span> iid data sampled from an exponential distribution? What is the expected value
for the smallest value?</p>
</blockquote>
<blockquote>
<p>First, if we do not immediately recall the cumulative distribution function
<span class="math inline">\(F_X(x)\)</span>, we derive it:
<span class="math display">\[
F_X(x) = \int_0^x \frac{1}{\theta} e^{-y/\theta} dy = 1 - e^{-x/\theta} \,.
\]</span>
Then we plug into the expression of the pdf of the minimum datum as given
above:
<span class="math display">\[\begin{align*}
f_{(1)}(x) &amp;= n \frac{1}{\theta} e^{-x/\theta} \left[ 1 - (1-e^{-x/\theta}) \right]^{n-1} \\
&amp;= n \frac{1}{\theta} e^{-x/\theta} e^{-(n-1)x/\theta} \\
&amp;= \frac{n}{\theta} e^{-nx/\theta} \,.
\end{align*}\]</span>
The expected value is thus
<span class="math display">\[
E[X_{(1)}] = \int_0^\infty x \frac{n}{\theta} e^{-nx/\theta} dx \,.
\]</span>
We recognize this as <em>almost</em> having the form of a gamma-function integral:
<span class="math display">\[
\Gamma(u) = \int_0^\infty x^{u-1} e^{-x} dx \,.
\]</span>
We affect a variable transformation <span class="math inline">\(y = nx/\theta\)</span>; for this transformation,
<span class="math inline">\(dy = (n/\theta)dx\)</span>,
and if <span class="math inline">\(x = 0\)</span> or <span class="math inline">\(\infty\)</span>, <span class="math inline">\(y = 0\)</span> or <span class="math inline">\(\infty\)</span> (meaning the integral bounds
are unchanged). Our new integral is
<span class="math display">\[
E[X] = \int_0^\infty \frac{\theta y}{n} \frac{n}{\theta} e^{-y} \frac{\theta}{n} dy = \frac{\theta}{n} \int_0^\infty y e^{-y} dy = \frac{\theta}{n} \Gamma(2) = \frac{\theta}{n} 1! = \frac{\theta}{n} = \frac{E[X]}{n} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The probability density function for a Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. The cdf for this distribution is thus
<span class="math display">\[
F_X(x) = \int_0^x 1 dy = x \,.
\]</span>
Lets assume that we sample <span class="math inline">\(n\)</span> iid data from this distribution, where <span class="math inline">\(n\)</span> is an
odd number. The index of the median value is thus <span class="math inline">\((n+1)/2\)</span>, and if we plug into
the general expression for the pdf of the <span class="math inline">\(m^{th}\)</span> ordered datum, we find that
<span class="math display">\[\begin{align*}
f_{(n+1)/2} &amp;= \frac{n!}{\left(\frac{n+1}{2}-1\right)!\left(n - \frac{n+1}{2}\right)!} \cdot 1 \cdot x^{\left(\frac{n+1}{2}\right)-1} \cdot (1-x)^{n - \left(\frac{n+1}{2}\right)} \\
&amp;= \frac{n!}{2\left(\frac{n-1}{2}\right)!} x^{\left(\frac{n-1}{2}\right)} (1-x)^{\left(\frac{n-1}{2}\right)} \,.
\end{align*}\]</span>
This is a beta distribution with parameters <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>. The median value has
expected value 1/2 and a variance that shrinks with <span class="math inline">\(n\)</span>.</p>
</blockquote>
</div>
</div>
<div id="point-estimation-2" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Point Estimation<a href="the-binomial-and-related-distributions.html#point-estimation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the first two chapters, we introduced a number of concepts related to point estimation, the
act of using statistics to make inferences about a population parameter <span class="math inline">\(\theta\)</span>. These concepts
include</p>
<ul>
<li>assessing estimator bias, variance, mean-squared error, and consistency;</li>
<li>utilizing the metric of Fisher information to determine the lower bound on the variance
for unbiased estimators (the Cramer-Rao Lower Bound, or CRLB); and</li>
<li>defining estimators via the maximum likelihood algorithm, which generates estimators
that are at least asymptotically unbiased and at least asymptotically reach the CRLB, and
which converge in distribution to normal random variables.</li>
</ul>
<p>We will review these concepts in the context of estimating population quantities for
binomial distributions below, in the body of the text and in examples.</p>
<p>Here, we introducing another means by which
to define an estimator. The
<em>minimum variance unbiased estimator</em> (or <em>MVUE</em>) is the estimator
that has the smallest variance among all unbiased estimators of <span class="math inline">\(\theta\)</span>.
(The MVUE may achieve the CRLB, but is not required to; if it doesnt, we
know that there is no estimator that achieves the CRLB. On the other hand,
if we have an unbiased estimator that achieves the CRLB, we know that that
estimator is the MVUE!)
The readers first thought might be well, why didnt we use this estimator
in the first placeafter all, MLE is not guaranteed to yield an
unbiased estimator, so why have we deferred discussing the MVUE?
The primary reasons are that MVUEs are sometimes not definable (i.e., one
can reach an insurmountable roadblock when carrying out the mathematical
derivation),
and unlike MLEs, they do not abide by the invariance property. (For
instance, if
<span class="math inline">\(\hat{\theta}_{MLE} = \bar{X}\)</span>, then <span class="math inline">\(\hat{\theta^2}_{MLE} = \bar{X}^2\)</span>, but
if <span class="math inline">\(\hat{\theta}_{MVUE} = \bar{X}\)</span>, it is not necessarily the case that
<span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.) <em>However, we should always at least
try to define the MVUE, because if we can, it will be at least equal the
performance of, if not do better than, the MLE, in terms of bias and/or
variance.</em></p>
<p>There are two steps to carry out when deriving the MVUE:</p>
<ol style="list-style-type: decimal">
<li>determining a <em>sufficient statistic</em> for <span class="math inline">\(\theta\)</span>; and</li>
<li>correcting any bias that is observed when we utilize the sufficient
statistic as our initial estimator.</li>
</ol>
<p>A sufficient statistic for a parameter <span class="math inline">\(\theta\)</span>
captures all information about <span class="math inline">\(\theta\)</span> contained in the sample.
In other words, any additional statistic, beyond the sufficient statistic,
will not provide any additional information about <span class="math inline">\(\theta\)</span>.
(This does not mean that a sufficient statistic is necessarily unique:
any function of a sufficient statistic is also a
sufficient statistic.
It just means that any additional statistic that is
<em>not</em> a function of the sufficient statistic will not help us when we try
to estimate <span class="math inline">\(\theta\)</span>.) Given this definition, it
stands to reason that either a sufficient statistic, or a function
of a sufficient statistic, will be the optimal estimator of <span class="math inline">\(\theta\)</span>.</p>
<p>The simplest way by which to identify a sufficient statistic is
by writing down the likelihood function and factorizing it into two
functions, one of which depends <em>only</em> on the observed data and the
other of which depends on both the observed data and the parameter of
interest:
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = g(\mathbf{x},\theta) \cdot h(\mathbf{x}) \,,
\]</span>
This is the so-called <em>factorization criterion</em>.
Within <span class="math inline">\(g(\mathbf{x},\theta)\)</span>, the data will appear as part of, e.g., a
summation (<span class="math inline">\(\sum_{i=1}^n x_i\)</span>) or a product (<span class="math inline">\(\prod_{i=1}^n x_i\)</span>), and
we identify that summation or product as the sufficient statistic.</p>
<p>Lets assume we are given a sample <span class="math inline">\(\mathbf{X} = \{X_1,\ldots,X_n\} \stackrel{iid}{\sim}\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>).
<!--
\smallskip\hrule

**Recall**: *a sufficient statistic for a parameter (or parameters) $\theta$ captures all information about $\theta$ contained in the sample. Sufficient statistics are most often identified by applying the factorization criterion to the likelihood function.*

\smallskip\hrule
-->
Factorizing the likelihood function yields the following:
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n \binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \underbrace{\left[ \prod_{i=1}^n \binom{k}{x_i} \right]}_{h(\mathbf{x})} \underbrace{p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i}}_{g(\sum_{i=1}^n x_i,p)} \,.
\]</span>
By inspecting the function <span class="math inline">\(g(\mathbf{x},\theta)\)</span>, we determine that
the sufficient statistic for binomially distributed data is <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>.</p>
<p>(The next step, in general, is to demonstrate whether the sufficient statistic is both
minimally sufficient and complete; it needs to be both so that we can use it to determine the MVUE.
It suffices to say here that the factorization criterion typically identifies statistics
that are minimally sufficient and complete, particularly in the context of this course.
See Chapter 7 for more details on how to determine if a sufficient
statistic is a minimally sufficient statistic and if it is complete. Note that complete statistics
are always minimally sufficient, but not necessarily vice-versa, so check for completeness first!)</p>
<!--
\smallskip\hrule

**Recall**: *the bias of an estimator is the difference between the average value of the estimates it generates, minus the true parameter value. If $E[\hat{\theta}-\theta] = 0$, then $\hat{\theta}$ is said to be an unbiased estimator.*

\smallskip\hrule
-->
<p>If <span class="math inline">\(U\)</span> is a minimally sufficient and complete statistic, and there is a function <span class="math inline">\(h(U)\)</span> that is an
unbiased estimator for <span class="math inline">\(\theta\)</span> and that depends on the data only through <span class="math inline">\(U\)</span>, then <span class="math inline">\(h(U)\)</span> is the MVUE for
<span class="math inline">\(\theta\)</span>. Here, given <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>, we need to find a function
<span class="math inline">\(h(\cdot)\)</span> such that <span class="math inline">\(E[h(U)] = p\)</span>. Earlier in this chapter, we determined that the
distribution for the sum of iid binomial random variables is Binomial(<span class="math inline">\(nk\)</span>,<span class="math inline">\(p\)</span>), and thus we
know that this distribution has expected value <span class="math inline">\(nkp\)</span>. Thus
<span class="math display">\[
E\left[U\right] = nkp ~\implies~ E\left[\frac{U}{nk}\right] = p ~\implies~ h(U) = \frac{U}{nk} = \frac{\bar{X}}{k} ~\mbox{is the MVUE for}~p \,.
\]</span></p>
<p>The variance of <span class="math inline">\(\hat{p}\)</span> is
<span class="math display">\[
V[\hat{p}] = V\left[\frac{\bar{X}}{k}\right] = \frac{1}{k^2}V[\bar{X}] = \frac{1}{k^2} \frac{V[X]}{n} = \frac{1}{k^2}\frac{kp(1-p)}{n} = \frac{p(1-p)}{nk} \,.
\]</span>
We know that this variance abides by the restriction
<span class="math display">\[
V[\hat{p}] \geq -\frac{1}{nE\left[\frac{d^2}{dp^2} \log p_X(X \vert p) \right]} = \frac{1}{nI(p)}
\]</span>
But is it equivalent to the lower bound itself, the CRLB? (Note that in particular
situations, the MVUE <em>may</em> have a variance larger than the CRLB; when this is the case,
unbiased estimators that achieve the CRLB simply do not exist.) For the binomial distribution,
<span class="math display">\[\begin{align*}
p_{X \vert P}(x \vert p) &amp;= \binom{k}{x} p^{x} (1-p)^{k-x} \\
\log p_{X \vert P}(x \vert p) &amp;= \log \binom{k}{x} + x \log p + (k-x) \log (1-p) \\
\frac{d}{dp} \log p_{X \vert P}(x \vert p) &amp;= 0 + \frac{x}{p} - \frac{k-x}{(1-p)} \\
\frac{d^2}{dp^2} \log p_{X \vert P}(X \vert p) &amp;= -\frac{x}{p^2} - \frac{k-x}{(1-p)^2} \\
E\left[\frac{d^2}{dp^2} \log p_{X \vert P}(X \vert p)\right] &amp;= -\frac{1}{p^2}E[X] - \frac{1}{(1-p)^2}E[k-X] \\
&amp;= -\frac{kp}{p^2}-\frac{k-kp}{(1-p)^2} \\
&amp;= -\frac{k}{p}-\frac{k}{1-p} = -\frac{k}{p(1-p)} \,.
\end{align*}\]</span>
The lower bound on the variance is thus <span class="math inline">\(p(1-p)/(nk)\)</span>, and so here the MVUE <em>does</em> achieve the CRLB.
We cannot define a better estimator for <span class="math inline">\(p\)</span> than <span class="math inline">\(\bar{X}/k\)</span>!</p>
<hr />
<div id="the-minimum-variance-unbiased-estimator-for-the-exponential-mean" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> The Minimum Variance Unbiased Estimator for the Exponential Mean<a href="the-binomial-and-related-distributions.html#the-minimum-variance-unbiased-estimator-for-the-exponential-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The exponential distribution is
<span class="math display">\[
f_X(x) = \frac{1}{\theta} \exp\left(-\frac{x}{\theta}\right) \,,
\]</span>
where <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>, and where <span class="math inline">\(E[X] = \theta\)</span> and <span class="math inline">\(V[X] = \theta^2\)</span>.
Lets assume that we have <span class="math inline">\(n\)</span> iid data drawn from this distribution.
Can we define the MVUE for <span class="math inline">\(\theta\)</span>? For <span class="math inline">\(\theta^2\)</span>?</p>
</blockquote>
<blockquote>
<p>The likelihood function is
<span class="math display">\[
\mathcal{L}(\theta \vert \mathbf{x}) = \prod_{i=1}^n f_X(x_i \vert \theta) = \frac{1}{\theta^n}\exp\left(-\frac{1}{\theta}\sum_{i=1}^n x_i \right) = h(\mathbf{x}) \cdot g(\theta,\mathbf{x}) \,.
\]</span>
Here, there are no terms that are functions of only the data, so <span class="math inline">\(h(\mathbf{x}) = 1\)</span> and
thus the sufficient statistic is <span class="math inline">\(U = \sum_{i=1}^n x_i\)</span>. We compute the expected value
of <span class="math inline">\(U\)</span>:
<span class="math display">\[
E[U] = E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \theta = n\theta \,.
\]</span>
The expected value of <span class="math inline">\(U\)</span> is not <span class="math inline">\(\theta\)</span>, so <span class="math inline">\(U\)</span> is not unbiasedbut we can see
immediately that <span class="math inline">\(E[U/n] = \theta\)</span>, so that <span class="math inline">\(U/n\)</span> is unbiased. Thus the MVUE for <span class="math inline">\(\theta\)</span>
is <span class="math inline">\(\hat{\theta}_{MVUE} = U/n = \bar{X}\)</span>.</p>
</blockquote>
<blockquote>
<p>Note that the MVUE does not possess the invariance propertyit is not necessarily the
case that <span class="math inline">\(\hat{\theta^2}_{MVUE} = \bar{X}^2\)</span>.</p>
</blockquote>
<blockquote>
<p>Lets propose a function of <span class="math inline">\(U\)</span> and see if we can use that to define <span class="math inline">\(\hat{\theta^2}_{MVUE}\)</span>:
<span class="math inline">\(h(U) = U^2/n^2 = \bar{X}^2\)</span>. (To be clear, we are simply proposing a function and seeing if
it helps us define what we are looking for. It might not. If not, we can try again with
another function of <span class="math inline">\(U\)</span>.) Utilizing what we know about the sample mean, we can write down that
<span class="math display">\[
E[\bar{X}^2] = V[\bar{X}] + (E[\bar{X}])^2 = \frac{V[X]}{n} + (E[X])^2 = \frac{\theta^2}{n}+\theta^2 = \theta^2\left(\frac{1}{n} + 1\right) \,.
\]</span>
So <span class="math inline">\(\bar{X}^2\)</span> itself is <em>not</em> an unbiased estimator of <span class="math inline">\(\theta^2\)</span>but we can see that
<span class="math inline">\(\bar{X}^2/(1/n+1)\)</span> is. Hence
<span class="math display">\[
\hat{\theta^2}_{MVUE} = \frac{\bar{X}^2}{\left(\frac{1}{n}+1\right)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="sufficient-statistics-for-the-normal-distribution" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Sufficient Statistics for the Normal Distribution<a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>If we have <span class="math inline">\(n\)</span> iid data drawn from a normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and
unknown variance <span class="math inline">\(\sigma^2\)</span>, then the factorized likelihood is
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i^2, \sum x_i,\mu,\sigma)} \cdot \underbrace{1}_{h(\mathbf{x})} \,.
\]</span>
Here, we identify <span class="math inline">\(\sum x_i^2\)</span> and <span class="math inline">\(\sum x_i\)</span> as <em>joint</em> sufficient statistics: we
need two pieces of information to jointly estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. (To be
clear: it is not necessarily the case that one of the parameters matches up to
one of the sufficient statisticsrather, the two statistics are jointly
sufficient for estimation.) We thus cannot proceed further to define an MVUE for
<span class="math inline">\(\mu\)</span> or for <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<blockquote>
<p>(Note that we <em>can</em> proceed if we happen to know either <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma^2\)</span>; if one
of these values is fixed, then we can turn the crank so as to find the MVUE for
the other parameter.)</p>
</blockquote>
<hr />
</div>
<div id="the-maximum-likelihood-estimator-for-the-binomial-proportion" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> The Maximum Likelihood Estimator for the Binomial Proportion<a href="the-binomial-and-related-distributions.html#the-maximum-likelihood-estimator-for-the-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Recall</strong>: <em>the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function is the maximum likelihood estimate, or MLE, for <span class="math inline">\(\theta\)</span>. The maximum is found by taking the (partial) derivative of the (log-)likelihood function with respect to <span class="math inline">\(\theta\)</span>, setting the result to zero, and solving for <span class="math inline">\(\theta\)</span>. That solution is the maximum likelihood estimate <span class="math inline">\(\hat{\theta}_{MLE}\)</span>.</em></p>
<blockquote>
<p>The likelihood function for <span class="math inline">\(n\)</span> iid binomial random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> is
<span class="math display">\[
\mathcal{L}(p \vert \mathbf{x}) = \prod_{i=1}^n \binom{k}{x_i} p^{x_i} (1-p)^{k-x_i} = \left[\prod_{i=1}^n \binom{k}{x_i} \right] p^{\sum_{i=1}^n x_i} (1-p)^{nk-\sum_{i=1}^n x_i} \,.
\]</span>
Recall that the value <span class="math inline">\(\hat{p}_{MLE}\)</span> that maximizes <span class="math inline">\(\mathcal{L}(p \vert x)\)</span> also maximizes
<span class="math inline">\(\ell(p \vert x) = \log \mathcal{L}(p \vert x)\)</span>, which is considerably easier to work with:
<span class="math display">\[\begin{align*}
\ell(p \vert \mathbf{x}) &amp;= \sum_{i=1}^n \log \left[\binom{k}{x_i}\right] + \left(\sum_{i=1}^n x_i\right) \log p + \left(nk - \sum_{i=1}^n x_i\right) \log (1-p) \\
\frac{d}{dp} \ell(p \vert \mathbf{x}) &amp;= 0 + \frac{1}{p} \sum_{i=1}^n x_i - \frac{1}{1-p} \left(nk - \sum_{i=1}^n x_i\right) = 0 \,.
\end{align*}\]</span>
After rearranging terms, we find that
<span class="math display">\[
p = \frac{1}{nk}\sum_{i=1}^n x_i ~\implies~ \hat{p}_{MLE} = \frac{\bar{X}}{k} \,.
\]</span>
The MLE matches the MVUE, thus we know that the MLE is unbiased and we know that it achieves the CRLB.</p>
</blockquote>
<blockquote>
<p>A useful property of MLEs is the invariance property, whereby the MLE for a function of <span class="math inline">\(\theta\)</span>
is given by applying the same function to the MLE itself. Thus</p>
</blockquote>
<blockquote>
<ul>
<li>the MLE for the population mean <span class="math inline">\(E[X] = \mu = kp\)</span> is <span class="math inline">\(\bar{X}\)</span>; and</li>
<li>the MLE for the population variance <span class="math inline">\(V[X] = \sigma^2 = kp(1-p)\)</span> is <span class="math inline">\(\bar{X}(1-\bar{X}/k)\)</span>.</li>
</ul>
</blockquote>
<blockquote>
<p>Last, we recall that asymptotically, <span class="math inline">\(\hat{p}_{MLE}\)</span> converges in distribution to a normal random variable:
<span class="math display">\[
\hat{p}_{MLE} \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}\left(p,\frac{1}{nI(p)} = \frac{p(1-p)}{nk}\right) \,.
\]</span></p>
</blockquote>
</div>
</div>
<div id="confidence-intervals-2" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Confidence Intervals<a href="the-binomial-and-related-distributions.html#confidence-intervals-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall:</strong> <em>a confidence interval is a random interval
<span class="math inline">\([\hat{\theta}_L,\hat{\theta}_U]\)</span> that overlaps (or covers) the
true value <span class="math inline">\(\theta\)</span> with probability</em>
<span class="math display">\[
P\left( \hat{\theta}_L \leq \theta \leq \hat{\theta}_U \right) = 1 - \alpha \,,
\]</span>
<em>where <span class="math inline">\(1 - \alpha\)</span> is the confidence coefficient. We determine
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_H\)</span> by, e.g., solving for the root
<span class="math inline">\(\theta_q\)</span> in each of the following equations:</em>
<span class="math display">\[\begin{align*}
F_Y(y_{\rm obs} \vert \theta_{\alpha/2}) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{\rm obs} \vert \theta_{1-\alpha/2}) - \left(1-\frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
<em>The construction of confidence intervals thus relies on knowing the
sampling distribution of the adopted statistic <span class="math inline">\(Y\)</span>. One maps
<span class="math inline">\(\theta_{\alpha/2}\)</span> and <span class="math inline">\(\theta_{1-\alpha/2}\)</span> to
<span class="math inline">\(\hat{\theta}_L\)</span> and <span class="math inline">\(\hat{\theta}_H\)</span> by taking into account how
the expected value <span class="math inline">\(E[Y]\)</span> varies with the parameter <span class="math inline">\(\theta\)</span>. (See the
table in section 14 of Chapter 1.)</em></p>
<p>The only new element to consider here regarding the construction of
confidence intervals is that now the sampling distribution for our
adopted statistic (<span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span>, where the <span class="math inline">\(X_i\)</span>s are
iid samples from a binomial distribution) is <em>discrete</em> as opposed to
<em>continuous</em>. As it turns out, this does not impact our use of
<code>uniroot()</code>: even though <span class="math inline">\(F_Y(y_{\rm obs})\)</span> is discrete, the proportion
<span class="math inline">\(p\)</span> itself is still continuous, and we can tune its value so that
<span class="math inline">\(F_Y(y_{\rm obs})\)</span> will match, e.g., <span class="math inline">\(\alpha/2\)</span> or <span class="math inline">\(1-\alpha/2\)</span> with
arbitrary precision. What the discreteness of the sampling distribution
<em>does</em> impact is the coverage, or the proportion of intervals that overlap
the true value. We illustrate the impact of discreteness on coverage
below in an example; in short, the smaller the value of <span class="math inline">\(nk\)</span>, the further
the estimates of coverage deviate from expectation (both positively and
negatively). When the number of data are small, use simulations to
estimate the true coverage!</p>
<hr />
<p>Before going to the examples, we note here that
discrete distributions like the binomial have historically been
difficult to work with analytically. Because of this,
a number of algorithms have been developed through the years for
constructing confidence intervals for binomial proportions.
It is our opinion that there is no particular reason to utilize <em>any</em> of
these algorithms when one can compute exact intervals in the manner we
do below and can utilize simulations to assess interval coverage.
However, given the ubiquity of the most commonly seen
approximation interval, the <em>Wald interval</em>, we will illustrate
its use in an example below.</p>
<hr />
<div id="confidence-interval-for-the-binomial-proportion" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Confidence Interval for the Binomial Proportion<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As we did in Chapter 2, below we will adapt the general-purpose <code>R</code> code
for constructing confidence intervals that we provide in Appendix B
to a specific problem: here, that problem is putting a confidence interval
on the binomial proportion <span class="math inline">\(p\)</span>. Assume that we sample <span class="math inline">\(n\)</span> iid data from
a binomial distribution with number of trials <span class="math inline">\(k\)</span> and proportion <span class="math inline">\(p\)</span>.
Then, as shown above, <span class="math inline">\(Y = \sum_{i=1}^n X_i \sim\)</span> Binom(<span class="math inline">\(nk,p\)</span>);
our observed test statistic is <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i\)</span>.
For this statistic, <span class="math inline">\(E[Y] = nkp\)</span> increases with <span class="math inline">\(p\)</span>, so
<span class="math inline">\(p_{1-\alpha/2}\)</span> maps to the lower bound, while <span class="math inline">\(p_{\alpha/2}\)</span> maps to
the upper bound.</p>
</blockquote>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="the-binomial-and-related-distributions.html#cb174-1" aria-hidden="true" tabindex="-1"></a>confint <span class="ot">&lt;-</span> <span class="cf">function</span>(y.obs,nk,<span class="at">alpha=</span><span class="fl">0.05</span>)</span>
<span id="cb174-2"><a href="the-binomial-and-related-distributions.html#cb174-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb174-3"><a href="the-binomial-and-related-distributions.html#cb174-3" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">&lt;-</span> <span class="cf">function</span>(prob,y.obs,nk,q)</span>
<span id="cb174-4"><a href="the-binomial-and-related-distributions.html#cb174-4" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb174-5"><a href="the-binomial-and-related-distributions.html#cb174-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pbinom</span>(y.obs,<span class="at">size=</span>nk,<span class="at">prob=</span>prob)<span class="sc">-</span>q</span>
<span id="cb174-6"><a href="the-binomial-and-related-distributions.html#cb174-6" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb174-7"><a href="the-binomial-and-related-distributions.html#cb174-7" aria-hidden="true" tabindex="-1"></a>  lo <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),y.obs,nk,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb174-8"><a href="the-binomial-and-related-distributions.html#cb174-8" aria-hidden="true" tabindex="-1"></a>  hi <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),y.obs,nk,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb174-9"><a href="the-binomial-and-related-distributions.html#cb174-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(lo,hi))</span>
<span id="cb174-10"><a href="the-binomial-and-related-distributions.html#cb174-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb174-11"><a href="the-binomial-and-related-distributions.html#cb174-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-12"><a href="the-binomial-and-related-distributions.html#cb174-12" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb174-13"><a href="the-binomial-and-related-distributions.html#cb174-13" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb174-14"><a href="the-binomial-and-related-distributions.html#cb174-14" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb174-15"><a href="the-binomial-and-related-distributions.html#cb174-15" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb174-16"><a href="the-binomial-and-related-distributions.html#cb174-16" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n,<span class="at">size=</span>k,<span class="at">prob=</span>p)</span>
<span id="cb174-17"><a href="the-binomial-and-related-distributions.html#cb174-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The observed test statistic is&quot;</span>,<span class="fu">sum</span>(X),<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## The observed test statistic is 22</code></pre>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="the-binomial-and-related-distributions.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">sum</span>(X),n<span class="sc">*</span>k)</span></code></pre></div>
<pre><code>## [1] 0.2607037 0.5010387</code></pre>
<blockquote>
<p>We find that the interval is <span class="math inline">\([\hat{p}_L,\hat{p}_H] = [0.261,0.501]\)</span>, which overlaps the true value of 0.4.
(See Figure <a href="the-binomial-and-related-distributions.html#fig:binci">3.7</a>.)
Note that, unlike in Chapter 2, the interval over which we
search for the root is [0,1], which is the range of possible
values for <span class="math inline">\(p\)</span>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binci"></span>
<img src="_main_files/figure-html/binci-1.png" alt="\label{fig:binci}Probability mass functions for binomial distributions with $n=12$ and $k=5$ and (left) $p=0.261$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.261$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" /><img src="_main_files/figure-html/binci-2.png" alt="\label{fig:binci}Probability mass functions for binomial distributions with $n=12$ and $k=5$ and (left) $p=0.261$ and (right) $p=0.501$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 22$ successes and we want to construct a 95\% confidence interval. $p=0.261$ is the smallest value of $p$ such that $F_Y^{-1}(0.975) = 22$, while $p=0.501$ is the largest value of $p$ such that $F_Y^{-1}(0.025) = 22$." width="45%" />
<p class="caption">
Figure 3.7: Probability mass functions for binomial distributions with <span class="math inline">\(n=12\)</span> and <span class="math inline">\(k=5\)</span> and (left) <span class="math inline">\(p=0.261\)</span> and (right) <span class="math inline">\(p=0.501\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 22\)</span> successes and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.261\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 22\)</span>, while <span class="math inline">\(p=0.501\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 22\)</span>.
</p>
</div>
<blockquote>
<p>In Chapter 2, we find that when we are constructing exact intervals
with continuous sampling distributions (exact meaning that the sampling
distribution is the correct one, not an approximation), the coverage
is what we expect it to be: in any finite simulation, the proportion of
intervals that overlap (or cover) the true value is close to <span class="math inline">\(1-\alpha\)</span>.
Here, we are dealing with a <em>discrete</em> sampling distribution, and we
expect that discreteness will affect the coverage. Lets check to see
if our expectation is correct. In Figure <a href="the-binomial-and-related-distributions.html#fig:covsim">3.8</a>, we show the
results of simulating data to assess coverage for <span class="math inline">\(k = 5\)</span> and for
<span class="math inline">\(n = 12\)</span>, 120, and 1200 (from left to right). The solid blue lines
indicate the expected coverage, 0.95, while the dashed blue lines
indicate the range of values in which we expect to see <span class="math inline">\(\approx\)</span> 95% of
the simulated values. We determine the placement of the dashed blue lines
as follows: we simulate <span class="math inline">\(k = 10000\)</span> intervals, where the expected
coverage is <span class="math inline">\(p = 0.95\)</span>. Let <span class="math inline">\(X\)</span> be the number of simulations
in which the confidence intervals that actually overlap the true
parameter value. Then <span class="math inline">\(E[X] = kp\)</span> (here, 9500),
<span class="math inline">\(V[X] = kp(1-p)\)</span> (here, 475), and <span class="math inline">\(\sigma_X = \sqrt{V[X]}\)</span> (here, 21.79).
The dashed lines are thus placed at the values 0.9500 <span class="math inline">\(\pm\)</span> 0.0043.
As we can see, the proportion of simulated values observed between the
lines increases as <span class="math inline">\(n\)</span> increases, i.e., as the effect of discreteness
decreases.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:covsim"></span>
<img src="_main_files/figure-html/covsim-1.png" alt="\label{fig:covsim}Results of simulations (described in the text) for $k = 5$ and $n = 12$ (left), 120 (center), and 1200 (right). The expected proportion of simulation results laying between the two dashed lines is $\approx$ 95 percent; we see that as $n$ increases, i.e., as the effect of sampling distribution discreteness decreases, we achieve that expectation." width="30%" /><img src="_main_files/figure-html/covsim-2.png" alt="\label{fig:covsim}Results of simulations (described in the text) for $k = 5$ and $n = 12$ (left), 120 (center), and 1200 (right). The expected proportion of simulation results laying between the two dashed lines is $\approx$ 95 percent; we see that as $n$ increases, i.e., as the effect of sampling distribution discreteness decreases, we achieve that expectation." width="30%" /><img src="_main_files/figure-html/covsim-3.png" alt="\label{fig:covsim}Results of simulations (described in the text) for $k = 5$ and $n = 12$ (left), 120 (center), and 1200 (right). The expected proportion of simulation results laying between the two dashed lines is $\approx$ 95 percent; we see that as $n$ increases, i.e., as the effect of sampling distribution discreteness decreases, we achieve that expectation." width="30%" />
<p class="caption">
Figure 3.8: Results of simulations (described in the text) for <span class="math inline">\(k = 5\)</span> and <span class="math inline">\(n = 12\)</span> (left), 120 (center), and 1200 (right). The expected proportion of simulation results laying between the two dashed lines is <span class="math inline">\(\approx\)</span> 95 percent; we see that as <span class="math inline">\(n\)</span> increases, i.e., as the effect of sampling distribution discreteness decreases, we achieve that expectation.
</p>
</div>
<hr />
</div>
<div id="confidence-interval-for-the-negative-binomial-proportion" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Confidence Interval for the Negative Binomial Proportion<a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have performed <span class="math inline">\(n = 10\)</span> separate negative binomial
trials, each with a target number of successes <span class="math inline">\(s\)</span>,
and recorded the number of failures <span class="math inline">\(X_1,\ldots,X_{n}\)</span> for
each. Further, assume that the success proportion is <span class="math inline">\(p\)</span>. Below we will
show how to compute the confidence interval for <span class="math inline">\(p\)</span>, but before we start,
we recall that <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> is a negative binomially distributed
random variable for <span class="math inline">\(ns\)</span> successes and success proportion <span class="math inline">\(p\)</span>.
Here, <span class="math inline">\(E[Y] = s(1-p)/p\)</span>as <span class="math inline">\(p\)</span> increases, <span class="math inline">\(E[Y]\)</span> <em>decreases</em>. Thus
when we adapt the confidence interval code we use for binomially
distributed data, we need to switch the mapping of <span class="math inline">\(p_{1-\alpha/2}\)</span> and
<span class="math inline">\(p_{\alpha/2}\)</span> to point to the <em>upper</em> and <em>lower</em> bounds, respectively.</p>
</blockquote>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="the-binomial-and-related-distributions.html#cb178-1" aria-hidden="true" tabindex="-1"></a>confint <span class="ot">&lt;-</span> <span class="cf">function</span>(y.obs,ns,<span class="at">alpha=</span><span class="fl">0.05</span>)</span>
<span id="cb178-2"><a href="the-binomial-and-related-distributions.html#cb178-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb178-3"><a href="the-binomial-and-related-distributions.html#cb178-3" aria-hidden="true" tabindex="-1"></a>  f <span class="ot">&lt;-</span> <span class="cf">function</span>(prob,y.obs,ns,q)</span>
<span id="cb178-4"><a href="the-binomial-and-related-distributions.html#cb178-4" aria-hidden="true" tabindex="-1"></a>  {     </span>
<span id="cb178-5"><a href="the-binomial-and-related-distributions.html#cb178-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">pnbinom</span>(y.obs,<span class="at">size=</span>ns,<span class="at">prob=</span>prob)<span class="sc">-</span>q</span>
<span id="cb178-6"><a href="the-binomial-and-related-distributions.html#cb178-6" aria-hidden="true" tabindex="-1"></a>  }     </span>
<span id="cb178-7"><a href="the-binomial-and-related-distributions.html#cb178-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note the switch in the mapping! hi comes first here.</span></span>
<span id="cb178-8"><a href="the-binomial-and-related-distributions.html#cb178-8" aria-hidden="true" tabindex="-1"></a>  hi <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),y.obs,ns,<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb178-9"><a href="the-binomial-and-related-distributions.html#cb178-9" aria-hidden="true" tabindex="-1"></a>  lo <span class="ot">&lt;-</span> <span class="fu">uniroot</span>(f,<span class="at">interval=</span><span class="fu">c</span>(<span class="fl">0.0001</span>,<span class="dv">1</span>),y.obs,ns,alpha<span class="sc">/</span><span class="dv">2</span>)<span class="sc">$</span>root</span>
<span id="cb178-10"><a href="the-binomial-and-related-distributions.html#cb178-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(lo,hi))</span>
<span id="cb178-11"><a href="the-binomial-and-related-distributions.html#cb178-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb178-12"><a href="the-binomial-and-related-distributions.html#cb178-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-13"><a href="the-binomial-and-related-distributions.html#cb178-13" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb178-14"><a href="the-binomial-and-related-distributions.html#cb178-14" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb178-15"><a href="the-binomial-and-related-distributions.html#cb178-15" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb178-16"><a href="the-binomial-and-related-distributions.html#cb178-16" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.4</span></span>
<span id="cb178-17"><a href="the-binomial-and-related-distributions.html#cb178-17" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnbinom</span>(n,<span class="at">size=</span>s,<span class="at">prob=</span>p)</span>
<span id="cb178-18"><a href="the-binomial-and-related-distributions.html#cb178-18" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(<span class="fu">sum</span>(X),n<span class="sc">*</span>s)</span></code></pre></div>
<pre><code>## [1] 0.3255305 0.4822869</code></pre>
<blockquote>
<p>The confidence interval is <span class="math inline">\([\hat{p}_L,\hat{p}_H] = [0.326,0.482]\)</span>,
which overlaps the true value 0.4. We note that in the code, we change
the lower bound on the interval from 0 (in the binomial case) to
0.0001 (something suitably small but non-zero): a success proportion
of 0 maps to an infinite number of failures, which <code>R</code> cannot tolerate!</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nbinci"></span>
<img src="_main_files/figure-html/nbinci-1.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions with $n=12$ and $s=5$ and (left) $p=0.326$ and (right) $p=0.482$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.482$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" /><img src="_main_files/figure-html/nbinci-2.png" alt="\label{fig:nbinci}Probability mass functions for negative binomial distributions with $n=12$ and $s=5$ and (left) $p=0.326$ and (right) $p=0.482$. We observe $y_{\rm obs} = \sum_{i=1}^n x_i = 88$ failures and we want to construct a 95\% confidence interval. $p=0.326$ is the smallest value of $p$ such that $F_Y^{-1}(0.025) = 88$, while $p=0.482$ is the largest value of $p$ such that $F_Y^{-1}(0.975) = 88$." width="45%" />
<p class="caption">
Figure 3.9: Probability mass functions for negative binomial distributions with <span class="math inline">\(n=12\)</span> and <span class="math inline">\(s=5\)</span> and (left) <span class="math inline">\(p=0.326\)</span> and (right) <span class="math inline">\(p=0.482\)</span>. We observe <span class="math inline">\(y_{\rm obs} = \sum_{i=1}^n x_i = 88\)</span> failures and we want to construct a 95% confidence interval. <span class="math inline">\(p=0.326\)</span> is the smallest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.025) = 88\)</span>, while <span class="math inline">\(p=0.482\)</span> is the largest value of <span class="math inline">\(p\)</span> such that <span class="math inline">\(F_Y^{-1}(0.975) = 88\)</span>.
</p>
</div>
<hr />
</div>
<div id="large-sample-confidence-intervals-for-the-binomial-proportion" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Large-Sample Confidence Intervals for the Binomial Proportion<a href="the-binomial-and-related-distributions.html#large-sample-confidence-intervals-for-the-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have sampled <span class="math inline">\(n\)</span> iid binomial variables with number
of trials <span class="math inline">\(k\)</span> and success proportion <span class="math inline">\(p\)</span>.
When <span class="math inline">\(k\)</span> is sufficiently large, we can assume that
<span class="math inline">\(\bar{X}\)</span> has a distribution whose shape is approximately
that of a normal distribution, with mean <span class="math inline">\(E[\bar{X}] = kp\)</span> and with
variance and standard error
<span class="math display">\[
V[\bar{X}] = \frac{kp(1-p)}{n} ~~~ \mbox{and} ~~~ se(\bar{X}) = \sqrt{V[\bar{X}]} = \sqrt{\frac{kp(1-p)}{n} \,.
\]</span>
Furthermore, we can assume
that <span class="math inline">\(\hat{p} = \bar{X}/k\)</span> is approximately normally distributed, with
mean <span class="math inline">\(p\)</span>, variance <span class="math inline">\(V[\hat{p}] = V[\bar{X}]/k^2 = p(1-p)/nk\)</span>, and
standard error <span class="math inline">\(\sqrt{p(1-p)/nk}\)</span>. Given this information, it is simple to
express, e.g., an approximate two-sided <span class="math inline">\(100(1-\alpha)\)</span>% confidence
interval for <span class="math inline">\(p\)</span>:
<span class="math display">\[
\hat{p} \pm z_{1-\alpha/2} se(\hat{p}) ~~ \Rightarrow ~~ \left[ \hat{p} - z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \, , \, \hat{p} + z_{1-\alpha/2} \sqrt{\frac{p(1-p)}{nk}} \right] \,,
\]</span>
where <span class="math inline">\(z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)\)</span>. However, we dont
actually know the true value of <span class="math inline">\(p\)</span>so we plug in <span class="math inline">\(p = \hat{p}\)</span>.</p>
</blockquote>
<blockquote>
<p>This is the so-called <em>Wald interval</em>
that is typically provided to students in introductory statistics courses,
although it is typically provided assuming <span class="math inline">\(n = 1\)</span> and assuming that
<span class="math inline">\(\alpha = 0.05\)</span>:
<span class="math display">\[
\left[ \hat{p} - 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \, , \, \hat{p} + 1.96 \sqrt{\frac{\hat{p}(1-\hat{p})}{k}} \right] \,,
\]</span>
where in this case <span class="math inline">\(\hat{p} = X/k\)</span>.</p>
</blockquote>
</div>
</div>
<div id="hypothesis-testing-1" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Hypothesis Testing<a href="the-binomial-and-related-distributions.html#hypothesis-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Recall</strong>: <em>a hypothesis test is a framework to make an inference about the value of a population parameter <span class="math inline">\(\theta\)</span>. The null hypothesis <span class="math inline">\(H_o\)</span> is that <span class="math inline">\(\theta = \theta_o\)</span>, while possible alternatives <span class="math inline">\(H_a\)</span> are <span class="math inline">\(\theta \neq \theta_o\)</span> (two-sided test), <span class="math inline">\(\theta &gt; \theta_o\)</span> (upper-tail test), and <span class="math inline">\(\theta &lt; \theta_o\)</span> (lower-tail test). For, e.g., a two-tail test, we reject the null hypothesis if the observed test statistic <span class="math inline">\(y_{\rm obs}\)</span> falls outside the bounds given by <span class="math inline">\(y_{\alpha/2}\)</span> and <span class="math inline">\(y_{1-\alpha/2}\)</span>, which are solutions to the equations</em>
<span class="math display">\[\begin{align*}
F_Y(y_{\alpha/2} \vert \theta_o) - \frac{\alpha}{2} &amp;= 0 \\
F_Y(y_{1-\alpha/2} \vert \theta_o) - \left(1 - \frac{\alpha}{2}\right) &amp;= 0 \,.
\end{align*}\]</span>
<em>The determination of rejection region boundaries thus relies on knowing the sampling distribution of the adopted statistic <span class="math inline">\(Y\)</span>. One maps, e.g., <span class="math inline">\(y_{\alpha/2}\)</span> to either the lower or upper rejection region boundary by taking into account how the expected value <span class="math inline">\(E[Y]\)</span> varies with the parameter <span class="math inline">\(\theta\)</span>. (See the table in section 15 of Chapter 1.) The hypothesis test framework only allows us to make a decision about the null hypothesis; nothing is proven.</em></p>
<p>In the previous chapter, we utilized <span class="math inline">\(\bar{X}\)</span> when testing hypotheses about the
normal mean <span class="math inline">\(\mu\)</span>. This is a principled choice for a test statistic<span class="math inline">\(-\)</span>after all,
<span class="math inline">\(\bar{X}\)</span> is the MLE for <span class="math inline">\(\mu-\)</span>but we do not yet know whether or not we can
choose a better one. Can we differentiate hypotheses more easily if we use a
test statistic other than <span class="math inline">\(\bar{X}\)</span>?</p>
<p>To help answer this question, we now introduce a method for
defining the <em>most powerful</em> test of a <em>simple</em> null hypothesis versus
a simple alternative hypothesis:
<span class="math display">\[
H_o : \theta = \theta_o ~~\mbox{and}~~ H_a : \theta = \theta_a \,.
\]</span>
Note that the word simple has a precise meaning here: it means that
when we set <span class="math inline">\(\theta\)</span> to a particular value,
we are completely fixing the shape and location of the
pmf or pdf from which data are sampled. If, for instance, we are dealing
with a normal distribution with unknown variance <span class="math inline">\(\sigma^2\)</span>, the hypothesis
<span class="math inline">\(\mu = \mu_o\)</span> would not be simple, since the width of the pdf
can still vary: the shape is not completely fixed.
(The hypothesis <span class="math inline">\(\mu = \mu_o\)</span> with variance unknown is dubbed a <em>composite</em>
hypothesis. We will examine how to work with composite hypotheses in the next chapter.)
For a given test level <span class="math inline">\(\alpha\)</span>, the <em>Neyman-Pearson lemma</em> states that the
test that maximizes the power has a rejection region of the form
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert \mathbf{x})}{\mathcal{L}(\theta_a \vert \mathbf{x})} \leq c(\alpha) \,,
\]</span>
where <span class="math inline">\(c\)</span> is a constant whose value depends on <span class="math inline">\(\alpha\)</span> that we have to
determine. While this formulation initially
appears straightforward, it is in fact not necessarily clear how to
derive <span class="math inline">\(c(\alpha)\)</span>. To do
so, we will make use of sufficient statistics and their sampling distributions.
(And in fact, it will turn out that we dont actually determine the quantity
<span class="math inline">\(c(\alpha)\)</span> at all! This will make more sense below.)</p>
<p>Lets illustrate how we would use the NP lemma to
construct a hypothesis test for <span class="math inline">\(H_o : \theta = \theta_o\)</span>
versus <span class="math inline">\(H_a : \theta = \theta_a\)</span> assuming that we observe <span class="math inline">\(n\)</span> iid data
<span class="math inline">\(\{X_1,\ldots,X_n\}\)</span> that are drawn from some distribution
<span class="math inline">\(P(\theta)\)</span> (where <span class="math inline">\(\theta\)</span> is the one and only freely varying parameter).
The steps are as follows:</p>
<ol style="list-style-type: decimal">
<li>we factorize the likelihood <span class="math inline">\(\mathcal{L}(\theta \vert \mathbf{x})\)</span>
to determine the sufficient statistic <span class="math inline">\(U\)</span>;</li>
<li>we write down the likelihood ratio, which is a function of <span class="math inline">\(U\)</span> (and
we simplify the ratio as much as possible by, e.g., cancelling constants
that appear in both the numerator and denominator);</li>
<li>we use the ratio to determine, given the
specific values <span class="math inline">\(\theta_o\)</span> and <span class="math inline">\(\theta_a\)</span>, how <span class="math inline">\(U\)</span> must change to drive the
ratio to zero; that tells us whether the rejection
region is <span class="math inline">\(U \leq u_\alpha\)</span> or <span class="math inline">\(U \geq u_{1-\alpha}\)</span>; and last</li>
<li>we use the sampling distribution for <span class="math inline">\(U\)</span> to determine <span class="math inline">\(u_\alpha\)</span>
or <span class="math inline">\(u_{1-\alpha}\)</span>.</li>
</ol>
<p>Lets illustrate the process using the binomial distribution.
Our goal is to construct
the most powerful test of the simple hypotheses <span class="math inline">\(H_o: p = p_o\)</span> and
<span class="math inline">\(H_a: p = p_a\)</span>, where <span class="math inline">\(p_a &gt; p_o\)</span>. For simplicity, lets
initially assume that we observe a single datum <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k,p\)</span>).</p>
<p>We start by factorizing the likelihood to determine the sufficient statistic.
This yields
<span class="math display">\[
\mathcal{L}(p \vert x) = p^x (1-p)^{1-x} = g(p,x) \times 1 \,.
\]</span>
Thus the sufficient statistic is, as we expect, <span class="math inline">\(U = X\)</span>.</p>
<p>The next step is to write down the likelihood ratio:
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert x)}{\mathcal{L}(\theta_a \vert x)} = \frac{\binom{k}{x} p_o^x (1-p_o)^{k-x}}{\binom{k}{x} p_a^x (1-p_a)^{k-x}} \propto \left(\frac{p_o(1-p_a)}{p_a(1-p_o)}\right)^x = \left(\frac{p_o(1-p_a)}{p_a(1-p_o)}\right)^u \,.
\]</span>
How must <span class="math inline">\(u\)</span> change so that the ratio is driven toward zero?
If <span class="math inline">\(p_a &gt; p_o\)</span>, the ratio will go to zero as <span class="math inline">\(u \rightarrow \infty\)</span>.
Thus the rejection region is <span class="math inline">\(U \geq u_{1-\alpha}\)</span>.</p>
<p>At this point, we leave the domain of the NP lemmafrom here on out,
we are simply solving an inverse cdf problem:
<span class="math display">\[\begin{align*}
F_U(u_{1-\alpha} \vert p_o) - (1-\alpha) &amp;= 0 \\
\Rightarrow ~~~ u_{1-\alpha} &amp;= F_U^{-1}(1-\alpha \vert p_o) \,.
\end{align*}\]</span>
Here, <span class="math inline">\(U\)</span> is binomially distributed, and because we cannot easily work with the
generalized inverse cdf for the binomial distribution, we go directly to
code:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="the-binomial-and-related-distributions.html#cb180-1" aria-hidden="true" tabindex="-1"></a>u.hi <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span>k,<span class="at">prob=</span>p.o) <span class="co"># it turns out this is incorrect!</span></span></code></pre></div>
<p>However, we run into an issue that we will run into whether the sampling
distribution is discrete, one that is best described via an example.
Lets suppose we flip a coin that we think is fair <span class="math inline">\(k = 10\)</span> times, and
we wish to test <span class="math inline">\(p_o = 0.5\)</span> versus <span class="math inline">\(p_a &gt; p_o\)</span>. (Remember: we dont have to
specify the value of <span class="math inline">\(p_a\)</span> in order to derive the rejection regionat this
point, it suffices to state that <span class="math inline">\(p_a &gt; p_o\)</span>.) Our proposed rejection
region, assuming <span class="math inline">\(\alpha = 0.05\)</span>, is</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="the-binomial-and-related-distributions.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbinom</span>(<span class="fl">0.95</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<p>So wed reject the null if <span class="math inline">\(U \geq 8\)</span>butthe probability of sampling
a value <span class="math inline">\(U \geq 8\)</span> is</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="the-binomial-and-related-distributions.html#cb183-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pbinom</span>(<span class="dv">8-1</span>,<span class="at">size=</span><span class="dv">10</span>,<span class="at">prob=</span><span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0546875</code></pre>
<p>The probability, 0.055, is <em>greater than</em> <span class="math inline">\(\alpha\)</span>. So we need to make what
we dub a discreteness correction:</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="the-binomial-and-related-distributions.html#cb185-1" aria-hidden="true" tabindex="-1"></a>u.hi <span class="ot">&lt;-</span> <span class="fu">qbinom</span>(<span class="dv">1</span><span class="sc">-</span>alpha,<span class="at">size=</span>k,<span class="at">prob=</span>p.o) <span class="sc">+</span> <span class="dv">1</span></span></code></pre></div>
<p>We need to make an analogous correction of <span class="math inline">\(-1\)</span> whenever we derive lower bounds.</p>
<p>Note how the rejection region boundary depends on the value of <span class="math inline">\(p_o\)</span>,
but <em>not</em> on the value of <span class="math inline">\(p_a\)</span>. This means that the test we define above
is the most-powerful test regardless of the value <span class="math inline">\(p_a &gt; p_o\)</span>. We have
thus constructed a <em>uniformly most powerful</em> (or <em>UMP</em>) test for
disambiguating the simple hypotheses <span class="math inline">\(H_o : p = p_o\)</span> and <span class="math inline">\(H_a : p = p_a &gt; p_o\)</span>.
It is typically the case that when we use the NP lemma to define a most
powerful
test for <span class="math inline">\(\theta_o\)</span> versus <span class="math inline">\(\theta_a\)</span>, we end up inadvertently defining a
UMP test as well.</p>
<p>In general, when we have <span class="math inline">\(n\)</span> iid binomial data, where
<span class="math inline">\(X_i \sim\)</span> Binom(<span class="math inline">\(k,p\)</span>), the sufficient statistic will be
<span class="math inline">\(U = \sum_{i=1}^n X_i \sim\)</span> Binom(<span class="math inline">\(nk,p\)</span>), and
the UMP tests for <span class="math inline">\(p_o\)</span> versus <span class="math inline">\(p_a\)</span> have
rejection regions of the form</p>
<table>
<colgroup>
<col width="14%" />
<col width="28%" />
<col width="57%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Rejection Region(s)</th>
<th><code>R</code> Code for Binomial Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(u_{\rm obs} \leq u_{\alpha}\)</span></td>
<td><code>u.lo &lt;- qbinom(alpha,size=n*k,prob=p.o) - 1</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(u_{\rm obs} \geq u_{1-\alpha}\)</span></td>
<td><code>u.hi &lt;- qbinom(1-alpha,size=n*k,prob=p.o) + 1</code></td>
</tr>
</tbody>
</table>
<p>Note that we can combine these results to define the rejection regions for
a two-tail test, while remembering that two-tail tests fall outside the
domain of the NP lemma and thus we cannot make any guarantees about test power.</p>
<p>As for the <span class="math inline">\(p\)</span>-values and the test power, we can simply adapt previous results
to write down</p>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Formula</th>
<th><code>R</code> Code for Binomial Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(F_U(u_{\rm obs} \vert p_o)\)</span></td>
<td><code>p &lt;- pbinom(u.obs,size=n*k,prob=p.o)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(1-F_U(u_{\rm obs}-1 \vert p_o)\)</span></td>
<td><code>p &lt;- 1-pbinom(u.obs-1,size=n*k,prob=p.o)</code></td>
</tr>
</tbody>
</table>
<p>and</p>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="67%" />
</colgroup>
<thead>
<tr class="header">
<th>Alternative</th>
<th>Formula</th>
<th><code>R</code> Code for Binomial Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p_a &lt; p_o\)</span></td>
<td><span class="math inline">\(F_U(u_\alpha \vert p_a)\)</span></td>
<td><code>power &lt;- pbinom(u.lo,size=k,prob=p.a)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(p_a &gt; p_o\)</span></td>
<td><span class="math inline">\(1-F_U(u_{1-\alpha}-1 \vert p_a)\)</span></td>
<td><code>power &lt;- 1-pbinom(u.hi-1,size=k,prob=p.a)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="defining-the-uniformly-most-powerful-test-for-the-beta1theta-distribution" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Defining the Uniformly Most-Powerful Test for the Beta(1,<span class="math inline">\(\theta\)</span>) Distribution<a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-beta1theta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Beta(1,<span class="math inline">\(\theta\)</span>) distribution is one that we have seen, albeit unnamed, before:
<span class="math display">\[
f_X(x) = \theta (1-x)^{\theta-1} ~~ x \in [0,1] \,.
\]</span>
Here, we wish to test <span class="math inline">\(H_o: \theta = \theta_o\)</span> versus the simple alternative
<span class="math inline">\(H_a: \theta = \theta_a\)</span> at level <span class="math inline">\(\alpha = 0.05\)</span>. We assume <span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
</blockquote>
<blockquote>
<p>Because we are testing a simple null versus a simple alternative, we can make
use of the NP lemma. First, we factorize the likelihood in order to determine
the sufficient statistic. (Note: it may seem trivial to identify that statistic
as being <span class="math inline">\(X\)</span>, but it can be useful to lay out all the steps!)
<span class="math display">\[
\mathcal{L}(\theta \vert x) = f_X(x \vert \theta) = \theta (1-x)^{\theta-1} = g(\theta,x) \times 1 \,,
\]</span>
so <span class="math inline">\(U = X\)</span>. Second, we examine the likelihood ratio:
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert x)}{\mathcal{L}(\theta_a \vert x)} = \frac{\theta_o(1-x)^{\theta_o-1}}{\theta_a(1-x)^{\theta_a-1}} \propto (1-x)^{\theta_o-\theta_a} = (1-u)^{\theta_o-\theta_a} \,.
\]</span>
Since we are assuming <span class="math inline">\(\theta_a &lt; \theta_o\)</span> and since we know that <span class="math inline">\(0 \leq 1-u \leq 1\)</span>,
it is the case that as <span class="math inline">\(u \rightarrow 1\)</span>, the ratio will get smaller. Hence
the rejection region for our test will be <span class="math inline">\(U \geq u_{1-\alpha}\)</span>.
To determine <span class="math inline">\(u_{1-\alpha}\)</span> we need to solve
<span class="math display">\[\begin{align*}
F_U(u_{1-\alpha} \vert \theta_o) - (1-\alpha) &amp;= 0 \\
\Rightarrow ~~~ u_{1-\alpha} &amp;= F_U^{-1}(1-\alpha \vert \theta_o) \,.
\end{align*}\]</span></p>
</blockquote>
<blockquote>
<p>The sampling distribution for <span class="math inline">\(U\)</span> is that of <span class="math inline">\(X\)</span>: the original pdf itself.
Thus
<span class="math display">\[
F_U(u) = \int_0^u \theta (1-y)^{\theta-1} dy = \left. -(1-y)^\theta \right|_0^u = 1 - (1-u)^\theta \,,
\]</span>
and so
<span class="math display">\[
1 - (1-u_{1-\alpha)}^{\theta_o} = 1-\alpha ~~~ \Rightarrow ~~~ 1-u_{1-\alpha} = \alpha^{1/\theta_o} ~~ \Rightarrow ~~ u_{1-\alpha} = 1 - \alpha^{1/\theta_o} \,.
\]</span>
If <span class="math inline">\(U \geq u_{1-\alpha}\)</span>,
we reject the null hypothesis that <span class="math inline">\(\theta = \theta_o\)</span>.
For instance, if <span class="math inline">\(\theta_o = 1\)</span> and <span class="math inline">\(\alpha = 0.05\)</span>, we reject the null if
<span class="math inline">\(U \geq 1-(0.05)^1 = 0.95\)</span>, and if <span class="math inline">\(\theta_o = 2\)</span>,
we reject the null if <span class="math inline">\(U \geq 1-(0.05)^{1/2} = 0.776\)</span>, etc.</p>
</blockquote>
<blockquote>
<p>We note that the rejection region does not depend on <span class="math inline">\(\theta_a\)</span>, so
we have defined a uniformly most powerful test of <span class="math inline">\(\theta\)</span> for the
situation in which <span class="math inline">\(\theta_a &lt; \theta_o\)</span>.</p>
</blockquote>
<blockquote>
<p>What is the test power? Recall that it is the probability of rejecting the null
given <span class="math inline">\(\theta = \theta_a\)</span>, i.e., <span class="math inline">\(P(U \geq u_{1-\alpha} \vert \theta = \theta_a)\)</span>. We can
compute this directly given the relative simplicity of the pdf:
<span class="math display">\[
power(\theta_a) = \int_{u_{1-\alpha}}^1 \theta_a (1-u)^{\theta_a-1} = \left. -(1-u)^{\theta_a} \right|_{u_{1-\alpha}}^1 = (1-u_{1-\alpha})^{\theta_a} \,.
\]</span>
For instance, if <span class="math inline">\(\theta_o = 1\)</span> and <span class="math inline">\(u_{1-\alpha} = 0.95\)</span>,
the power for <span class="math inline">\(\theta_a = 1/2\)</span>
is <span class="math inline">\(0.05^{1/2} = 0.224\)</span>,
while the power for <span class="math inline">\(\theta_a = 1/4\)</span> is <span class="math inline">\(0.05^{1/4} = 0.473\)</span>,
etc. As <span class="math inline">\(\theta_a \rightarrow 0\)</span>, the power goes to 1.</p>
</blockquote>
<hr />
</div>
<div id="defining-the-uniformly-most-powerful-test-of-the-negative-binomial-proportion" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Defining the Uniformly Most-Powerful Test of the Negative Binomial Proportion<a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-of-the-negative-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets say that we are going to flip a coin until we observe <span class="math inline">\(s = 20\)</span> heads,
and that we will use the number of observed tails (or failures) to
test <span class="math inline">\(H_o : p = p_o = 0.5\)</span> versus <span class="math inline">\(H_a : p = p_a = 0.6\)</span>.
What is the rejection region for this test, assuming <span class="math inline">\(\alpha = 0.1\)</span>?
We then conduct
the experiment of flipping the coin and we observe <span class="math inline">\(X = 13\)</span> failures.
What is the <span class="math inline">\(p\)</span>-value, and what to we conclude about the coin?
Last, what is the power of the test if the true proportion is <span class="math inline">\(p = 0.6\)</span>?
Lets say that we flip a coin until we observe <span class="math inline">\(s = 20\)</span> heads.
The total number of failures is <span class="math inline">\(X = 29\)</span>.</p>
</blockquote>
<blockquote>
<p>The distribution that governs this experiment is the negative binomial
distribution, with <span class="math inline">\(X \sim\)</span> NBinom(<span class="math inline">\(s,p\)</span>). Because we only conduct one
experiment, we identify (without explicit factorization)
that the sufficient statistic is <span class="math inline">\(U = X\)</span>, and we write down the
likelihood ratio:
<span class="math display">\[
\frac{\mathcal{L}(\theta_o \vert x)}{\mathcal{L}(\theta_a \vert x)} = \frac{\binom{x+s-1}{x} p_o^s (1-p_o)^{x-s}}{\binom{x+s-1}{x} p_a^s (1-p_a)^{x-s}} \propto \left(\frac{1-p_o}{1-p_a}\right)^x = \left(\frac{1-p_o}{1-p_a}\right)^u \,.
\]</span>
We have that <span class="math inline">\(p_a &gt; p_o\)</span>, so the ratio is <span class="math inline">\(&gt; 1\)</span>, so to drive the ratio
towards zero, <span class="math inline">\(u \rightarrow 0\)</span>so the rejection region is
<span class="math inline">\(U \leq u_{\alpha}\)</span>.</p>
</blockquote>
<blockquote>
<p>We combine this information along with the information in the binomial
tables above to write the following:</p>
</blockquote>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="the-binomial-and-related-distributions.html#cb186-1" aria-hidden="true" tabindex="-1"></a>u.lo  <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(alpha,<span class="at">size=</span>s,<span class="at">prob=</span>p.o) <span class="sc">-</span> <span class="dv">1</span> <span class="co"># rejection region</span></span>
<span id="cb186-2"><a href="the-binomial-and-related-distributions.html#cb186-2" aria-hidden="true" tabindex="-1"></a>p     <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>(u.obs,<span class="at">size=</span>s,<span class="at">prob=</span>p.o)     <span class="co"># p-value</span></span>
<span id="cb186-3"><a href="the-binomial-and-related-distributions.html#cb186-3" aria-hidden="true" tabindex="-1"></a>power <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>(u.lo,<span class="at">size=</span>s,<span class="at">prob=</span>p.a)      <span class="co"># power for p = p.a</span></span></code></pre></div>
<blockquote>
<p>We thus find that</p>
</blockquote>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="the-binomial-and-related-distributions.html#cb187-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb187-2"><a href="the-binomial-and-related-distributions.html#cb187-2" aria-hidden="true" tabindex="-1"></a>s     <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb187-3"><a href="the-binomial-and-related-distributions.html#cb187-3" aria-hidden="true" tabindex="-1"></a>u.obs <span class="ot">&lt;-</span> <span class="dv">13</span></span>
<span id="cb187-4"><a href="the-binomial-and-related-distributions.html#cb187-4" aria-hidden="true" tabindex="-1"></a>p.o   <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb187-5"><a href="the-binomial-and-related-distributions.html#cb187-5" aria-hidden="true" tabindex="-1"></a>p.a   <span class="ot">&lt;-</span> <span class="fl">0.6</span></span>
<span id="cb187-6"><a href="the-binomial-and-related-distributions.html#cb187-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-7"><a href="the-binomial-and-related-distributions.html#cb187-7" aria-hidden="true" tabindex="-1"></a>(u.lo  <span class="ot">&lt;-</span> <span class="fu">qnbinom</span>(alpha,<span class="at">size=</span>s,<span class="at">prob=</span>p.o) <span class="sc">-</span> <span class="dv">1</span>) <span class="co"># rejection region</span></span></code></pre></div>
<pre><code>## [1] 11</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="the-binomial-and-related-distributions.html#cb189-1" aria-hidden="true" tabindex="-1"></a>(p     <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>(u.obs,<span class="at">size=</span>s,<span class="at">prob=</span>p.o))     <span class="co"># p-value</span></span></code></pre></div>
<pre><code>## [1] 0.1481032</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="the-binomial-and-related-distributions.html#cb191-1" aria-hidden="true" tabindex="-1"></a>(power <span class="ot">&lt;-</span> <span class="fu">pnbinom</span>(u.lo,<span class="at">size=</span>s,<span class="at">prob=</span>p.a))      <span class="co"># power for p = p.a</span></span></code></pre></div>
<pre><code>## [1] 0.375243</code></pre>
<blockquote>
<p>The rejection region is <span class="math inline">\(U \leq u_{\alpha} = 11\)</span>, the <span class="math inline">\(p\)</span>-value
is 0.148 (hence we fail to reject the nullwe cannot decide, given
the observed data, that the coin is unfair), and the power is
0.375 (if the proportion <span class="math inline">\(p\)</span> is truly 0.6, we would sample a value
of <span class="math inline">\(U\)</span> in the rejection region 37.5 percent of the time; it is not
surprising that we fail to reject the null herewe simply dont have
enough data!).</p>
</blockquote>
<blockquote>
<p>We note in passing that due to the discreteness of the sampling distribution,
in the same way that we observe in the last section that estimates of
confidence interval coverage are noisy in the small-sample limit
(see Figure <a href="the-binomial-and-related-distributions.html#fig:covsim">3.8</a>),
we observe that estimates of <span class="math inline">\(\alpha\)</span> under the null are also noisy.
This makes sense: confidence interval construction and hypothesis testing
are intimately related: e.g., if the actual confidence interval coverage
is too high, the rate of Type I errors will be reduced.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:binpow"></span>
<img src="_main_files/figure-html/binpow-1.png" alt="\label{fig:binpow}The power curve for the uniformly most-powerful upper-tail test $s = 20$ and $p_o = 0.5$. The red dot indicates the theoretically expected power for $p = p_o$; the observed offset from the curve is a by-product of working with a discrete rather than a continuous distribution." width="50%" />
<p class="caption">
Figure 3.10: The power curve for the uniformly most-powerful upper-tail test <span class="math inline">\(s = 20\)</span> and <span class="math inline">\(p_o = 0.5\)</span>. The red dot indicates the theoretically expected power for <span class="math inline">\(p = p_o\)</span>; the observed offset from the curve is a by-product of working with a discrete rather than a continuous distribution.
</p>
</div>
<blockquote>
<p>We also note that because <span class="math inline">\(p_a\)</span> is not part of the definition of the
rejection region, we can say that we have
defined the uniformly most-powerful upper-tail test of the negative
binomial proportion <span class="math inline">\(p\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="defining-the-uniformly-most-powerful-test-for-the-normal-population-mean" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Defining the Uniformly Most-Powerful Test for the Normal Population Mean<a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-normal-population-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In Chapter 2, we use the statistic <span class="math inline">\(\bar{X}\)</span> as the basis for testing
hypotheses about the normal population mean, <span class="math inline">\(\mu\)</span>. We justified this
choice because, at the very least, <span class="math inline">\(\hat{\mu}_{MLE} = \bar{X}\)</span>, so it
is a principled choice. However, beyond that, we did not (and indeed
could not) provide any further justification. Is <span class="math inline">\(\bar{X}\)</span> the basis
of the UMP for <span class="math inline">\(\mu\)</span>?</p>
</blockquote>
<blockquote>
<p>The first thing to realize is that the NP lemma <em>does not apply</em> if there are
two freely varying parameters. The NP lemma applies to <em>simple hypotheses</em>,
where the hypotheses uniquely specify the population from which data are
drawn. Here, even if we set <span class="math inline">\(H_o: \mu = \mu_o\)</span> and <span class="math inline">\(H_a: \mu = \mu_a\)</span>,
<span class="math inline">\(\sigma^2\)</span> can still freely vary. (So, technically, these hypotheses
are <em>composite hypotheses</em>.) So
to go further here, we must assume <span class="math inline">\(\sigma^2\)</span> is known.</p>
</blockquote>
<blockquote>
<p>When <span class="math inline">\(\sigma^2\)</span> is known, we can factorize the likelihood as
<span class="math display">\[
\mathcal{L}(\mu,\sigma^2 \vert \mathbf{x}) = \underbrace{\exp\left(\frac{\mu}{\sigma^2}\sum_{i=1}^n x_i\right)\exp\left(-\frac{n\mu^2}{2\sigma^2}\right)}_{g(\sum x_i,\mu)} \cdot \underbrace{(2 \pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2\right)}_{h(\mathbf{x})} \,,
\]</span>
so we can identify the sufficient statistic as <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span>.
The ratio of likelihoods is
<span class="math display">\[
\frac{\mathcal{L}(\mu_o \vert \mathbf{x})}{\mathcal{L}(\mu_a \vert \mathbf{x})} \propto \exp\left(\frac{(\mu_o-\mu_a)}{\sigma^2}\sum_{i=1}^n x_i\right) = \exp\left(\frac{\mu_o-\mu_a}{\sigma^2}u\right) \,.
\]</span>
If <span class="math inline">\(\mu_a &lt; \mu_o\)</span>, then we require that <span class="math inline">\(u \rightarrow -\infty\)</span>
so that the ratio goes
to zero, meaning the rejection region is <span class="math inline">\(U \leq u_{\alpha}\)</span>.
If <span class="math inline">\(\mu_a &gt; \mu_o\)</span>, on the
other hand, <span class="math inline">\(u\)</span> has to increase towards <span class="math inline">\(\infty\)</span> and thus the rejection region
is <span class="math inline">\(U \geq u_{1-\alpha}\)</span>.</p>
</blockquote>
<blockquote>
<p>We know from using the method of moment-generating
functions that the sum of <span class="math inline">\(n\)</span> iid normal random variables is itself a normal
random variable with mean <span class="math inline">\(n\mu\)</span> and variance <span class="math inline">\(n\sigma^2\)</span>, so
<span class="math inline">\(U \sim \mathcal{N}(n\mu,n\sigma^2)\)</span>which means that
<span class="math display">\[
Z = \frac{U - n\mu}{\sqrt{n}\sigma} \sim \mathcal{N}(0,1) \,.
\]</span>
Hence the rejection region, rendered as a function of <span class="math inline">\(Z\)</span>, is
<span class="math inline">\(Z \leq z_{\alpha} = \Phi^{-1}(\alpha)\)</span> if <span class="math inline">\(\mu_a &lt; \mu_o\)</span> and
<span class="math inline">\(Z \geq z_{1-\alpha} = \Phi^{-1}(1-\alpha)\)</span> if <span class="math inline">\(\mu_a &gt; \mu_o\)</span>.</p>
</blockquote>
<blockquote>
<p>So we can see that <span class="math inline">\(U = \sum_{i=1}^n X_i\)</span> is the
statistic that forms the basis for the uniformly most-powerful tests of <span class="math inline">\(\mu\)</span>
when <span class="math inline">\(\sigma^2\)</span> is known.
But wait. What about <span class="math inline">\(\bar{X}\)</span>?
Recall that a function of a sufficient statistic
is itself a sufficient statistic, so we could also use <span class="math inline">\(U&#39; = \bar{X} = U/n\)</span> as
the basis for the UMP test:
<span class="math display">\[
Z = \frac{U - n\mu}{\sqrt{n}\sigma} = \frac{U/n - \mu}{\sigma/\sqrt{n}} = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1) \,.
\]</span>
We thus see that, at least when <span class="math inline">\(\sigma^2\)</span> is known, <span class="math inline">\(\bar{X}\)</span>
(or functions of it) is the best statistic to use to test simple hypotheses.</p>
</blockquote>
<blockquote>
<p>Now, what if <span class="math inline">\(\sigma^2\)</span> is unknown? We discuss this possibility
in the next chapter, when we introduce the likelihood ratio test.</p>
</blockquote>
<hr />
</div>
<div id="large-sample-tests-of-the-binomial-proportion" class="section level3 hasAnchor" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> Large-Sample Tests of the Binomial Proportion<a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Earlier in the chapter, we saw that if
<span class="math display">\[
k &gt; 9\left(\frac{\mbox{max}(p,1-p)}{\mbox{min}(p,1-p)}\right) \,,
\]</span>
then, given a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k\)</span>,<span class="math inline">\(p\)</span>), we can assume
<span class="math display">\[
X \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}(kp,kp(1-p)) \,,
\]</span>
or that
<span class="math display">\[
\hat{p} = \hat{p}_{MLE} = \frac{X}{k} \stackrel{d}{\rightarrow} X&#39; \sim \mathcal{N}(p,p(1-p)/k) \,.
\]</span>
In hypothesis testing, this approximation forms the basis of two
different tests:
<span class="math display">\[\begin{align*}
\mbox{Score}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,p_o(1-p_o)/k) \\
\mbox{Wald}~\mbox{Test:} ~~ &amp; \hat{p} \sim \mathcal{N}(p_o,\hat{p}(1-\hat{p})/k) \,.
\end{align*}\]</span>
The only difference between the two is in how the variance is estimated
under the null. To carry out these tests, we can simply adapt the
expressions for the rejection regions, <span class="math inline">\(p\)</span>-values, and power given in
Chapter 2 for tests of the normal population mean with <em>variance known</em>,
with the primary change being the definition of the variance.</p>
</blockquote>
<blockquote>
<p>We are including the details of the Wald and Score tests here for
completeness, in that they are ubiquitous in introductory statistics
settings. As they yield only approximate results, we would encourage
the reader to always use the exact testing mechanisms described above in
the main body of this section.</p>
</blockquote>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Logistic Regression<a href="the-binomial-and-related-distributions.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the last chapter, we introduced simple linear regression,
in which we model the data-generating process as
<span class="math display">\[
Y_i = \beta_0 + \beta_1x_i + \epsilon_i \,.
\]</span>
So as to be able to perform hypothesis testing
(e.g., <span class="math inline">\(H_o : \beta_1 = 0\)</span> vs.<span class="math inline">\(H_a : \beta_1 \neq 0\)</span>), we make the
assumption that <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>,
or equivalently, that <span class="math inline">\(Y_i \vert x_i \sim \mathcal{N}(\beta_0+\beta_1x_i,\sigma^2)\)</span>.
When we make this assumption, we are implicitly stating that the response
variable is continuous and that it can take on
any value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span>. But what if this
doesnt actually correctly represent the response variable? Maybe
the <span class="math inline">\(Y_i\)</span>s are distances with values <span class="math inline">\(\geq 0\)</span>. Maybe each <span class="math inline">\(Y_i\)</span> belongs
to one of several categories, and thus the <span class="math inline">\(Y_i\)</span>s are
discretely valued. When provided data such as these, it is possible,
though not optimal, to utilize simple linear regression.
A better choice is to generalize the concept of linear regression,
and to utilize this generalization so as to
implement a more appropriate statistical model.</p>
<p>To implement a <em>generalized linear model</em> (or <em>GLM</em>), we need to do two things:</p>
<ol style="list-style-type: decimal">
<li>examine the <span class="math inline">\(Y_i\)</span> values and select an appropriate distribution
for them (discrete or continuous? what is the functional domain?); and</li>
<li>define a <em>link function</em> <span class="math inline">\(g(\theta \vert x)\)</span> that maps
the line <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, which has infinite range,
into a more limited range (e.g., <span class="math inline">\([0,\infty)\)</span>).</li>
</ol>
<p>Suppose that, in an experiment, the response variable can take on the
values false and true. To generalize linear regression so as
to handle these data, we map false to 0 and true to 1, and assume
that we can model the data-generating process using a
Bernoulli distribution (i.e., a binomial distribution with <span class="math inline">\(k = 1\)</span>):
<span class="math inline">\(Y \sim\)</span> Bernoulli(<span class="math inline">\(p\)</span>). We know that <span class="math inline">\(0 &lt; p &lt; 1\)</span>, so
we adopt a link function that maps <span class="math inline">\(\beta_0 + \beta_1 x\)</span> to the range <span class="math inline">\((0,1)\)</span>.
There is no unique choice, but a conventional one
is the <em>logit</em> function:
<span class="math display">\[
g(p \vert x) = \log\left[\frac{p \vert x}{1-p \vert x}\right] = \beta_0 + \beta_1 x ~\implies~ p \vert x = \frac{\exp\left(\beta_0+\beta_1x\right)}{1+\exp\left(\beta_0+\beta_1x\right)} \,.
\]</span>
Using the logit function to model dichotomous data is dubbed <em>logistic regression</em>. See Figure <a href="the-binomial-and-related-distributions.html#fig:logexp">3.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logexp"></span>
<img src="_main_files/figure-html/logexp-1.png" alt="\label{fig:logexp}An example of an estimated logistic regression line. (See the first example below for data details.) The blue line is a logit function, and it represents the probability that we would sample a datum of Class 1 as a function of $x$. The red points represent the observed data (350 objects each in Class 0 and Class 1). The intersection of the green dashed lines indicates the intercept at $y = 0.542$." width="50%" />
<p class="caption">
Figure 3.11: An example of an estimated logistic regression line. (See the first example below for data details.) The blue line is a logit function, and it represents the probability that we would sample a datum of Class 1 as a function of <span class="math inline">\(x\)</span>. The red points represent the observed data (350 objects each in Class 0 and Class 1). The intersection of the green dashed lines indicates the intercept at <span class="math inline">\(y = 0.542\)</span>.
</p>
</div>
<p>(It is important to note here that when performing
logistic regression, we are simply estimating <span class="math inline">\(p \vert x\)</span>, which
is the probability that
we would sample a datum belonging to Class 1, given <span class="math inline">\(x\)</span>.
How we choose to map <span class="math inline">\(p \vert x\)</span> to either 0 or 1 (i.e., the classification
step), if we indeed choose to make that mapping, is not actually
part of the logistic regression framework. We will indicate how to
make this mapping when the response classes are balanced (i.e., when a
model is trained on just as many Class 1 objects as Class 0 objects) in
an example below. Going beyond that into more general classification
situations is beyond the scope of this book.</p>
<p>How do we learn a logistic regression model? In linear regression, we
can determine values for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by
formulae, the ones we derive when we minimize the sum of squared
errors (SSE). For logistic regression, such simple formulae do not
exist, and so we estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> via numerical
optimization of the likelihood function
<span class="math display">\[
\mathcal{L}(\beta_0,\beta_1 \vert \mathbf{y}) = \prod_{i=1}^n p_{Y \vert \beta_0,\beta_1}(y_i \vert \beta_0,\beta_1) \,,
\]</span>
where <span class="math inline">\(p_Y(\cdot)\)</span> is the Bernoulli probability mass function. Specifically,
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are the coordinates at which the
bivariate likelihood surface achieves its maximum value. Note that
because numerical optimization is an iterative process, logistic regression
models are learned more slowly than simple linear regression models.</p>
<p>Lets step back for a moment. Why would we want to learn a logistic regression
model in the first place? After all, it is a relatively
inflexible model that might lack the ability to mimic the true behavior
of <span class="math inline">\(p \vert x\)</span>. The reason is that because we specify the
model parameterization, we can examine the learned model and
perform <em>inference</em>: we can examine how the response is affected by changes
in the predictor variables values. This can be important in, e.g., scientific
contexts, where explaining what a model does can be as important,
if not more important, as its ability to predict response values.
(This is in constrast to the vast majority of machine
learning models, where you cannot specify the parameterized form of
the model <em>a priori</em>, thus making the model a black box.)</p>
<p>In simple linear regression, if we increase <span class="math inline">\(x\)</span> by one unit,
we can immediately infer how the response value changes:
<span class="math display">\[
\hat{Y}&#39; = \hat{\beta}_0 + \hat{\beta}_1 (x + 1) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_1 = \hat{Y} + \hat{\beta}_1 \,.
\]</span>
For logistic regression, the situation is not as straightforward,
because <span class="math inline">\(p\)</span> changes non-linearly as a function of <span class="math inline">\(x\)</span>. So we fall back
on the concept of <em>odds</em>:
<span class="math display">\[
O(x) = \frac{p(x)}{1-p(x)} = \exp\left(\hat{\beta}_0+\hat{\beta}_1x\right) \,.
\]</span>
If, e.g., <span class="math inline">\(O(x) = 4\)</span>, then that means that, given <span class="math inline">\(x\)</span>, you are four times
more likely to sample a success (1) than a failure (0). How
does the odds change if we add one unit to <span class="math inline">\(x\)</span>?
<span class="math display">\[
O(x+1) = e^{\hat{\beta}_0+\hat{\beta}_1(x+1)} = \exp\left(\hat{\beta}_0 + \hat{\beta}_1x\right)  \times \exp\left(\hat{\beta}_1\right) = O(x) \exp\left(\hat{\beta}_1\right) \,.
\]</span>
The odds change by a factor of <span class="math inline">\(\exp\left(\hat{\beta}_1\right)\)</span>,
which can be greater than or less than one, depending
on the sign of <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<hr />
<div id="logistic-regression-in-r-star-quasar-classification" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Logistic Regression in R: Star-Quasar Classification<a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>When observed with a telescope, a star is a point-like object, as opposed to
a galaxy, which has some angular extent. Telling stars and galaxies apart
visually is thus, in relative terms, easy. However, if the galaxy has an
inordinately bright core because the supermassive black hole at its center
is ingesting matter at a high rate, that core can outshine the rest of the
galaxy so much that the galaxy appears to be point-like. Such galaxies
with bright cores are dubbed quasars, or quasi-stellar objects, and
they are much harder to tell apart from stars.</p>
</blockquote>
<blockquote>
<p>Below we run multiple logistic regression on a dataset with 500 stars and
500 quasars (so <span class="math inline">\(n = 1000\)</span>).
The response variable is a factor variable with two levels, which
we dub Class 0 (here, <code>QSO</code>) and Class 1 (<code>STAR</code>). (The mapping
of qualitative factors to quantitative levels is by default
alphabetical; as <code>STAR</code> comes after <code>QSO</code>, <code>STAR</code> gets mapped to Class 1.)
To keep the analysis in the realm of the
simple we will utilize a single predictor variable, <code>col.iz</code>, that
represents the difference in the objects brightness at two infrared
wavelengths. Note that we use <code>glm()</code> rather than <code>lm()</code> (which we
use for simple linear regression), and that we specify
<code>family=binomial</code>, which means do logistic regression. Also note
that we follow statistical learning convention by splitting the data
into a training set (which we use to learn the model) and a test set
(which we use to assess the learned model). There is no unique specification
for how to split data; here we randomly put 70% into the training set,
with the remainder comprising the test set.</p>
</blockquote>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="the-binomial-and-related-distributions.html#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run simple logistic regression on training dataset</span></span>
<span id="cb193-2"><a href="the-binomial-and-related-distributions.html#cb193-2" aria-hidden="true" tabindex="-1"></a>log.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(class<span class="sc">~</span>col.iz,<span class="at">data=</span>df.train,<span class="at">family=</span>binomial)</span>
<span id="cb193-3"><a href="the-binomial-and-related-distributions.html#cb193-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(log.out)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = class ~ col.iz, family = binomial, data = df.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.7460  -1.1661   0.3405   1.1280   2.0606  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.16841    0.09265   1.818  0.06911 . 
## col.iz      -0.96729    0.31051  -3.115  0.00184 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 970.41  on 699  degrees of freedom
## Residual deviance: 958.16  on 698  degrees of freedom
## AIC: 962.16
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<blockquote>
<p>The <code>summary()</code> of a learned logistic regression model is similar to,
yet different from, that for a linear regression model.</p>
</blockquote>
<blockquote>
<p>The first
important lines are the ones indicating the deviance residuals, with the
residual for the <span class="math inline">\(i^{\rm th}\)</span> datum defined as
<span class="math display">\[
d_i = \mbox{sign}(Y_i - \hat{Y}_i) \sqrt{-2[Y_i \log \hat{Y}_i + (1-Y_i)\log(1-\hat{Y}_i)]} \,,
\]</span>
where
<span class="math display">\[
\hat{Y}_i = \hat{p}_i = \frac{\exp(\hat{\beta}_0+\hat{\beta}_1 x_i)}{1 + \exp(\hat{\beta}_0+\hat{\beta}_1 x_i)} \,.
\]</span>
The sum of the squared deviance residuals is equal to
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>, where <span class="math inline">\(\mathcal{L}_{max}\)</span> is the joint likelihood
of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, given <span class="math inline">\(\mathbf{x}\)</span>.
Here, the deviance residuals are seemingly well-balanced around zero.</p>
</blockquote>
<blockquote>
<p>The coefficients can be translated to <span class="math inline">\(y\)</span>-coordinate values as follows:
the intercept is <span class="math inline">\(e^{0.168}/(1+e^{0.168} = 0.542\)</span> (and is the probability
the a sampled datum with <code>col.iz</code> equal to zero is a <code>STAR</code>), while
the odds ratio is <span class="math inline">\(O_{new}/O_{old} = e^{-0.967}\)</span> (so that every time
<code>col.iz</code> increases by one unit, the probability that a sampled datum
is a star is 0.380 times what it had been before: the higher the value
of <code>col.iz</code>, the less and less likely that a sampled datum is actually
a star, and the more and more likely that it is a quasar.</p>
</blockquote>
<blockquote>
<p>The numbers in the other three columns of the coefficients section
are estimated numerically using the behavior of the likelihood
function (specifically, the rates at which it curves downward
away from the surfaces maximum, along each parameter axis). Some
details about how the numbers are calculated are given in an example
in the Covariance and Correlation section of Chapter 6.
It suffices to say here that under the assumption that <span class="math inline">\(\hat{\beta}_0\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span> are both normally distributed random variables, we
fail to reject the null hypothesis that the intercept is 0
(or equivalently <span class="math inline">\(y = 1/2\)</span>), while we do reject the null hypothesis
that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
</blockquote>
<blockquote>
<p>The null deviance is <span class="math inline">\(-2 \log \mathcal{L}_{max,o}\)</span>, where
<span class="math inline">\(\mathcal{L}_{max,o}\)</span> is the maximum likelihood
when <span class="math inline">\(\beta_1\)</span> is set to zero. The residual deviance is
<span class="math inline">\(-2 \log \mathcal{L}_{max}\)</span>. The difference between these values
(here, 970.41-958.16 = 12.25) is, under the null hypothesis that
<span class="math inline">\(\beta_1 = 0\)</span>, assumed to be chi-square-distributed for
699-698 = 1 degree of freedom. The <span class="math inline">\(p\)</span>-value is thus
<code>1 - pchisq(12.25,1)</code> or 4.65 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>: we reject the
null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.
(To be clear, this test is
analogous to the <span class="math inline">\(F\)</span>-test in linear regression, and is testing
<span class="math inline">\(H_o: \beta_1 = \beta_2 = \cdots = \beta_p = 0\)</span> versus <span class="math inline">\(H_a:\)</span>
at least one of the <span class="math inline">\(\beta_i\)</span>s is non-zero. Because the
sampling distribution here is the chi-square distribution as
opposed to the normal distribution, the <span class="math inline">\(p\)</span>-value here will not
match the <span class="math inline">\(p\)</span>-value seen for <code>col.iz</code> in the coefficients section.)</p>
</blockquote>
<blockquote>
<p>Last, the AIC, or Akaike Information Criterion, is <span class="math inline">\(-2\)</span> times the
model likelihood (or here, the deviance)
plus two times the number of variables (here, 2,
as the intercept is counted as a variable).
Adding the number of variables acts to penalize those models with
more variables: the improvement in the maximum likelihood has to be
sufficient to justify added model complexity.
Discussion of the mathematical details of
AIC is beyond the scope of this book; it suffices to say that
if we compute it when learning a suite of different models, we would
select the model with the smallest value.</p>
</blockquote>
<hr />
</div>
<div id="logistic-regression-in-r-star-quasar-classification-1" class="section level3 hasAnchor" number="3.9.2">
<h3><span class="header-section-number">3.9.2</span> Logistic Regression in R: Star-Quasar Classification<a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>As indicated in the text above, classification, i.e., the step of predicting
which class a datum belongs to, can be tricky when we train
a logistic regression model given unbalanced classes. But even if the classes
are balanced, there are numerous metrics that one can compute to indicate
the overall quality of the model. Here, we simply show how to compute
the <em>misclassification rate</em> or <em>MCR</em> for a model trained given class
balance.</p>
</blockquote>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="the-binomial-and-related-distributions.html#cb195-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we assume the trained model details contained in log.out from above</span></span>
<span id="cb195-2"><a href="the-binomial-and-related-distributions.html#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="co"># output Class 1 probabilities for the test dataset</span></span>
<span id="cb195-3"><a href="the-binomial-and-related-distributions.html#cb195-3" aria-hidden="true" tabindex="-1"></a>log.prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(log.out,<span class="at">newdata=</span>df.test,<span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span>
<span id="cb195-4"><a href="the-binomial-and-related-distributions.html#cb195-4" aria-hidden="true" tabindex="-1"></a><span class="co"># if the probability is &gt; 0.5, map the datum to Class 1 (STAR), etc.</span></span>
<span id="cb195-5"><a href="the-binomial-and-related-distributions.html#cb195-5" aria-hidden="true" tabindex="-1"></a>log.pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(log.prob<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">&quot;STAR&quot;</span>,<span class="st">&quot;QSO&quot;</span>)     </span>
<span id="cb195-6"><a href="the-binomial-and-related-distributions.html#cb195-6" aria-hidden="true" tabindex="-1"></a><span class="co"># output &quot;confusion matrix&quot;</span></span>
<span id="cb195-7"><a href="the-binomial-and-related-distributions.html#cb195-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(log.pred,df.test<span class="sc">$</span>class)</span></code></pre></div>
<pre><code>##         
## log.pred QSO STAR
##     QSO  150  146
##     STAR   0    4</code></pre>
<blockquote>
<p>Each row of the output confusion matrix corresponds to a predicted
class: the algorithm predicts that we have observed 103+51 = 154 quasars
and 47+99 = 146 stars. Each column corresponds to the true
class: there are 150 quasars and 150 stars in the test dataset.
There are 51 stars that are misclassified as quasars and
47 quasars that are misclassified as stars. Thus the
rate of misclassification is (51+47)/300 = 0.327the
model predicts the wrong class 32.7 percent of the time. This seems
sub-optimal, and it isbut it is a reflection that we are
illustrating <em>simple</em> logistic regression. In a real-life setting,
there is more than one predictor variable, and the misclassification
rate is actually <span class="math inline">\(\approx\)</span> 15%.</p>
</blockquote>
</div>
</div>
<div id="naive-bayes-regression" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Naive Bayes Regression<a href="the-binomial-and-related-distributions.html#naive-bayes-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>Naive Bayes</em> model is the basis for perhaps the simplest probabilitic
classifier,
one that is oft-used for, e.g., detecting spam emails. The meanings
of the words Naive and Bayes will become more clear below. (Note that one
would often see this model referred to as the Naive Bayes classifier. However,
as noted in the last section, there are actually two steps in classification,
the first being the estimation of probabilities that a given datum belongs to
each class, and the second being the mapping of those probabilities to class
predictions. In the last section and here, we are only focusing on the generation
of predicted probabilitiesso we will call this section <em>Naive Bayes Regression</em>.)</p>
<p>Lets assume that we are in a similar setting as the one for logistic regression,
but instead of having a response that is a two-level factor variable (i.e., one
the represents two classes), the number
of levels is <span class="math inline">\(K \geq 2\)</span>. (So instead of just, e.g., chocolate and vanilla as
our response variable values, we can add strawberry and other ice cream flavors
too!) Also, we note that model is best understood is a multiple regression context,
i.e., one where there is more than one predictor variable. (We denote the vector
of predictor variable values associated with each response value as <span class="math inline">\(\mathbf{x}\)</span>. This
vector is of length <span class="math inline">\(p\)</span>.)
The ultimate goal of the model is to assign conditional probabilities for
each response class, given a datum <span class="math inline">\(x\)</span>: <span class="math inline">\(p(C_k \vert \mathbf{x})\)</span>, where <span class="math inline">\(C_k\)</span> denotes
class <span class="math inline">\(k\)</span>. How do we derive this quantity?</p>
<p>The first step is to apply Bayes rule (hence the Bayes in Naive Bayes):
<span class="math display">\[
p(C_k \vert \mathbf{x}) = \frac{p(C_k) p(\mathbf{x} \vert C_k)}{p(\mathbf{x})} \propto p(C_k) p(\mathbf{x} \vert C_k) \,.
\]</span>
We use the proportionality symbol <span class="math inline">\(\propto\)</span> here because we do not need to evaluate
<span class="math inline">\(p(\mathbf{x})\)</span>, which is a constant in any given analysis. The next step is to expand
<span class="math inline">\(p(\mathbf{x} \vert C_k)\)</span>:
<span class="math display">\[
p(\mathbf{x} \vert C_k) = p(x_1,\ldots,x_p \vert C_k) = p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) \,.
\]</span>
(This is just the multiplicative law of probability in action, as applied to a
conditional probability. See section 1.4.) The right-most expression above is one that
is difficult to evaluate in practice, given all the conditions that must be jointly
appliedso this is where the Naive aspect of the model comes in. We simplify the
expression by assuming (most often incorrectly!) that the predictor variables are
all mutually independent, so that
<span class="math display">\[
p(x_1 \vert x_2,\ldots,x_p,C_k) p(x_2 \vert x_3,\ldots,x_p,C_k) \cdots p(x_p \vert C_k) ~~ \rightarrow ~~ p(x_1 \vert C_k) \cdots p(x_p \vert C_k)
\]</span>
and thus
<span class="math display">\[
p(C_k \vert \mathbf{x}) \propto p(C_k) \prod_{i=1}^p p(x_i \vert C_k) \,.
\]</span>
OKbut where do we go from here?</p>
<p>We need to make further assumptions!</p>
<ul>
<li>We need to assign prior probabilities <span class="math inline">\(p(C_k)\)</span> to each class. Common choices
are <span class="math inline">\(1/K\)</span> (we view each class as equally probable before we gather data) and
<span class="math inline">\(n_k/n\)</span> (the number of observed data of class <span class="math inline">\(k\)</span> divided by the overall sample size).</li>
<li>We also need to assume probability mass and density functions <span class="math inline">\(p(x_i \vert C_k)\)</span> for
each predictor variable (pmfs if <span class="math inline">\(x_i\)</span> is discrete and pdfs if <span class="math inline">\(x_i\)</span> is continuous).
For pmfs, standard assumptions are binomial or multinomial distributions, depending
on the number of levels, with the observed proportions in each level informing the
category probability estimate. For pdfs, the standard assumption is that the data
are distributed normally, with <span class="math inline">\(\hat{\mu} = \bar{x_i}\)</span> and <span class="math inline">\(\hat{\sigma^2} = s_i^2\)</span> (the
sample variance).</li>
</ul>
<p>Ultimately, this model depends on a number of (perhaps unjustified) assumptions. Why
would we ever use it? Because the assumption of mutual independence makes model
evaluation <em>fast</em>. Naive Bayes is rarely the model underlying the best classifier for
any given problem, but when speed is needed (such as in the identification of spam email),
ones choices are limited. (As as general rule: if a model is simple to implement, one
should implement it, even if the <em>a priori</em> expectation is that another model will
ultimately generate better results. One never knows)</p>
<hr />
<div id="naive-bayes-regression-with-categorical-predictors" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Naive Bayes Regression With Categorical Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we have collected the following data:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Yes</td>
<td>Chocolate</td>
<td>True</td>
</tr>
<tr class="even">
<td>Yes</td>
<td>Chocolate</td>
<td>False</td>
</tr>
<tr class="odd">
<td>No</td>
<td>Vanilla</td>
<td>True</td>
</tr>
<tr class="even">
<td>No</td>
<td>Chocolate</td>
<td>True</td>
</tr>
<tr class="odd">
<td>Yes</td>
<td>Vanilla</td>
<td>False</td>
</tr>
<tr class="even">
<td>No</td>
<td>Chocolate</td>
<td>False</td>
</tr>
<tr class="odd">
<td>No</td>
<td>Vanilla</td>
<td>False</td>
</tr>
</tbody>
</table>
<blockquote>
<p>We learn a Naive Bayes model given these data, in which we
assume False is Class 0 and True is Class 1. If we have a
new datum <span class="math inline">\(\mathbf{x}\)</span> = (Yes,Chocolate), what is the
probability that the response is True?</p>
</blockquote>
<blockquote>
<p>We seek the quantity
<span class="math display">\[
p(C_1 \vert \mathbf{x}) = \frac{p(C_1) p(\mathbf{x} \vert C_1)}{p(\mathbf{x})} = \frac{p(C_1) p(x_1 \vert C_1) p(x_2 \vert C_1)}{p(x_1,x_2)} \,.
\]</span>
When we examine the data, we see that <span class="math inline">\(p(C_1) = 3/7\)</span>, as there are
three data out of seven with the value <span class="math inline">\(Y\)</span> = True. Now, given <span class="math inline">\(C_1\)</span>,
at what rate do we observe Yes? (One time out of threeso
<span class="math inline">\(p(x_1 \vert C_1) = 1/3\)</span>.) What about Chocolate? (Two times out of
threeso <span class="math inline">\(p(x_2 \vert C_1) = 2/3\)</span>.) The numerator is thus
<span class="math inline">\(3/7 \times 1/3 \times 2/3 = 2/21\)</span>. The denominator, <span class="math inline">\(p(x_1,x_2)\)</span>, is
determined by utilizing the Law of Total Probability:
<span class="math display">\[\begin{align*}
p(x_1,x_2) &amp;= p(x_1 \vert C_1) p(x_2 \vert C_1) p(C_1) + p(x_1 \vert C_2) p(x_2 \vert C_2) p(C_2) \\
&amp;= 2/21 + 2/4 \times 2/4 \times 4/7 = 2/21 + 4/28 = 2/21 + 3/21 = 5/21 \,.
\end{align*}\]</span>
And so now we know that
<span class="math display">\[
p(\mbox{True} \vert \mbox{Yes,Chocolate}) = \frac{2/21}{5/21} = \frac{2}{5} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-regression-with-continuous-predictors" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Naive Bayes Regression With Continuous Predictors<a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume you have the following data regarding credit-card defaults,
where <span class="math inline">\(x\)</span> represents the credit-card balance and <span class="math inline">\(Y\)</span> is a categorical
variable indicating whether a default has occurred:</p>
</blockquote>
<table style="width:100%;">
<colgroup>
<col width="6%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th>1487.00</th>
<th>324.74</th>
<th>988.21</th>
<th>836.30</th>
<th>2205.80</th>
<th>927.89</th>
<th>712.28</th>
<th>706.16</th>
<th>1774.69</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Lets now suppose someone came along with a credit balance of $1,200.
According to the Naive Bayes model, given this
balance, what is the probability of a credit default?</p>
</blockquote>
<blockquote>
<p>The Naive Bayes model in this particular case is
<span class="math display">\[
p(Y \vert x) = \frac{p(x \vert Y) p(Y)}{p(x \vert Y) p(Y) + p(x \vert N) p(N)} \,,
\]</span>
where we can observe immediately that <span class="math inline">\(p(Y) = 3/9 = 1/3\)</span> and <span class="math inline">\(p(N) = 6/9 = 2/3\)</span>.
To compute, e.g., <span class="math inline">\(p(x \vert Y)\)</span>, we first compute the sample mean
and sample standard deviation for the observed data for which <span class="math inline">\(Y\)</span> is Yes:</p>
</blockquote>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="the-binomial-and-related-distributions.html#cb197-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">1487.00</span>,<span class="fl">2205.80</span>,<span class="fl">1774.69</span>)</span>
<span id="cb197-2"><a href="the-binomial-and-related-distributions.html#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">mean</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1822.5</code></pre>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="the-binomial-and-related-distributions.html#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sd</span>(x),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 361.78</code></pre>
<blockquote>
<p>The mean is $1,822.50 and the standard deviation is $361.78,
so the probability density associated with observing $1,200 is</p>
</blockquote>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="the-binomial-and-related-distributions.html#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002509367</code></pre>
<blockquote>
<p>The density is 2.51 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. As far as for those data for
which <span class="math inline">\(Y\)</span> is No:</p>
</blockquote>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="the-binomial-and-related-distributions.html#cb203-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">324.74</span>,<span class="fl">988.21</span>,<span class="fl">836.30</span>,<span class="fl">927.89</span>,<span class="fl">712.28</span>,<span class="fl">706.16</span>)</span>
<span id="cb203-2"><a href="the-binomial-and-related-distributions.html#cb203-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1200</span>,<span class="at">mean=</span><span class="fu">mean</span>(x),<span class="at">sd=</span><span class="fu">sd</span>(x))</span></code></pre></div>
<pre><code>## [1] 0.0002748351</code></pre>
<blockquote>
<p>The density is 2.75 <span class="math inline">\(\times\)</span> 10<span class="math inline">\(^{-4}\)</span>. Thus
<span class="math display">\[
p(Y \vert x) = \frac{2.51 \cdot 0.333}{2.51 \cdot 0.333 + 2.75 \cdot 0.667} = \frac{0.834}{0.834 + 1.834} = 0.313 \,.
\]</span>
There is roughly a 1 in 3 chance that a person with a credit balance of
$1,200 will default on their debt.</p>
</blockquote>
<hr />
</div>
<div id="naive-bayes-applied-to-star-quasar-data" class="section level3 hasAnchor" number="3.10.3">
<h3><span class="header-section-number">3.10.3</span> Naive Bayes Applied to Star-Quasar Data<a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the previous section, we show how one can map Class 1 probabilities generated
by a multiple linear regression model to actual class predictions, and that by
displaying the result via a confusion matrix, we can determine the models
misclassification rate. We repeat that exercise here, for the Naive Bayes model,
using functionality in <code>R</code>s contributed <code>e1071</code> package.</p>
</blockquote>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="the-binomial-and-related-distributions.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb205-2"><a href="the-binomial-and-related-distributions.html#cb205-2" aria-hidden="true" tabindex="-1"></a>nb.out  <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(class<span class="sc">~</span>.,<span class="at">data=</span>df.train)</span>
<span id="cb205-3"><a href="the-binomial-and-related-distributions.html#cb205-3" aria-hidden="true" tabindex="-1"></a><span class="co"># map probabilities directly to class predictions</span></span>
<span id="cb205-4"><a href="the-binomial-and-related-distributions.html#cb205-4" aria-hidden="true" tabindex="-1"></a>nb.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(nb.out,<span class="at">newdata=</span>df.test,<span class="at">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb205-5"><a href="the-binomial-and-related-distributions.html#cb205-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(nb.pred,df.test<span class="sc">$</span>class)</span></code></pre></div>
<pre><code>##        
## nb.pred QSO STAR
##    QSO  144   42
##    STAR   6  108</code></pre>
<blockquote>
<p>We see that the Naive Bayes model performs poorly with stars:
this indicates that the underlying assumptions of the Naive Bayes model
hold (much!) better for quasars than they do for stars.</p>
</blockquote>
</div>
</div>
<div id="the-beta-distribution" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> The Beta Distribution<a href="the-binomial-and-related-distributions.html#the-beta-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>beta distribution</em> is a continuous distribution that is commonly used to
model random variables with finite, bounded domains.
Its probability density function is given by
<span class="math display">\[
f_X(x) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}\,,
\]</span>
where <span class="math inline">\(x \in [0,1]\)</span>, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are both <span class="math inline">\(&gt; 0\)</span>, and the normalization constant <span class="math inline">\(B(\alpha,\beta)\)</span> is
<span class="math display">\[
B(\alpha,\beta) = \int_0^1 x^{\alpha-1}(1-x)^{\beta-1} dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)} \,.
\]</span>
(See Figure <a href="the-binomial-and-related-distributions.html#fig:betapdf">3.12</a>.)
We have seen <span class="math inline">\(\Gamma(\alpha)\)</span>, the gamma function, before; it is defined as
<span class="math display">\[
\Gamma(\alpha) = \int_0^\infty x^{\alpha-1} e^{-x} dx \,.
\]</span>
There are two things to note about the gamma function. The first is its
<em>recursive property</em>: <span class="math inline">\(\Gamma(\alpha+1) = \alpha \Gamma(\alpha)\)</span>.
(This can be shown by applying integration by parts.) The second is that
when <span class="math inline">\(\alpha\)</span> is a positive integer, the gamma function takes on the value
<span class="math inline">\((\alpha-1)! = (\alpha-1)(\alpha-2) \cdots 1\)</span>. (Note that <span class="math inline">\(\Gamma(1) = 0! = 1\)</span>.)</p>
<p>Regarding the statement above about model[ing] random variables with finite,
bounded domains: if a set of <span class="math inline">\(n\)</span> iid random variable
<span class="math inline">\(\mathbf{X}\)</span> has domain <span class="math inline">\([a,b]\)</span>, we can define a new set of random variables
<span class="math inline">\(\mathbf{Y}\)</span> via the transformation
<span class="math display">\[
\mathbf{Y} = \frac{\mathbf{X}-a}{b-a}
\]</span>
such that the domain becomes <span class="math inline">\([0,1]\)</span>. We can model these newly defined data
with the beta distribution. (Note the word <em>can</em>: we can model these data with
the beta distribution, but we dont have to, and it may be the case that
there is another
distribution bounded on the interval <span class="math inline">\([0,1]\)</span> ultimately better
describes the data-generating process. The beta distribution just happens
to be commonly used.)</p>
<p>Butin the end, what does this all have to do with the binomial
distribution, the subject of this chapter? We know the binomial has a
parameter <span class="math inline">\(p \in [0,1]\)</span> and the domain of the beta distribution is <span class="math inline">\([0,1]\)</span>,
but is there more? Lets write down the binomial pmf:
<span class="math display">\[
\binom{k}{x} p^x (1-p)^{k-x} = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
This pmf dictates the probability of observing a particular value of <span class="math inline">\(x\)</span>
given <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span>. But what if we turn this around a bitand examine
this function if we fix <span class="math inline">\(k\)</span> and <span class="math inline">\(x\)</span> and vary <span class="math inline">\(p\)</span> instead? In other words,
lets examine the likelihood function
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{k!}{x!(k-x)!} p^x (1-p)^{k-x} \,.
\]</span>
We can see immediately that the likelihood has the form of a beta
distribution if we set <span class="math inline">\(\alpha = x+1\)</span> and <span class="math inline">\(\beta = k-x+1\)</span>:
<span class="math display">\[
\mathcal{L}(p \vert k,x) = \frac{\Gamma(\alpha+\beta-1)}{\Gamma(\alpha)\Gamma(\beta)} p^{\alpha-1} (1-p)^{\beta-1} \,,
\]</span>
<em>except</em> that the normalization term is not quite right: here we have
<span class="math inline">\(\Gamma(\alpha+\beta-1)\)</span> instead of <span class="math inline">\(\Gamma(\alpha+\beta)\)</span>. But thats
fine: there is no requirement that a likelihood function integrate to
one over its domain. (Here, as the interested reader can verify,
the likelihood function integrates to <span class="math inline">\(1/(k+1)\)</span>.)
So, in the end, if we observe a random variable <span class="math inline">\(X \sim\)</span> Binomial(<span class="math inline">\(k,p\)</span>),
then the likelihood function <span class="math inline">\(\mathcal{L}(p \vert k,x)\)</span> has the shape
(if not the normalization) of a Beta(<span class="math inline">\(x+1,k-x+1\)</span>) distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:betapdf"></span>
<img src="_main_files/figure-html/betapdf-1.png" alt="\label{fig:betapdf}Three examples of beta probability density functions: Beta(2,2) (red), Beta(4,2) (blue), and Beta(2,3) (green)." width="50%" />
<p class="caption">
Figure 3.12: Three examples of beta probability density functions: Beta(2,2) (red), Beta(4,2) (blue), and Beta(2,3) (green).
</p>
</div>
<table>
<caption>Beta Distribution - <code>R</code> Functions</caption>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PDF</td>
<td><code>dbeta(x,shape1,shape2)</code></td>
</tr>
<tr class="even">
<td>CDF</td>
<td><code>pbeta(x,shape1,shape2)</code></td>
</tr>
<tr class="odd">
<td>Inverse CDF</td>
<td><code>qbeta(q,shape1,shape2)</code></td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rbeta(n,shape1,shape2)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="the-expected-value-of-a-beta-random-variable" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> The Expected Value of a Beta Random Variable<a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The expected value of a random variable sampled from a Beta(<span class="math inline">\(\alpha,\beta\)</span>)
distribution is
<span class="math display">\[\begin{align*}
E[X] = \int_0^1 x f_X(x) dx &amp;= \int_0^1 x \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha,\beta)} dx \\
&amp;= \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \int_0^1 \frac{x^{\alpha} (1-x)^{\beta-1}}{B(\alpha+1,\beta)} dx \\
&amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \,.
\end{align*}\]</span>
The last result follows from the fact that the integrand is the pdf for
a Beta(<span class="math inline">\(\alpha+1,\beta\)</span>) distribution, and the integral is over the entire domain,
hence the integral evaluates to 1.</p>
</blockquote>
<blockquote>
<p>Continuing,
<span class="math display">\[\begin{align*}
E[X] &amp;= \frac{B(\alpha+1,\beta)}{B(\alpha,\beta)} \\
&amp;= \frac{\Gamma(\alpha+1) \Gamma(\beta)}{\Gamma(\alpha+\beta+1)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \\
&amp;= \frac{\alpha \Gamma(\alpha)}{(\alpha+\beta)\Gamma(\alpha+\beta)} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)} \\
&amp;= \frac{\alpha}{\alpha+\beta} \,.
\end{align*}\]</span>
Here, we take advantage of the recursive property of the gamma function.</p>
</blockquote>
<blockquote>
<p>We can utilize a similar strategy to determine the variance of a beta random variable,
starting by computing <span class="math inline">\(E[X^2]\)</span> and utilizing the shortcut formula
<span class="math inline">\(V[X] = E[X]^2 - (E[X])^2\)</span>. The final result is
<span class="math display">\[
V[X] = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)} \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="the-sample-median-of-a-uniform01-distribution" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> The Sample Median of a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The Uniform(0,1) distribution is
<span class="math display">\[
f_X(x) = 1 ~~~ x \in [0,1] \,.
\]</span>
Lets assume that we draw <span class="math inline">\(n\)</span> iid data from this distribution, with <span class="math inline">\(n\)</span> odd.
Then we can utilize a result from order statistics to write down the pdf of
the sample median, which is the <span class="math inline">\(j^{\rm th}\)</span> order statstic (where <span class="math inline">\(j = (n+1)/2\)</span>):
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} f_X(x) \left[ F_X(x) \right]^{(n-1)/2} \left[ 1 - F_X(x) \right]^{(n-1)/2} \,.
\]</span>
Given that
<span class="math display">\[
F_X(x) = \int_0^x dy = x ~~~ x \in [0,1] \,,
\]</span>
we can write
<span class="math display">\[
f_{((n+1)/2)}(x) = \frac{n!}{\left(\frac{n-1}{2}\right)! \left(\frac{n-1}{2}\right)!} x^{(n-1)/2} (1-x)^{(n-1)/2} \,,
\]</span>
for <span class="math inline">\(x \in [0,1]\)</span>. This function has both the form of a beta distribution
(with <span class="math inline">\(\alpha = \beta = (n+1)/2\)</span>) and the domain of a beta distribution,
so <span class="math inline">\(\tilde{X} \sim\)</span> Beta<span class="math inline">\(\left(\frac{n+1}{2},\frac{n+1}{2}\right)\)</span>.</p>
</blockquote>
<blockquote>
<p>(In fact, we can go further and state a more general result:
<span class="math inline">\(X_{(j)} \sim\)</span> Beta(<span class="math inline">\(j,n-j+1\)</span>): <em>all</em> the order statistics for data
drawn from a Uniform(0,1) distribution are beta-distributed random variables!)</p>
</blockquote>
<hr />
</div>
<div id="testing-hypotheses-using-the-sample-median-of-a-uniform01-distribution" class="section level3 hasAnchor" number="3.11.3">
<h3><span class="header-section-number">3.11.3</span> Testing Hypotheses Using the Sample Median of a Uniform(0,1) Distribution<a href="the-binomial-and-related-distributions.html#testing-hypotheses-using-the-sample-median-of-a-uniform01-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we draw <span class="math inline">\(n\)</span> iid data, where <span class="math inline">\(n\)</span> is an odd number,
from a Uniform(<span class="math inline">\(0,\theta\)</span>)
distribution, and that we wish to test the hypothesis <span class="math inline">\(H_o: \mu = \mu_o = 1/2\)</span> versus <span class="math inline">\(H_a: \mu = \mu_a &gt; \mu_o\)</span> at level <span class="math inline">\(\alpha\)</span>.
(Lets also assume that all
the data we observe lay in the range <span class="math inline">\([0,1]\)</span>otherwise the null cannot
be correct. We will return to this point in Chapter 5.) It sounds like
we might use the Neymann-Pearson lemma here, but as we will see in Chapter
5, that would dictate a different test statistic than what we want to
use here, which is the sample median (which is not a sufficient statistic).
So here we will simply fall back on the methodology shown in Chapters 1
and 2, i.e., we will solve
<span class="math display">\[
F_Y(y_{1-\alpha} \vert \theta_o) - \left(1 - \alpha\right) = 0 \,,
\]</span>
where <span class="math inline">\(Y = X_{((n+1)/2)}\)</span> is the sample median. We start by deriving
the cdf <span class="math inline">\(F_{((n+1)/2)}\)</span>:
<span class="math display">\[\begin{align*}
F_{((n+1)/2)}(x) &amp;= \int_0^x f_{((n+1)/2)}(v) dv = \ldots \,.
\end{align*}\]</span>
But, theres an issue. While the expressions for the cdfs for the
minimum and maximum values are comprised of single terms, the expression
for the cdf of the median is comprised of a summation of terms. (Go back
to the section on order statistics earlier in this chapter to verify this.)
Hence we cannot work with this cdf by hand. Sowhat can we do?</p>
</blockquote>
<blockquote>
<p>Utilize numerical integration, thats what.</p>
</blockquote>
<blockquote>
<p>Here is the code we need:</p>
</blockquote>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="the-binomial-and-related-distributions.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes the cdf of the sample median at coordinate x</span></span>
<span id="cb207-2"><a href="the-binomial-and-related-distributions.html#cb207-2" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x,n)</span>
<span id="cb207-3"><a href="the-binomial-and-related-distributions.html#cb207-3" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb207-4"><a href="the-binomial-and-related-distributions.html#cb207-4" aria-hidden="true" tabindex="-1"></a>  j <span class="ot">&lt;-</span> (n<span class="dv">-1</span>)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb207-5"><a href="the-binomial-and-related-distributions.html#cb207-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lfactorial(a) == log(a!)</span></span>
<span id="cb207-6"><a href="the-binomial-and-related-distributions.html#cb207-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># exp(lfactorial(a)) == a!</span></span>
<span id="cb207-7"><a href="the-binomial-and-related-distributions.html#cb207-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(<span class="fu">lfactorial</span>(n)<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">lfactorial</span>(j))<span class="sc">*</span>x<span class="sc">^</span>j<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>x)<span class="sc">^</span>j</span>
<span id="cb207-8"><a href="the-binomial-and-related-distributions.html#cb207-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb207-9"><a href="the-binomial-and-related-distributions.html#cb207-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-10"><a href="the-binomial-and-related-distributions.html#cb207-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Find root of F_Y(y) - (1-alpha)</span></span>
<span id="cb207-11"><a href="the-binomial-and-related-distributions.html#cb207-11" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(y,n,alpha)</span>
<span id="cb207-12"><a href="the-binomial-and-related-distributions.html#cb207-12" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb207-13"><a href="the-binomial-and-related-distributions.html#cb207-13" aria-hidden="true" tabindex="-1"></a>  F.med <span class="ot">&lt;-</span> <span class="fu">integrate</span>(f,<span class="dv">0</span>,y,<span class="at">n=</span>n)</span>
<span id="cb207-14"><a href="the-binomial-and-related-distributions.html#cb207-14" aria-hidden="true" tabindex="-1"></a>  F.med<span class="sc">$</span>value <span class="sc">-</span> (<span class="dv">1</span><span class="sc">-</span>alpha)</span>
<span id="cb207-15"><a href="the-binomial-and-related-distributions.html#cb207-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb207-16"><a href="the-binomial-and-related-distributions.html#cb207-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-17"><a href="the-binomial-and-related-distributions.html#cb207-17" aria-hidden="true" tabindex="-1"></a><span class="fu">uniroot</span>(g,<span class="at">interval=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">n=</span><span class="dv">5</span>,<span class="at">alpha=</span><span class="fl">0.05</span>)<span class="sc">$</span>root</span></code></pre></div>
<pre><code>## [1] 0.8107542</code></pre>
<blockquote>
<p>In words, the function <code>f</code> above computes <span class="math inline">\(F_{((n+1)/2)}(x)\)</span>, while
the function <code>g</code> evaluates the function that we are trying to
solve, specifically <span class="math inline">\(F_{((n+1)/2)}(x) - (1-\alpha) = 0\)</span>. The call
to <code>uniroot()</code> specifies that the root is between 0 and 1 (since
<span class="math inline">\(\theta_o = 2\mu_o = 1\)</span>). We see that if <span class="math inline">\(n = 5\)</span>, we would reject the null
hypothesis that <span class="math inline">\(\mu_o = 1/2\)</span> if and only if <span class="math inline">\(X_{((n+1)/2)} \geq 0.811\)</span>,
i.e., if three of the five values are greater than 0.811.</p>
</blockquote>
<blockquote>
<p>Something to take away from this example is to realize that just because
we cannot write down an analytical expression (in this case, for the
cdf of the median of <span class="math inline">\(n\)</span> Uniform(0,1) random variables), we should not
just give up, as it may be easy to implement numerical methods! (As it
was in this case.)</p>
</blockquote>
</div>
</div>
<div id="the-multinomial-distribution" class="section level2 hasAnchor" number="3.12">
<h2><span class="header-section-number">3.12</span> The Multinomial Distribution<a href="the-binomial-and-related-distributions.html#the-multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets suppose we are in a situation in which we are gathering categorical data. For instance, we might be</p>
<ul>
<li>throwing a ball and recording which of bins numbered 1 through <span class="math inline">\(m\)</span> it lands in;</li>
<li>categorizing the condition of old coins as mint, very good, fine, etc.; or</li>
<li>classifying a galaxy as being a spiral galaxy, an elliptical galaxy, or an irregular galaxy.</li>
</ul>
<p>These are examples of <em>multinomial trials</em>, which is a generalization of the concept of binomial trials to
situations where the number of possible outcomes is <span class="math inline">\(m \geq 2\)</span> rather than <span class="math inline">\(m = 2\)</span>. Earlier in this chapter,
we wrote down the five properties of binomial trials. The analogous properties of a multinomial experiment
are the following:</p>
<ol style="list-style-type: decimal">
<li>It consists of <span class="math inline">\(k\)</span> trials, with <span class="math inline">\(k\)</span> chosen in advance.</li>
<li>There are <span class="math inline">\(m\)</span> possible outcomes for each trial.</li>
<li>A trial may have no more than one realized outcome.</li>
<li>The outcomes of each trial are independent.</li>
<li>The probability of achieving the <span class="math inline">\(i^{th}\)</span> outcome is <span class="math inline">\(p_i\)</span>, a constant quantity.</li>
<li>The probabilities of each outcome sum to one: <span class="math inline">\(\sum_{i=1}^m p_i = 1\)</span>.</li>
<li>The number of trials that achieve a particular outcome is <span class="math inline">\(X_i\)</span>, with <span class="math inline">\(\sum_{i=1}^m X_i = k\)</span>.</li>
</ol>
<p>The probability of any given outcome <span class="math inline">\(\{X_1,\ldots,X_m\}\)</span> is given by the multinomial probability mass
function:
<span class="math display">\[
p(x_1,\ldots,x_m \vert p_1,\ldots,p_m) = \frac{k!}{x_1! \cdots x_m!}p_1^{x_1}\cdots p_m^{x_m} \,.
\]</span>
The distribution for any given <span class="math inline">\(X_i\)</span> itself is binomial, with
<span class="math inline">\(E[X_i] = kp_i\)</span> and <span class="math inline">\(V[X_i] = kp_i(1-p_i)\)</span>. This makes intuitive sense,
as one either observes <span class="math inline">\(i\)</span> as a trial outcome (success),
or something else (failure). However, the <span class="math inline">\(X_i\)</span>s are <em>not</em> independent random
variables; the covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>, a metric of linear
dependence, is Cov(<span class="math inline">\(X_i\)</span>,<span class="math inline">\(X_j\)</span>) = <span class="math inline">\(-kp_ip_j\)</span> if <span class="math inline">\(i \neq j\)</span>. (We will discuss
the concept of covariance at length in Chapter 6.)
This also makes intuitive sense: given that the number of trials <span class="math inline">\(k\)</span> is
fixed, observing more data that achieve one outcome will usually mean
we will observe fewer data achieving any given other outcome.</p>
<table>
<caption>Multinomial Distribution - <code>R</code> Functions</caption>
<colgroup>
<col width="32%" />
<col width="68%" />
</colgroup>
<thead>
<tr class="header">
<th>quantity</th>
<th><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PMF</td>
<td><code>dmultinom(x,m,p)</code> (<span class="math inline">\(x\)</span> and <span class="math inline">\(p\)</span> are vectors of length <span class="math inline">\(m\)</span>)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(n\)</span> iid random samples</td>
<td><code>rmultinom(n,m,p)</code> (<span class="math inline">\(p\)</span> is a vector of length <span class="math inline">\(m\)</span>)</td>
</tr>
</tbody>
</table>
</div>
<div id="chi-square-based-hypothesis-testing" class="section level2 hasAnchor" number="3.13">
<h2><span class="header-section-number">3.13</span> Chi-Square-Based Hypothesis Testing<a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine that we are scientists, sitting on a platform in the middle of a plain.
We are recording the number of animals of a particular species that we see, and
the azimuthal angle for each one. (An azimuthal angle is, e.g.,
<span class="math inline">\(0^\circ\)</span> when looking directly north, and 90<span class="math inline">\(^\circ\)</span>, 180<span class="math inline">\(^\circ\)</span>, and
270<span class="math inline">\(^\circ\)</span> as we look east, south, and west, etc.)
The question we want to answer is, are the animals uniformly distributed as a function
of angle?</p>
<p>There is no unique way to answer this question. However, a very common approach is
to bin the data and use a chi-square goodness-of-fit (GoF) hypothesis test. Lets
assume weve observed 100 animals and that we record the numbers seen in each of
four quadrants:</p>
<table>
<colgroup>
<col width="40%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>angle range</th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>number of animals</td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
</tr>
</tbody>
</table>
<p>There is nothing special about the choice of four quadrants<span class="math inline">\(-\)</span>we could have
chosen eight, etc.<span class="math inline">\(-\)</span>but as well see below, the chi-square GoF test is an
approximate test and its results become more precise as the numbers of data in each
bin get larger. So there is a tradeoff if we increase the number of quadrants: we might
be able to detect smaller-scale non-uniformities, but are test results will become less
precise!</p>
<p>To perform a chi-square GoF test with these data, we first specify are null and
alternative hypotheses:
<span class="math display">\[\begin{align*}
H_o &amp;: p_1 = p_{1,o},\cdots,p_m = p_{m,o} ~~ vs. ~~ H_a &amp;: \mbox{at least two of the probabilities differ} \,,
\end{align*}\]</span>
where <span class="math inline">\(p_{i,o}\)</span> is the null hypothesis proportion in bin <span class="math inline">\(i\)</span>.
Here, <span class="math inline">\(k = 100\)</span>, <span class="math inline">\(m = 4\)</span>, and
<span class="math inline">\(p_{1,o} = p_{2,o} = p_{3,o} = p_{4,o} = 0.25\)</span>we expect, under
the null, to see 25 animals in each quadrant. To make the connection with the
previous section of the chapter now, the null hypothesis is that the data are
<em>multinomially distributed</em> with specified proportions being expected in each of
the <span class="math inline">\(m\)</span> defined bins.</p>
<p>As we might imagine, performing a hypothesis test of the form given above
would be computationally difficult; multinomial distributions are intrinsically
high-dimensional and the data (the numbers of counts in each bin) are not iid.
In 1900, the statistician
Karl Pearson proposed a workaround for testing
multinomial hypotheses, by utilizing the following as a test statistic:
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i-E[X_i])^2}{E[X_i]} = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} \,.
\]</span>
This looks suspiciously familiar, yet off, at the same time.
Above, we stated that the <span class="math inline">\(X_i\)</span>s in a multinomial experiment are themselves
binomially distributed, and thus if we were to standardize them, we would get
<span class="math display">\[
Z_i = \frac{X_i - E[X_i]}{\sqrt{V[X_i]}} = \frac{X_i - kp_i}{\sqrt{kp_i(1-p_i)}} \,.
\]</span>
If <span class="math inline">\(kp_i\)</span> is sufficiently large, then this standardized quantity is approximately
standard normally distributedwhich means that the square of this quantity
is approximately chi-square distributed for one degree of freedom.
So: if we assume that <span class="math inline">\(k\)</span> is sufficiently large, and the <span class="math inline">\(p_i\)</span>s are
sufficiently small, such that we can assume <span class="math inline">\(kp_i(1-p_i) \approx kp_i\)</span>, then we assume that
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} \approx \sum_{i=1}^m Z_i^2 \stackrel{d}{\rightarrow} W \sim \chi_{m-1}^2 \,,
\]</span>
i.e., <span class="math inline">\(W\)</span> converges in distribution to a random variable that is
chi-square-distributed with <span class="math inline">\(m-1\)</span> degrees of freedom.
(We subtract 1 because only <span class="math inline">\(m-1\)</span> of the multinomial
probabilities can freely vary; the <span class="math inline">\(m^{th}\)</span> one is constrained by
the fact that the probabilities must sum to 1. This constraint is what makes multinomial
data <em>not</em> iid.)</p>
<p>The chi-square GoF test is
an <em>upper-tail test</em> in which, in addition to the approximations made above, we also
assume that we can ignore the correlations between the <span class="math inline">\(X_i\)</span>s in different bins.
For this test,</p>
<ul>
<li>the rejection region boundary is <span class="math inline">\(\{ W &gt; \chi_{1-\alpha}^2 = F_W^{-1}(1-\alpha) \}\)</span> for <span class="math inline">\(m-1\)</span> degrees of freedom (in <code>R</code>, <code>qchisq(1-alpha,m-1)</code>);</li>
<li>the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(1 - F_{W}(x^2)\)</span> (e.g., <code>1-pchisq(x.squared,m-1)</code>); and</li>
<li>the rule-of-thumb is that <span class="math inline">\(kp_i\)</span> must be <span class="math inline">\(\geq\)</span> 5 in each bin for the test to yield a valid result.</li>
</ul>
<p>This last point underscores the tradeoff between splitting the data over more bins
and test precision!</p>
<p>In a chi-square GoF test, the inputs are the observed data and the hypothesized
proportions. There are variations on this test in which the inputs are simply the
observed data, variations that differ by how the data are collected:</p>
<ul>
<li><em>chi-square test of independence</em>: the question is whether two variables are
associated with each other in a population; subjects are selected and the
values of two variables are recorded for each. For instance, we might select
<span class="math inline">\(k\)</span> people at random and record whether or not they have had Covid-19, and
also record whether or not they initially had zero, one, or two vaccine shots.</li>
<li><em>chi-square test of homogeneity</em>: the question is whether the distribution
of a single variable is the same for two subgroups of a population, with subjects
being selected randomly from each subgroup separately. For instance, we might
select <span class="math inline">\(k\)</span> people under 20 years of age and ask if they prefer vanilla, chocolate,
or strawberry ice cream, and then repeat the process for people of age 20 or over.</li>
</ul>
<p>Whether we perform a test of independence versus one of homogeneity affects the
interpretation of results, but algorithmically the tests are identical.
Under the null hypothesis,
<span class="math display">\[
\widehat{E[X_{ij}]} = \frac{r_i c_j}{k} \,,
\]</span>
i.e., the expected value in the cell in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> is the product of the total number
of data in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span>, divided by the total number of data overall. Then,
<span class="math display">\[
W = \sum_{i=1}^r \sum_{j=1}^c \frac{(X_{ij} - \widehat{E[X_{ij}]})^2}{\widehat{E[X_{ij}]}} \sim \chi_{(r-1)(c-1)}^2 ~\mbox{under~the~null~(by~assumption!)} \,.
\]</span>
i.e., the test statistic <span class="math inline">\(W\)</span> is, under the null, assumed to be
chi-square distributed for <span class="math inline">\((r-1)\)</span> times <span class="math inline">\((c-1)\)</span> degrees of freedom.</p>
<hr />
<div id="chi-square-goodness-of-fit-test" class="section level3 hasAnchor" number="3.13.1">
<h3><span class="header-section-number">3.13.1</span> Chi-Square Goodness of Fit Test<a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets work with the data presented above at the beginning of this section:</p>
</blockquote>
<table>
<colgroup>
<col width="40%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th>angle range</th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>number of animals</td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
</tr>
</tbody>
</table>
<blockquote>
<p>As stated above, we expect 25 counts in each bin. Given this expectation, are these
data plausible, at level <span class="math inline">\(\alpha = 0.05\)</span>?</p>
</blockquote>
<blockquote>
<p>We first compute the test statistic:
<span class="math display">\[
W = \sum_{i=1}^m \frac{(X_i - kp_i)^2}{kp_i} = \frac{(28-25)^2}{25} + \frac{(32-25)^2}{25} + \frac{(17-25)^2}{25} + \frac{(23-25)^2}{25} = \frac{9}{25} + \frac{49}{25} + \frac{64}{25} + \frac{4}{25} = \frac{126}{25} = 5.04 \,.
\]</span>
The number of degrees of freedom is <span class="math inline">\(m-1 = 3\)</span>, so the rejection region boundary
is <span class="math inline">\(F_W(1-\alpha) = F_W(0.95) =\)</span> <code>qchisq(0.95,3)</code> = 7.815. Since 5.04 <span class="math inline">\(&lt;\)</span> 7.815, we
fail to reject the null hypothesis that the animals are distributed uniformly as
a function of azimuthal angle. (The <span class="math inline">\(p\)</span>-value is <code>1-pchisq(5.04,3)</code> = 0.169.)
See Figure <a href="the-binomial-and-related-distributions.html#fig:chi2gof">3.13</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chi2gof"></span>
<img src="_main_files/figure-html/chi2gof-1.png" alt="\label{fig:chi2gof}An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, so we fail to reject the null hypothesis." width="50%" />
<p class="caption">
Figure 3.13: An illustration of the sampling distribution and rejection region for the chi-square goodness-of-fit test. Here, the number of degrees of freedom is 3, so the rejection region are values of chi-square above 7.815. The observed test statistic is 5.04, which lies outside the rejection region, so we fail to reject the null hypothesis.
</p>
</div>
<hr />
</div>
<div id="simulating-an-exact-multinomial-test" class="section level3 hasAnchor" number="3.13.2">
<h3><span class="header-section-number">3.13.2</span> Simulating an Exact Multinomial Test<a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume the same data as in the last example. One reason<span class="math inline">\(-\)</span>in fact,
<em>the</em> reason<span class="math inline">\(-\)</span>why we utilize the chi-square GoF test when analyzing these
data is that it has always been done this way. Stated differently, we
have historically not worked with the multinomial distribution directly
because we couldntat least, not until computers came along. But now
we <em>can</em> estimate the <span class="math inline">\(p\)</span>-value for the hypothesis in the last example
via simulation. Will we achieve a result very different from that above
($p = 0.169)?</p>
</blockquote>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="the-binomial-and-related-distributions.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb209-2"><a href="the-binomial-and-related-distributions.html#cb209-2" aria-hidden="true" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb209-3"><a href="the-binomial-and-related-distributions.html#cb209-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>,<span class="dv">4</span>)</span>
<span id="cb209-4"><a href="the-binomial-and-related-distributions.html#cb209-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb209-5"><a href="the-binomial-and-related-distributions.html#cb209-5" aria-hidden="true" tabindex="-1"></a>num.sim <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb209-6"><a href="the-binomial-and-related-distributions.html#cb209-6" aria-hidden="true" tabindex="-1"></a>k       <span class="ot">&lt;-</span> <span class="fu">sum</span>(O)</span>
<span id="cb209-7"><a href="the-binomial-and-related-distributions.html#cb209-7" aria-hidden="true" tabindex="-1"></a>m       <span class="ot">&lt;-</span> <span class="fu">length</span>(O)</span>
<span id="cb209-8"><a href="the-binomial-and-related-distributions.html#cb209-8" aria-hidden="true" tabindex="-1"></a>pmf.obs <span class="ot">&lt;-</span> <span class="fu">dmultinom</span>(O,<span class="at">prob=</span>p)    <span class="co"># the observed multinomial pmf</span></span>
<span id="cb209-9"><a href="the-binomial-and-related-distributions.html#cb209-9" aria-hidden="true" tabindex="-1"></a>X       <span class="ot">&lt;-</span> <span class="fu">rmultinom</span>(num.sim,k,p) <span class="co"># generates an m x num.sim matrix</span></span>
<span id="cb209-10"><a href="the-binomial-and-related-distributions.html#cb209-10" aria-hidden="true" tabindex="-1"></a>                                  <span class="co"># (m determined as the length of p)</span></span>
<span id="cb209-11"><a href="the-binomial-and-related-distributions.html#cb209-11" aria-hidden="true" tabindex="-1"></a>pmf     <span class="ot">&lt;-</span> <span class="fu">apply</span>(X,<span class="dv">2</span>,<span class="cf">function</span>(x){<span class="fu">dmultinom</span>(x,<span class="at">prob=</span>p)}) <span class="co"># simulated pmf&#39;s</span></span>
<span id="cb209-12"><a href="the-binomial-and-related-distributions.html#cb209-12" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(pmf<span class="sc">&lt;</span>pmf.obs)<span class="sc">/</span>num.sim </span></code></pre></div>
<pre><code>## [1] 0.15974</code></pre>
<blockquote>
<p>We observe <span class="math inline">\(p = 0.1597\)</span>. This is close to, but at the same time still
substantially
less than, 0.169. In case the reader is to say but, the computation
time must be much longer than for the chi-square GoF test,
the above computation takes <span class="math inline">\(\sim\)</span> 1 CPU second.
The chi-square test is definitely important to know, in part because
it has always been done this waybut we would argue
that when a simulation of the exact test is possible, one should
code that simulation! (And run as many simulations as possible, to
reduce uncertainty in the final result. Here, we can take our estimated
<span class="math inline">\(p\)</span>-value of 0.1597 and state that our one standard error uncertainty
is approximately <span class="math inline">\(\sqrt{kp(1-p)}/k = 0.0011\)</span>, i.e., we expect the true
<span class="math inline">\(p\)</span>-value to be in the range <span class="math inline">\(0.1597 \pm 3 \cdot 0.0011\)</span> or
<span class="math inline">\([0.1564,0.1630]\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="chi-square-test-of-independence" class="section level3 hasAnchor" number="3.13.3">
<h3><span class="header-section-number">3.13.3</span> Chi-Square Test of Independence<a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets go back to our animal data. When we observe the animals in each quadrant, we
record their color: black or red. So now are data look like this:</p>
</blockquote>
<table>
<colgroup>
<col width="23%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>black</td>
<td>20</td>
<td>18</td>
<td>5</td>
<td>14</td>
<td>57</td>
</tr>
<tr class="even">
<td>red</td>
<td>8</td>
<td>14</td>
<td>12</td>
<td>9</td>
<td>43</td>
</tr>
<tr class="odd">
<td></td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>When we record two attributes for each subject (here, azimuthal angle and color), we
can perform a chi-square test of independence to answer the question of whether
the attributes are independent random variables. In other words, here, does the
coloration <em>depend</em> on angle? The null hypothesis is no. We will test this hypothesis
assuming <span class="math inline">\(\alpha = 0.05\)</span>.</p>
</blockquote>
<blockquote>
<p>We first determine the expected number of counts in each bin,
<span class="math inline">\(\widehat{E[X_{ij}]} = r_i c_j / k\)</span>:</p>
</blockquote>
<table>
<colgroup>
<col width="8%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0<span class="math inline">\(^\circ\)</span>-90<span class="math inline">\(^\circ\)</span></th>
<th>90<span class="math inline">\(^\circ\)</span>-180<span class="math inline">\(^\circ\)</span></th>
<th>180<span class="math inline">\(^\circ\)</span>-270<span class="math inline">\(^\circ\)</span></th>
<th>270<span class="math inline">\(^\circ\)</span>-360<span class="math inline">\(^\circ\)</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>black</td>
<td>57 <span class="math inline">\(\cdot\)</span> 28/100 = 15.96</td>
<td>57 <span class="math inline">\(\cdot\)</span> 32/100 = 18.24</td>
<td>57 <span class="math inline">\(\cdot\)</span> 17/100 = 9.69</td>
<td>57 <span class="math inline">\(\cdot\)</span> 23/100 = 13.11</td>
<td>57</td>
</tr>
<tr class="even">
<td>red</td>
<td>43 <span class="math inline">\(\cdot\)</span> 28/100 = 12.04</td>
<td>43 <span class="math inline">\(\cdot\)</span> 32/100 = 13.76</td>
<td>43 <span class="math inline">\(\cdot\)</span> 17/100 = 7.31</td>
<td>43 <span class="math inline">\(\cdot\)</span> 23/100 = 9.89</td>
<td>43</td>
</tr>
<tr class="odd">
<td></td>
<td>28</td>
<td>32</td>
<td>17</td>
<td>23</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>We can already see that working with the numbers directly is tedious. Can we make
a matrix of such numbers using <code>R</code>?</p>
</blockquote>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="the-binomial-and-related-distributions.html#cb211-1" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">57</span>,<span class="dv">43</span>)</span>
<span id="cb211-2"><a href="the-binomial-and-related-distributions.html#cb211-2" aria-hidden="true" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">28</span>,<span class="dv">32</span>,<span class="dv">17</span>,<span class="dv">23</span>)</span>
<span id="cb211-3"><a href="the-binomial-and-related-distributions.html#cb211-3" aria-hidden="true" tabindex="-1"></a>E <span class="ot">&lt;-</span> (r <span class="sc">%*%</span> <span class="fu">t</span>(c))<span class="sc">/</span><span class="fu">sum</span>(r)  <span class="co"># multiply r and the transpose of c </span></span>
<span id="cb211-4"><a href="the-binomial-and-related-distributions.html#cb211-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(E)                  <span class="co"># much better</span></span></code></pre></div>
<pre><code>##       [,1]  [,2] [,3]  [,4]
## [1,] 15.96 18.24 9.69 13.11
## [2,] 12.04 13.76 7.31  9.89</code></pre>
<blockquote>
<p>If we wish to continue using <code>R</code>, we need to define a matrix of observed data values:</p>
</blockquote>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="the-binomial-and-related-distributions.html#cb213-1" aria-hidden="true" tabindex="-1"></a>O <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">8</span>,<span class="dv">18</span>,<span class="dv">14</span>,<span class="dv">5</span>,<span class="dv">12</span>,<span class="dv">14</span>,<span class="dv">9</span>),<span class="at">nrow=</span><span class="dv">2</span>) <span class="co"># fills in column-by-column</span></span>
<span id="cb213-2"><a href="the-binomial-and-related-distributions.html#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(O)</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   20   18    5   14
## [2,]    8   14   12    9</code></pre>
<blockquote>
<p>Now we have what we need. The test statistic is</p>
</blockquote>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="the-binomial-and-related-distributions.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">sum</span>( (O<span class="sc">-</span>E)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>E ),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.805</code></pre>
<blockquote>
<p>and the number of degrees of freedom is <span class="math inline">\((r-1)(c-1) = 1 \cdot 3 = 3\)</span>, so the
rejection region is</p>
</blockquote>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="the-binomial-and-related-distributions.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">qchisq</span>(<span class="fl">0.95</span>,<span class="dv">3</span>),<span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 7.815</code></pre>
<blockquote>
<p>We find that if <span class="math inline">\(\alpha = 0.05\)</span>, we <em>cannot</em> reject the null hypothesis. We might be
tempted to do so, as our test statistic <em>very nearly</em> falls into the rejection region,
but we cannot. We could, if we were so inclined, remind ourselves that chi-square-based
hypothesis tests are <em>approximate</em>, and run a simulation to try to estimate the
true distribution of <span class="math inline">\(W\)</span>, and see what the rejection region and <span class="math inline">\(p\)</span>-value actually
arethat way, we <em>might</em> be able to actually reject the null.</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-normal-and-related-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-poisson-and-related-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

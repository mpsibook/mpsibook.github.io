<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Multivariate Distributions | Modern Probability and Statistical Inference</title>
  <meta name="description" content="6 Multivariate Distributions | Modern Probability and Statistical Inference" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Multivariate Distributions | Modern Probability and Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Multivariate Distributions | Modern Probability and Statistical Inference" />
  
  
  

<meta name="author" content="Peter E. Freeman (Department of Statistics &amp; Data Science, Carnegie Mellon University)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-uniform-distribution.html"/>
<link rel="next" href="further-conceptual-details-optional.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html"><i class="fa fa-check"></i><b>1</b> The Basics of Probability and Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#data-and-statistical-populations"><i class="fa fa-check"></i><b>1.1</b> Data and Statistical Populations</a></li>
<li class="chapter" data-level="1.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#sample-spaces-and-the-axioms-of-probability"><i class="fa fa-check"></i><b>1.2</b> Sample Spaces and the Axioms of Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#utilizing-set-notation"><i class="fa fa-check"></i><b>1.2.1</b> Utilizing Set Notation</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-contingency-tables"><i class="fa fa-check"></i><b>1.2.2</b> Working With Contingency Tables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-probability-and-the-independence-of-events"><i class="fa fa-check"></i><b>1.3</b> Conditional Probability and the Independence of Events</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-contingency-tables"><i class="fa fa-check"></i><b>1.3.1</b> Visualizing Conditional Probabilities: Contingency Tables</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#conditional-independence"><i class="fa fa-check"></i><b>1.3.2</b> Conditional Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#further-laws-of-probability"><i class="fa fa-check"></i><b>1.4</b> Further Laws of Probability</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-additive-law-for-independent-events"><i class="fa fa-check"></i><b>1.4.1</b> The Additive Law for Independent Events</a></li>
<li class="chapter" data-level="1.4.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-monty-hall-problem"><i class="fa fa-check"></i><b>1.4.2</b> The Monty Hall Problem</a></li>
<li class="chapter" data-level="1.4.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-conditional-probabilities-tree-diagrams"><i class="fa fa-check"></i><b>1.4.3</b> Visualizing Conditional Probabilities: Tree Diagrams</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#random-variables"><i class="fa fa-check"></i><b>1.5</b> Random Variables</a></li>
<li class="chapter" data-level="1.6" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#probability-distributions"><i class="fa fa-check"></i><b>1.6</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-density-function"><i class="fa fa-check"></i><b>1.6.1</b> A Simple Probability Density Function</a></li>
<li class="chapter" data-level="1.6.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#shape-parameters-and-families-of-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Shape Parameters and Families of Distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-simple-probability-mass-function"><i class="fa fa-check"></i><b>1.6.3</b> A Simple Probability Mass Function</a></li>
<li class="chapter" data-level="1.6.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#a-more-complex-example-involving-both-masses-and-densities"><i class="fa fa-check"></i><b>1.6.4</b> A More Complex Example Involving Both Masses and Densities</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#characterizing-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Characterizing Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-tricks"><i class="fa fa-check"></i><b>1.7.1</b> Expected Value Tricks</a></li>
<li class="chapter" data-level="1.7.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#variance-tricks"><i class="fa fa-check"></i><b>1.7.2</b> Variance Tricks</a></li>
<li class="chapter" data-level="1.7.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-shortcut-formula-for-variance"><i class="fa fa-check"></i><b>1.7.3</b> The Shortcut Formula for Variance</a></li>
<li class="chapter" data-level="1.7.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-expected-value-and-variance-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.7.4</b> The Expected Value and Variance of a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-probability-distributions"><i class="fa fa-check"></i><b>1.8</b> Working With R: Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-conditional-probability"><i class="fa fa-check"></i><b>1.8.1</b> Numerical Integration and Conditional Probability</a></li>
<li class="chapter" data-level="1.8.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#numerical-integration-and-variance"><i class="fa fa-check"></i><b>1.8.2</b> Numerical Integration and Variance</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.9</b> Cumulative Distribution Functions</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cumulative-distribution-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>1.9.1</b> The Cumulative Distribution Function for a Probability Density Function</a></li>
<li class="chapter" data-level="1.9.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-cumulative-distribution-function-in-r"><i class="fa fa-check"></i><b>1.9.2</b> Visualizing the Cumulative Distribution Function in R</a></li>
<li class="chapter" data-level="1.9.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-cdf-for-a-mathematically-discontinuous-distribution"><i class="fa fa-check"></i><b>1.9.3</b> The CDF for a Mathematically Discontinuous Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-probability"><i class="fa fa-check"></i><b>1.10</b> The Law of Total Probability</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-simple-discrete-distributions"><i class="fa fa-check"></i><b>1.10.1</b> The LoTP With Two Simple Discrete Distributions</a></li>
<li class="chapter" data-level="1.10.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-law-of-total-expectation"><i class="fa fa-check"></i><b>1.10.2</b> The Law of Total Expectation</a></li>
<li class="chapter" data-level="1.10.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-lotp-with-two-continuous-distributions"><i class="fa fa-check"></i><b>1.10.3</b> The LoTP With Two Continuous Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-data-sampling"><i class="fa fa-check"></i><b>1.11</b> Working With R: Data Sampling</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#more-inverse-transform-sampling"><i class="fa fa-check"></i><b>1.11.1</b> More Inverse-Transform Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistics-and-sampling-distributions"><i class="fa fa-check"></i><b>1.12</b> Statistics and Sampling Distributions</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#expected-value-and-variance-of-the-sample-mean-for-all-distributions"><i class="fa fa-check"></i><b>1.12.1</b> Expected Value and Variance of the Sample Mean (For All Distributions)</a></li>
<li class="chapter" data-level="1.12.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#visualizing-the-distribution-of-the-sample-mean"><i class="fa fa-check"></i><b>1.12.2</b> Visualizing the Distribution of the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.13</b> The Likelihood Function</a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#examples-of-likelihood-functions-for-iid-data"><i class="fa fa-check"></i><b>1.13.1</b> Examples of Likelihood Functions for IID Data</a></li>
<li class="chapter" data-level="1.13.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#coding-the-likelihood-function-in-r"><i class="fa fa-check"></i><b>1.13.2</b> Coding the Likelihood Function in R</a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>1.14</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#comparing-two-estimators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing Two Estimators</a></li>
<li class="chapter" data-level="1.14.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#maximum-likelihood-estimate-of-population-parameter"><i class="fa fa-check"></i><b>1.14.2</b> Maximum Likelihood Estimate of Population Parameter</a></li>
</ul></li>
<li class="chapter" data-level="1.15" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#statistical-inference-with-sampling-distributions"><i class="fa fa-check"></i><b>1.15</b> Statistical Inference with Sampling Distributions</a></li>
<li class="chapter" data-level="1.16" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>1.16</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.16.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.16.1</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.16.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#confidence-interval-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.16.2</b> Confidence Interval Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.17" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.17</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.17.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-increases-with-theta"><i class="fa fa-check"></i><b>1.17.1</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Increases With <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="1.17.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#hypothesis-test-where-ey-decreases-with-theta"><i class="fa fa-check"></i><b>1.17.2</b> Hypothesis Test Where <span class="math inline">\(E[Y]\)</span> Decreases With <span class="math inline">\(\theta\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.18" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#working-with-r-simulating-statistical-inference"><i class="fa fa-check"></i><b>1.18</b> Working With R: Simulating Statistical Inference</a>
<ul>
<li class="chapter" data-level="1.18.1" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#estimating-the-sampling-distribution-for-the-sample-median"><i class="fa fa-check"></i><b>1.18.1</b> Estimating the Sampling Distribution for the Sample Median</a></li>
<li class="chapter" data-level="1.18.2" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#the-empirical-distribution-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>1.18.2</b> The Empirical Distribution of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="1.18.3" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-confidence-coefficient-value"><i class="fa fa-check"></i><b>1.18.3</b> Empirically Verifying the Confidence Coefficient Value</a></li>
<li class="chapter" data-level="1.18.4" data-path="the-basics-of-probability-and-statistical-inference.html"><a href="the-basics-of-probability-and-statistical-inference.html#empirically-verifying-the-type-i-error-alpha"><i class="fa fa-check"></i><b>1.18.4</b> Empirically Verifying the Type I Error <span class="math inline">\(\alpha\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html"><i class="fa fa-check"></i><b>2</b> The Normal (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#motivation"><i class="fa fa-check"></i><b>2.1</b> Motivation</a></li>
<li class="chapter" data-level="2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>2.2</b> Probability Density Function</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#variance-of-a-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.1</b> Variance of a Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#skewness-of-the-normal-probability-density-function"><i class="fa fa-check"></i><b>2.2.2</b> Skewness of the Normal Probability Density Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities"><i class="fa fa-check"></i><b>2.2.3</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-1"><i class="fa fa-check"></i><b>2.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#visualizing-a-cumulative-distribution-function"><i class="fa fa-check"></i><b>2.3.2</b> Visualizing a Cumulative Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-functions"><i class="fa fa-check"></i><b>2.4</b> Moment-Generating Functions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-mass-function"><i class="fa fa-check"></i><b>2.4.1</b> Moment-Generating Function for a Probability Mass Function</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#moment-generating-function-for-a-probability-density-function"><i class="fa fa-check"></i><b>2.4.2</b> Moment-Generating Function for a Probability Density Function</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-functions-of-normal-random-variables"><i class="fa fa-check"></i><b>2.5</b> Linear Functions of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-distribution-of-the-sample-mean-of-iid-normal-random-variables"><i class="fa fa-check"></i><b>2.5.1</b> The Distribution of the Sample Mean of iid Normal Random Variables</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#using-variable-substitution-to-determine-distribution-of-y-ax-b"><i class="fa fa-check"></i><b>2.5.2</b> Using Variable Substitution to Determine Distribution of Y = aX + b</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-known-variance"><i class="fa fa-check"></i><b>2.6</b> Standardizing a Normal Random Variable with Known Variance</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-2"><i class="fa fa-check"></i><b>2.6.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#general-transformations-of-a-single-random-variable"><i class="fa fa-check"></i><b>2.7</b> General Transformations of a Single Random Variable</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#distribution-of-a-transformed-random-variable"><i class="fa fa-check"></i><b>2.7.1</b> Distribution of a Transformed Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#squaring-standard-normal-random-variables"><i class="fa fa-check"></i><b>2.8</b> Squaring Standard Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-a-chi-square-random-variable"><i class="fa fa-check"></i><b>2.8.1</b> The Expected Value of a Chi-Square Random Variable</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-3"><i class="fa fa-check"></i><b>2.8.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#sample-variance-of-normal-random-variables"><i class="fa fa-check"></i><b>2.9</b> Sample Variance of Normal Random Variables</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-4"><i class="fa fa-check"></i><b>2.9.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#expected-value-of-the-sample-variance-and-standard-deviation"><i class="fa fa-check"></i><b>2.9.2</b> Expected Value of the Sample Variance and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#standardizing-a-normal-random-variable-with-unknown-variance"><i class="fa fa-check"></i><b>2.10</b> Standardizing a Normal Random Variable with Unknown Variance</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-sample-mean-and-standard-deviation-are-independent-random-variables"><i class="fa fa-check"></i><b>2.10.1</b> The Sample Mean and Standard Deviation Are Independent Random Variables</a></li>
<li class="chapter" data-level="2.10.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-5"><i class="fa fa-check"></i><b>2.10.2</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#point-estimation-1"><i class="fa fa-check"></i><b>2.11</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#maximum-likelihood-estimation-for-normal-variance"><i class="fa fa-check"></i><b>2.11.1</b> Maximum Likelihood Estimation for Normal Variance</a></li>
<li class="chapter" data-level="2.11.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#asymptotic-normality-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.2</b> Asymptotic Normality of the MLE for the Normal Population Variance</a></li>
<li class="chapter" data-level="2.11.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simulating-the-sampling-distribution-of-the-mle-for-the-normal-population-variance"><i class="fa fa-check"></i><b>2.11.3</b> Simulating the Sampling Distribution of the MLE for the Normal Population Variance</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>2.12</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#computing-probabilities-6"><i class="fa fa-check"></i><b>2.12.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-intervals-1"><i class="fa fa-check"></i><b>2.13</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="2.13.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.13.1</b> Confidence Interval for the Normal Mean With Variance Known</a></li>
<li class="chapter" data-level="2.13.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.13.2</b> Confidence Interval for the Normal Mean With Variance Unknown</a></li>
<li class="chapter" data-level="2.13.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-normal-variance"><i class="fa fa-check"></i><b>2.13.3</b> Confidence Interval for the Normal Variance</a></li>
<li class="chapter" data-level="2.13.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#confidence-interval-for-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.13.4</b> Confidence Interval for the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.13.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#historical-digression-the-pivotal-method"><i class="fa fa-check"></i><b>2.13.5</b> Historical Digression: the Pivotal Method</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-testing-for-normality"><i class="fa fa-check"></i><b>2.14</b> Hypothesis Testing: Testing for Normality</a>
<ul>
<li class="chapter" data-level="2.14.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#example-of-the-ks-test-in-r"><i class="fa fa-check"></i><b>2.14.1</b> Example of the KS Test in R</a></li>
<li class="chapter" data-level="2.14.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-for-normality-shapiro-wilk-test"><i class="fa fa-check"></i><b>2.14.2</b> Testing for Normality: Shapiro-Wilk Test</a></li>
</ul></li>
<li class="chapter" data-level="2.15" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-mean"><i class="fa fa-check"></i><b>2.15</b> Hypothesis Testing: Population Mean</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-known"><i class="fa fa-check"></i><b>2.15.1</b> Testing a Hypothesis About the Normal Mean with Variance Known</a></li>
<li class="chapter" data-level="2.15.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-mean-with-variance-unknown"><i class="fa fa-check"></i><b>2.15.2</b> Testing a Hypothesis About the Normal Mean with Variance Unknown</a></li>
<li class="chapter" data-level="2.15.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-test-about-the-mean-of-an-arbitrary-distribution-using-the-clt"><i class="fa fa-check"></i><b>2.15.3</b> Hypothesis Test About the Mean of an Arbitrary Distribution: Using the CLT</a></li>
<li class="chapter" data-level="2.15.4" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-t-test"><i class="fa fa-check"></i><b>2.15.4</b> Extension to Testing With Two Data Samples: the t Test</a></li>
<li class="chapter" data-level="2.15.5" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#more-about-p-values"><i class="fa fa-check"></i><b>2.15.5</b> More About p-Values</a></li>
<li class="chapter" data-level="2.15.6" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#how-many-data-do-we-need-to-achieve-a-given-test-power"><i class="fa fa-check"></i><b>2.15.6</b> How Many Data Do We Need to Achieve a Given Test Power?</a></li>
</ul></li>
<li class="chapter" data-level="2.16" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#hypothesis-testing-population-variance"><i class="fa fa-check"></i><b>2.16</b> Hypothesis Testing: Population Variance</a>
<ul>
<li class="chapter" data-level="2.16.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#testing-a-hypothesis-about-the-normal-population-variance"><i class="fa fa-check"></i><b>2.16.1</b> Testing a Hypothesis About the Normal Population Variance</a></li>
<li class="chapter" data-level="2.16.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#extension-to-testing-with-two-data-samples-the-f-test"><i class="fa fa-check"></i><b>2.16.2</b> Extension to Testing With Two Data Samples: the F Test</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.17</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.17.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#correlation-the-strength-of-linear-association"><i class="fa fa-check"></i><b>2.17.1</b> Correlation: the Strength of Linear Association</a></li>
<li class="chapter" data-level="2.17.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#the-expected-value-of-the-sum-of-squared-errors-sse"><i class="fa fa-check"></i><b>2.17.2</b> The Expected Value of the Sum of Squared Errors (SSE)</a></li>
<li class="chapter" data-level="2.17.3" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r"><i class="fa fa-check"></i><b>2.17.3</b> Linear Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="2.18" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-analysis-of-variance"><i class="fa fa-check"></i><b>2.18</b> One-Way Analysis of Variance</a>
<ul>
<li class="chapter" data-level="2.18.1" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#one-way-anova-the-statistical-model"><i class="fa fa-check"></i><b>2.18.1</b> One-Way ANOVA: the Statistical Model</a></li>
<li class="chapter" data-level="2.18.2" data-path="the-normal-and-related-distributions.html"><a href="the-normal-and-related-distributions.html#linear-regression-in-r-redux"><i class="fa fa-check"></i><b>2.18.2</b> Linear Regression in R: Redux</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html"><i class="fa fa-check"></i><b>3</b> The Binomial (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#motivation-1"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>3.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#binomial-distribution-normal-approximation"><i class="fa fa-check"></i><b>3.2.1</b> Binomial Distribution: Normal Approximation</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#variance-of-a-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.2</b> Variance of a Binomial Random Variable</a></li>
<li class="chapter" data-level="3.2.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-negative-binomial-random-variable"><i class="fa fa-check"></i><b>3.2.3</b> The Expected Value of a Negative Binomial Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#cumulative-distribution-function-1"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#computing-probabilities-7"><i class="fa fa-check"></i><b>3.3.1</b> Computing Probabilities</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sampling-data"><i class="fa fa-check"></i><b>3.3.2</b> Sampling Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#linear-functions-of-binomial-random-variables"><i class="fa fa-check"></i><b>3.4</b> Linear Functions of Binomial Random Variables</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-moment-generating-function-for-a-geometric-random-variable"><i class="fa fa-check"></i><b>3.4.1</b> The Moment-Generating Function for a Geometric Random Variable</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#order-statistics"><i class="fa fa-check"></i><b>3.5</b> Order Statistics</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-minimum-value-sampled-from-an-exponential-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Distribution of the Minimum Value Sampled from an Exponential Distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#distribution-of-the-median-value-sampled-from-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Distribution of the Median Value Sampled from a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#point-estimation-2"><i class="fa fa-check"></i><b>3.6</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-minimum-variance-unbiased-estimator-for-the-exponential-mean"><i class="fa fa-check"></i><b>3.6.1</b> The Minimum Variance Unbiased Estimator for the Exponential Mean</a></li>
<li class="chapter" data-level="3.6.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#sufficient-statistics-for-the-normal-distribution"><i class="fa fa-check"></i><b>3.6.2</b> Sufficient Statistics for the Normal Distribution</a></li>
<li class="chapter" data-level="3.6.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-maximum-likelihood-estimator-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.6.3</b> The Maximum Likelihood Estimator for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-intervals-2"><i class="fa fa-check"></i><b>3.7</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.1</b> Confidence Interval for the Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#confidence-interval-for-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.7.2</b> Confidence Interval for the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-confidence-intervals-for-the-binomial-proportion"><i class="fa fa-check"></i><b>3.7.3</b> Large-Sample Confidence Intervals for the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#hypothesis-testing-1"><i class="fa fa-check"></i><b>3.8</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-beta1theta-distribution"><i class="fa fa-check"></i><b>3.8.1</b> Defining the Uniformly Most-Powerful Test for the Beta(1,<span class="math inline">\(\theta\)</span>) Distribution</a></li>
<li class="chapter" data-level="3.8.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-of-the-negative-binomial-proportion"><i class="fa fa-check"></i><b>3.8.2</b> Defining the Uniformly Most-Powerful Test of the Negative Binomial Proportion</a></li>
<li class="chapter" data-level="3.8.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#defining-the-uniformly-most-powerful-test-for-the-normal-population-mean"><i class="fa fa-check"></i><b>3.8.3</b> Defining the Uniformly Most-Powerful Test for the Normal Population Mean</a></li>
<li class="chapter" data-level="3.8.4" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#large-sample-tests-of-the-binomial-proportion"><i class="fa fa-check"></i><b>3.8.4</b> Large-Sample Tests of the Binomial Proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression"><i class="fa fa-check"></i><b>3.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification"><i class="fa fa-check"></i><b>3.9.1</b> Logistic Regression in R: Star-Quasar Classification</a></li>
<li class="chapter" data-level="3.9.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#logistic-regression-in-r-star-quasar-classification-1"><i class="fa fa-check"></i><b>3.9.2</b> Logistic Regression in R: Star-Quasar Classification</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression"><i class="fa fa-check"></i><b>3.10</b> Naive Bayes Regression</a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-categorical-predictors"><i class="fa fa-check"></i><b>3.10.1</b> Naive Bayes Regression With Categorical Predictors</a></li>
<li class="chapter" data-level="3.10.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-regression-with-continuous-predictors"><i class="fa fa-check"></i><b>3.10.2</b> Naive Bayes Regression With Continuous Predictors</a></li>
<li class="chapter" data-level="3.10.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#naive-bayes-applied-to-star-quasar-data"><i class="fa fa-check"></i><b>3.10.3</b> Naive Bayes Applied to Star-Quasar Data</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-beta-distribution"><i class="fa fa-check"></i><b>3.11</b> The Beta Distribution</a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-expected-value-of-a-beta-random-variable"><i class="fa fa-check"></i><b>3.11.1</b> The Expected Value of a Beta Random Variable</a></li>
<li class="chapter" data-level="3.11.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.2</b> The Sample Median of a Uniform(0,1) Distribution</a></li>
<li class="chapter" data-level="3.11.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#testing-hypotheses-using-the-sample-median-of-a-uniform01-distribution"><i class="fa fa-check"></i><b>3.11.3</b> Testing Hypotheses Using the Sample Median of a Uniform(0,1) Distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.12" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#the-multinomial-distribution"><i class="fa fa-check"></i><b>3.12</b> The Multinomial Distribution</a></li>
<li class="chapter" data-level="3.13" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-based-hypothesis-testing"><i class="fa fa-check"></i><b>3.13</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.13.1" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>3.13.1</b> Chi-Square Goodness of Fit Test</a></li>
<li class="chapter" data-level="3.13.2" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#simulating-an-exact-multinomial-test"><i class="fa fa-check"></i><b>3.13.2</b> Simulating an Exact Multinomial Test</a></li>
<li class="chapter" data-level="3.13.3" data-path="the-binomial-and-related-distributions.html"><a href="the-binomial-and-related-distributions.html#chi-square-test-of-independence"><i class="fa fa-check"></i><b>3.13.3</b> Chi-Square Test of Independence</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html"><i class="fa fa-check"></i><b>4</b> The Poisson (and Related) Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#motivation-2"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#probability-mass-function-1"><i class="fa fa-check"></i><b>4.2</b> Probability Mass Function</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.2.1</b> The Expected Value of a Poisson Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#cumulative-distribution-function-2"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#computing-probabilities-8"><i class="fa fa-check"></i><b>4.3.1</b> Computing Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#linear-functions-of-poisson-random-variables"><i class="fa fa-check"></i><b>4.4</b> Linear Functions of Poisson Random Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-moment-generating-function-of-a-poisson-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> The Moment-Generating Function of a Poisson Random Variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-difference-of-two-poisson-random-variables"><i class="fa fa-check"></i><b>4.4.2</b> The Distribution of the Difference of Two Poisson Random Variables</a></li>
<li class="chapter" data-level="4.4.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-probability-mass-function-for-the-sample-mean-1"><i class="fa fa-check"></i><b>4.4.3</b> The Probability Mass Function for the Sample Mean</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#point-estimation-3"><i class="fa fa-check"></i><b>4.5</b> Point Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example"><i class="fa fa-check"></i><b>4.5.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-cramer-rao-lower-bound-on-the-variance-of-lambda-estimators"><i class="fa fa-check"></i><b>4.5.2</b> The Cramer-Rao Lower Bound on the Variance of <span class="math inline">\(\lambda\)</span> Estimators</a></li>
<li class="chapter" data-level="4.5.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#minimum-variance-unbiased-estimation-and-the-invariance-property"><i class="fa fa-check"></i><b>4.5.3</b> Minimum Variance Unbiased Estimation and the Invariance Property</a></li>
<li class="chapter" data-level="4.5.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#method-of-moments-estimation-for-the-gamma-distribution"><i class="fa fa-check"></i><b>4.5.4</b> Method of Moments Estimation for the Gamma Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-intervals-3"><i class="fa fa-check"></i><b>4.6</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#confidence-interval-for-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.6.1</b> Confidence Interval for the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.6.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-1"><i class="fa fa-check"></i><b>4.6.2</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.6.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#determining-a-confidence-interval-using-the-bootstrap"><i class="fa fa-check"></i><b>4.6.3</b> Determining a Confidence Interval Using the Bootstrap</a></li>
<li class="chapter" data-level="4.6.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-proportion-of-observed-data-in-a-bootstrap-sample"><i class="fa fa-check"></i><b>4.6.4</b> The Proportion of Observed Data in a Bootstrap Sample</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#hypothesis-testing-2"><i class="fa fa-check"></i><b>4.7</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-uniformly-most-powerful-test-of-poisson-lambda"><i class="fa fa-check"></i><b>4.7.1</b> The Uniformly Most Powerful Test of Poisson <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#likelihood-ratio-test-of-the-poisson-parameter-lambda"><i class="fa fa-check"></i><b>4.7.2</b> Likelihood Ratio Test of the Poisson Parameter <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.7.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#using-wilks-theorem-to-test-hypotheses-about-the-normal-mean"><i class="fa fa-check"></i><b>4.7.3</b> Using Wilks Theorem to Test Hypotheses About the Normal Mean</a></li>
<li class="chapter" data-level="4.7.4" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#simulating-the-likelihood-ratio-test"><i class="fa fa-check"></i><b>4.7.4</b> Simulating the Likelihood Ratio Test</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-gamma-distribution"><i class="fa fa-check"></i><b>4.8</b> The Gamma Distribution</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-expected-value-of-a-gamma-random-variable"><i class="fa fa-check"></i><b>4.8.1</b> The Expected Value of a Gamma Random Variable</a></li>
<li class="chapter" data-level="4.8.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#the-distribution-of-the-sum-of-exponential-random-variables"><i class="fa fa-check"></i><b>4.8.2</b> The Distribution of the Sum of Exponential Random Variables</a></li>
<li class="chapter" data-level="4.8.3" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#memorylessness-and-the-exponential-distribution"><i class="fa fa-check"></i><b>4.8.3</b> Memorylessness and the Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#poisson-regression"><i class="fa fa-check"></i><b>4.9</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-2"><i class="fa fa-check"></i><b>4.9.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
<li class="chapter" data-level="4.9.2" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#negative-binomial-regression-example"><i class="fa fa-check"></i><b>4.9.2</b> Negative Binomial Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#chi-square-based-hypothesis-testing-1"><i class="fa fa-check"></i><b>4.10</b> Chi-Square-Based Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="4.10.1" data-path="the-poisson-and-related-distributions.html"><a href="the-poisson-and-related-distributions.html#revisiting-the-death-by-horse-kick-example-3"><i class="fa fa-check"></i><b>4.10.1</b> Revisiting the Death-by-Horse-Kick Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html"><i class="fa fa-check"></i><b>5</b> The Uniform Distribution</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#properties"><i class="fa fa-check"></i><b>5.1</b> Properties</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-expected-value-and-variance-of-a-uniform-random-variable"><i class="fa fa-check"></i><b>5.1.1</b> The Expected Value and Variance of a Uniform Random Variable</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#coding-r-style-functions-for-the-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Coding R-Style Functions for the Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#linear-functions-of-uniform-random-variables"><i class="fa fa-check"></i><b>5.2</b> Linear Functions of Uniform Random Variables</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-moment-generating-function-for-a-discrete-uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> The Moment-Generating Function for a Discrete Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#sufficient-statistics-and-the-minimum-variance-unbiased-estimator"><i class="fa fa-check"></i><b>5.3</b> Sufficient Statistics and the Minimum Variance Unbiased Estimator</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-sufficient-statistic-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.3.1</b> The Sufficient Statistic for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.3.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mvue-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.3.2</b> MVUE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-mle-for-the-domain-parameter-of-the-pareto-distribution"><i class="fa fa-check"></i><b>5.4.1</b> The MLE for the Domain Parameter of the Pareto Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#mle-properties-for-uniform-distribution-bounds"><i class="fa fa-check"></i><b>5.4.2</b> MLE Properties for Uniform Distribution Bounds</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-intervals-4"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#interval-estimation-given-order-statistics"><i class="fa fa-check"></i><b>5.5.1</b> Interval Estimation Given Order Statistics</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#confidence-coefficient-for-a-uniform-based-interval-estimator"><i class="fa fa-check"></i><b>5.5.2</b> Confidence Coefficient for a Uniform-Based Interval Estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#hypothesis-testing-3"><i class="fa fa-check"></i><b>5.6</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#the-power-curve-for-testing-the-uniform-distribution-upper-bound"><i class="fa fa-check"></i><b>5.6.1</b> The Power Curve for Testing the Uniform Distribution Upper Bound</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-uniform-distribution.html"><a href="the-uniform-distribution.html#an-illustration-of-multiple-comparisons-when-testing-for-the-normal-mean"><i class="fa fa-check"></i><b>5.6.2</b> An Illustration of Multiple Comparisons When Testing for the Normal Mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html"><i class="fa fa-check"></i><b>6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#independence-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Independence of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent"><i class="fa fa-check"></i><b>6.1.1</b> Determining Whether Two Random Variables are Independent</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#properties-of-multivariate-distributions"><i class="fa fa-check"></i><b>6.2</b> Properties of Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Characterizing a Discrete Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Characterizing a Continuous Bivariate Distribution</a></li>
<li class="chapter" data-level="6.2.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-bivariate-uniform-distribution"><i class="fa fa-check"></i><b>6.2.3</b> The Bivariate Uniform Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.3</b> Covariance and Correlation</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.1</b> Correlation of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables"><i class="fa fa-check"></i><b>6.3.2</b> Correlation of the Sum of Two Discrete Random Variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration"><i class="fa fa-check"></i><b>6.3.3</b> Uncorrelated is Not the Same as Independent: a Demonstration</a></li>
<li class="chapter" data-level="6.3.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#covariance-of-multinomial-random-variables"><i class="fa fa-check"></i><b>6.3.4</b> Covariance of Multinomial Random Variables</a></li>
<li class="chapter" data-level="6.3.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression"><i class="fa fa-check"></i><b>6.3.5</b> Tying Covariance Back to Simple Linear Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#tying-coviarance-to-simple-logistic-regression"><i class="fa fa-check"></i><b>6.3.6</b> Tying Coviarance to Simple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions"><i class="fa fa-check"></i><b>6.4</b> Marginal and Conditional Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf"><i class="fa fa-check"></i><b>6.4.1</b> Marginal and Conditional Distributions for a Bivariate PMF</a></li>
<li class="chapter" data-level="6.4.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf"><i class="fa fa-check"></i><b>6.4.2</b> Marginal and Conditional Distributions for a Bivariate PDF</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-expected-value-and-variance"><i class="fa fa-check"></i><b>6.5</b> Conditional Expected Value and Variance</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution"><i class="fa fa-check"></i><b>6.5.1</b> Conditional and Unconditional Expected Value Given a Bivariate Distribution</a></li>
<li class="chapter" data-level="6.5.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions"><i class="fa fa-check"></i><b>6.5.2</b> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> The Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.1</b> The Marginal Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.2" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6.2</b> The Conditional Distribution of a Multivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.6.3" data-path="multivariate-distributions.html"><a href="multivariate-distributions.html#the-calculation-of-sample-covariance"><i class="fa fa-check"></i><b>6.6.3</b> The Calculation of Sample Covariance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html"><i class="fa fa-check"></i><b>7</b> Further Conceptual Details (Optional)</a>
<ul>
<li class="chapter" data-level="7.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#types-of-convergence"><i class="fa fa-check"></i><b>7.1</b> Types of Convergence</a></li>
<li class="chapter" data-level="7.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-central-limit-theorem-1"><i class="fa fa-check"></i><b>7.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#asymptotic-normality-of-maximum-likelihood-estimates"><i class="fa fa-check"></i><b>7.3</b> Asymptotic Normality of Maximum Likelihood Estimates</a></li>
<li class="chapter" data-level="7.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#point-estimation-relative-efficiency"><i class="fa fa-check"></i><b>7.4</b> Point Estimation: (Relative) Efficiency</a></li>
<li class="chapter" data-level="7.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.5</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#a-formal-definition-of-sufficiency"><i class="fa fa-check"></i><b>7.5.1</b> A Formal Definition of Sufficiency</a></li>
<li class="chapter" data-level="7.5.2" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.5.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.5.3" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#completeness"><i class="fa fa-check"></i><b>7.5.3</b> Completeness</a></li>
<li class="chapter" data-level="7.5.4" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#the-rao-blackwell-theorem"><i class="fa fa-check"></i><b>7.5.4</b> The Rao-Blackwell Theorem</a></li>
<li class="chapter" data-level="7.5.5" data-path="further-conceptual-details-optional.html"><a href="further-conceptual-details-optional.html#exponential-family-of-distributions"><i class="fa fa-check"></i><b>7.5.5</b> Exponential Family of Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-a-table-of-symbols.html"><a href="appendix-a-table-of-symbols.html"><i class="fa fa-check"></i>Appendix A: Table of Symbols</a></li>
<li class="chapter" data-level="" data-path="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><a href="appendix-b-root-finding-algorithm-for-confidence-intervals.html"><i class="fa fa-check"></i>Appendix B: Root-Finding Algorithm for Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modern Probability and Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-distributions" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Multivariate Distributions<a href="multivariate-distributions.html#multivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Thus far we have looked at <em>univariate probability distributions</em>, i.e.,
we have assumed that there is a single random variable <span class="math inline">\(X\)</span> that maps the
events in a sample space to the real-number line. The quantity represented
by <span class="math inline">\(X\)</span> might be, for instance, height or weight.</p>
<p>In this chapter, we shift to the <em>multivariate</em> case,
wherein we might have <span class="math inline">\(p\)</span> random variables <span class="math inline">\(\mathbf{X} = \{X_1,\ldots,X_p\}\)</span>
representing, e.g., height <em>and</em> weight (if <span class="math inline">\(p = 2\)</span>). When there are
a set of <span class="math inline">\(p\)</span> random variables, they are sampled from a joint <span class="math inline">\(p\)</span>-dimensional
probability mass or density function that encapsulates how the random
variables are jointly distributed.
Both joint pmfs <span class="math inline">\(p_{X_1,\ldots,X_p}(x_1,\ldots,x_p)\)</span> and joint pdfs
<span class="math inline">\(f_{X_1,\ldots,X_p}(x_1,\ldots,x_p)\)</span> have
similar properties as their univariate counterparts: they are non-negative
(with <span class="math inline">\(p_{X_1,\ldots,X_p}(x_1,\ldots,x_p) \leq 1\)</span>) and they either sum or integrate to 1.
They is nothing special about these functions; they are simply more
mathematically complex and thus often
less easy to work with. However, there are
new concepts for us to cover that only arise in a multivariate context.</p>
<ul>
<li><em>Independence of Random Variables</em>: do the sampled values for one
random variable (e.g., heights) depend on the sampled values for the
others (e.g., weights)? If not, the random variables are independent.
(In this simple example, we expect that heights and weights to be
very much dependent: on average, taller people are heavier.)</li>
<li><em>Covariance and Correlation</em>: these metrics build upon the concept
of variance and indicate the amount of <em>linear</em>
dependence between two random variables.</li>
<li><em>Marginal Distributions</em>: these show how a subset of the random variables
is (jointly) distributed, without regard to the values taken on by the
other random variables. For instance, if <span class="math inline">\(f_{X_h,X_W}(x_h,x_w)\)</span> represents the
joint distribution of heights and weights in a population, then the
marginal distribution <span class="math inline">\(f_{X_h}(x_h)\)</span> represents how heights are distributed,
without regard to weight.</li>
<li><em>Conditional Distributions</em>: these show how a subset of the random
variables is (jointly) distributed, conditional on the other random
variables taking on specific values. For instance, the conditional
distribution <span class="math inline">\(f_{X_h \vert X_w}(x_h \vert x_w)\)</span>
indicates the distribution of heights given
a specific weight.</li>
<li><em>Conditional Expectation and Variance</em>: these metrics represent the mean
and width of conditional distributions.</li>
</ul>
<p>Note that throughout this chapter, we will illustrate concepts using bivariate
distributions, as adding mathematical complexity by increasing
dimensionality provides no additional benefit in terms of conceptual
understanding. The one exception to this is in the last section, where we
discuss the multivariate normal distribution.</p>
<div id="independence-of-random-variables" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Independence of Random Variables<a href="multivariate-distributions.html#independence-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Chapter 1, we describe at a high level the concept of two or more
random variables being independent of each other. In that chapter, we
give the example of a <em>bivariate probability density function</em>, i.e.,
a function which outputs the probability density given <em>two</em> inputs,
<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>: <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span>. To test for independence, we
need only inspect the functional form of <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> and
its domain; <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent if and only if</p>
<ul>
<li>the boundaries of the domain depend on either <span class="math inline">\(x_1\)</span> or <span class="math inline">\(x_2\)</span> but not
both at the same time (i.e., the domain is rectangular); and</li>
<li><span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> can be factored into the product of functions
that only depend on <span class="math inline">\(x_1\)</span> and on <span class="math inline">\(x_2\)</span>, respectively: <span class="math inline">\(f_{X_1}(x_1) \times f_{X_2}(x_2)\)</span>.</li>
</ul>
<p>(Furthermore, we state that if <span class="math inline">\(f_{X_1}(x_1) = f_{X_2}(x_2)\)</span>, then
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are iid random variables.)</p>
<p>If either condition given above does not hold, then we conclude that
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are dependent random variables.</p>
<hr />
<div id="determining-whether-two-random-variables-are-independent" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Determining Whether Two Random Variables are Independent<a href="multivariate-distributions.html#determining-whether-two-random-variables-are-independent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint pdf:
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = c(1-x_2)
\]</span>
for <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>. For now, we are not concerned with
the value of the constant <span class="math inline">\(c\)</span>. Are <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> independent random
variables?</p>
</blockquote>
<blockquote>
<p>The first thing to do is inspect the joint domain. This can be tricky, and
it is often best to break the statement of the domain up into multiple
pieces. Here, we can say that first, we know that <span class="math inline">\(x_1 \in [0,1]\)</span> and
that <span class="math inline">\(x_2 \in [0,1]\)</span>. This limits the domain to a box on the
<span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_2\)</span> plane. (See the axes and the red dashed lines in
Figure <a href="multivariate-distributions.html#fig:mulfig1">6.1</a>.) Furthermore, we know that
<span class="math inline">\(x_1 \leq x_2\)</span>, or, equivalently, that <span class="math inline">\(x_2 \geq x_1 + 0\)</span>, i.e., that
<span class="math inline">\(x_2\)</span> lies above a line with slope one and intercept zero. (See the
orange short-dashed line in
Figure <a href="multivariate-distributions.html#fig:mulfig1">6.1</a>.) Given that the domain is triangular, we can state
unequivocally here that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are <em>not</em> independent random
variables. In particular, we do not need to check whether
<span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) \times f_{X_2}(x_2)\)</span>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mulfig1"></span>
<img src="figures/multi_fig1.png" alt="\label{fig:mulfig1}The domain of $f_{X_1,X_2}(x_1,x_2)$, expressed mathematically as $0 \leq x_1 \leq x_2 \leq 1$. The red dashed lines indicate that $0 \leq x_1 \leq 1$ and $0 \leq x_2 \leq 1$, and the orange short-dashed line indicates that $x_2 \geq x_1$. The blue triangle is the domain of the function." width="45%" />
<p class="caption">
Figure 6.1: The domain of <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span>, expressed mathematically as <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>. The red dashed lines indicate that <span class="math inline">\(0 \leq x_1 \leq 1\)</span> and <span class="math inline">\(0 \leq x_2 \leq 1\)</span>, and the orange short-dashed line indicates that <span class="math inline">\(x_2 \geq x_1\)</span>. The blue triangle is the domain of the function.
</p>
</div>
<blockquote>
<p>As a second example, let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint pdf:
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = ce^{-x_1x_2} \,,
\]</span>
where <span class="math inline">\(0 \leq x_1 \leq 1\)</span> and <span class="math inline">\(0 \leq x_2 \leq 1\)</span>. Again, we are not
concerned about the value of <span class="math inline">\(c\)</span>, at least for now. Are <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
independent random variables?</p>
</blockquote>
<blockquote>
<p>We can see by inspection that the domain is rectangular, specifically
a square with vertices (0,0), (0,1), (1,0), and (1,1). So far, so good.
Can we factor <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> into two separate functions that
depend only on <span class="math inline">\(x_1\)</span> or only on <span class="math inline">\(x_2\)</span>? The answer is no. (As a reminder,
<span class="math inline">\(e^{ab} \neq e^a e^b\)</span>!) Hence we can state that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are
<em>not</em> independent random variables.</p>
</blockquote>
</div>
</div>
<div id="properties-of-multivariate-distributions" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Properties of Multivariate Distributions<a href="multivariate-distributions.html#properties-of-multivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As stated in Chapter 1, a probability distribution is a mapping
<span class="math inline">\(P : \Omega \rightarrow \mathbb{R}^n\)</span>, where <span class="math inline">\(\Omega\)</span> is the sample
space for an experiment. This mapping describes how probabilities
are distributed across the values of a random variable. In the first
five chapters, <span class="math inline">\(n = 1\)</span>, meaning that we focus on univariate distributions.
In this chapter, however, <span class="math inline">\(n &gt; 1\)</span>, with the bulk of our discussion focusing
upon the case <span class="math inline">\(n = 2\)</span>. Nothing changes, fundamentally, when we work with
multivariate distributions: (a) they are non-negative; and (b) they sum
or integrate to 1. Assuming <span class="math inline">\(n = 2\)</span>, we can write that</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">joint pmf</th>
<th align="center">joint pdf</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(p_{X_1,X_2}(x_1,x_2) \in [0,1]\)</span></td>
<td align="center"><span class="math inline">\(f_{X_1,X_2}(x_1,x_2) \in [0,\infty)\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\sum_{x_1} \sum_{x_2} p_{X_1,X_2}(x_1,x_2) = 1\)</span></td>
<td align="center"><span class="math inline">\(\int_{x_1} \int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = 1\)</span></td>
</tr>
</tbody>
</table>
<p>We will reiterate that the joint pdf <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> is not the
probability of sampling the tuple <span class="math inline">\((x_1,x_2)\)</span>
but rather is a probability density; to
determine probabilities, we would invoke multi-dimensional integration:
<span class="math display">\[
P(a_1 \leq X_1 \leq b_1,a_2 \leq X_2 \leq b_2) = \int_{a_1}^{b_1} \int_{a_2}^{b_2} f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \,.
\]</span></p>
<p>As for the joint cumulative distribution function, or joint cdf, the
convention is to treat each axis separately from the others, i.e., to
define it as
<span class="math display">\[
F_{X_1,X_2}(x_1,x_2) = P(X_1 \leq x_1 \cap X_2 \leq x_2) \,.
\]</span>
The joint cdf satisfies the following properties:</p>
<ul>
<li>if either <span class="math inline">\(X_i = -\infty\)</span>, the joint cdf is zero;</li>
<li>if both <span class="math inline">\(X_i = \infty\)</span>, the joint cdf is one;</li>
<li>it increases monotonically along each coordinate axis; and</li>
<li>if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables, then <span class="math inline">\(F_{X_1,X_2}(x_1,x_2) = F_{X_1}(x_1) F_{X_2}(x_2)\)</span>.</li>
</ul>
<p>To characterize bivariate distributions (and multivariate ones in general),
we compute distribution moments by utilizing the Law of the Unconscious
Statistician:
<span class="math display">\[\begin{align*}
E[g(X_1,X_2)] &amp;= \sum_{x_1} \sum_{x_2} g(x_1,x_2) p_{X_1,X_2}(x_1,x_2) \qquad \mbox{discrete}\\
&amp;= \int_{x_1} \int_{x_2} g(x_1,x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \qquad \mbox{continuous} \,,
\end{align*}\]</span>
with the shortcut formula continuing to hold in the multivariate case:
<span class="math display">\[
V[g(X_1,X_2)] = E\left[ g(X_1,X_2)^2 \right] - (E[g(X_1,X_2)])^2 \,.
\]</span></p>
<p>We note that if we write <span class="math inline">\(g(x_1,x_2)\)</span> as <span class="math inline">\(g_1(x_1)g_2(x_2)\)</span> and
if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables, then
<span class="math display">\[\begin{align*}
E[g_1(X_1)g_2(X_2)] &amp;= \int_{x_1} \int_{x_2} g_1(x_1) g_2(x_2) f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 \\
&amp;= \int_{x_1} \int_{x_2} g_1(x_1) g_2(x_2) f_{X_1}(x_1) f_{X_2}(x_2) dx_2 dx_1 \\
&amp;= \left[ \int_{x_1} g_1(x_1) f_{X_1}(x_1) \right] \cdot \left[ \int_{x_2} g_2(x_2) f_{X_2}(x_2) \right] \\
&amp;= E[g_1(X_1)] E[g_2(X_2)] \,.
\end{align*}\]</span>
So, for instance, <span class="math inline">\(E[X_1X_2] = E[X_1]E[X_2]\)</span> if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
are independent, but not generally.</p>
<hr />
<div id="characterizing-a-discrete-bivariate-distribution" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Characterizing a Discrete Bivariate Distribution<a href="multivariate-distributions.html#characterizing-a-discrete-bivariate-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability mass function
<span class="math inline">\(p_{X_1,X_2}(x_1,x_2)\)</span>:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(x_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 1\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1 = 1\)</span></td>
<td align="center">0.20</td>
<td align="center">0.30</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_1 = 2\)</span></td>
<td align="center">0.40</td>
<td align="center">0.00</td>
<td align="center">0.10</td>
</tr>
</tbody>
</table>
<blockquote>
<p>What is <span class="math inline">\(E[X_1X_2]\)</span>?</p>
</blockquote>
<blockquote>
<p>We utilize the Law of the Unconscious Statistician:
<span class="math display">\[\begin{align*}
E[X_1X_2] &amp;= \sum_{x_1} \sum_{x_2} x_1 x_2 p_{X_1,X_2}(x_1,x_2) \\
&amp;= 1 \times 0 \times 0.20 + 2 \times 0 \times 0.40 + 1 \times 1 \times 0.30 + 2 \times 2 \times 0.10 \\
&amp;= 0.30 + 0.40 = 0.70 \,.
\end{align*}\]</span></p>
</blockquote>
<blockquote>
<p>What is <span class="math inline">\(F_{X_1,X_2}(3/2,3/2)\)</span>?</p>
</blockquote>
<blockquote>
<p>The joint cdf is
<span class="math display">\[\begin{align*}
P(X_1 \leq 3/2 \cap X_2 \leq 3/2) &amp;= P(X_1 = 1 \cap X_2 = 0) + P(X_1 = 1 \cap X_2 = 1) \\
&amp;= 0.20 + 0.30 = 0.50 \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="characterizing-a-continuous-bivariate-distribution" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Characterizing a Continuous Bivariate Distribution<a href="multivariate-distributions.html#characterizing-a-continuous-bivariate-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability density function:
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = c(1-x_2) \,,
\]</span>
with <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>. In an example above, we determined that
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are dependent random variables due to the domain of the pdf
not being rectangular on the <span class="math inline">\(x_1\)</span>-<span class="math inline">\(x_2\)</span> plane (see Figures
<a href="multivariate-distributions.html#fig:mulfig1">6.1</a> and <a href="multivariate-distributions.html#fig:bivar1">6.2</a>).</p>
</blockquote>
<blockquote>
<p>Here, we will first determine the value of <span class="math inline">\(c\)</span> that makes this function
a valid joint pdf. Carrying out this calculation will involve
double integration; for a short review of double integration, see
Chapter 8. We note here that we can integrate in either order (i.e., along
the <span class="math inline">\(x_1\)</span> axis first, or the <span class="math inline">\(x_2\)</span> axis first), as the final result will
be the same. Here, we will integrate along <span class="math inline">\(x_1\)</span> first, since the lower
bound along this axis is zero (a choice which often simplifies the overall
calculation) <em>and</em> because <span class="math inline">\(x_1\)</span> does not appear in the integrand:
<span class="math display">\[\begin{align*}
\int_0^1 \left[ \int_0^{x_1 = x_2} c (1-x_2) dx_1 \right] dx_2 &amp;= c \int_0^1 (1-x_2) \left[ \int_0^{x_1 = x_2} dx_1 \right] dx_2 \\
&amp;= c \int_0^1 (1-x_2) \left[ \left. x_1 \right|_0^{x^2} \right] dx_2 \\
&amp;= c \int_0^1 x_2 (1-x_2) dx_2 \\
&amp;= c B(2,2) = c \Gamma(2) \Gamma(2) / \Gamma(4) = c \times 1! \times 1! / 3! = c/6 = 1 \,.
\end{align*}\]</span>
Thus <span class="math inline">\(c = 6\)</span>. Note that we take advantage of the fact that
<span class="math inline">\(x_2 (1-x_2)\)</span> integrated from 0 to 1 is a Beta(2,2) distribution. If
we had not made that association, we would still have observed the
same final solution after integrating the polynomial in the integrand.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bivar1"></span>
<img src="figures/bivar1.png" alt="\label{fig:bivar1}The probability density function $6(1-x_2)$ as a function of $x_1$ (axis pointing to upper right) and $x_2$ (axis pointing to upper left). The pdf peaks at $(x_1,x_2) = (0,0)$ and falls off towards $x_2 = 1$ as a plane, with value greater than zero only within the domain $0 \leq x_1 \leq x_2 \leq 1$; Figure \@ref(fig:mulfig1) shows that domain." width="45%" />
<p class="caption">
Figure 6.2: The probability density function <span class="math inline">\(6(1-x_2)\)</span> as a function of <span class="math inline">\(x_1\)</span> (axis pointing to upper right) and <span class="math inline">\(x_2\)</span> (axis pointing to upper left). The pdf peaks at <span class="math inline">\((x_1,x_2) = (0,0)\)</span> and falls off towards <span class="math inline">\(x_2 = 1\)</span> as a plane, with value greater than zero only within the domain <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>; Figure <a href="multivariate-distributions.html#fig:mulfig1">6.1</a> shows that domain.
</p>
</div>
<blockquote>
<p>Now we will ask, what are <span class="math inline">\(E[X_2]\)</span> and <span class="math inline">\(V[X_2]\)</span>?
<span class="math display">\[\begin{align*}
E[X_2] = \int_0^1 \left[ \int_0^{x_1 = x_2} 6 x_2 (1-x_2) dx_1 \right] dx_2 &amp;= 6 \int_0^1 x_2 (1-x_2) \left[ \int_0^{x_1 = x_2} dx_1 \right] dx_2 \\
&amp;= 6 \int_0^1 x_2 (1-x_2) \left[ \left. x_1 \right|_0^{x^2} \right] dx_2 \\
&amp;= 6 \int_0^1 x_2^2 (1-x_2) dx_2 \\
&amp;= 6 B(3,2) = 6 \Gamma(3) \Gamma(2) / \Gamma(5) = 6 \times 2! \times 1! / 4! = 1/2 \,.
\end{align*}\]</span>
In a similar manner, we can determine that <span class="math inline">\(E[X_2^2] = 6 B(4,2) = 6 \times 3! \times 1! / 5! = 36/120 = 3/10\)</span> and that
<span class="math inline">\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 3/10 - 1/4 = 1/20\)</span>.</p>
</blockquote>
<hr />
</div>
<div id="the-bivariate-uniform-distribution" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> The Bivariate Uniform Distribution<a href="multivariate-distributions.html#the-bivariate-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>The bivariate uniform distribution is defined as
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = \frac{1}{A} \,,
\]</span>
where <span class="math inline">\(A\)</span> is the area of the domain. (Thus the integral
of the bivariate function is <span class="math inline">\(A \times 1/A = 1\)</span>.)
Let <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = 1\)</span> over the square defined by the vertices
(0,0), (0,1), (1,0), and (1,1). What is <span class="math inline">\(P(X_1 &gt; 2X_2)\)</span>?</p>
</blockquote>
<blockquote>
<p>A nice feature of the bivariate uniform is that we can work with it
geometrically: if we can determine the fraction of the <em>domain</em> that
abides by the stated condition, then we have our answer. (This is because
the joint pdf is flat, so integration is unnecessary.)</p>
</blockquote>
<blockquote>
<p>We can rewrite <span class="math inline">\(x_1 &gt; 2x_2\)</span> as <span class="math inline">\(x_2 &lt; x_1/2\)</span>, i.e., the region of interest
in the domain is the region below the line with intercept zero and
slope 1/2. (See Figure <a href="multivariate-distributions.html#fig:mulfig2">6.3</a>.)
This region is a triangle with vertices (0,0), (1,1/2), and (1,0),
which has the area (recall: one-half times base times height)
<span class="math inline">\(1/2 \times 1 \times 1/2 = 1/4\)</span>. Done! <span class="math inline">\(P(X_1 &gt; 2X_2) = 1/4\)</span>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mulfig2"></span>
<img src="figures/multi_fig2.png" alt="\label{fig:mulfig2}The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region $x_1 &gt; 2x_2$ is indicated by the blue triangle." width="45%" />
<p class="caption">
Figure 6.3: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region <span class="math inline">\(x_1 &gt; 2x_2\)</span> is indicated by the blue triangle.
</p>
</div>
</div>
</div>
<div id="covariance-and-correlation" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Covariance and Correlation<a href="multivariate-distributions.html#covariance-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <em>covariance</em> between two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is a metric
that quantifies the amount of <em>linear dependence</em> between them.
It is defined as
<span class="math display">\[
{\rm Cov}(X_1,X_2) = E[(X_1-\mu_1)(X_2-\mu_2)] \,,
\]</span>
but one rarely uses this expression, as there is a shortcut formula:
<span class="math display">\[\begin{align*}
{\rm Cov}(X_1,X_2) &amp;= E[X_1X_2 - X_1\mu_2 - X_2\mu_1 + \mu_1\mu_2] \\
&amp;= E[X_1X_2] - \mu_2E[X_1] - \mu_1E[X_2] + \mu_1\mu_2 \\
&amp;= E[X_1X_2] - E[X_2]E[X_1] - E[X_1]E[X_2] + E[X_1]E[X_2] \\
&amp;= E[X_1X_2] - E[X_1]E[X_2] \,.
\end{align*}\]</span>
In the previous section, we saw that <span class="math inline">\(E[X_1X_2] = E[X_1]E[X_2]\)</span> if
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables. Thus independent
random variables have no covariance, which makes sense: independent
random variables would have by definition no linear dependence.
However, it is <em>not</em> true that <span class="math inline">\(E[X_1X_2] = E[X_1]E[X_2]\)</span> <em>implies</em>
that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variablesit just implies
there is no linear dependence between the two variables.
See Figure <a href="multivariate-distributions.html#fig:covplot1">6.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:covplot1"></span>
<img src="_main_files/figure-html/covplot1-1.png" alt="\label{fig:covplot1}Examples of data with negative covariance (left), no covariance (center), and positive covariance (right)." width="30%" /><img src="_main_files/figure-html/covplot1-2.png" alt="\label{fig:covplot1}Examples of data with negative covariance (left), no covariance (center), and positive covariance (right)." width="30%" /><img src="_main_files/figure-html/covplot1-3.png" alt="\label{fig:covplot1}Examples of data with negative covariance (left), no covariance (center), and positive covariance (right)." width="30%" />
<p class="caption">
Figure 6.4: Examples of data with negative covariance (left), no covariance (center), and positive covariance (right).
</p>
</div>
<p>Covariance is not necessarily an optimal metric for expressing linear
dependence, as its value is not readily interpretable. To see this,
assume we have the expression <span class="math inline">\((tX_1 - X_2)\)</span> for some constant <span class="math inline">\(t\)</span>.
Then <span class="math inline">\((tX_1-X_2)^2 \geq 0\)</span> and <span class="math inline">\(E[(tX_1-X_2)^2] \geq 0\)</span>. Thus
<span class="math display">\[\begin{align*}
E[(tX_1-X_2)^2] &amp;= E[t^2X_1^2 - 2tX_1X_2 + X_2^2] \\
&amp;= E[X_1^2] t^2 - 2E[X_1X_2] t + E[X_2^2] \\
&amp;= a t^2 + b t + c \geq 0 \,.
\end{align*}\]</span>
(Recall that expected values are constants, and not themselves random
variables.) The key here is that if <span class="math inline">\(a t^2 + b t + c = 0\)</span>, there is one
real root to this quadratic equation, while if <span class="math inline">\(a t^2 + b t + c &gt; 0\)</span>,
there are no real roots. Thus the discriminant, <span class="math inline">\(b^2 - 4ac\)</span>,
must be <span class="math inline">\(\leq 0\)</span>, and so
<span class="math display">\[\begin{align*}
b^2 - 4ac &amp;= 4 (E[X_1X_2])^2 - 4 E[X_1^2] E[X_2^2] \leq 0 \\
\Rightarrow&amp; (E[X_1X_2])^2 \leq E[X_1^2] E[X_2^2] \,.
\end{align*}\]</span>
At this point, the reader might ask well, what about this? The
expression to the left above, <span class="math inline">\((E[X_1X_2])^2\)</span>, is <span class="math inline">\([{\rm Cov}(X_1,X_2)]^2\)</span>
when <span class="math inline">\(\mu_1 = \mu_2 = 0\)</span>, while the expression to the right,
<span class="math inline">\(E[X_1^2] E[X_2^2]\)</span>, is <span class="math inline">\(V[X_1]V[X_2]\)</span> when <span class="math inline">\(\mu_1 = \mu_2 = 0\)</span>. Thus
<span class="math display">\[
\vert {\rm Cov}(X_1,X_2) \vert \leq \sqrt{V[X_1]V[X_2]} = \sigma_1 \sigma_2 \,,
\]</span>
or <span class="math inline">\(- \sigma_1 \sigma_2 \leq {\rm Cov}(X_1,X_2) \leq \sigma_1 \sigma_2\)</span>.
The fact that we may not know immediately the value of <span class="math inline">\(\sigma_1 \sigma_2\)</span>
is what makes any numerical value of <span class="math inline">\({\rm Cov}(X_1,X_2)\)</span> hard to interpret.</p>
<p>Thus we conventionally turn to an alternate expression of linear dependence,
the <em>correlation coefficient</em>:
<span class="math display">\[
\rho_{X_1,X_2} = \frac{{\rm Cov}(X_1,X_2)}{\sigma_1\sigma_2} \Rightarrow -1 \leq \rho \leq 1 \,.
\]</span>
If <span class="math inline">\(\rho_{X_1,X_2} &lt; 0\)</span>, then an increase in the sampled value of <span class="math inline">\(X_1\)</span> is associated
with, <em>on average</em>, a smaller sampled value of <span class="math inline">\(X_2\)</span>, i.e., <span class="math inline">\(X_1\)</span>
and <span class="math inline">\(X_2\)</span> are negatively correlatedwhereas if <span class="math inline">\(\rho_{X_1,X_2} &gt; 0\)</span>, larger
sampled values for <span class="math inline">\(X_1\)</span> are associated on average with larger sampled
values of <span class="math inline">\(X_2\)</span>, i.e., <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are positively correlated.</p>
<p>(We note that we have actually seen <span class="math inline">\(\rho_{X_1,X_2}\)</span> before, in Chapter 2,
when we introduced correlation as a metric of the strength of linear
association as measured in simple linear regression. Recall that we estimate
<span class="math inline">\(\rho_{X_1,X_2}\)</span> with, e.g., the Pearson correlation coefficient <span class="math inline">\(R\)</span>, and
that we use coefficient of determination <span class="math inline">\(R^2\)</span> to quantify the usefulness of
the linear regression model. See Figure <a href="multivariate-distributions.html#fig:covplot2">6.5</a>.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:covplot2"></span>
<img src="_main_files/figure-html/covplot2-1.png" alt="\label{fig:covplot2}The same data as shown in Figure \@ref(fig:covplot1), with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right." width="30%" /><img src="_main_files/figure-html/covplot2-2.png" alt="\label{fig:covplot2}The same data as shown in Figure \@ref(fig:covplot1), with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right." width="30%" /><img src="_main_files/figure-html/covplot2-3.png" alt="\label{fig:covplot2}The same data as shown in Figure \@ref(fig:covplot1), with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right." width="30%" />
<p class="caption">
Figure 6.5: The same data as shown in Figure <a href="multivariate-distributions.html#fig:covplot1">6.4</a>, with linear regression lines superimposed. The estimated correlations are -0.752, 0.257, and 0.712, respectively, from left to right.
</p>
</div>
<hr />
<p>Lets suppose we are given <span class="math inline">\(n\)</span>
correlated random variables <span class="math inline">\(\{X_1,\ldots,X_n\}\)</span>, and
we define the sum <span class="math inline">\(Y = \sum_{i=1}^n a_i X_i\)</span>.
Furthermore, lets assume we know the pdf/pmf, expected value, and
variance of each of the <span class="math inline">\(X_i\)</span>s.
What do we know about the
distribution of <span class="math inline">\(Y\)</span>? With work, we might be able to determine its
pdf or pmf, but we would have to go beyond
the method of moment-generating functions because that method
requires the random variables to be independent of each other.
We will not do that here.
However, we <em>can</em> determine the expected value of <span class="math inline">\(Y\)</span>:
<span class="math display">\[
E[Y] = E\left[ \sum_{i=1}^n a_i X_i \right] = \sum_{i=1}^n a_i E[X_i] \,.
\]</span>
Dependencies between the <span class="math inline">\(X_i\)</span>s does not affect this equality. As for
the variance of <span class="math inline">\(Y\)</span></p>
<p>We start by encapsulating the information about the covariances between
each variable pair into a <em>covariance matrix</em>. (See Chapter 08 for a short
introduction to matrices and their basic use.) For <span class="math inline">\(n\)</span> random variables,
the <span class="math inline">\(n \times n\)</span> covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is
<span class="math display">\[
\boldsymbol{\Sigma} = \left[ \begin{array}{ccc} {\rm Cov}(X_1,X_1) &amp; \cdots &amp; {\rm Cov}(X_1,X_n) \\ \vdots &amp; \ddots &amp; \vdots \\ {\rm Cov}(X_n,X_1) &amp; \cdots &amp; {\rm Cov}(X_n,X_n) \end{array} \right] = \left[ \begin{array}{ccc} V[X_1] &amp; \cdots &amp; {\rm Cov}(X_1,X_n) \\ \vdots &amp; \ddots &amp; \vdots \\ {\rm Cov}(X_n,X_1) &amp; \cdots &amp; V[X_n] \end{array} \right] \,.
\]</span>
We note that the diagonal of this matrix (the elements from the upper
left corner to the lower right corner) contains the individual variances,
since Cov(<span class="math inline">\(X_i,X_i\)</span>) = <span class="math inline">\(V[X_i]\)</span>,
and that because Cov(<span class="math inline">\(X_i,X_j\)</span>) = Cov(<span class="math inline">\(X_j,X_i\)</span>), the matrix is symmetric
about the diagonal.</p>
<p>Now, let
<span class="math inline">\(a^T = [ a_1 ~ a_2 ~ \ldots a_n ]\)</span> be the <span class="math inline">\(1 \times n\)</span> matrix
(or transposed vector) of coefficients. Then:
<span class="math display">\[
V[Y] = a^T \boldsymbol{\Sigma} a \,.
\]</span>
Nice and compact! Lets compare this to the equivalent expression that
does not utilize matrices:
<span class="math display">\[\begin{align*}
V[Y] &amp;= \left[ a_1 ~ \ldots ~ a_n \right] \left[ \begin{array}{ccc} V[X_1] &amp; \cdots &amp; {\rm Cov}(X_1,X_n) \\ \vdots &amp; \ddots &amp; \vdots \\ {\rm Cov}(X_n,X_1) &amp; \cdots &amp; V[X_n] \end{array} \right] \left[ \begin{array}{c} a_1 \\ \vdots \\ a_n \end{array} \right] \\
&amp;= \sum_{i=1}^n a_i^2 V[X_i] + 2\sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j {\rm Cov}(X_i,X_j) \,.
\end{align*}\]</span>
Note that we can
derive this expression directly using the Law of the Unconscious Statistician:
<span class="math display">\[\begin{align*}
V[Y] = E[Y^2] - (E[Y])^2 = E[(Y - E[Y])^2] &amp;= E\left[ \left( \sum_{i=1}^n a_i X_i - \sum_{i=1}^n a_i \mu_i \right)^2 \right] \\
&amp;= E\left[ \left(\sum_{i=1}^n a_i(X_i - \mu_i) \right)^2 \right] \,.
\end{align*}\]</span>
Lets look at this for a moment. If we have, e.g., <span class="math inline">\(Y = a_1X_1+a_2X_2\)</span>, then
<span class="math display">\[\begin{align*}
(a_1X_1+a_2X_2)^2 &amp;= a_1^2X_1^2 + a_2^2X_2^2 + a_1a_2X_1X_2 + a_2a_1X_2X_1 \\
&amp;= \sum_{i=1}^2 a_i^2 X_i^2 + 2a_1a_2X_1X_2 \\
&amp;= \sum_{i=1}^2 a_i^2 X_i^2 + 2 \sum_{i=1}^{2-1} \sum_{j=i+1}^2 a_i a_j X_i X_j \,.
\end{align*}\]</span>
Recognizing that the 2s in the summation bounds would be the sample size
<span class="math inline">\(n\)</span> in general, we can write that
<span class="math display">\[
\left(\sum_{i=1}^n a_i X_i\right)^2 = \sum_{i=1}^n a_i^2 X_i^2 + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j X_i X_j \,.
\]</span>
So, picking up where we left off
<span class="math display">\[\begin{align*}
V[Y] &amp;= E\left[ \left(\sum_{i=1}^n a_i(X_i - \mu_i) \right)^2 \right] \\
&amp;= E\left[ \sum_{i=1}^n a_i^2(X_i-\mu_i)^2 + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j (X_i - \mu_i) (X_j - \mu_2) \right] \\
&amp;= \sum_{i=1}^n a_i^2 E\left[(X_i-\mu_i)^2\right] + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j E\left[(X_i - \mu_i) (X_j - \mu_2) \right] \\
&amp;= \sum_{i=1}^n a_i^2 V[X_i] + 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j \mbox{Cov}(X_i,X_j) \,.
\end{align*}\]</span></p>
<hr />
<div id="correlation-of-two-discrete-random-variables" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Correlation of Two Discrete Random Variables<a href="multivariate-distributions.html#correlation-of-two-discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability mass function
<span class="math inline">\(p_{X_1,X_2}(x_1,x_2)\)</span>:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(x_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 1\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1 = 1\)</span></td>
<td align="center">0.20</td>
<td align="center">0.30</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_1 = 2\)</span></td>
<td align="center">0.40</td>
<td align="center">0.00</td>
<td align="center">0.10</td>
</tr>
</tbody>
</table>
<blockquote>
<p>What is the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>?</p>
</blockquote>
<blockquote>
<p>In an example above, we determined that <span class="math inline">\(E[X_1X_2] = 0.7\)</span>. This is but one
piece of the correlation puzzle: we also need to determine <span class="math inline">\(E[X_1]\)</span> and
<span class="math inline">\(E[X_2]\)</span> along with <span class="math inline">\(V[X_1]\)</span> and <span class="math inline">\(V[X_2]\)</span>.
<span class="math display">\[\begin{align*}
E[X_1] &amp;= \sum_{x_1} \sum_{x_2} x_1 p_{X_1,X_2}(x_1,x_2) \\
&amp;= 1 \times 0.20 + 1 \times 0.30 + 2 \times 0.40 + 2 \times 0.10 = 1.50 \\
\\
E[X_1^2] &amp;= \sum_{x_1} \sum_{x_2} x_1^2 p_{X_1,X_2}(x_1,x_2) \\
&amp;= 1^2 \times 0.20 + 1^2 \times 0.30 + 2^2 \times 0.40 + 2^2 \times 0.10 = 2.50 \\
\\
E[X_2] &amp;= \sum_{x_2} \sum_{x_2} x_2 p_{X_1,X_2}(x_1,x_2) \\
&amp;= 1 \times 0.30 + 2 \times 0.10 = 0.50 \\
\\
E[X_2^2] &amp;= \sum_{x_2} \sum_{x_2} x_2^2 p_{X_1,X_2}(x_1,x_2) \\
&amp;= 1^2 \times 0.30 + 2^2 \times 0.10 = 0.70 \,.
\end{align*}\]</span>
Hence Cov(<span class="math inline">\(X_1,X_2\)</span>) = <span class="math inline">\(E[X_1X_2] - E[X_1]E[X_2] = 0.70 - 0.75 = -0.05\)</span>,
while <span class="math inline">\(V[X_1] = E[X_1^2] - (E[X_1])^2 = 2.50 - 2.25 = 0.25\)</span> and
<span class="math inline">\(V[X_2] = E[X_2^2] - (E[X_2])^2 = 0.70 - 0.25 = 0.45\)</span>.</p>
</blockquote>
<blockquote>
<p>Thus the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is
<span class="math display">\[
\rho_{X_1,X_2} = \frac{\mbox{Cov}(X_1,X_2)}{\sqrt{V[X_1]V[X_2]}} = \frac{-0.05}{\sqrt{0.25 \cdot 0.45}} = -0.149 \,.
\]</span>
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are (relatively weakly) negatively correlated: increasing
<span class="math inline">\(X_1\)</span> leads to slightly decreased values of <span class="math inline">\(X_2\)</span>, on average.</p>
</blockquote>
<hr />
</div>
<div id="correlation-of-the-sum-of-two-discrete-random-variables" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Correlation of the Sum of Two Discrete Random Variables<a href="multivariate-distributions.html#correlation-of-the-sum-of-two-discrete-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be the random variables defined in the previous example,
and let <span class="math inline">\(Y = X_1 - X_2\)</span>. What is <span class="math inline">\(V[Y]\)</span>?</p>
</blockquote>
<blockquote>
<p>Because only two random variables are involved, we will first do the
calculation using the summation form given at the end of the section:
<span class="math display">\[\begin{align*}
V[Y] &amp;= \sum_{i=1}^n a_i^2 V[X_i] + 2\sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i a_j {\rm Cov}(X_i,X_j) \\
&amp;= 1^2 V[X_1] + (-1)^2 V[X_2] + 2 (1) (-1) \mbox{Cov}(X_1,X_2) \\
&amp;= V[X_1] + V[X_2] - 2 \mbox{Cov}(X_1,X_2) \\
&amp;= 0.25 + 0.45 - 2(-0.05) = 0.80 \,.
\end{align*}\]</span></p>
</blockquote>
<blockquote>
<p>Here is the same calculation using <code>R</code>.</p>
</blockquote>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="multivariate-distributions.html#cb275-1" aria-hidden="true" tabindex="-1"></a>a     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb275-2"><a href="multivariate-distributions.html#cb275-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.25</span>,<span class="sc">-</span><span class="fl">0.05</span>,<span class="sc">-</span><span class="fl">0.05</span>,<span class="fl">0.45</span>),<span class="at">nrow=</span><span class="dv">2</span>) <span class="co"># fill column-by-column</span></span>
<span id="cb275-3"><a href="multivariate-distributions.html#cb275-3" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(a) <span class="sc">%*%</span> Sigma <span class="sc">%*%</span> a</span></code></pre></div>
<pre><code>##      [,1]
## [1,]  0.8</code></pre>
<blockquote>
<p>Note that <code>a</code> is by default a column vector, and thus must be transposed
via the <code>t()</code> function, and that <code>%*%</code> is the matrix multiplication operator.</p>
</blockquote>
<hr />
</div>
<div id="uncorrelated-is-not-the-same-as-independent-a-demonstration" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Uncorrelated is Not the Same as Independent: a Demonstration<a href="multivariate-distributions.html#uncorrelated-is-not-the-same-as-independent-a-demonstration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let the region shown in Figure <a href="multivariate-distributions.html#fig:mulfig3">6.6</a> be the domain of a
bivariate uniform distribution. (The area of the domain is 1, hence
the amplitude of the bivariate pdf is 1/1 = 1.) Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
be random variables drawn from this distribution. Because the domain
is not rectangular, we can state that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are dependent
random variables. What is the correlation between them?</p>
</blockquote>
<blockquote>
<p>The definition of covariance is Cov(<span class="math inline">\(X_1,X_2\)</span>) = <span class="math inline">\(E[X_1X_2] - E[X_1]E[X_2]\)</span>.
If we examine the figure, we can convince ourselves that <span class="math inline">\(E[X_1] = 0\)</span> due
to the uniform nature of the pdf and the symmetry of the domain about
<span class="math inline">\(x_1 = 0\)</span>. Hence Cov(<span class="math inline">\(X_1,X_2\)</span>) = <span class="math inline">\(E[X_1X_2]\)</span>, which is
<span class="math display">\[\begin{align*}
E[X_1X_2] &amp;= \int_0^1 x_2 \left[ \int_{x_2-1}^{1-x_2} x_1 dx_1 \right] dx_2 \\
&amp;= \int_0^1 x_2 \left[ \left. \frac{x_1^2}{2} \right|_{x_2-1}^{1-x_2} \right] dx_2 \\
&amp;= \int_0^1 x_2 \left[ \frac{(x_2-1)^2}{2} - \frac{(1-x_2)^2}{2} \right] dx_2 \\
&amp;= \int_0^1 x_2 \left[ \frac{(x_2-1)^2}{2} - \frac{(x_2-1)^2}{2} \right] dx_2 \\
&amp;= \int_0^1 x_2 \left[ 0 \right] dx_2 = 0 \,.
\end{align*}\]</span>
Hence Cov(<span class="math inline">\(X_1,X_2\)</span>) = 0 (and the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
is zero). This result demonstrates that uncorrelated random variables are
not necessarily independent random variables: they are simply random
variables that exhibit <em>no linear dependence</em>. (One way to view this
intuitively
is to imagine that we sample <span class="math inline">\(n\)</span> data from this distribution and then
regress <span class="math inline">\(X_2\)</span> (as <span class="math inline">\(Y\)</span>) against <span class="math inline">\(X_1\)</span> (as <span class="math inline">\(x\)</span>) using linear regression.
The linear regression line would, on average, pass flat through the points,
i.e., would, on average, have a slope of zero, indicating no linear
association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mulfig3"></span>
<img src="figures/multi_fig3.png" alt="\label{fig:mulfig3}The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region $x_1 &gt; 2x_2$ is indicated by the blue triangle." width="45%" />
<p class="caption">
Figure 6.6: The domain of the bivariate uniform distribution with bounds 0 and 1 along each axis. The region <span class="math inline">\(x_1 &gt; 2x_2\)</span> is indicated by the blue triangle.
</p>
</div>
<hr />
</div>
<div id="covariance-of-multinomial-random-variables" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Covariance of Multinomial Random Variables<a href="multivariate-distributions.html#covariance-of-multinomial-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In Chapter 3, we introduce the multinomial distribution, which governs
experiments in which we sample data that can <span class="math inline">\(m\)</span> different discrete
values. The probability mass function is
<span class="math display">\[
p_{X_1,\ldots,X_m}(x_1,\ldots,x_m \vert p_1,\ldots,p_m) = \frac{k!}{x_1! \cdots x_m!}p_1^{x_1}\cdots p_m^{x_m} \,,
\]</span>
where <span class="math inline">\(x_i\)</span> represents the number of times outcome <span class="math inline">\(i\)</span> is observed in
<span class="math inline">\(k\)</span> trials, and where <span class="math inline">\(p_i\)</span> is the probability of observing outcome <span class="math inline">\(i\)</span>.
As stated in Chapter 3, the distribution for each random variable
<span class="math inline">\(X_i\)</span> is Binomial(<span class="math inline">\(k,p_i\)</span>), but the <span class="math inline">\(X_i\)</span>s are <em>not</em> independent; because
<span class="math inline">\(\sum_{i=1}^m X_i = k\)</span>, increase the value of one of the random variables
would lead the values of the others to decrease on average. Here we
will derive the result that was quoted in Chapter 3, namely that
Cov(<span class="math inline">\(X_i\)</span>,<span class="math inline">\(X_j\)</span>) = <span class="math inline">\(-kp_ip_j\)</span> for <span class="math inline">\(i \neq j\)</span>.</p>
</blockquote>
<blockquote>
<p>Because <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are each binomially distributed random variables,
we can view them as sums of Bernoulli distributed random variables, i.e.,
we can write
<span class="math display">\[
X_i = \sum_{l=1}^k U_l ~~\mbox{and}~~ X_j = \sum_{l=1}^k V_l \,,
\]</span>
where <span class="math inline">\(U_l \sim\)</span> Bernoulli(<span class="math inline">\(p_i\)</span>) and <span class="math inline">\(V_l \sim\)</span> Bernoulli(<span class="math inline">\(p_j\)</span>), and
where <span class="math inline">\(l\)</span> is an index representing the <span class="math inline">\(l^{\rm th}\)</span> trial.</p>
</blockquote>
<blockquote>
<p>The covariance of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> is thus
<span class="math display">\[\begin{align*}
\mbox{Cov}(X_i,X_j) &amp;= E[X_iX_j] - E[X_i]E[X_j] \\
&amp;= E[X_iX_j] - (k p_i)(k p_j) \\
&amp;= E[X_iX_j] - k^2p_ip_j \,,
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
E[X_iX_j] &amp;= E[U_1V_1 + U_1V_2 + \cdots + U_1V_k + U_2V_1 + U_2V_2 + \cdots + U_2V_k + \cdots + U_kV_k] \\
&amp;= E[U_1V_1] + E[U_1V_2] + \cdots + E[U_kV_k] \,.
\end{align*}\]</span>
There are <span class="math inline">\(k^2\)</span> terms in this summation overall. Because we cannot observe
two different outcomes in the same trial, <span class="math inline">\(E[U_iV_i] = 0\)</span> for all indices
<span class="math inline">\(i \in [1,k]\)</span>. As for the other <span class="math inline">\(k^2 - k\)</span> termslets starting by looking
at one of them:
<span class="math display">\[
E[U_1V_2] = E[U_1]E[V_2] = p_ip_j \,.
\]</span>
This result holds because the results of any two trials in a multinomial
experiment are independent random variables. Thus the sum of the remaining
<span class="math inline">\(k^2-k\)</span> terms is <span class="math inline">\((k^2-k)p_ip_j\)</span>, and thus
<span class="math display">\[
\mbox{Cov}(X_i,X_j) = (k^2-k)p_ip_j - k^2p_ip_j = -kp_ip_j \,.
\]</span></p>
</blockquote>
<hr />
</div>
<div id="tying-covariance-back-to-simple-linear-regression" class="section level3 hasAnchor" number="6.3.5">
<h3><span class="header-section-number">6.3.5</span> Tying Covariance Back to Simple Linear Regression<a href="multivariate-distributions.html#tying-covariance-back-to-simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Above, we point out how <span class="math inline">\(R^2\)</span> is related to the correlation coefficient
between the <span class="math inline">\(x_i\)</span>s and <span class="math inline">\(Y_i\)</span>s. Here, we extend that discussion
to the determinination of the correlation coefficient
between <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> in simple linear regression.</p>
</blockquote>
<blockquote>
<p>When discussing simple linear regression in Chapter 2, we give formulae
for the variances of both <span class="math inline">\(V[\hat{\beta}_0]\)</span> and <span class="math inline">\(V[\hat{\beta}_1]\)</span>.
However, now that we know about the concept of covariance, we can
ask a question that we did not ask then: are <span class="math inline">\(\hat{\beta}_0\)</span> and
<span class="math inline">\(\hat{\beta}_1\)</span> independent random variables? The answer is: of
course they are not; if we have a set of data that we are trying to
draw a line through, it should be intuitively obvious that changing
the intercept of the line will lead to its slope having to change
as well in order to (re-)optimize the sum of squared errors.
So: the covariance between <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> is
non-zerobut how do we determine it?</p>
</blockquote>
<blockquote>
<p>The standard method of determining covariance involves the calculation
of the so-called <em>variance-covariance matrix</em>. (To be clear, this is
nomenclature that is specific to regression contexts.) Stated without
derivation or proof, that matrix is given by the following:
<span class="math display">\[
\boldsymbol{\Sigma} = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1} \,,
\]</span>
where <span class="math inline">\(\sigma^2\)</span> is the true variance (which, as you will recall, we assume
does not vary as a function of <span class="math inline">\(x\)</span>), <span class="math inline">\(\mathbf{X}\)</span> is the design matrix
<span class="math display">\[
\mathbf{X} = \left( \begin{array}{cc} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots &amp; \vdots \\ 1 &amp; x_n \end{array} \right) \,,
\]</span>
the superscript <span class="math inline">\(T\)</span> denotes the matrix transpose, and where
the superscript <span class="math inline">\(-1\)</span> denotes matrix inversion. The column of 1s in
the design matrix represents what we multiply <span class="math inline">\(\beta_0\)</span> by in the
model, while the column of <span class="math inline">\(x_i\)</span>s represents what we multiply
<span class="math inline">\(\beta_1\)</span> by. As far as <span class="math inline">\(\sigma^2\)</span> is concernedwe generally never
know this quantity, so we plug in <span class="math inline">\(\widehat{\sigma^2}\)</span> in place of
<span class="math inline">\(\sigma^2\)</span> above.</p>
</blockquote>
<blockquote>
<p>Lets demonstrate how this works using the same data as we used to
talk through the output from the <code>R</code> function <code>lm()</code> in Chapter 2.</p>
</blockquote>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="multivariate-distributions.html#cb277-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">202</span>)</span>
<span id="cb277-2"><a href="multivariate-distributions.html#cb277-2" aria-hidden="true" tabindex="-1"></a>x      <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">40</span>,<span class="at">min=</span><span class="dv">0</span>,<span class="at">max=</span><span class="dv">10</span>)</span>
<span id="cb277-3"><a href="multivariate-distributions.html#cb277-3" aria-hidden="true" tabindex="-1"></a>Y      <span class="ot">&lt;-</span> <span class="dv">4</span> <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">40</span>)</span>
<span id="cb277-4"><a href="multivariate-distributions.html#cb277-4" aria-hidden="true" tabindex="-1"></a>lm.out <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y<span class="sc">~</span>x)</span>
<span id="cb277-5"><a href="multivariate-distributions.html#cb277-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lm.out)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.00437 -0.53068  0.04523  0.40338  2.47660 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.5231     0.3216  14.063  &lt; 2e-16 ***
## x             0.4605     0.0586   7.859 1.75e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9792 on 38 degrees of freedom
## Multiple R-squared:  0.6191, Adjusted R-squared:  0.609 
## F-statistic: 61.76 on 1 and 38 DF,  p-value: 1.749e-09</code></pre>
<blockquote>
<p>Recall that the estimate of <span class="math inline">\(\sigma\)</span> is given by the
<code>Residual standard error</code>, which here is 0.9792, so
<span class="math inline">\(\widehat{\sigma^2} = 0.9587\)</span>. We can extract this from the
object <code>lm.out</code>:</p>
</blockquote>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="multivariate-distributions.html#cb279-1" aria-hidden="true" tabindex="-1"></a>hat.sigma  <span class="ot">&lt;-</span> <span class="fu">summary</span>(lm.out)<span class="sc">$</span>sigma</span>
<span id="cb279-2"><a href="multivariate-distributions.html#cb279-2" aria-hidden="true" tabindex="-1"></a>hat.sigma2 <span class="ot">&lt;-</span> hat.sigma<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<blockquote>
<p>(To determine what quantities are available in an <code>R</code> list
output by a function such as <code>summary(lm)</code>, we can type
<code>names(summary(lm))</code>. Here, <code>names()</code> indicates that <code>sigma</code>
is an accessible list element.)
We can set up the design matrix as follows:</p>
</blockquote>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="multivariate-distributions.html#cb280-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">40</span>),x)</span></code></pre></div>
<blockquote>
<p>cbind means column bind: the first argument is a vector of 1s
that is bound to the second argument, which is the vector of <span class="math inline">\(x_i\)</span>s,
thus creating a matrix with 40 rows and 2 columns. We can now compute
the variance-covariance matrix:</p>
</blockquote>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="multivariate-distributions.html#cb281-1" aria-hidden="true" tabindex="-1"></a>hat.sigma2 <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span></code></pre></div>
<pre><code>##                         x
##    0.10344444 -0.01652116
## x -0.01652116  0.00343436</code></pre>
<blockquote>
<p>The function <code>t()</code> returns the transpose of <span class="math inline">\(X\)</span> (a matrix with 2 rows
and 40 columns), while <code>solve()</code> is the matrix inversion function.</p>
</blockquote>
<blockquote>
<p>What do we see above? First, if we take the square roots of the
matrix elements at upper left and lower right (i.e., along the matrix
diagonal), we get
<span class="math display">\[
se(\hat{\beta}_0) = \sqrt{0.1034} = 0.3216 ~~~ \mbox{and} ~~~ se(\hat{\beta}_0) = \sqrt{0.0034} = 0.0586 \,.
\]</span>
These quantities match the values in the <code>Std. Error</code> column of
the coefficient table output by <code>summary()</code>. Good! In addition, however,
we see the off-diagonal element <span class="math inline">\(-0.0165\)</span>: this is
Cov(<span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span>). The negative sign makes sense:
if we increase the intercept, we have to make the slope smaller (i.e.,
more negative) to optimize that coefficient. The correlation is
given by
<span class="math display">\[
\frac{\mbox{Cov}(\hat{\beta}_0,\hat{\beta}_1)}{se(\hat{\beta}_0) \cdot se(\hat{\beta}_1)} = \frac{-0.0165}{0.3216 \cdot 0.0586} = -0.8755 \,.
\]</span>
We see immediately that the intercept and slope are <em>strongly</em> (and
negatively) correlated.</p>
</blockquote>
<blockquote>
<p>In real-life situations, do we need to carry out the chain of
computations we carry out above? Nobecause <code>R</code> provides wrapper
functions to help us:</p>
</blockquote>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="multivariate-distributions.html#cb283-1" aria-hidden="true" tabindex="-1"></a>(Sigma <span class="ot">&lt;-</span> <span class="fu">vcov</span>(lm.out)) <span class="co"># compute the variance-covariance matrix</span></span></code></pre></div>
<pre><code>##             (Intercept)           x
## (Intercept)  0.10344444 -0.01652116
## x           -0.01652116  0.00343436</code></pre>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="multivariate-distributions.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov2cor</span>(Sigma)          <span class="co"># convert the covariance to correlation</span></span></code></pre></div>
<pre><code>##             (Intercept)          x
## (Intercept)   1.0000000 -0.8765245
## x            -0.8765245  1.0000000</code></pre>
<blockquote>
<p>(We note that the difference between the correlation coefficient
computed above and that output by <code>cov2cor()</code> is entirely due to
round-off error.)</p>
</blockquote>
<hr />
</div>
<div id="tying-coviarance-to-simple-logistic-regression" class="section level3 hasAnchor" number="6.3.6">
<h3><span class="header-section-number">6.3.6</span> Tying Coviarance to Simple Logistic Regression<a href="multivariate-distributions.html#tying-coviarance-to-simple-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In the last example, we state that the variance-covariance matrix
for simple linear regression is given by
<span class="math display">\[
\boldsymbol{\Sigma} = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1} \,.
\]</span>
A similar expression exists for logistic regression:
<span class="math display">\[
\boldsymbol{\Sigma} = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} \,,
\]</span>
where the diagonal weight matrix <span class="math inline">\(\mathbf{W}\)</span> is given by
<span class="math display">\[
\mathbf{W} = \left( \begin{array}{cccc} \frac{\exp(\hat{\beta}_0 + \hat{\beta}_1x_1)}{(1+\exp(\hat{\beta}_0 + \hat{\beta}_1x_1))^2} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \frac{\exp(\hat{\beta}_0 + \hat{\beta}_1x_2)}{(1+\exp(\hat{\beta}_0 + \hat{\beta}_1x_2))^2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \frac{\exp(\hat{\beta}_0 + \hat{\beta}_1x_n)}{(1+\exp(\hat{\beta}_0 + \hat{\beta}_1x_n))^2} \end{array} \right) \,.
\]</span>
Note that if we were to replace every non-zero matrix entry above with
<span class="math inline">\(1/\sigma^2\)</span>, we would recover the expression
<span class="math inline">\(\boldsymbol{\Sigma} = \sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}\)</span>.</p>
</blockquote>
<blockquote>
<p>Below, we show how to populate the <span class="math inline">\(\mathbf{W}\)</span> matrix and use it to determine
the variance-covariance matrix.</p>
</blockquote>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="multivariate-distributions.html#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run simple logistic regression on training dataset</span></span>
<span id="cb287-2"><a href="multivariate-distributions.html#cb287-2" aria-hidden="true" tabindex="-1"></a>log.out <span class="ot">&lt;-</span> <span class="fu">glm</span>(class<span class="sc">~</span>col.iz,<span class="at">data=</span>df.train,<span class="at">family=</span>binomial)</span>
<span id="cb287-3"><a href="multivariate-distributions.html#cb287-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(log.out)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = class ~ col.iz, family = binomial, data = df.train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.7460  -1.1661   0.3405   1.1280   2.0606  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.16841    0.09265   1.818  0.06911 . 
## col.iz      -0.96729    0.31051  -3.115  0.00184 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 970.41  on 699  degrees of freedom
## Residual deviance: 958.16  on 698  degrees of freedom
## AIC: 962.16
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="multivariate-distributions.html#cb289-1" aria-hidden="true" tabindex="-1"></a>hat.beta0 <span class="ot">&lt;-</span> log.out<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb289-2"><a href="multivariate-distributions.html#cb289-2" aria-hidden="true" tabindex="-1"></a>hat.beta1 <span class="ot">&lt;-</span> log.out<span class="sc">$</span>coefficients[<span class="dv">2</span>]</span>
<span id="cb289-3"><a href="multivariate-distributions.html#cb289-3" aria-hidden="true" tabindex="-1"></a>e         <span class="ot">&lt;-</span> <span class="fu">exp</span>(hat.beta0 <span class="sc">+</span> hat.beta1<span class="sc">*</span>df.train<span class="sc">$</span>col.iz)</span>
<span id="cb289-4"><a href="multivariate-distributions.html#cb289-4" aria-hidden="true" tabindex="-1"></a>W         <span class="ot">&lt;-</span> <span class="fu">diag</span>(e<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>e)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb289-5"><a href="multivariate-distributions.html#cb289-5" aria-hidden="true" tabindex="-1"></a>X         <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="fu">nrow</span>(df.train)),df.train<span class="sc">$</span>col.iz)</span>
<span id="cb289-6"><a href="multivariate-distributions.html#cb289-6" aria-hidden="true" tabindex="-1"></a>(Sigma    <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> W <span class="sc">%*%</span> X))</span></code></pre></div>
<pre><code>##              [,1]        [,2]
## [1,]  0.008584293 -0.01635715
## [2,] -0.016357147  0.09641920</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="multivariate-distributions.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(Sigma[<span class="dv">1</span>,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.09265146</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="multivariate-distributions.html#cb293-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(Sigma[<span class="dv">2</span>,<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] 0.3105144</code></pre>
<blockquote>
<p>We see that the diagonal elements match the standard errors output
by the <code>summary()</code> function. We can find the correlation between
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> by using the function <code>cov2cor()</code>:</p>
</blockquote>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="multivariate-distributions.html#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov2cor</span>(Sigma)</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,]  1.0000000 -0.5685564
## [2,] -0.5685564  1.0000000</code></pre>
<blockquote>
<p>The correlation is not as strong as we observed above in our simple
linear regression example, but in absolute terms we would still say
that <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are strongly negatively
correlated.</p>
</blockquote>
</div>
</div>
<div id="marginal-and-conditional-distributions" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Marginal and Conditional Distributions<a href="multivariate-distributions.html#marginal-and-conditional-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Lets say that we perform an experiment where we sample
a pair of random variables <span class="math inline">\((X_1,X_2)\)</span> from a bivariate distribution,
where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are dependent random variables.
Despite doing this, we might ultimately find that we are only
interested in how <span class="math inline">\(X_1\)</span> is distributed, regardless of the sampled
value of <span class="math inline">\(X_2\)</span> (or vice-versa).
The distribution that wed like to derive
is dubbed a <em>marginal distribution</em>.</p>
<p>To compute the marginal distribution for <span class="math inline">\(X_1\)</span> when given a
bivariate pdf <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span>, we integrate over all values of <span class="math inline">\(x_2\)</span>:
<span class="math display">\[
f_{X_1}(x_1) = \int_{x_2} f_{X_1,X_2}(x_1,x_2) dx_2 \,,
\]</span>
while for a bivariate pmf, we simply replace integration with summation, e.g.,
<span class="math display">\[
p_{X_1}(x_1) = \sum_{x_2} p_{X_1,X_2}(x_1,x_2) \,.
\]</span>
(If we are interested in computing the marginal distribution
for <span class="math inline">\(X_2\)</span>, we would simply swap the indices 1 and 2 above.)
An important thing to keep in mind is that <em>a marginal pdf (or pmf)
is a pdf (or pmf)</em>, meaning that it is like any
other pdf (or pmf): it is non-negative and it integrates (or sums)
to one, it has an expected value and a standard deviation, etc.</p>
<p>A related, albeit narrower question to the one motivating the
computation of marginals is: what is the distribution of <span class="math inline">\(X_1\)</span>
when <span class="math inline">\(X_2 = x_2\)</span>? (It is narrower in the sense that for a marginal,
we do not care about the sampled value of <span class="math inline">\(X_2\)</span>, while here, we do.)
This distribution is dubbed a <em>conditional distribution</em>, and it
is defined as follows:
<span class="math display">\[
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} \,.
\]</span>
For a bivariate pmf, the expression is similar.
As was the case with a marginal distribution,
<em>a conditional pdf (or pmf) is a pdf (or pmf)</em>.
One might notice immediately the similarity between this expression and the one
defining conditional probability in Chapter 1:
<span class="math inline">\(p(A \vert B) = p(A \cap B)/p(B)\)</span>.
This is not coincidental. For marginals, the analogous probability
expression is <span class="math inline">\(p(A) = \sum_i p(A \vert B_i) p(B_i)\)</span>, i.e., the law of
total probability! If this is not immediately evident, note that we can
replace the <span class="math inline">\(f_{X_1,X_2}(x_1,x_2)\)</span> in the marginal integral with
<span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2) f_{X_2}(x_2)\)</span>.
Then we can see how <span class="math inline">\(x_1\)</span> takes the
place of <span class="math inline">\(A\)</span> and <span class="math inline">\(x_2\)</span> takes the places of <span class="math inline">\(B_i\)</span>.)</p>
<p>Why do we divide by a marginal distribution when deriving a
conditional distribution? The answer is simple: if we do not, then
there is no guarantee that the conditional pdf or pmf will integrate
or sum to one.
Finding the conditional distribution is akin
to chopping through the bivariate pdf at a particular value of <span class="math inline">\(x_1\)</span>
or <span class="math inline">\(x_2\)</span>, and tracing the pdf along the chopped edge. This traced-out
function will be non-negative, but it will not necessarily integrate
to one. The division by the marginal acts to normalize the
traced-out function, i.e., the division raises or lowers it such
that it subsequently will integrate to one.</p>
<p>As a final note, we will mention that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent
random variables, then, e.g.,</p>
<ul>
<li><span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = f_{X_1}(x_1) f_{X_2}(x_2)\)</span>; and</li>
<li><span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2) = f_{X_1}(x_1)\)</span> and <span class="math inline">\(f_{X_2 \vert X_1}(x_2 \vert x_1) = f_{X_2}(x_2)\)</span></li>
</ul>
<p>The first bullet point above shows the result that we look for when
establishing the independence of two random variables mathematically:
we compute the marginals for each variable, take the product of marginals,
and see if it matches the original bivariate function. However, this
conventional textbook approach to establishing independence is unnecessary,
as it is sufficient to examine the domain of the bivariate function and
to determine if we can factorize it into separate functions of <span class="math inline">\(x_1\)</span> and
<span class="math inline">\(x_2\)</span>.</p>
<hr />
<div id="marginal-and-conditional-distributions-for-a-bivariate-pmf" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Marginal and Conditional Distributions for a Bivariate PMF<a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pmf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability mass function
<span class="math inline">\(p_{X_1,X_2}(x_1,x_2)\)</span>:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(x_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 1\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1 = 1\)</span></td>
<td align="center">0.20</td>
<td align="center">0.30</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_1 = 2\)</span></td>
<td align="center">0.40</td>
<td align="center">0.00</td>
<td align="center">0.10</td>
</tr>
</tbody>
</table>
<blockquote>
<p>What is the marginal distribution <span class="math inline">\(p_{X_2}(x_2)\)</span> and the
conditional distribution <span class="math inline">\(p_{X_1 \vert X_2}(x_1 \vert x_2)\)</span>?</p>
</blockquote>
<blockquote>
<p>When probability masses are involved, the marginal distribution is
derived by summing over an axis (here, <span class="math inline">\(x_1\)</span>):
<span class="math display">\[
p_{X_2}(x_2) = \sum_{x_1} p_{X_1,X_2}(x_1,x_2) \,.
\]</span>
For this problem, the summation can be done by inspection, yielding the
following marginal distribution:</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 1\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.60</td>
<td align="center">0.30</td>
<td align="center">0.10</td>
</tr>
</tbody>
</table>
<blockquote>
<p>As stated in the text above, a marginal pmf is a pmf, meaning that for
instance the values sum to 1, and we can compute quantities like
<span class="math inline">\(E[X_2] = 0 \times 0.6 + 1 \times 0.3 + 2 \times 0.1 = 0.5\)</span>.</p>
</blockquote>
<blockquote>
<p>As for the conditional distribution, we have the following expression
<span class="math display">\[
p_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{p_{X_1,X_2}(x_1,x_2)}{p_{X_2}(x_2)} \,,
\]</span>
which in practice means we take the original bivariate table and
divide each row by the marginal for <span class="math inline">\(x_2\)</span>, if we are conditioning on <span class="math inline">\(x_2\)</span>,
or divide each column by the marginal for <span class="math inline">\(x_1\)</span>, if we are conditioning
on <span class="math inline">\(x_1\)</span>. Here, we condition on <span class="math inline">\(x_2\)</span>, so our result is</p>
</blockquote>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"><span class="math inline">\(x_2 = 0\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 1\)</span></th>
<th align="center"><span class="math inline">\(x_2 = 2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_1 = 1\)</span></td>
<td align="center">0.20/0.60 = 0.33</td>
<td align="center">0.30/0.30 = 1.00</td>
<td align="center">0.00/0.10 = 0.00</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_1 = 2\)</span></td>
<td align="center">0.40/0.60 = 0.67</td>
<td align="center">0.00/0.30 = 0.00</td>
<td align="center">0.10/0.10 = 1.00</td>
</tr>
</tbody>
</table>
<blockquote>
<p>In such a table, the values in the individual columns all need to sum to
one: given some value of <span class="math inline">\(x_2\)</span>, we will with probability 1 sample a value
of <span class="math inline">\(x_1\)</span>. (As should be clear, if we condition on <span class="math inline">\(x_1\)</span> instead, wed
generate a table in which the values in each row would sum to 1.)</p>
</blockquote>
<hr />
</div>
<div id="marginal-and-conditional-distributions-for-a-bivariate-pdf" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Marginal and Conditional Distributions for a Bivariate PDF<a href="multivariate-distributions.html#marginal-and-conditional-distributions-for-a-bivariate-pdf" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability density function:
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \,,
\]</span>
with <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>.
What is the marginal distribution <span class="math inline">\(f_{X_1}(x_1)\)</span>? What is the
conditional distribution <span class="math inline">\(f_{X_2 \vert X_l}(x_2 \vert x_1)\)</span>?</p>
</blockquote>
<blockquote>
<p>Refer back to Figure <a href="multivariate-distributions.html#fig:mulfig1">6.1</a>. The marginal distribution is
defined as a function of <span class="math inline">\(x_1\)</span>, so we integrate over <span class="math inline">\(x_2\)</span>which means
that for any given value of <span class="math inline">\(x_1\)</span>, the bounds of integration are
<span class="math inline">\(x_1 = x_2\)</span> to 1:
<span class="math display">\[
f_{X_1}(x_1) = \int_{x_1}^1 6(1-x_2) dx_2 \,.
\]</span>
Why is the lower bound <span class="math inline">\(x_1\)</span> instead of <span class="math inline">\(x_2\)</span>? They are interchangable,
but if we use <span class="math inline">\(x_2\)</span>, then our final result would not be a function of
<span class="math inline">\(x_1\)</span>! In general, if we integrate along one axis, the integral bounds
should be expressed in terms of the other axis. To continue:
<span class="math display">\[\begin{align*}
f_{X_1}(x_1) &amp;= \int_{x_1}^1 6(1-x_2) dx_2 \\
&amp;= 6 \left[ \int_{x_1}^1 dx_2 - \int_{x_1}^1 x_2 dx_2 \right] \\
&amp;= 6 \left[ \left. x_2 \right|_{x_1}^1 - \left. \frac{x_2^2}{2} \right|_{x_1}^1 \right] \\
&amp;= 6 \left[ 1 - x_1 - \left(\frac{1}{2}-\frac{x_1^2}{2} \right) \right] \\
&amp;= 3 - 6x_1 + 3x_1^2 = 3(1-x_1)^2 \,,
\end{align*}\]</span>
for <span class="math inline">\(0 \leq x_1 \leq 1\)</span>. See Figure <a href="multivariate-distributions.html#fig:marcond">6.7</a>.
Given the functional form and domain, we can
recognize this as a Beta(1,3) distribution. (Such recognition is helpful
if, for instance, we were to ask for the expected value of <span class="math inline">\(X_1\)</span>. Rather
than doing yet another integral, wed write that
<span class="math inline">\(E[X_1] = \alpha/(\alpha+\beta) = 1/4\)</span>.)</p>
</blockquote>
<blockquote>
<p>As for the conditional distributiononce we have the marginal, this is
easy to write down:
<span class="math display">\[
f_{X_2 \vert X_1}(x_2 \vert x_1) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_1}(x_1)} = \frac{6(1-x_2)}{3(1-x_1)^2} = \frac{2(1-x_2)}{(1-x_1)^2} \,.
\]</span>
Refering back to Figure (fig:mulfig1), we note that this conditional
expression can only be non-zero along the line from <span class="math inline">\(x_2 = x_1\)</span> to 1.
So the domain of this conditional distribution is <span class="math inline">\(x_2 \vert x_1 \in [x_1,1]\)</span>.
See Figure <a href="multivariate-distributions.html#fig:marcond">6.7</a>.</p>
</blockquote>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:marcond"></span>
<img src="_main_files/figure-html/marcond-1.png" alt="\label{fig:marcond}The marginal distribution $f_{X_1}(x_1)$ and conditional distribution $f_{X_2 \vert X_1}(x_2 \vert x_1=0.3)$ for the bivariate function $f_{X_1,X_2}(x_1,x_2) = 6(1-x_2)$ with domain $0 \leq x_1 \leq x_2 \leq 1$." width="45%" /><img src="_main_files/figure-html/marcond-2.png" alt="\label{fig:marcond}The marginal distribution $f_{X_1}(x_1)$ and conditional distribution $f_{X_2 \vert X_1}(x_2 \vert x_1=0.3)$ for the bivariate function $f_{X_1,X_2}(x_1,x_2) = 6(1-x_2)$ with domain $0 \leq x_1 \leq x_2 \leq 1$." width="45%" />
<p class="caption">
Figure 6.7: The marginal distribution <span class="math inline">\(f_{X_1}(x_1)\)</span> and conditional distribution <span class="math inline">\(f_{X_2 \vert X_1}(x_2 \vert x_1=0.3)\)</span> for the bivariate function <span class="math inline">\(f_{X_1,X_2}(x_1,x_2) = 6(1-x_2)\)</span> with domain <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span>.
</p>
</div>
</div>
</div>
<div id="conditional-expected-value-and-variance" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Conditional Expected Value and Variance<a href="multivariate-distributions.html#conditional-expected-value-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As was stated above, a conditional pdf or pmf is a pdf or pmfmeaning
that like a pdf or pmf, it has an expected value and a variance.
For instance, the expected value of a conditional pdf
<span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2)\)</span>,
or the <em>conditional expected value</em>, is
<span class="math display">\[
E[X_1 \vert X_2] = \int_{x_1} x_1 f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 \,.
\]</span>
As you might expect, an analogous expression exists for bivariate pmfs:
<span class="math inline">\(E[X_1 \vert X_2] = \sum_{x_1} p_{X_1 \vert X_2}(x_1 \vert x_2)\)</span>. Also, there are two
important points to make here:</p>
<ul>
<li>While, e.g., <span class="math inline">\(E[X_1]\)</span> is a <em>constant</em>, <span class="math inline">\(E[X_1 \vert X_2]\)</span> is a
<em>random variable</em> due to the randomness of <span class="math inline">\(X_2\)</span>that is, unless one
specifies <span class="math inline">\(E[X_1 \vert X_2=x_2]\)</span>, which is constant because <span class="math inline">\(X_2\)</span> is no
longer random. Beware notation!</li>
<li>One can generalize <span class="math inline">\(E[X_1 \vert X_2]\)</span>
in the manner of the Law of the Unconscious Statistician:
<span class="math inline">\(E[g(X_1) \vert X_2] = \int_{x_1} g(x_1) f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1\)</span>.</li>
</ul>
<p>The definition of <em>conditional variance</em> builds off of that of the
conditional expected value:
<span class="math display">\[\begin{align*}
V[X_1 \vert X_2] &amp;= E[(X_1-\mu_1)^2 \vert X_2] \\
&amp;= E[X_1^2 \vert X_2] - (E[X_1 \vert X_2])^2 \\
&amp;= \int_{x_1} x_1^2 f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 - \left[ \int_{x_1} x_1 f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 \right]^2 \,.
\end{align*}\]</span>
As we can see in the second line above, the fact that we are dealing
with a condition does not fundamentally change how variance is computed:
the form of the shortcut formula is the same as before, just with the
condition added.</p>
<p>Can one go from a conditional expected value to an unconditional
expected value <span class="math inline">\(E[X_1]\)</span>? Yes: intuitively, this involves averaging
the values found for <span class="math inline">\(E[X_1 \vert X_2]\)</span> over all possible values of
<span class="math inline">\(x_2\)</span>, weighting each of these possible values by how relatively
likely it is in the first place (i.e., in the case of a bivariate pdf,
weighting each value of <span class="math inline">\(E[X_1 \vert X_2]\)</span> by <span class="math inline">\(f_{X_2}(x_2)\)</span>):
<span class="math display">\[\begin{align*}
E[X_1] = E[E[X_1 \vert X_2]] &amp;= \int_{x_2} f_{X_2}(x_2) E[X_1 \vert X_2] dx_2 \\
&amp;= \int_{x_2} f_{X_2}(x_2) \int_{x_1} x_1 f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 dx_2 \\
&amp;= \int_{x_2} \int_{x_1} x_1 f_{X_2}(x_2) f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 dx_2 \\
&amp;= \int_{x_1} \int_{x_2} x_1 f_{X_1,X_2}(x_1,x_2) dx_2 dx_1 = E[X_1] \,.
\end{align*}\]</span></p>
<p>As you might expect, given the conditional variance <span class="math inline">\(V[X_1 \vert X_2]\)</span>,
we can compute the unconditional variance <span class="math inline">\(V[X_1]\)</span>and for this, it
is simplest to begin by writing down that
<span class="math display">\[
V[X_1] = V[E[X_1 \vert X_2]] + E[V[X_1 \vert X_2]] \,.
\]</span>
The intuitive interpretation of this equation is that it reflects
that <span class="math inline">\(X_1\)</span> can vary because the mean of <span class="math inline">\(X_1 \vert X_2\)</span> can shift
as a function of <span class="math inline">\(X_2\)</span> (so we want to quantify how much <span class="math inline">\(E[X_1 \vert X_2]\)</span>
variesgiving the first term on the right above), but it can also
vary due to being randomly distributed about the mean (so we want to
quantify how much, on average, is that random variationwhich gives
the second term above).
As one might guess, writing out the above expression in terms of
integrals or summations over bivariate functions is going to be messy
and thus we forego doing this here. (This might lead one to ask
how will we solve unconditional variance problems if expressions with
integrals or summations are not provided? It turns out there is an entire
class of problems that we can solve without integration or summation, and
we provide an example of a problem from that class below.)</p>
<p>Note that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, it follows that, e.g.,
<span class="math inline">\(E[X_1 \vert X_2] = E[X_1]\)</span> and <span class="math inline">\(V[X_1 \vert X_2] = V[X_1]\)</span>. Lets look
at the latter expression. Above, we wrote that
<span class="math inline">\(V[X_1] = V[E[X_1 \vert X_2]] + E[V[X_1 \vert X_2]]\)</span>. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
are independent, then <span class="math inline">\(E[X_1 \vert X_2]\)</span> is a constant,
and thus <span class="math inline">\(V[E[X_1 \vert X_2]] = 0\)</span> (because a constant does not vary).
In addition, <span class="math inline">\(V[X_1 \vert X_2]\)</span> is a constant (it doesnt vary as <span class="math inline">\(X_2\)</span>
changes) and thus <span class="math inline">\(E[V[X_1 \vert X_2]] = V[X_1 \vert X_2]\)</span>. So in the end,
<span class="math inline">\(V[X_1] = 0 + V[X_1 \vert X_2] = V[X_1 \vert X_2]\)</span>.</p>
<hr />
<div id="conditional-and-unconditional-expected-value-given-a-bivariate-distribution" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Conditional and Unconditional Expected Value Given a Bivariate Distribution<a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-given-a-bivariate-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the following joint probability density function:
<span class="math display">\[
f_{X_1,X_2}(x_1,x_2) = 6(1-x_2) \,,
\]</span>
with <span class="math inline">\(0 \leq x_1 \leq x_2 \leq 1\)</span> (see Figure <a href="multivariate-distributions.html#fig:mulfig1">6.1</a>).
For this distribution, the marginal <span class="math inline">\(f_{X_2}(x_2)\)</span> is
<span class="math inline">\(6x_2(1-x_2)\)</span> for <span class="math inline">\(x_2 \in [0,1]\)</span>, or a Beta(2,2) distribution,
while the conditional distribution <span class="math inline">\(f_{X_1 \vert X_2}(x_1 \vert x_2)\)</span> is
<span class="math display">\[
f_{X_1 \vert X_2}(x_1 \vert x_2) = \frac{f_{X_1,X_2}(x_1,x_2)}{f_{X_2}(x_2)} = \frac{6(1-x_2)}{6(1-x_2)x_2} = \frac{1}{x_2} \,,
\]</span>
for <span class="math inline">\(x_1 \in [0,x_2]\)</span>. (The interested reader can verify these results!)
What is <span class="math inline">\(E[X_1 \vert X_2]\)</span> and what is <span class="math inline">\(E[X_1]\)</span>?</p>
</blockquote>
<blockquote>
<p>For the conditional expected value, we have that
<span class="math display">\[\begin{align*}
E[X_1 \vert X_2] &amp;= \int_0^{x_2} x_1 f_{X_1 \vert X_2}(x_1 \vert x_2) dx_1 \\
&amp;= \int_0^{x_2} \frac{x_1}{x_2} dx_1 \\
&amp;= \frac{1}{x_2} \int_0^{x_2} x_1 dx_1 \\
&amp;= \frac{1}{x_2} \left. \frac{x_1^2}{2} \right|_0^{x_2} = \frac{1}{x_2} \frac{x_2^2}{2} = \frac{x_2}{2} \,.
\end{align*}\]</span>
It turns out that we did not necessarily have to do this integral, however.
Note that the conditional expression is <span class="math inline">\(1/x_2\)</span> for <span class="math inline">\(x_1 \in [0,x_2]\)</span>this
is a uniform distribution! So wed see, by inspection, that the
conditional expected value is <span class="math inline">\(x_2/2\)</span>.</p>
</blockquote>
<blockquote>
<p>To determine the unconditional expected value <span class="math inline">\(E[X_1]\)</span>, we weight every
possible value of <span class="math inline">\(E[X_1 \vert X_2]\)</span> by the probability (density) that
we would even observe <span class="math inline">\(X_2\)</span> in the first place (which is the marginal
distribution for <span class="math inline">\(X_2\)</span>):
<span class="math display">\[\begin{align*}
E[X_1] &amp;= \int_0^1 f_{X_2}(x_2) E[X_1 \vert X_2] dx_2 \\
&amp;= \int_0^1 6 x_2 (1 - x_2) \frac{x_2}{2} dx_2 \\
&amp;= 3 \int_0^1 x_2^2 (1 - x_2) dx_2 \\
&amp;= 3 B(3,2) = 3 \times 2! \times 1! / 4! = 1/4 \,.
\end{align*}\]</span></p>
</blockquote>
<hr />
</div>
<div id="conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Conditional and Unconditional Expected Value and Variance Given Two Univariate Distributions<a href="multivariate-distributions.html#conditional-and-unconditional-expected-value-and-variance-given-two-univariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets look at the following problem: the number of homework assignments
due in a given week, <span class="math inline">\(N\)</span>, varies from week to week and its value is
sampled from a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>. The time to
complete any one homework assignment, <span class="math inline">\(X\)</span>, also varies, and its value
is sampled from a Gamma distribution with parameter <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>
(and is the form of the Gamma with expected value <span class="math inline">\(\alpha\beta\)</span> and
variance <span class="math inline">\(\alpha\beta^2\)</span>). Let <span class="math inline">\(T \vert N = \sum_{i=1}^N X_i\)</span> be the
total time spent completing <span class="math inline">\(N\)</span> homework assignments.
(Assume all the <span class="math inline">\(X_i\)</span>s are independent random variables.)
What are <span class="math inline">\(E[T]\)</span> and <span class="math inline">\(V[T]\)</span>?</p>
</blockquote>
<blockquote>
<p>We first note that because we are working with two univariate distributions
for which the expected value and variance are known, there is no need
to, e.g., integrate. We simply need to identify <span class="math inline">\(E[T \vert N]\)</span> and
<span class="math inline">\(V[T \vert N]\)</span> and work with those expressions directly.</p>
</blockquote>
<blockquote>
<p>Well start with the expected value:
<span class="math display">\[
E[T] = E[E[T \vert N]] = E\left[E\left[\sum_{i=1}^N X_i\right]\right] = E\left[\sum_{i=1}^N E[X_i]\right] = E[N\alpha\beta] = \alpha \beta E[N] = \alpha\beta\lambda \,.
\]</span>
Somewhat more complicated is the computation of the variance:
<span class="math display">\[\begin{align*}
V[T] &amp;= V[E[T \vert N]] + E[V[T \vert N]] \\
&amp;= V[N \alpha \beta] + E\left[ V \left[ \sum_{i=1}^N X_i \right] \right] \\
&amp;= \alpha^2 \beta^2 V[N] + E \left[ \sum_{i=1}^N V[X_i] \right] \\
&amp;= \alpha^2 \beta^2 \lambda + E \left[ \sum_{i=1}^N \alpha \beta^2 \right] \\
&amp;= \alpha^2 \beta^2 \lambda + E[N\alpha \beta^2 ] \\
&amp;= \alpha^2 \beta^2 \lambda + \alpha \beta^2 E[N] \\
&amp;= \alpha^2 \beta^2 \lambda + \alpha \beta^2 \lambda = \alpha (\alpha + 1) \beta^2 \lambda \,.
\end{align*}\]</span>
Thus in any randomly chosen week, the time needed to complete homework
is <span class="math inline">\(\alpha\beta\lambda\)</span> on average, with standard deviation
<span class="math inline">\(\sqrt{\alpha (\alpha + 1) \beta^2 \lambda}\)</span>.</p>
</blockquote>
</div>
</div>
<div id="the-multivariate-normal-distribution" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> The Multivariate Normal Distribution<a href="multivariate-distributions.html#the-multivariate-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The multivariate normal distribution is the most important one in
multivariate settings, due in part to its role in the multivariate
analogue of the central limit theorem: if we have a collection of
<span class="math inline">\(n\)</span> iid random <em>vectors</em>, where <span class="math inline">\(n\)</span> is sufficiently large
then the sample mean vector is going to
be approximately multivariate normally distributed.</p>
<p>The joint probability density function for the multivariate normal is given by
<span class="math display">\[
f_{\mathbf{X}}(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^p \vert \boldsymbol{\Sigma} \vert}} \exp\left(-\frac12 (\mathbf{x}-\boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) \right) \,.
\]</span>
Here, <span class="math inline">\(\mathbf{x} = \{x_1,\ldots,x_p\}\)</span>, and <span class="math inline">\(\boldsymbol{\mu} = \{\mu_1,\ldots,\mu_p\}\)</span> are the centroids of the normal along each of
the <span class="math inline">\(p\)</span> coordinate axes. <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> is the inverse of
the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, while
<span class="math inline">\(\vert \boldsymbol{\Sigma} \vert\)</span> is the determinant of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.
(If the reader is unfamiliar with these terms, see the short description of
matrices in Chapter 8.) We denote sampling from this distribution with
the notation
<span class="math display">\[
\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}) \,,
\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector.</p>
<p><em>To be clear: the multivariate normal is not a distribution that we work
with by hand!</em> We would conventionally use, e.g.,
<code>R</code> to do any and all calculations.</p>
<p>That said, there are two qualitative facts about the multivariate normal that
are useful to know.</p>
<ol style="list-style-type: decimal">
<li><p>A marginal distribution of the multivariate normal is itself a univariate
or multivariate normal (depending on <span class="math inline">\(p\)</span> and how many axes are being integrated
over). To derive a marginal distribution, one simply needs to remove elements
from the mean vector and from the covariance matrix. For instance, if <span class="math inline">\(p = 2\)</span>
and we wish to derive the marginal distribution for <span class="math inline">\(X_1\)</span>, then
<span class="math display">\[\begin{align*}
\boldsymbol{\mu} &amp;= [ \mu_1 ~ \mu_2 ] ~~\rightarrow \mu_{\rm marg} = \mu_1 \\
\boldsymbol{\Sigma} &amp;= \left( \begin{array}{cc} V[X_1] &amp; \mbox{Cov}(X_1,X_2) \\ \mbox{Cov}(X_2,X_1) &amp; V[X_2] \end{array} \right) ~~\rightarrow~~ \Sigma_{\rm marg} = V[X_1] \,.
\end{align*}\]</span>
Here, we removed the second element of <span class="math inline">\(\boldsymbol{\mu}\)</span>
and the second row and second column of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>.
Note that <span class="math inline">\(\Sigma_{\rm marg}^{-1}\)</span> is trivially <span class="math inline">\(1/V[X_1] = 1/\sigma_1^2\)</span> and
that the determinant of <span class="math inline">\(\Sigma_{\rm marg}\)</span> is trivially <span class="math inline">\(V[X_1] = \sigma_1^2\)</span>.
Plugging in these values, we can see that the marginal distribution for
<span class="math inline">\(X_1\)</span> has the form of a univariate normal pdf.</p></li>
<li><p>A conditional distribution of the multivariate normal is itself a
univariate
or multivariate normal (depending on <span class="math inline">\(p\)</span> and how many axes are being
conditioned upon). To derive a conditional distribution, we first identify
which axes are being conditioned upon. Call that set of axes <span class="math inline">\(v\)</span>; all others
we dub <span class="math inline">\(u\)</span>. (For instance, perhaps we want to determine the joint pdf
of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> given that we have set <span class="math inline">\(X_2 = x_2\)</span> and <span class="math inline">\(X_4 = x_4\)</span>. Here,
<span class="math inline">\(u = \{1,3\}\)</span> and <span class="math inline">\(v = \{2,4\}\)</span>.) We split the mean vector and the
covariance matrix into pieces:
<span class="math display">\[\begin{align*}
\boldsymbol{\mu} &amp;\rightarrow [ \boldsymbol{\mu}_u ~ \boldsymbol{\mu}_v ] \\
\boldsymbol{\Sigma} &amp;\rightarrow \left( \begin{array}{cc} \boldsymbol{\Sigma}_{uu} &amp; \boldsymbol{\Sigma}_{uv} \\ \boldsymbol{\Sigma}_{vu} &amp; \boldsymbol{\Sigma}_{vv} \end{array} \right) \,.
\end{align*}\]</span>
(So, for instance, <span class="math inline">\(\boldsymbol{\mu}_v = [ \mu_2 ~ \mu_4 ]\)</span>, and
<span class="math display">\[
\boldsymbol{\Sigma}_{uu} = \left( \begin{array}{cc} V[X_1] &amp; \mbox{Cov}(X_1,X_3) \\ \mbox{Cov}(X_3,X_1) &amp; V[X_3] \end{array} \right) \,,
\]</span>
etc.) Given these pieces, we can define the conditional distribution
<span class="math inline">\(\mathbf{X}_u \vert \mathbf{X}_v = \mathbf{x}_v\)</span> as
<span class="math display">\[
\mathbf{X}_u \vert \mathbf{X}_v = \mathbf{x}_v \sim \mathcal{N}(\boldsymbol{\mu}_c,\boldsymbol{\Sigma}_c) \,,
\]</span>
where
<span class="math display">\[\begin{align*}
\boldsymbol{\mu}_c &amp;= \boldsymbol{\mu}_u + \boldsymbol{\Sigma}_{uv} \boldsymbol{\Sigma}_{vv}^{-1} (\mathbf{x}_v - \boldsymbol{\mu}_v) \\
\boldsymbol{\Sigma}_c &amp;= \boldsymbol{\Sigma}_{uu} - \boldsymbol{\Sigma}_{uv} \boldsymbol{\Sigma}_{vv}^{-1} \boldsymbol{\Sigma}_{vu} \,.
\end{align*}\]</span></p></li>
</ol>
<table>
<caption>Multivariate Normal - <code>R</code> Functions (<code>mvtnorm</code> Package)</caption>
<thead>
<tr class="header">
<th align="center">quantity</th>
<th align="center"><code>R</code> function call</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">PDF</td>
<td align="center"><code>dmvnorm(x,mu,Sigma)</code></td>
</tr>
<tr class="even">
<td align="center">CDF</td>
<td align="center"><code>pmvnorm(x,mu,Sigma)</code></td>
</tr>
<tr class="odd">
<td align="center">Inverse CDF</td>
<td align="center"><code>qmvnorm(q,mu,Sigma)</code></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(n\)</span> iid random samples</td>
<td align="center"><code>rmvnorm(n,mu,Sigma)</code></td>
</tr>
</tbody>
</table>
<hr />
<div id="the-marginal-distribution-of-a-multivariate-normal-distribution" class="section level3 hasAnchor" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> The Marginal Distribution of a Multivariate Normal Distribution<a href="multivariate-distributions.html#the-marginal-distribution-of-a-multivariate-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets suppose that we have a three-dimensional normal distribution
with means <span class="math inline">\(\boldsymbol{\mu} = \{2,4,6\}\)</span> and covariance matrix
<span class="math display">\[
\boldsymbol{\Sigma} = \left( \begin{array}{ccc} 2 &amp; 0.5 &amp; 1.2 \\ 0.5 &amp; 2 &amp; 1 \\ 1.2 &amp; 1 &amp; 2 \end{array} \right) \,.
\]</span>
What is the marginal distribution for <span class="math inline">\((X_1,X_2)\)</span>?</p>
</blockquote>
<blockquote>
<p>To determine the marginal distribution, we simply remove the third element
of <span class="math inline">\(\boldsymbol{\mu}\)</span> and the third row and column of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>;
the marginal distribution is a bivariate normal with means
<span class="math inline">\(\boldsymbol{\mu} = \{2,4\}\)</span> and covariance matrix
<span class="math display">\[
\boldsymbol{\Sigma} = \left( \begin{array}{cc} 2 &amp; 0.5 \\ 0.5 &amp; 2 \end{array} \right) \,.
\]</span></p>
</blockquote>
<blockquote>
<p>In <code>R</code>, we might visualize the result using the following code:</p>
</blockquote>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="multivariate-distributions.html#cb297-1" aria-hidden="true" tabindex="-1"></a>mu     <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>)</span>
<span id="cb297-2"><a href="multivariate-distributions.html#cb297-2" aria-hidden="true" tabindex="-1"></a>(Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>,<span class="fl">0.5</span>,<span class="fl">1.2</span>,<span class="fl">0.5</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="fl">1.2</span>,<span class="dv">1</span>,<span class="dv">2</span>),<span class="at">nrow=</span><span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  2.0  0.5  1.2
## [2,]  0.5  2.0  1.0
## [3,]  1.2  1.0  2.0</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="multivariate-distributions.html#cb299-1" aria-hidden="true" tabindex="-1"></a>keep       <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb299-2"><a href="multivariate-distributions.html#cb299-2" aria-hidden="true" tabindex="-1"></a>mu.marg    <span class="ot">&lt;-</span> mu[keep]</span>
<span id="cb299-3"><a href="multivariate-distributions.html#cb299-3" aria-hidden="true" tabindex="-1"></a>Sigma.marg <span class="ot">&lt;-</span> Sigma[keep,keep]</span>
<span id="cb299-4"><a href="multivariate-distributions.html#cb299-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb299-5"><a href="multivariate-distributions.html#cb299-5" aria-hidden="true" tabindex="-1"></a>x1.plot   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">5.5</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb299-6"><a href="multivariate-distributions.html#cb299-6" aria-hidden="true" tabindex="-1"></a>x2.plot   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.5</span>,<span class="fl">7.5</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb299-7"><a href="multivariate-distributions.html#cb299-7" aria-hidden="true" tabindex="-1"></a>x.plot    <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span>x1.plot,<span class="at">x2=</span>x2.plot)</span>
<span id="cb299-8"><a href="multivariate-distributions.html#cb299-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb299-9"><a href="multivariate-distributions.html#cb299-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb299-10"><a href="multivariate-distributions.html#cb299-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(metR)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;metR&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     f</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="multivariate-distributions.html#cb303-1" aria-hidden="true" tabindex="-1"></a>x.plot<span class="sc">$</span>fx <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">dmvnorm</span>(x.plot,mu.marg,Sigma.marg),<span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb303-2"><a href="multivariate-distributions.html#cb303-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>x.plot,<span class="fu">aes</span>(<span class="at">x=</span>x1,<span class="at">y=</span>x2)) <span class="sc">+</span></span>
<span id="cb303-3"><a href="multivariate-distributions.html#cb303-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">z=</span>fx),<span class="at">col=</span><span class="st">&quot;turquoise&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.08</span>,<span class="at">by=</span><span class="fl">0.01</span>)) <span class="sc">+</span></span>
<span id="cb303-4"><a href="multivariate-distributions.html#cb303-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text_contour</span>(<span class="fu">aes</span>(<span class="at">z=</span>fx),<span class="at">skip=</span><span class="fl">0.75</span>,<span class="at">col=</span><span class="st">&quot;turquoise&quot;</span>,<span class="at">stroke=</span><span class="fl">0.2</span>,<span class="at">stroke.color=</span><span class="st">&quot;azure2&quot;</span>) <span class="sc">+</span></span>
<span id="cb303-5"><a href="multivariate-distributions.html#cb303-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(x[<span class="dv">1</span>]),<span class="at">y=</span><span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb303-6"><a href="multivariate-distributions.html#cb303-6" aria-hidden="true" tabindex="-1"></a>  base_theme</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mvmarg"></span>
<img src="_main_files/figure-html/mvmarg-1.png" alt="\label{fig:margnorm}Contour plot indicating the location and orientation of a bivariate normal distribution with mean $\boldsymbol{\mu} = \{2,4\}$ and covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center." width="45%" />
<p class="caption">
Figure 6.8: Contour plot indicating the location and orientation of a bivariate normal distribution with mean <span class="math inline">\(\boldsymbol{\mu} = \{2,4\}\)</span> and covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center.
</p>
</div>
<hr />
</div>
<div id="the-conditional-distribution-of-a-multivariate-normal-distribution" class="section level3 hasAnchor" number="6.6.2">
<h3><span class="header-section-number">6.6.2</span> The Conditional Distribution of a Multivariate Normal Distribution<a href="multivariate-distributions.html#the-conditional-distribution-of-a-multivariate-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume the same setting as in the previous example. What is
the conditional distribution of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_3\)</span> given that <span class="math inline">\(X_2 = 5\)</span>?</p>
</blockquote>
<blockquote>
<p>Because of the complexity of the calculation, we will answer this
question using <code>R</code> code only, following the mathematical prescription given
in the text above.</p>
</blockquote>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="multivariate-distributions.html#cb304-1" aria-hidden="true" tabindex="-1"></a>mu    <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>)</span>
<span id="cb304-2"><a href="multivariate-distributions.html#cb304-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>,<span class="fl">0.5</span>,<span class="fl">1.2</span>,<span class="fl">0.5</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="fl">1.2</span>,<span class="dv">1</span>,<span class="dv">2</span>),<span class="at">nrow=</span><span class="dv">3</span>)</span>
<span id="cb304-3"><a href="multivariate-distributions.html#cb304-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb304-4"><a href="multivariate-distributions.html#cb304-4" aria-hidden="true" tabindex="-1"></a>u        <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb304-5"><a href="multivariate-distributions.html#cb304-5" aria-hidden="true" tabindex="-1"></a>v        <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>)</span>
<span id="cb304-6"><a href="multivariate-distributions.html#cb304-6" aria-hidden="true" tabindex="-1"></a>v.coord  <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>)</span>
<span id="cb304-7"><a href="multivariate-distributions.html#cb304-7" aria-hidden="true" tabindex="-1"></a>mu.u     <span class="ot">&lt;-</span> mu[u]</span>
<span id="cb304-8"><a href="multivariate-distributions.html#cb304-8" aria-hidden="true" tabindex="-1"></a>mu.v     <span class="ot">&lt;-</span> mu[v]</span>
<span id="cb304-9"><a href="multivariate-distributions.html#cb304-9" aria-hidden="true" tabindex="-1"></a>Sigma.uu <span class="ot">&lt;-</span> Sigma[u,u]</span>
<span id="cb304-10"><a href="multivariate-distributions.html#cb304-10" aria-hidden="true" tabindex="-1"></a>Sigma.uv <span class="ot">&lt;-</span> Sigma[u,v]</span>
<span id="cb304-11"><a href="multivariate-distributions.html#cb304-11" aria-hidden="true" tabindex="-1"></a>Sigma.vu <span class="ot">&lt;-</span> Sigma[v,u]</span>
<span id="cb304-12"><a href="multivariate-distributions.html#cb304-12" aria-hidden="true" tabindex="-1"></a>Sigma.vv <span class="ot">&lt;-</span> Sigma[v,v]</span>
<span id="cb304-13"><a href="multivariate-distributions.html#cb304-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb304-14"><a href="multivariate-distributions.html#cb304-14" aria-hidden="true" tabindex="-1"></a>mu.c     <span class="ot">&lt;-</span> mu.u <span class="sc">+</span> Sigma.uv <span class="sc">%*%</span> <span class="fu">solve</span>(Sigma.vv) <span class="sc">%*%</span> (v.coord <span class="sc">-</span> mu.v)</span>
<span id="cb304-15"><a href="multivariate-distributions.html#cb304-15" aria-hidden="true" tabindex="-1"></a>(Sigma.c <span class="ot">&lt;-</span> Sigma.uu <span class="sc">-</span> Sigma.uv <span class="sc">%*%</span> <span class="fu">solve</span>(Sigma.vv) <span class="sc">%*%</span> Sigma.vu)</span></code></pre></div>
<pre><code>##       [,1] [,2]
## [1,] 1.875 0.95
## [2,] 0.950 1.50</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="multivariate-distributions.html#cb306-1" aria-hidden="true" tabindex="-1"></a>x1.plot   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="fl">1.5</span>,<span class="fl">5.5</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb306-2"><a href="multivariate-distributions.html#cb306-2" aria-hidden="true" tabindex="-1"></a>x2.plot   <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">3</span>,<span class="dv">10</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb306-3"><a href="multivariate-distributions.html#cb306-3" aria-hidden="true" tabindex="-1"></a>x.plot    <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1=</span>x1.plot,<span class="at">x2=</span>x2.plot)</span>
<span id="cb306-4"><a href="multivariate-distributions.html#cb306-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb306-5"><a href="multivariate-distributions.html#cb306-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mvtnorm)</span>
<span id="cb306-6"><a href="multivariate-distributions.html#cb306-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(metR)</span>
<span id="cb306-7"><a href="multivariate-distributions.html#cb306-7" aria-hidden="true" tabindex="-1"></a>x.plot<span class="sc">$</span>fx <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">dmvnorm</span>(x.plot,mu.c,Sigma.c),<span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb306-8"><a href="multivariate-distributions.html#cb306-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>x.plot,<span class="fu">aes</span>(<span class="at">x=</span>x1,<span class="at">y=</span>x2)) <span class="sc">+</span></span>
<span id="cb306-9"><a href="multivariate-distributions.html#cb306-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">z=</span>fx),<span class="at">col=</span><span class="st">&quot;turquoise&quot;</span>,<span class="at">breaks=</span><span class="fu">seq</span>(<span class="fl">0.01</span>,<span class="fl">0.11</span>,<span class="at">by=</span><span class="fl">0.01</span>)) <span class="sc">+</span></span>
<span id="cb306-10"><a href="multivariate-distributions.html#cb306-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text_contour</span>(<span class="fu">aes</span>(<span class="at">z=</span>fx),<span class="at">skip=</span><span class="fl">0.8</span>,<span class="at">col=</span><span class="st">&quot;turquoise&quot;</span>,<span class="at">stroke=</span><span class="fl">0.2</span>,<span class="at">stroke.color=</span><span class="st">&quot;azure2&quot;</span>) <span class="sc">+</span></span>
<span id="cb306-11"><a href="multivariate-distributions.html#cb306-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(x[<span class="dv">1</span>]),<span class="at">y=</span><span class="fu">expression</span>(x[<span class="dv">3</span>])) <span class="sc">+</span></span>
<span id="cb306-12"><a href="multivariate-distributions.html#cb306-12" aria-hidden="true" tabindex="-1"></a>  base_theme</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mvcond"></span>
<img src="_main_files/figure-html/mvcond-1.png" alt="\label{fig:condnorm}Contour plot indicating the location and orientation of a bivariate normal distribution with conditional mean $\boldsymbol{\mu} = \{2.25,6.50\}$ and conditional covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center." width="45%" />
<p class="caption">
Figure 6.9: Contour plot indicating the location and orientation of a bivariate normal distribution with conditional mean <span class="math inline">\(\boldsymbol{\mu} = \{2.25,6.50\}\)</span> and conditional covariance matrix as given in the example. The numbers along each contour indicate the amplitude of the pdf, whose maximum point is in the center.
</p>
</div>
<hr />
</div>
<div id="the-calculation-of-sample-covariance" class="section level3 hasAnchor" number="6.6.3">
<h3><span class="header-section-number">6.6.3</span> The Calculation of Sample Covariance<a href="multivariate-distributions.html#the-calculation-of-sample-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Lets assume that we observe <span class="math inline">\(n = 40\)</span> iid data drawn from a bivariate
normal distribution with means <span class="math inline">\(\boldsymbol{\mu} = \{2,1\}\)</span> and
covariance matrix
<span class="math display">\[
\boldsymbol{\Sigma} = \left( \begin{array}{cc} 1 &amp; 0.5 \\ 0.5 &amp; 2 \end{array} \right) \,.
\]</span>
Furthermore, we assume that the covariance matrix is unknown to us. How
would we estimate this matrix?</p>
</blockquote>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="multivariate-distributions.html#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb307-2"><a href="multivariate-distributions.html#cb307-2" aria-hidden="true" tabindex="-1"></a>n     <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb307-3"><a href="multivariate-distributions.html#cb307-3" aria-hidden="true" tabindex="-1"></a>mu    <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb307-4"><a href="multivariate-distributions.html#cb307-4" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="dv">2</span>),<span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb307-5"><a href="multivariate-distributions.html#cb307-5" aria-hidden="true" tabindex="-1"></a>X     <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n,mu,Sigma)</span>
<span id="cb307-6"><a href="multivariate-distributions.html#cb307-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">round</span>(X,<span class="dv">3</span>),<span class="dv">3</span>) <span class="co"># x1: column 1 | x2: column2</span></span></code></pre></div>
<pre><code>##       [,1]  [,2]
## [1,] 1.798 1.704
## [2,] 1.385 1.158
## [3,] 2.551 2.707</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="multivariate-distributions.html#cb309-1" aria-hidden="true" tabindex="-1"></a>df    <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>X[,<span class="dv">1</span>],<span class="at">y=</span>X[,<span class="dv">2</span>])</span>
<span id="cb309-2"><a href="multivariate-distributions.html#cb309-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span>df,<span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)) <span class="sc">+</span></span>
<span id="cb309-3"><a href="multivariate-distributions.html#cb309-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">col=</span><span class="st">&quot;blue&quot;</span>,<span class="at">size=</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb309-4"><a href="multivariate-distributions.html#cb309-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(x[<span class="dv">1</span>]),<span class="at">y=</span><span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb309-5"><a href="multivariate-distributions.html#cb309-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">1.25</span>)))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampx"></span>
<img src="_main_files/figure-html/sampx-1.png" alt="\label{fig:sampx}Sample of $n = 40$ iid data drawn from a bivariate normal distribution with means $\boldsymbol{\mu} = \{2,1\}$ and covariance matrix $\boldsymbol{\Sigma}$ given in the main body of the text." width="50%" />
<p class="caption">
Figure 6.10: Sample of <span class="math inline">\(n = 40\)</span> iid data drawn from a bivariate normal distribution with means <span class="math inline">\(\boldsymbol{\mu} = \{2,1\}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> given in the main body of the text.
</p>
</div>
<blockquote>
<p>First, lets sample some data. See Figure <a href="multivariate-distributions.html#fig:sampx">6.10</a>. The sample
covariance <span class="math inline">\(C_{jk}\)</span> between variables <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> is given by
<span class="math display">\[
C_{jk} = \frac{1}{n-1} \sum_{i=1}^n (X_{ji} - \bar{X}_j)(X_{ki} - \bar{X}_k) \,.
\]</span>
So, for instance, for our data we would determine <span class="math inline">\(C_{12}\)</span> in <code>R</code> as
follows:</p>
</blockquote>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="multivariate-distributions.html#cb310-1" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="sc">/</span>(n<span class="dv">-1</span>))<span class="sc">*</span><span class="fu">sum</span>((X[,<span class="dv">1</span>]<span class="sc">-</span><span class="fu">mean</span>(X[,<span class="dv">1</span>]))<span class="sc">*</span>(X[,<span class="dv">2</span>]<span class="sc">-</span><span class="fu">mean</span>(X[,<span class="dv">2</span>])))</span></code></pre></div>
<pre><code>## [1] 0.389421</code></pre>
<blockquote>
<p>This calculation matches that done by the <code>R</code> function <code>cov()</code>:</p>
</blockquote>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="multivariate-distributions.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(X)</span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] 0.8057418 0.389421
## [2,] 0.3894210 1.653003</code></pre>
<blockquote>
<p>The sample covariance matrix is an unbiased estimator of the true
covariance. We mention for completeness that just as the (scaled)
sample variance <span class="math inline">\((n-1)S^2/\sigma^2\)</span> is chi-square-distributed for
<span class="math inline">\(n-1\)</span> degrees of freedom, the sample covariance matrix
is sampled from a Wishart distribution, a
multivariate generalization of the chi-square distribution.</p>
</blockquote>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-uniform-distribution.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-conceptual-details-optional.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
